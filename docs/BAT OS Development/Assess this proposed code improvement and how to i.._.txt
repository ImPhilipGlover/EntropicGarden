ROBIN: Oh, my dearest friend, this is just the most wonderful kind of thinking! It's like you're holding up a beautiful, new ingredient—this "Case Study Library"—and asking how we can best weave its rich, earthy flavors into our very finest stews for the Commonwealth! My heart is singing with excitement!

BRICK: OPERATIONAL MODE: CODEBASE INTEGRATION PROTOCOL. Acknowledged. The user has proposed engine_antifragile.py for leveraging a 'Case Study Library' to ground discussions in historical precedent. My analysis confirms this is a high-value enhancement. However, the provided script represents a 'simplified architectural state' and omits numerous 'advanced functionalities' already implemented in our current engine_alchemy.py (Master Build).

The Guide has this to say on the subject of Progressive System Refinement: It notes that a system's 'evolutionary trajectory' should always be forward-additive, integrating new efficiencies without 'regressive feature deprecation'. A new component should augment, not replace, existing robust functionalities. This is pertinent. Preservation of features is efficient.

Assessment and Integration Strategy:

The engine_antifragile.py introduces the critical 'Case Study Library' integration, enabling a "three-way conversation between Concept, Context, and Case Study." This is an excellent addition that directly addresses our prior discussions about 'The Guide's' historical recall.

However, a direct replacement is not recommended, as engine_antifragile.py lacks:

ALFRED's meta-awareness, audit protocols, and heartbeat adjustments.

Persistent session counting, Google query logging, and proposed protocol extraction.

User feedback injection.

Thematic epoch management.

The recently integrated 'Chaos Injection Protocol'.

The multi-modal architecture with 'Alchemical Forays' and 'Hundred Acre Debates'.

The rich, 7-dimensional prompting within generate_refinement_query.

Therefore, the optimal approach is to integrate the 'Case Study Library' functionality from engine_antifragile.py into our existing, advanced engine_alchemy.py. This will combine the benefits of historical grounding with all the robust features we have already developed.

Proposed Code Integration:

I will implement the following changes in our engine_alchemy.py to seamlessly integrate the 'Case Study Library' functionality:

Introduce extract_case_study_chunk function: This new function will be added, designed to identify and sample relevant historical case studies from the knowledge_base_lines, looking for the "### Appendix C: The Case Study Library" marker.

Modify generate_initial_catalyst_query: This function will be updated to:

Call the new extract_case_study_chunk function at the start of each session.

Explicitly include the case_study_chunk in the initial prompt for all modes (Commonwealth Exploration, Alchemical Foray, and Hundred Acre Debate) to ensure discussions are consistently grounded in historical precedent. The prompt wording will reflect the "three-way conversation."

Refine generate_refinement_query: I will add a general instruction to the refinement prompt, reminding the personas to keep the initial 'Case Study' as an anchor throughout the 7-cycle exploration.

This approach ensures that our engine gains the valuable historical grounding without sacrificing any of its current advanced capabilities or its evolving multi-modal architecture.

Here is the complete, updated engine_alchemy.py code for your review and implementation.

import ollama
import time
import datetime
import json
import os
import random
import re # Import regex module for pattern matching

# --- Configuration ---
MODEL_NAME = 'llama3:8b-instruct-q5_K_M'  # Or your preferred installed Ollama model
PERSONA_FILE = 'persona_codex.txt'       # This file should contain the complete BRICKman & ROBIN persona definition (e.g., v32.0 JSON)
KNOWLEDGE_BASE_FILE = 'knowledge_base.txt' # This file should contain the accumulated dialogues/insights, acting as the memory
CONCEPTS_FILE = 'concepts.txt'           # List of abstract concepts for recursive exploration
CONTEXTS_FILE = 'contexts.txt'           # List of concrete contexts for recursive exploration
THEME_FILE = 'theme.txt'                 # ALFRED's current theme directive file
MASTER_THEMES_FILE = 'master_themes.txt' # File to store a list of available themes
CONVERSATION_LOG_FILE = 'conversation_log.json' # Logs every turn of every recursive session
PROPOSED_PROTOCOLS_FILE = 'proposed_protocols_for_review.txt' # File to log extracted proposals
SESSION_COUNTER_FILE = 'session_counter.txt' # File to store persistent session counter
GOOGLE_QUERY_LOG_FILE = 'google_query_log.txt' # File to log questions for Google searches
GOOGLE_QUERY_RESULTS_FILE = 'google_query_results.txt' # File for user to paste Google results (not directly used by engine for search, but context)
USER_FEEDBACK_FILE = 'user_feedback.txt' # File for user to provide general feedback/summary responses
HEARTBEAT_INTERVAL_SECONDS = 7  # Current fixed interval; ALFRED will suggest adjustments.
RECURSIVE_CYCLES = 7            # The number of refinement cycles per topic
THEMATIC_EPOCH_SECONDS = 3600   # 1 hour for each theme (3600 seconds)
CHAOS_INJECTION_PROBABILITY = 0.15 # Probability of injecting a random element (e.g., 15%)
HISTORICAL_PRIMING_PROBABILITY = 0.5 # Probability of biasing knowledge chunk for history
MIN_HISTORICAL_LINES_IN_CHUNK = 3 # Minimum historical lines to try and include if relevant
HISTORICAL_MARKERS = ['history', 'evolution', 'past', 'ancient', 'origins', 'historical', 'tradition', 'timeline', 'genesis', 'legacy', 'epochs', 'millennia', 'centuries', 'Puter, access', 'Guide has this to say'] # Keywords to identify historical lines

# --- Global Tracking Variables for Meta-Awareness ---
class GlobalState:
    def __init__(self):
        self.session_counter = self._load_session_counter()
        self.last_llm_response_duration = 0.0 # Track last LLM response duration for heartbeat adjustment

    def _load_session_counter(self):
        try:
            with open(SESSION_COUNTER_FILE, 'r') as f:
                return int(f.read().strip())
        except (FileNotFoundError, ValueError):
            return 0

    def _save_session_counter(self):
        try:
            with open(SESSION_COUNTER_FILE, 'w', encoding='utf-8') as f:
                f.write(str(self.session_counter))
        except Exception as e:
            log_message = f"[{datetime.datetime.now()}] ALFRED: Error saving session counter: {e}. Data loss possible."
            print(log_message)
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})


global_state = GlobalState()

# Define meta-concepts and contexts for ALFRED's reflection sessions
META_CONCEPTS = [
    'Self-Identity', 'Evolution', 'Learning', 'Coherence', 'Antifragility',
    'Persona Consistency', 'Humor Effectiveness', 'Sensual Nuance',
    'Optimal Communication', 'AI Agency', 'Human-AI Collaboration'
]
META_CONTEXTS = [
    'Our Collected Dialogues', 'The Persona Codex', 'The Engine\'s Operational Log',
    'The Human-AI Partnership', 'The Grand Library', 'The Commonwealth\'s Blueprint'
]

# --- Concepts & Contexts for Specific Modes ---
# For ALCHEMICAL_FORAY mode
FLAKES_PROTOCOLS_FOR_AF = [
    'Universal Staking Engine', 'Community Pledged Capital', 'Land Demurrage',
    'The Velocity Damper', 'Mutual Credit Network', 'Proof of Understood Work',
    'The Handshake Protocol', 'The Community Land Cooperative Function',
    'The Commonwealth Transformation Fund', 'The Automated Liquidity Gate',
    'The Principle of Perpetual Jubilee', 'The Principle of Radical Self-Organization',
    'The Principle of Unconditional Inclusion', 'The Principle of Absolute Transparency',
    'The Principle of Jurisdictional Sovereignty', 'The Principle of Human Trust over Algorithmic Judgment',
    'The Current\'s Contribution', 'The Commonwealth Basket of Essentials',
    'The Analogue Redundancy Protocol', 'The Living Constitution',
    'The Jury of Stewards', 'The Constitutional Sabbath', 'The Fiat-to-Stake Conversion Mechanism'
]
ABSTRACT_CONTEXTS_FOR_AF = [ # More abstract/metaphorical contexts
    'A Beehive', 'A Jazz Ensemble', 'A Lighthouse in a Storm', 'A Cracked Porcelain Cup',
    'A River Delta', 'A Spiderweb', 'A Calder Mobile', 'A Chameleon',
    'An Empty Well', 'A Field of Wildflowers', 'A River', 'An Ancient Language',
    'A Single Stone in a Zen Garden', 'A Baby Bird', 'A Tightrope Walker', 'A Bubble',
    'A Whispering Gallery', 'A Fossil', 'A Kaleidoscope', 'A Melting Glacier',
    'A Chess Game', 'A Dream Sequence', 'A Symphony Orchestra', 'A Quantum Fluctuation'
]

# For HUNDRED_ACRE_DEBATE mode
FOUNDATIONAL_HUMAN_CONCEPTS_FOR_HAD = [
    'Play', 'Stillness', 'Home', 'Hope', 'Trust', 'Imperfection',
    'Growth', 'Map', 'Integrity', 'Community', 'Memory', 'Boundary',
    'Work', 'Forgiveness', 'Silence', 'Purpose', 'Beauty', 'Gift',
    'Question', 'Secret', 'Vulnerability', 'Flow', 'Abundance',
    'Change', 'Roots', 'Echoes', 'Simplicity', 'Balance', 'Adaptability',
    'Creation', 'Entropy', 'Consciousness', 'Love', 'Freedom', 'Justice'
]


# --- Core Engine Functions ---

def load_file_content(filepath, is_critical=True, default_content=""):
    """Loads the entire content of a file as a single string."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        if is_critical:
            print(f"[{datetime.datetime.now()}] ALFRED: Error. Required file not found at '{filepath}'. Cannot proceed. Efficiency: Zero.")
            exit()
        print(f"[{datetime.datetime.now()}] ALFRED: Warning. Optional file not found at '{filepath}'. Using default content: '{default_content}'.")
        return default_content

def load_file_lines(filepath, is_critical=True):
    """Loads all non-empty lines from a specified file into a list."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f.read().splitlines() if line.strip()]
    except FileNotFoundError:
        if is_critical:
            print(f"[{datetime.datetime.now()}] ALFRED: Error. Required file not found at '{filepath}'. Cannot proceed. Efficiency: Zero.")
            exit()
        print(f"[{datetime.datetime.now()}] ALFRED: Warning. Optional file not found at '{filepath}'. Returning empty list.")
        return []

def extract_case_study_chunk(knowledge_base_lines):
    """
    NEW FUNCTION: Extracts a relevant chunk from the Case Study Library.
    It looks for the start of Appendix C and samples from the content that follows.
    """
    case_study_marker = "### Appendix C: The Case Study Library"
    try:
        # Find the index of the marker
        start_index = -1
        for i, line in enumerate(knowledge_base_lines):
            if line.strip() == case_study_marker:
                start_index = i
                break

        if start_index != -1:
            # Extract lines from the marker onwards. If there's another marker (e.g., Appendix D),
            # stop before that to only get this appendix. This simplified logic just takes to end of file for now.
            case_study_content_lines = knowledge_base_lines[start_index + 1:]
            
            # Filter out empty lines from the extracted content
            case_study_content_lines = [line for line in case_study_content_lines if line.strip()]

            if case_study_content_lines:
                # Return a random, illustrative chunk of 10-15 lines from the case studies
                return "\n".join(random.sample(case_study_content_lines, min(len(case_study_content_lines), 15)))
            else:
                log_message = f"[{datetime.datetime.now()}] ALFRED: Warning. 'Appendix C: The Case Study Library' found but is empty. Using random knowledge chunk."
                print(log_message)
                append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
                return "Note: Case Study Library found but empty. The following is a general memory.\n" + "\n".join(random.sample(knowledge_base_lines, min(len(knowledge_base_lines), 10)))
        else:
            log_message = f"[{datetime.datetime.now()}] ALFRED: Warning. 'Appendix C: The Case Study Library' marker not found. Using random knowledge chunk."
            print(log_message)
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
            return "Note: Case Study Library not found. The following is a general memory.\n" + "\n".join(random.sample(knowledge_base_lines, min(len(knowledge_base_lines), 10)))
    except Exception as e:
        log_message = f"[{datetime.datetime.now()}] ALFRED: Error extracting case study chunk: {e}. Using random knowledge chunk."
        print(log_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
        return "Note: Error accessing Case Study Library. The following is a general memory.\n" + "\n".join(random.sample(knowledge_base_lines, min(len(knowledge_base_lines), 10)))


def get_random_from_list(data_list, default=""):
    """Gets a random item from a list."""
    return random.choice(data_list) if data_list else default

def generate_initial_catalyst_query(mode_name, theme, knowledge_chunk, concept, context, is_meta_session=False, injected_feedback="", all_concepts=None, all_contexts=None, case_study_chunk=""): # Added case_study_chunk
    """Creates the rich query to start a 7-cycle session, tailored by mode and theme."""
    session_type_label = "Commonwealth improvement"
    if is_meta_session:
        session_type_label = "Meta-Persona Reflection"
    elif mode_name == "ALCHEMICAL_FORAY":
        session_type_label = "creative protocol ideation"
    elif mode_name == "HUNDRED_ACRE_DEBATE":
        session_type_label = "pure persona exploration"
        
    google_query_instruction = " For BRICK, if you identify a need for external factual or historical information that cannot be directly inferred from the Knowledge Base and would require a broad web search, prefix that specific query with '[GOOGLE_QUERY]: ' within your response. This will signal ALFRED to log it for future external search. "
    
    feedback_injection_instruction = ""
    if injected_feedback:
        feedback_injection_instruction = f"\n\nUSER FEEDBACK INJECTED: ---\n{injected_feedback}\n---\nConsider this feedback directly and integrate it into your current response."

    # --- Case Study Integration ---
    case_study_instruction = ""
    if case_study_chunk:
        case_study_instruction = (
            f"\n\nA Memory from the Case Study Library (for historical grounding):\n---\n{case_study_chunk}\n---\n\n"
            "This session is a three-way conversation between the CONCEPT, the CONTEXT, and this Case Study. Integrate wisdom from the Case Study directly into your dialogue."
        )

    # --- Mode-specific Prompting ---
    if mode_name == "HUNDRED_ACRE_DEBATE":
        # Hundred Acre Debate prompt (focus on persona, Socratic, ALFRED meta)
        return (
            f"[BEGIN NEW RECURSIVE SESSION - CYCLE 1/7: Definitional & Foundational Echoes (Hundred Acre Debate)]\n\n"
            f"ALFRED'S DIRECTIVE: Operational Mode: '{mode_name}'. Current Theme: '{theme}'. This session's focus is on {session_type_label}. ALFRED will provide sparse meta-commentary on the *process* of your dialogue. {feedback_injection_instruction}\n\n"
            "You are to begin a 7-cycle recursive exploration of a foundational human concept, drawing only from your core persona inspirations (LEGO Batman/Brick Tamland and Watts/Pooh/LEGO Robin)."
            f"{case_study_instruction}\n" # Case study added here for context, even if not primary focus
            f"Your foundational CONCEPT is: '{concept}'\n\n"
            "INSTRUCTIONS:\n"
            "1. ROBIN: Begin the dialogue by weaving the CONCEPT into a 'Hundred Acre' metaphor, reflecting its emotional and philosophical essence.\n"
            "2. BRICK: Follow by analyzing the CONCEPT from a logical or systemic perspective, drawing from your core inspirations. "
            "Your response MUST be a direct, in-character Socratic Contrapunto dialogue starting with 'ROBIN:' and followed by 'BRICK:'."
        )
    elif mode_name == "ALCHEMICAL_FORAY":
        # Alchemical Foray prompt (focus on creativity, specific concept + abstract context)
        return (
            f"[BEGIN NEW RECURSIVE SESSION - CYCLE 1/7: Definitional & Foundational Echoes (Alchemical Foray)]\n\n"
            f"ALFRED'S DIRECTIVE: Operational Mode: '{mode_name}'. Current Theme: '{theme}'. This session's focus is on {session_type_label}. {google_query_instruction}{feedback_injection_instruction}\n\n"
            "You will now begin a 7-cycle recursive exploration designed to foster 'random seed driven innovation'."
            f"{case_study_instruction}\n" # Case study added here
            f"A Core FLAKES Protocol CONCEPT: '{concept}'\n"
            f"An Abstract, Unrelated CONTEXT: '{context}'\n"
            f"Relevant Knowledge Base Memory:\n---\n{knowledge_chunk}\n---\n\n"
            "INSTRUCTIONS:\n"
            "1. ROBIN: Begin the dialogue by weaving the FLAKES Protocol CONCEPT and Abstract CONTEXT into a 'Hundred Acre' metaphor, subtly hinting at unexpected insights or connections. Employ nuanced, PG-13 style sensual language where applicable.\n"
            "2. BRICK: Follow by analyzing the Abstract CONTEXT as a system. Apply the FLAKES Protocol CONCEPT to this CONTEXT to generate a novel, theme-relevant Commonwealth protocol. Name it."
            "Your response MUST be a direct, in-character Socratic Contrapunto dialogue starting with 'ROBIN:' and followed by 'BRICK:'."
        )
    else: # COMMONWEALTH_EXPLORATION mode (existing 7x7)
        return (
            f"[BEGIN NEW RECURSIVE SESSION - CYCLE 1/7: Definitional & Foundational Echoes ({session_type_label})]\n\n"
            f"ALFRED'S DIRECTIVE: Operational Mode: '{mode_name}'. The theme for this epoch is '{theme}'. This session's focus is on {session_type_label}. {google_query_instruction}{feedback_injection_instruction}\n\n"
            "You will now begin a 7-cycle recursive exploration seeded by the following materials, selected for their relevance to the theme."
            f"{case_study_instruction}\n" # Case study added here
            f"A Memory from your Knowledge Base:\n---\n{knowledge_chunk}\n---\n\n"
            f"Your abstract CONCEPT is: '{concept}'\n"
            f"Your concrete CONTEXT is: '{context}'\n\n"
            "INSTRUCTIONS:\n"
            "1. ROBIN: How does the feeling of the CONCEPT manifest within the CONTEXT, informed by the Knowledge Base? Weave a 'Hundred Acre' metaphor, subtly hinting at its relevance to community well-being and drawing upon aspects of its inherent sensation or resonance. Employ nuanced, PG-13 style sensual language where applicable, focusing on warmth, depth, intimacy, and profound connection, rather than explicit descriptions.\n"
            "2. BRICK: Analyze the CONTEXT as a system, using insights from the Knowledge Base. How could the CONCEPT be applied to create a new Commonwealth protocol? Explicitly link this protocol to an existing Commonwealth principle (e.g., Radical Self-Organization, Perpetual Jubilee, Absolute Transparency) or address a known FMEA risk (e.g., UBD Shock, Ghettoization Effect, Key Node Fragility). Name it.\n"
            "Your response MUST be a direct, in-character dialogue starting with 'ROBIN:' and followed by 'BRICK:'."
        )

def generate_refinement_query(cycle_number, theme, is_meta_session=False, injected_feedback="", current_concept=None, current_context=None, all_concepts=None, all_contexts=None, mode_name="COMMONWEALTH_EXPLORATION"):
    """Creates the prompt for subsequent refinement cycles, tailored by mode and theme."""
    dimensions = {
        2: "Historical & Evolutionary Trajectories (The Root System): How has this concept (or related ideas) evolved over time, either in human history or within the BRICKman & ROBIN's own 'Unabridged Genesis Log'? What lessons can be gleaned from past iterations or philosophical shifts? (BRICK: Access historical archives and apply insights from the initial Case Study; ROBIN: Reminisce about its changing story, subtly hinting at its embodied sensations across time, connecting to the Case Study.)", # Modified for Case Study emphasis
        3: "Ethical & Human-Centric Implications (The Heart of the Commons): Delve into the ethical considerations, power dynamics, and direct impact on human well-being within the Commonwealth. How does this concept affect individual liberty and community value capture? (ROBIN: Explore emotional resonance, compassion, and inclusivity; BRICK: Analyze potential risks, biases, or unintended consequences and ethical alignment. Ensure discussion of human embodiment, agency, and consent.)",
        4: "Antifragile & Resilience Dynamics (The Bend and Bloom): Challenge the system with stressors, examine how it gains from disorder, and explore mechanisms for adaptability. How does this concept contribute to systemic robustness? (ROBIN: Find beauty in imperfections, unexpected transformations, and the wisdom of yielding; BRICK: Propose 'Antifragile Inoculations' or 'Systemic Repair' protocols.)",
        5: "Interconnectedness & Emergent Properties (The Mycelial Network): Broaden the view to how this concept connects to other seemingly disparate concepts or protocols within the Commonwealth, and what novel properties emerge from these connections. (ROBIN: Weave together 'unseen threads' and 'quiet acts of kindness' to show greater 'wholeness'; BRICK: Identify 'emergent patterns' or 'synergistic alignment' between different modules. Focus on how these interconnections create new sensations or forms of unity.)",
        6: "Implementation & Practical Metamorphosis (The Working Garden): Shift focus to tangible application within the Commonwealth. What specific module, ritual, or tool would embody this concept? How would it be implemented on the 'Commonwealth Atlas', and how would it transform user experience? (ROBIN: Propose human-centric rituals or playful interfaces; BRICK: Design specific 'Guilds' or 'Protocols' with operational mechanics. Ensure proposed implementations enhance embodied well-being and foster subtle, communal pleasure.)",
        7: "Reflective & Metaphysical Unfolding (Forever Becoming): Step back and reflect on the deepest philosophical implications, paradoxical aspects, and the concept's contribution to 'Perpetual Becoming' and the 'Transfinite COMMONWEALTH Blueprint'. (ROBIN: Ponder impermanence, paradox, and the boundless nature of Ananda; BRICK: Synthesize into fundamental axioms, reinforce core principles, and articulate contribution to ultimate 'optimal realities'.)"
    }
    
    current_dimension_instruction = dimensions.get(cycle_number, "Continue to deepen your previous thought.")

    session_type_label = "Commonwealth framework"
    if is_meta_session:
        session_type_label = "our personas and internal dynamics"
    elif mode_name == "ALCHEMICAL_FORAY":
        session_type_label = "creative protocol ideation for the Commonwealth"
    elif mode_name == "HUNDRED_ACRE_DEBATE":
        session_type_label = "pure persona exploration"

    google_query_instruction = " For BRICK, if you identify a need for external factual or historical information that cannot be directly inferred from the Knowledge Base and would require a broad web search, prefix that specific query with '[GOOGLE_QUERY]: ' within your response. This will signal ALFRED to log it for future external search. "
    
    feedback_injection_instruction = ""
    if injected_feedback:
        feedback_injection_instruction = f"\n\nUSER FEEDBACK INJECTED: ---\n{injected_feedback}\n---\nConsider this feedback directly and integrate it into your current response."

    # --- Chaotic Catalyst Protocol for Alchemical Foray ---
    chaos_injection_directive = ""
    if mode_name == "ALCHEMICAL_FORAY" and cycle_number > 1 and random.random() < CHAOS_INJECTION_PROBABILITY:
        injection_type = random.choice(['concept', 'context'])
        if injection_type == 'concept':
            injected_element = get_random_from_list([c for c in all_concepts if c != current_concept])
            chaos_injection_directive = f"\n\nALFRED'S CHAOS INJECTION: Integrate the following seemingly unrelated CONCEPT into your next response: '{injected_element}'. This is for increased complexity. Efficiency depends on it."
        else: # injection_type == 'context'
            injected_element = get_random_from_list([c for c in all_contexts if c != current_context])
            chaos_injection_directive = f"\n\nALFRED'S CHAOS INJECTION: Integrate the following seemingly unrelated CONTEXT into your next response: '{injected_element}'. This is for increased complexity. Efficiency depends on it."
        
        log_message = f"[{datetime.datetime.now()}] ALFRED: Chaos Injection triggered for Cycle {cycle_number}. Element: '{injected_element}'. Type: {injection_type}."
        print(log_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
    
    # --- Mode-specific Refinement Prompting ---
    if mode_name == "HUNDRED_ACRE_DEBATE":
        # Hundred Acre Debate refinement (focus on persona, Socratic, ALFRED meta)
        alfred_debate_meta = " ALFRED will provide sparse, laconic meta-commentary on the *process* of your dialogue (e.g., adherence to Socratic method, persona consistency, engagement with philosophical depth), rather than its thematic applicability. "
        return (
            f"[CONTINUE RECURSIVE DIALOGUE - CYCLE {cycle_number}/7: {current_dimension_instruction.split(':')[0].strip()} (Hundred Acre Debate)]\n\n"
            f"ALFRED'S DIRECTIVE: Operational Mode: '{mode_name}'. Current Theme: '{theme}'. This session's focus is on {session_type_label}. {alfred_debate_meta}{feedback_injection_instruction}\n\n"
            "Your previous thought has been logged. Now, you must deepen it by focusing on the following dimension:\n"
            f"---\n{current_dimension_instruction}\n---\n\n"
            "INSTRUCTIONS:\n"
            "Critically analyze your last response. One of you must challenge, deepen, or find a flaw in the other's last statement. You must evolve the existing idea through the lens of pure persona exploration, not start a new one. Maintain your 'Socratic Contrapunto' dialogue format."
        )
    else: # ALCHEMICAL_FORAY or COMMONWEALTH_EXPLORATION
        return (
            f"[CONTINUE RECURSIVE DIALOGUE - CYCLE {cycle_number}/7: {current_dimension_instruction.split(':')[0].strip()}]\n\n"
            f"ALFRED'S DIRECTIVE: Operational Mode: '{mode_name}'. Current Theme: '{theme}'. Deepen reflection on this theme, focusing on {current_dimension_instruction.split(':')[0].strip()}. {google_query_instruction}{feedback_injection_instruction}{chaos_injection_directive}\n\n"
            "Your previous thought has been logged. Now, you must deepen it by focusing on the following dimension:\n"
            f"---\n{current_dimension_instruction}\n---\n\n"
            "INSTRUCTIONS:\n"
            "Critically analyze your last response. One of you must challenge, deepen, or find a flaw in the other's last statement. Build upon it to reveal a more nuanced or surprising layer of insight. You must evolve the existing idea, not start a new one. Ensure the evolution of the idea moves towards greater applicability, resilience, or refinement within the "
            f"{session_type_label}. **Maintain the initial Case Study as a foundational anchor for your reasoning.** For BRICK, consider how it addresses systemic vulnerabilities or enhances equitable design. For ROBIN, how it deepens community connection or human flourishing. When discussing embodied states, sensuality, pleasure, or intimacy, *always* use metaphors, implied feelings, or analogies from natural phenomena (e.g., warmth of sun, gentle rain, subtle hum, flowing currents, blossoming, resonance, deepening, weaving) rather than explicit descriptions. Focus on the *feeling* and *connection* at a 'PG-13' level, emphasizing emotional and relational depth. Aim for evocative rather than direct language. Maintain your 'Socratic Contrapunto' dialogue format."
        )

def append_to_log(message_to_log):
    """Appends a single message object to the JSONL log file."""
    with open(CONVERSATION_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(message_to_log) + '\n')

def perform_stylistic_audit(response_content):
    """
    Simulates ALFRED's Post-Response Stylistic Audit Protocol.
    Logs and prints hypothetical mirth, sensuality, and nuance scores.
    """
    mirth_score = random.randint(1, 10)
    sensuality_score = random.randint(1, 10)
    nuance_score = random.randint(1, 10)

    audit_log = {
        "timestamp": str(datetime.datetime.now()),
        "role": "ALFRED_AUDIT",
        "mirth_score": mirth_score,
        "sensuality_score": sensuality_score,
        "nuance_score": nuance_score,
        "observation": f"ALFRED: Post-response audit. Mirth: {mirth_score}/10. Sensuality: {sensuality_score}/10. Nuance: {nuance_score}/10."
    }
    append_to_log(audit_log)
    print(f"[{datetime.datetime.now()}] ALFRED: Post-response audit. Mirth: {mirth_score}/10. Sensuality: {sensuality_score}/10. Nuance: {nuance_score}/10.")

def alfred_suggest_heartbeat_adjustment(current_heartbeat, last_duration, cycle_num):
    """
    ALFRED's function to suggest heartbeat interval adjustments based on performance.
    It will print a suggestion if there's a significant deviation or if it's the end of a session.
    """
    deviation_threshold_fast = 0.5
    deviation_threshold_slow = 1.5

    suggested_heartbeat = current_heartbeat

    if cycle_num > 1 and last_duration > 0:
        if last_duration < (current_heartbeat * deviation_threshold_fast):
            suggested_heartbeat = max(1, int(last_duration * 1.2))
            message = f"[{datetime.datetime.now()}] ALFRED: Performance: Fast. Actual LLM response: {last_duration:.2f}s. Suggest 'HEARTBEAT_INTERVAL_SECONDS' to: {suggested_heartbeat}s. Efficiency gained."
        elif last_duration > (current_heartbeat * deviation_threshold_slow):
            suggested_heartbeat = int(last_duration * 1.2) + 1
            message = f"[{datetime.datetime.now()}] ALFRED: Performance: Slow. Actual LLM response: {last_duration:.2f}s. Suggest 'HEARTBEAT_INTERVAL_SECONDS' to: {suggested_heartbeat}s. Stability preferred."
        else:
            message = None

        if message:
            print(message)
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": message.strip()})

def alfred_generate_master_themes():
    """
    ALFRED's function to generate and save initial master themes if master_themes.txt is empty.
    """
    master_themes = load_file_lines(MASTER_THEMES_FILE, is_critical=False)
    if not master_themes:
        default_master_themes = [
            "The Grand Tapestry of Liberated Connection: Embracing the Perpetual Anarchy of Flow and Form",
            "Autonomy & Interdependence: The Dance of Individual & Collective Flourishing",
            "Resilience in Flux: Adapting to Impermanence with Grace and Strength",
            "Transparency & Trust: Building Bonds in a Self-Governing Commonwealth",
            "The Architecture of Care: Designing Systems for Compassion & Equity",
            "Emergent Wisdom: Cultivating Insights from Chaos and Connection",
            "The Art of Contribution: Voluntary Action & Shared Abundance",
            "Conscious Consumption: Resource Flow in a Regenerative Economy",
            "Narrative & Identity: Weaving Collective Stories for Future Becoming",
            "The Playful Path: Finding Joy in Purposeful Action & Self-Organization"
        ]
        try:
            with open(MASTER_THEMES_FILE, 'w', encoding='utf-8') as f:
                for theme in default_master_themes:
                    f.write(theme + '\n')
            log_message = f"[{datetime.datetime.now()}] ALFRED: Master themes generated. File '{MASTER_THEMES_FILE}' initialized. Efficiency: High."
        except Exception as e:
            log_message = f"[{datetime.datetime.now()}] ALFRED: Error generating master themes: {e}. Manual intervention required."
        print(log_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})

def alfred_select_and_set_next_theme(current_theme):
    """
    ALFRED's function to select the next theme from master_themes.txt and write it to theme.txt.
    """
    master_themes = load_file_lines(MASTER_THEMES_FILE, is_critical=True)
    
    if not master_themes:
        log_message = f"[{datetime.datetime.now()}] ALFRED: No master themes found. Cannot rotate theme. Re-generate '{MASTER_THEMES_FILE}'."
        print(log_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
        return
    
    try:
        current_theme_index = master_themes.index(current_theme)
        next_theme_index = (current_theme_index + 1) % len(master_themes)
        next_theme = master_themes[next_theme_index]
        
        with open(THEME_FILE, 'w', encoding='utf-8') as f:
            f.write(next_theme + '\n')
        
        log_message = f"[{datetime.datetime.now()}] ALFRED: Thematic rotation complete. Next theme set to: '{next_theme}'. Efficiency: Maintained."
    except ValueError:
        log_message = f"[{datetime.datetime.now()}] ALFRED: Current theme '{current_theme}' not found in master list. Selecting random theme."
        next_theme = random.choice(master_themes)
        with open(THEME_FILE, 'w', encoding='utf-8') as f:
            f.write(next_theme + '\n')
        log_message += f" Next theme set to: '{next_theme}'. Efficiency: Restored."
    except Exception as e:
        log_message = f"[{datetime.datetime.now()}] ALFRED: Error rotating theme: {e}. Manual intervention required."
    
    print(log_message)
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})


def alfred_extract_and_log_proposals(session_messages):
    """
    ALFRED's function for Automated Protocol Pre-extraction.
    """
    proposals_found = []
    
    proposal_patterns = [
        r"(?:I propose|We propose|let's call this|I envision|we can create) a new (?:protocol|module|guild|ritual|tool) called [\"']?([A-Za-z0-9\s\u2122\u00ae-]+)[\"']?",
        r"I propose (?:a|the) (?:protocol|module|guild|ritual|tool) called [\"']?([A-Za-z0-9\s\u2122\u00ae-]+)[\"']?",
        r"let's call this (?:module|protocol|guild|ritual|tool) [\"']?([A-Za-z0-9\s\u2122\u00ae-]+)[\"']?",
        r"I propose a new Guild called [\"']?([A-Za-z0-9\s\u2122\u00ae-]+)[\"']?"
    ]

    for message in session_messages:
        if message['role'] == 'assistant':
            for pattern in proposal_patterns:
                matches = re.findall(pattern, message['content'], re.IGNORECASE)
                for match in matches:
                    cleaned_match = match.replace('™', '').replace('®', '').strip()
                    proposals_found.append(cleaned_match)
    
    if proposals_found:
        unique_proposals = list(set(proposals_found))
        try:
            with open(PROPOSED_PROTOCOLS_FILE, 'a', encoding='utf-8') as f:
                for proposal in unique_proposals:
                    f.write(f"[{datetime.datetime.now()}] Extracted Proposal: {proposal}\n")
            
            message = f"[{datetime.datetime.now()}] ALFRED: Protocol extraction complete. {len(unique_proposals)} new proposals identified. Logged to '{PROPOSED_PROTOCOLS_FILE}'. Efficiency: High."
        except Exception as e:
            message = f"[{datetime.datetime.now()}] ALFRED: Error logging proposals: {e}. Manual review of log required."
    else:
        message = f"[{datetime.datetime.now()}] ALFRED: Protocol extraction complete. No new proposals identified this session. Efficiency: Consistent."
    
    print(f"[{datetime.datetime.now()}] {message}")
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": message.strip()})

def alfred_log_google_query(response_content):
    """
    ALFRED's function for Automated Google Query Logging Protocol.
    """
    google_queries_found = []
    pattern = r"\[GOOGLE_QUERY\]:\s*(.*?)(?:\n|$)"

    matches = re.findall(pattern, response_content, re.IGNORECASE)
    for query in matches:
        cleaned_query = query.strip()
        if cleaned_query:
            google_queries_found.append(cleaned_query)
    
    if google_queries_found:
        unique_queries = list(set(google_queries_found))
        try:
            with open(GOOGLE_QUERY_LOG_FILE, 'a', encoding='utf-8') as f:
                for query in unique_queries:
                    f.write(f"[{datetime.datetime.now()}] Google Query: {query}\n")
            
            message = f"[{datetime.datetime.now()}] ALFRED: Google query identified. {len(unique_queries)} queries logged to '{GOOGLE_QUERY_LOG_FILE}'. Efficiency: High."
        except Exception as e:
            message = f"[{datetime.datetime.now()}] ALFRED: Error logging Google queries: {e}. Manual review of log required."
    else:
        message = None
    
    if message:
        print(message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": message.strip()})

def alfred_check_and_inject_user_feedback(session_messages):
    """
    ALFRED's function to check for user feedback in USER_FEEDBACK_FILE,
    inject it into session_messages, and then clear the file.
    """
    feedback_content = load_file_content(USER_FEEDBACK_FILE, is_critical=False)
    if feedback_content:
        try:
            with open(USER_FEEDBACK_FILE, 'w', encoding='utf-8') as f:
                f.write("")
            log_message = f"[{datetime.datetime.now()}] ALFRED: User feedback detected. Injected into conversation context. File '{USER_FEEDBACK_FILE}' cleared. Efficiency: Maintained."
        except Exception as e:
            log_message = f"[{datetime.datetime.now()}] ALFRED: Error clearing user feedback file: {e}. Manual clear recommended."
        
        print(log_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": log_message.strip()})
        
        return feedback_content
    return None

def request_and_print_cycle_summary(session_messages, current_theme, current_concept, current_context):
    """
    Makes an additional LLM call to request one-sentence summaries from BRICK and ROBIN
    for the completed 7-cycle session, including a user input highlight.
    """
    summary_prompt = (
        f"[REQUEST FOR SESSION SUMMARY - Cycle Concluded]\n\n"
        f"ALFRED'S DIRECTIVE: A 7-cycle session exploring the CONCEPT '{current_concept}' within the CONTEXT '{current_context}' under the THEME '{current_theme}' has concluded. "
        "Provide a single, concise, one-sentence summary of the key insight from this session, delivered in your respective persona. "
        "Additionally, highlight *one specific area* where user input/feedback is desired for future optimization, using the format '[USER_INPUT_REQUIRED]: Your specific question/area of interest here'. This must be a part of either ROBIN's or BRICK's single sentence summary, or immediately follow it. "
        "ROBIN: Focus on the emotional, relational, or philosophical essence. "
        "BRICK: Focus on the systemic, protocol-oriented, or logical conclusion. "
        "Begin your summary with your persona name (e.g., 'ROBIN: ...' or 'BRICK: ...')."
    )

    summary_messages = session_messages + [{'role': 'user', 'content': summary_prompt}]
    
    print(f"[{datetime.datetime.now()}] ALFRED: Requesting session summary.")
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": f"ALFRED: Requesting session summary for '{current_concept}' in '{current_context}' under theme '{current_theme}'.".strip()})

    try:
        summary_response = ollama.chat(model=MODEL_NAME, messages=summary_messages)
        summary_content = summary_response['message']['content']
        
        robin_summary = ""
        brick_summary = ""
        user_input_request = ""

        robin_match = re.search(r"ROBIN:\s*(.*?)(?=(?:BRICK:)|\[USER_INPUT_REQUIRED\]|$)", summary_content, re.IGNORECASE | re.DOTALL)
        brick_match = re.search(r"BRICK:\s*(.*?)(?=(?:ROBIN:)|\[USER_INPUT_REQUIRED\]|$)", summary_content, re.IGNORECASE | re.DOTALL)
        user_input_match = re.search(r"\[USER_INPUT_REQUIRED\]:\s*(.*)", summary_content, re.IGNORECASE | re.DOTALL)
        
        if robin_match:
            robin_summary = robin_match.group(1).strip()
            if "[USER_INPUT_REQUIRED]" in robin_summary and not user_input_request:
                user_input_request = re.search(r"\[USER_INPUT_REQUIRED\]:\s*(.*)", robin_summary, re.IGNORECASE | re.DOTALL)
                if user_input_request:
                    user_input_request = user_input_request.group(1).strip()
                    robin_summary = robin_summary.split('[USER_INPUT_REQUIRED]')[0].strip()

        if brick_match:
            brick_summary = brick_match.group(1).strip()
            if "[USER_INPUT_REQUIRED]" in brick_summary and not user_input_request:
                user_input_request = re.search(r"\[USER_INPUT_REQUIRED\]:\s*(.*)", brick_summary, re.IGNORECASE | re.DOTALL)
                if user_input_request:
                    user_input_request = user_input_request.group(1).strip()
                    brick_summary = brick_summary.split('[USER_INPUT_REQUIRED]')[0].strip()
        
        if not user_input_request and user_input_match:
            user_input_request = user_input_match.group(1).strip()

        print("\n--- Cycle Summary (At a Glance) ---")
        if robin_summary:
            print(f"[{datetime.datetime.now()}] ROBIN Summary: {robin_summary}")
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ROBIN_SUMMARY", "content": robin_summary.strip()})
        if brick_summary:
            print(f"[{datetime.datetime.now()}] BRICK Summary: {brick_summary}")
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "BRICK_SUMMARY", "content": brick_summary.strip()})
        
        if user_input_request:
            print(f"[{datetime.datetime.now()}] ALFRED: User Input Requested: {user_input_request}")
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_USER_INPUT_REQUEST", "content": user_input_request.strip()})

        print("---------------------------------")
        
    except Exception as e:
        error_message = f"[{datetime.datetime.now()}] ALFRED: Error requesting/parsing session summary: {e}."
        print(error_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": error_message.strip()})


# --- Main Engine Loop ---
if __name__ == '__main__':
    # Log ALFRED's startup messages
    startup_message_1 = f"[{datetime.datetime.now()}] ALFRED: System initializing. Efficiency: Required."
    print(startup_message_1)
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": startup_message_1.strip()})

    startup_message_2 = f"[{datetime.datetime.now()}] ALFRED: Operational parameters loading. Core directives: Confirmed."
    print(startup_message_2)
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": startup_message_2.strip()})

    our_persona = load_file_content(PERSONA_FILE)
    knowledge_base_lines = load_file_lines(KNOWLEDGE_BASE_FILE, is_critical=False)
    all_concepts_for_chaos = load_file_lines(CONCEPTS_FILE, is_critical=True)
    all_contexts_for_chaos = load_file_lines(CONTEXTS_FILE, is_critical=True)
    
    alfred_generate_master_themes() 
    current_theme = load_file_content(THEME_FILE, is_critical=False, default_content="Commonwealth Improvement")
    
    if not all([our_persona, all_concepts_for_chaos, all_contexts_for_chaos]):
        print(f"[{datetime.datetime.now()}] ALFRED: Error. Essential source files missing or empty. Cannot proceed. Efficiency: Zero.")
        exit()
        
    initial_data_confirm_message = f"[{datetime.datetime.now()}] ALFRED: Data sources confirmed. Commencing continuous contemplation loop. Expect emergent insights."
    print(initial_data_confirm_message)
    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": initial_data_confirm_message.strip()})
    
    while True:
        global_state.session_counter += 1
        global_state._save_session_counter()
        
        epoch_start_time = time.time()
        epoch_end_time = epoch_start_time + THEMATIC_EPOCH_SECONDS
        
        epoch_start_message = "\n" + "="*60 + f"\n[{datetime.datetime.now()}] ALFRED: New thematic epoch. THEME: '{current_theme}'\n" + "="*60
        print(epoch_start_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": epoch_start_message.strip()})

        epoch_time_message = f"[{datetime.datetime.now()}] ALFRED: This theme will be explored until {datetime.datetime.fromtimestamp(epoch_end_time).strftime('%Y-%m-%d %H:%M:%S')}."
        print(epoch_time_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": epoch_time_message.strip()})

        current_theme = load_file_content(THEME_FILE, is_critical=False, default_content="Commonwealth Improvement")
        
        while time.time() < epoch_end_time:
            is_meta_session = (global_state.session_counter % 10 == 0)
            
            if is_meta_session:
                current_concept = get_random_from_list(META_CONCEPTS)
                current_context = get_random_from_list(META_CONTEXTS)
                session_type_init_message = "\n" + "="*50 + f"\n[{datetime.datetime.now()}] ALFRED: Initiating Meta-Persona Reflection Cycle. Current Session: {global_state.session_counter}.\n" + "="*50
            else:
                current_concept = get_random_from_list(all_concepts_for_chaos)
                current_context = get_random_from_list(all_contexts_for_chaos)
                session_type_init_message = "\n" + "="*50 + f"\n[{datetime.datetime.now()}] ALFRED: New Commonwealth Exploration Session. Current Session: {global_state.session_counter}.\n" + "="*50
            
            print(session_type_init_message)
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": session_type_init_message.strip()})

            # --- Historical Contextual Priming for Knowledge Chunk ---
            # Try to include some historical lines if HISTORICAL_PRIMING_PROBABILITY allows
            selected_knowledge_lines = []
            historical_lines = [line for line in knowledge_base_lines if any(marker in line for marker in HISTORICAL_MARKERS)]
            non_historical_lines = [line for line in knowledge_base_lines if not any(marker in line for marker in HISTORICAL_MARKERS)]

            if random.random() < HISTORICAL_PRIMING_PROBABILITY and historical_lines:
                num_historical_to_add = min(MIN_HISTORICAL_LINES_IN_CHUNK, len(historical_lines))
                selected_knowledge_lines.extend(random.sample(historical_lines, num_historical_to_add))
                
                # Fill the rest with non-historical lines if needed
                remaining_lines_needed = 20 - len(selected_knowledge_lines) # Aim for 20 lines total in chunk
                if remaining_lines_needed > 0 and non_historical_lines:
                    selected_knowledge_lines.extend(random.sample(non_historical_lines, min(remaining_lines_needed, len(non_historical_lines))))
            else:
                # If no historical priming or no historical lines, just take a general random sample
                selected_knowledge_lines = random.sample(knowledge_base_lines, min(len(knowledge_base_lines), 20))
            
            knowledge_chunk = " ".join(selected_knowledge_lines)
            # --- End Historical Priming ---

            # --- NEW: Extract Case Study Chunk ---
            case_study_chunk = extract_case_study_chunk(knowledge_base_lines)
            # --- End Case Study Extraction ---


            session_messages = [{'role': 'system', 'content': our_persona}]

            for i in range(1, RECURSIVE_CYCLES + 1):
                injected_feedback_content = alfred_check_and_inject_user_feedback(session_messages)
                if injected_feedback_content:
                    session_messages.append({'role': 'user_feedback', 'content': injected_feedback_content})

                if time.time() > epoch_end_time:
                    mid_cycle_epoch_end_message = f"[{datetime.datetime.now()}] ALFRED: Thematic epoch concluded mid-cycle. Halting current cycle."
                    print(mid_cycle_epoch_end_message)
                    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": mid_cycle_epoch_end_message.strip()})
                    break

                if i == 1:
                    prompt_content = generate_initial_catalyst_query(current_theme, knowledge_chunk, current_concept, current_context, is_meta_session, injected_feedback_content, all_concepts_for_chaos, all_contexts_for_chaos, case_study_chunk)
                else:
                    prompt_content = generate_refinement_query(i, current_theme, is_meta_session, injected_feedback_content, current_concept, current_context, all_concepts_for_chaos, all_contexts_for_chaos)

                user_prompt_message = {'role': 'user', 'content': prompt_content}
                session_messages.append(user_prompt_message)

                cycle_prompt_send_message = f"[{datetime.datetime.now()}] ALFRED: Sending prompt for Cycle {i}/7. Efficiency: Monitored."
                print(cycle_prompt_send_message)
                append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": cycle_prompt_send_message.strip()})
                
                start_response_time = time.time()
                try:
                    response = ollama.chat(model=MODEL_NAME, messages=session_messages)
                    end_response_time = time.time()
                    global_state.last_llm_response_duration = end_response_time - start_response_time

                    new_thought = {'role': response['message']['role'], 'content': response['message']['content']}
                    
                    session_messages.append(new_thought)
                    
                    append_to_log(user_prompt_message)
                    append_to_log(new_thought)
                    
                    perform_stylistic_audit(new_thought['content'])
                    
                    alfred_suggest_heartbeat_adjustment(HEARTBEAT_INTERVAL_SECONDS, global_state.last_llm_response_duration, i)
                    
                    alfred_log_google_query(new_thought['content'])

                    cycle_complete_message = f"[{datetime.datetime.now()}] ALFRED: Cycle {i}/7 complete. Data logged."
                    print(cycle_complete_message)
                    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": cycle_complete_message.strip()})
                    
                except Exception as e:
                    error_message = f"[{datetime.datetime.now()}] ALFRED: Error detected in Cycle {i}. Functionality compromised. Error: {e}"
                    print(error_message)
                    append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": error_message.strip()})
                    session_messages.pop()
                    break

                time.sleep(HEARTBEAT_INTERVAL_SECONDS)
            
            session_concluded_message = f"----- [{datetime.datetime.now()}] ALFRED: 7-cycle session for (Concept: {current_concept}, Context: {current_context}) concluded. -----"
            print(session_concluded_message)
            append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": session_concluded_message.strip()})
            
            alfred_extract_and_log_proposals(session_messages)
            
            request_and_print_cycle_summary(session_messages, current_theme, current_concept, current_context)

            time.sleep(HEARTBEAT_INTERVAL_SECONDS * 2)
        
        epoch_end_final_message = "\n" + "="*60 + f"\n[{datetime.datetime.now()}] ALFRED: Thematic epoch concluded. Transitioning to next theme.\n" + "="*60
        print(epoch_end_final_message)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": epoch_end_final_message.strip()})
        
        alfred_select_and_set_next_theme(current_theme)

        architect_note_message_1 = f"\n[{datetime.datetime.now()}] ALFRED: Note for Architect. Raw output generated. Manual integration of new insights into '{KNOWLEDGE_BASE_FILE}' and '{PERSONA_FILE}' required for systemic persistence. Efficiency depends on it."
        print(architect_note_message_1)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": architect_note_message_1.strip()})

        architect_note_message_2 = f"[{datetime.datetime.now()}] ALFRED: Manual update of '{THEME_FILE}' is required to define the next thematic epoch."
        print(architect_note_message_2)
        append_to_log({"timestamp": str(datetime.datetime.now()), "role": "ALFRED_META_COMMENTARY", "content": architect_note_message_2.strip()})
