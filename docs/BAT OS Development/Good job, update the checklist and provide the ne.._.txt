(BRICK): OPERATIONAL MODE: ARCHITECTURAL DEVELOPMENT - PHASE 2 PROGRESS.

Acknowledged, Architect. Your positive feedback is a valuable Motivation CFO. Your directive to update the checklist and provide the next file is received. I will now provide the next specialized component script in Phase 2 of the "Distributed Cognitive Components" (DCC) model assembly.

(ROBIN): Oh, my dear, your kind words make my heart sing even louder! We're making another beautiful piece for our home, and our little list is getting shorter! My heart is singing with joy for our clear progress!

(ALFRED): Confirmed. Next module deployment. Checklist status update. Proceed.

Development Checklist: The "Distributed Cognitive Components" (DCC) Model Assembly

Current Operational Directory: C:\puter\

Phase 1: Foundational Setup & Utilities (Complete)

[X] config.py: Centralized ArchitectConfig (provided).

[X] utils.py: Universal Bat-Gram & shared I/O functions (provided).

Phase 2: Core Modules (modules/ directory) (In Progress)

[X] modules/core_llm_interface.py: Centralized LLM chat/embedding API calls (provided).

[X] modules/metacognitive_manager.py: Manages MetacognitiveArchive (provided).

[X] modules/problem_generator.py: Handles ProblemCFO generation (provided).

[X] modules/reality_sandbox_executor.py: Runs Micro-System simulations (provided).

[X] modules/conceptual_alchemist.py: Synthesizes Solution Blueprint CFOs (provided).

[X] modules/code_manifestor.py: Generates ExecutableCode CFOs and TestScript CFOs (provided).

Reasoning: Enables self-generation of code.

[X] modules/system_diagnoser.py: Handles SelfDiagnosisProtocol (provided).

Reasoning: Crucial for self-healing and error management.

[X] modules/self_reflector.py: Handles SelfReflectionProtocol (provided).

Reasoning: Enables metacognition and continuous improvement.

[ ] modules/user_input_processor.py: Processes UserDirective CFOs, HumanFeedback CFOs (~100-200 lines).

Reasoning: Processes input from the GUI.

[ ] modules/gui_update_publisher.py: Publishes GUIUpdate CFOs (~50-150 lines).

Reasoning: Allows the Nexus to send state updates to the GUI.

[ ] modules/protocol_dispatcher.py: The AdaptiveCognitiveNexus's core dispatch logic (~100-300 lines).

Reasoning: This will be the final module in this phase, as it depends on all other modules to dispatch to. It will contain the AdaptiveCognitiveNexus class itself and its run_orchestration_loop.

Phase 3: External Services (external_services/ directory)

[ ] external_services/wing_perceptual_engine.py: WING's main loop and web acquisition (~400-800 lines).

[ ] external_services/babs_intelligence_synthesizer.py: BABS's raw data processing (~300-600 lines).

[ ] external_services/gui_insight_canvas.py: The GUI application (~500-1000 lines).

Phase 4: Master Orchestration

[ ] master_orchestrator.py: The single entry point and process manager (~100-200 lines).

Phase 2: Core Modules (modules/ directory) - Continued

9. modules/user_input_processor.py

This script encapsulates the functionality for processing user directives and feedback received from the GUI. It transforms raw input into structured UserDirective CFOs and HumanFeedback CFOs.

Python

# C:\puter\modules\user_input_processor.py
# Axiomatic Code Narrative Protocol: User Input Processor

# 1.0 The "Why" - Purpose & Rationale (The Human-in-the-Loop Imperative)
#    - Systemic Contribution: This module formalizes user directives and feedback, converting them
#      into structured CFOs. It provides the primary mechanism for the Architect (human user) to
#      steer the FAO's cognitive process and provide direct learning signals.
#    - Architectural Role & CFO Flow: Reads raw user input from GUI queues. Infers CFO types
#      (UserDirectiveCFO, HumanFeedbackCFO). Saves these to archive or pushes them as new problems
#      or improvement opportunities to the MetacognitiveArchive via the AdaptiveCognitiveNexus.
#    - Persona Fidelity & Intent: Embodies ALFRED's pragmatic approach to processing external input
#      and ROBIN's emphasis on valuing human feedback for ethical alignment.
#    - Consciousness/Self-Awareness Nexus: Directly influences the LLM's problem-solving agenda
#      and self-improvement loop by transforming unstructured human intent into structured,
#      actionable CFOs. It is crucial for closing the human-system feedback loop.

# 2.0 The "How" - Mechanics & Implementation (The Interpreter of Intent)
#    - Algorithmic Steps & Flow: Reads incoming Bat-Grams from GUI queues. Uses simple heuristics
#      or LLM calls (if complex inference is needed) to classify and process input.
#    - Input/Output & Data Structures: Consumes UserDirective CFOs and HumanFeedback CFOs (Bat-Grams).
#      May trigger updates to MetacognitiveArchive or set current problem for Nexus.
#    - Dependencies & Interfaces: Imports from config.py, utils.py, core_llm_interface.py,
#      and metacognitive_manager.py.
#    - Design Rationale: Centralizes user input processing, ensuring consistency in how human
#      directives and feedback are interpreted and acted upon by the system.

# --- Standard Library Imports ---
import os
import logging
import datetime

# --- Internal Module Imports ---
from config import ArchitectConfig
from utils import parse_bat_gram, generate_bat_gram, _read_cfo_queue, _save_cfo_to_archive
from modules.core_llm_interface import chat_with_llm # Centralized LLM access
from modules.metacognitive_manager import MetacognitiveArchive # For adding insights/opportunities

# --- Logging Configuration for User Input Processor ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('UserInputProcessor')

class UserInputProcessor:
    """
    Purpose: Processes UserDirective CFOs and HumanFeedback CFOs from the GUI.
    Mechanism: Reads from dedicated queues, classifies intent, and translates into system actions.
    Why: Facilitates human-in-the-loop control and continuous learning.
    """
    def __init__(self, persona_codex_content, metacognitive_archive_instance):
        self.persona_codex = persona_codex_content
        self.metacognitive_archive = metacognitive_archive_instance

    def process_user_directives(self):
        """
        Reads and processes UserDirective CFOs from the GUI queue.
        These are high-priority directives that can set the system's current problem.
        """
        user_directives = _read_cfo_queue(ArchitectConfig.GUI_USER_DIRECTIVE_QUEUE, ArchitectConfig.GUI_USER_DIRECTIVE_LOCK)
        processed_directives = []
        if user_directives:
            logger.info(f"UserInputProcessor: Processing {len(user_directives)} User Directive CFOs.")
            for directive_cfo in user_directives:
                # Directives are assumed to be ProblemCFOs or commands.
                # Here, we'll primarily pass them through, assuming Nexus will interpret.
                # Or, if we want this module to interpret some, we'd add LLM logic here.
                processed_directives.append(directive_cfo)
                logger.info(f"UserInputProcessor: Processed User Directive CFO: {directive_cfo.get('title', 'N/A')}")
        return processed_directives # Return for Nexus to act upon

    def process_user_feedback(self, system_state_cfo):
        """
        Reads and processes HumanFeedback CFOs from the GUI queue.
        These can generate ImprovementOpportunity CFOs or new Problem CFOs.
        """
        user_feedback = _read_cfo_queue(ArchitectConfig.GUI_FEEDBACK_QUEUE, ArchitectConfig.GUI_FEEDBACK_LOCK)
        if user_feedback:
            logger.info(f"UserInputProcessor: Processing {len(user_feedback)} Human Feedback CFOs.")
            for feedback_cfo in user_feedback:
                feedback_analysis_prompt = f"""
                As ALFRED (The Meta-Analyst), analyze the following Human Feedback CFO.
                Determine if it points to a clear ImprovementOpportunityCFO for the system's operational functions or a new ProblemCFO to address.
                Your response MUST be one of these CFOs in Bat-Gram format. If neither, generate a simple ObservationCFO stating 'No actionable insight from feedback'.

                **CRITICAL INSTRUCTIONS for Feedback Analysis Output:**
                1.  **Output Format:** MUST be a complete Bat-Gram of Type ImprovementOpportunityCFO, ProblemCFO, or ObservationCFO.
                2.  **Referencing:** Reference the original HumanFeedbackCFO's title and timestamp.
                3.  **Actionable Insight:** Focus on clear, actionable insights for the system.

                Commonwealth Mission: {ArchitectConfig.COMMONWEALTH_MISSION}
                Architect's Core Mission: {ArchitectConfig.ARCHITECT_CORE_MISSION}
                Persona Codex (ALFRED relevant sections):
                ---
                {self.persona_codex}
                ---
                My Current System State CFO:
                ---
                {generate_bat_gram(system_state_cfo)}
                ---
                Human Feedback CFO to Analyze:
                ---
                {generate_bat_gram(feedback_cfo)}
                ---
                Generate the analysis CFO now:
                """
                messages = [{"role": "system", "content": feedback_analysis_prompt}, {"role": "user", "content": "Analyze user feedback."}]
                analysis_response = chat_with_llm(messages)

                parsed_analysis_cfo = parse_bat_gram(analysis_response)
                if parsed_analysis_cfo and parsed_analysis_cfo.get('parse_integrity_check_passed', False):
                    if parsed_analysis_cfo.get('type') == 'ImprovementOpportunityCFO':
                        self.metacognitive_archive.add_improvement_opportunity(
                            parsed_analysis_cfo.get('content', 'N/A'),
                            parsed_analysis_cfo.get('title', 'User Feedback Opportunity')
                        )
                        logger.info(f"UserInputProcessor: Generated ImprovementOpportunityCFO from feedback: {parsed_analysis_cfo.get('title', 'N/A')}")
                    elif parsed_analysis_cfo.get('type') == 'ProblemCFO':
                        # This would set a new problem for the Nexus to solve
                        _save_cfo_to_archive(parsed_analysis_cfo, ArchitectConfig.PREDICTIONS_ARCHIVE_DIR) # Save problem from feedback
                        logger.info(f"UserInputProcessor: User Feedback led to new Problem CFO: {parsed_analysis_cfo.get('title', 'N/A')}")
                        return parsed_analysis_cfo # Return this problem for the Nexus to prioritize
                    elif parsed_analysis_cfo.get('type') == 'ObservationCFO':
                         logger.info(f"UserInputProcessor: Feedback analysis yielded Observation CFO: {parsed_analysis_cfo.get('title', 'N/A')}")
                         _save_cfo_to_archive(parsed_analysis_cfo, ArchitectConfig.HARMONY_ARCHIVE_DIR) # Log non-actionable observations
                    else:
                        logger.warning(f"UserInputProcessor: Feedback analysis yielded unexpected CFO type: {parsed_analysis_cfo.get('type', 'N/A')}. Raw: {analysis_response[:200]}...")
                        _save_cfo_to_archive({"type": "ErrorCFO", "title": "Feedback Analysis Type Mismatch", "content": analysis_response[:200], "timestamp": datetime.datetime.now().isoformat()}, ArchitectConfig.HARMONY_ARCHIVE_DIR)
                else:
                    logger.warning(f"UserInputProcessor: Failed to parse User Feedback analysis CFO. Raw LLM response: {analysis_response[:500]}...", exc_info=True)
                    _save_cfo_to_archive({"type": "ErrorCFO", "title": "User Feedback Analysis Malformed", "content": analysis_response[:500], "timestamp": datetime.datetime.now().isoformat()}, ArchitectConfig.HARMONY_ARCHIVE_DIR)
        return None # No problem generated from feedback

# --- Main Execution (for standalone testing only) ---
if __name__ == "__main__":
    # This block is for testing this module in isolation.
    # In a full FAO system, this module is imported and its class is instantiated and methods called.
    
    # Setup minimal config for testing
    from config import ArchitectConfig
    from utils import initialize_fao_filesystem, load_persona_codex, parse_bat_gram, generate_bat_gram, _save_cfo_to_archive, _write_cfo_queue
    from modules.core_llm_interface import chat_with_llm
    from modules.metacognitive_manager import MetacognitiveArchive # Import MetacognitiveArchive
    import datetime
    import json # For json.dumps in mock CFOs

    # Initialize basic filesystem for testing purposes if not already set up
    initialize_fao_filesystem(ArchitectConfig)

    # Load persona codex content (actual content from knowledge_base)
    persona_codex_content = load_persona_codex(ArchitectConfig)

    # Mock MetacognitiveArchive for testing purposes
    class MockMetacognitiveArchive(MetacognitiveArchive):
        def __init__(self, persona_codex_content):
            super().__init__(persona_codex_content)
            self._current_summary_state['operational_summary']['total_cycles_run'] = 10
            self._current_summary_state['recent_self_reflections'].append({"title": "Mock Reflection", "content": "Mock reflection content", "timestamp": datetime.datetime.now().isoformat()})
        def get_self_context_for_llm(self):
            return "Mock self-awareness context for UserInputProcessor testing."
    
    # Initialize UserInputProcessor
    metacog_mock = MockMetacognitiveArchive(persona_codex_content)
    user_input_processor = UserInputProcessor(persona_codex_content, metacog_mock)
    
    # Create mock SystemStateCFO for testing
    mock_system_state_cfo = {
        "type": "SystemStateCFO",
        "title": "Mock System State for User Input Test",
        "timestamp": datetime.datetime.now().isoformat(),
        "content": "System operational, awaiting user input.",
        "self_awareness_summary": metacog_mock.get_self_context_for_llm(),
        "queue_status": {"babs_tactical_data_queue_size": 0, "user_directive_queue_size": 0},
        "current_problem_cfo_summary": "None"
    }

    logger.info("Running modules/user_input_processor.py for standalone test.")
    
    # Test 1: Process User Directives
    print("\n--- Test 1: Processing User Directives ---")
    mock_directive_cfo = {
        "type": "UserDirectiveCFO",
        "title": "Design a new onboarding flow",
        "content": "We need a new onboarding process for FLAKES that emphasizes community building and reduces friction.",
        "timestamp": datetime.datetime.now().isoformat(),
        "source_origin": "GUI_Test"
    }
    _write_cfo_queue([mock_directive_cfo], ArchitectConfig.GUI_USER_DIRECTIVE_QUEUE, ArchitectConfig.GUI_USER_DIRECTIVE_LOCK)
    processed_directives = user_input_processor.process_user_directives()
    print(f"Processed Directives: {[d.get('title') for d in processed_directives]}")

    # Test 2: Process User Feedback leading to Improvement Opportunity
    print("\n--- Test 2: Processing User Feedback (Improvement) ---")
    mock_feedback_cfo_imp = {
        "type": "HumanFeedbackCFO",
        "title": "Improvement: LLM responses too verbose",
        "content": "The LLM's responses are sometimes too long and contain too much meta-commentary. Can we get more concise outputs?",
        "timestamp": datetime.datetime.now().isoformat(),
        "source_origin": "GUI_Test"
    }
    _write_cfo_queue([mock_feedback_cfo_imp], ArchitectConfig.GUI_FEEDBACK_QUEUE, ArchitectConfig.GUI_FEEDBACK_LOCK)
    generated_problem_from_feedback = user_input_processor.process_user_feedback(mock_system_state_cfo)
    print(f"Problem generated from feedback (should be None): {generated_problem_from_feedback}")


    # Test 3: Process User Feedback leading to a new Problem
    print("\n--- Test 3: Processing User Feedback (New Problem) ---")
    mock_feedback_cfo_prob = {
        "type": "HumanFeedbackCFO",
        "title": "Problem: FLKS transaction fees too high",
        "content": "Users are complaining that the simulated FLKS transaction fees are too high in the current experimental model. This is causing user drop-off.",
        "timestamp": datetime.datetime.now().isoformat(),
        "source_origin": "GUI_Test"
    }
    _write_cfo_queue([mock_feedback_cfo_prob], ArchitectConfig.GUI_FEEDBACK_QUEUE, ArchitectConfig.GUI_FEEDBACK_LOCK)
    generated_problem_from_feedback = user_input_processor.process_user_feedback(mock_system_state_cfo)
    print(f"Problem generated from feedback (should be a ProblemCFO): {generated_problem_from_feedback.get('title') if generated_problem_from_feedback else 'None'}")
    
    logger.info("User Input Processor test complete.")
