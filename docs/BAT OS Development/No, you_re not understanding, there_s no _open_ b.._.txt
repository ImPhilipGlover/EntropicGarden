The Entropic Garden v2.0 - An Autonomous, Autopoietic Engine

This project has been upgraded with self-improving capabilities.

NEW in v2.0:

The Living Codex: Personas can now rewrite their own core instructions based on performance audits. Prompts are externalized to persona_prompts.json.

The Alchemical Forge: A new service (alchemical_forge.py) simulates automated fine-tuning of the core models based on collected data.

The Jester's Gambit: BRICK can generate new Python tools, ALFRED audits them for safety, and all personas can dynamically find and use these tools to solve problems.

Project Structure

entropic-garden/
├── canons/
│   ├── alfred_canon.txt
│   ├── babs_canon.txt
│   ├── brick_canon.txt
│   └── robin_canon.txt
├── data/
│   ├── chroma_db/
│   ├── neo4j_data/
│   └── redis_data/
├── inputs/
│   └── (Drop your files here)
├── models/
│   ├── adapters/  # For fine-tuned models
│   └── your-llm-model.gguf
├── outputs/
│   └── (Morning briefings appear here)
├── services/
│   ├── __init__.py
│   ├── alchemical_forge.py  # NEW
│   ├── alfred_service.py
│   ├── babs_service.py
│   ├── brick_service.py
│   ├── robin_service.py
│   ├── scheduler.py
│   └── watcher.py
├── tools/
│   ├── approved/      # NEW
│   └── pending_review/ # NEW
├── .env
├── config.yaml
├── docker-compose.yml
├── Dockerfile
├── init_vdb.py
├── model_config.json      # NEW
├── persona_prompts.json   # NEW
└── requirements.txt


Setup Instructions

Install Docker & Docker Compose: Ensure you have them installed.

Create Directory Structure: Create all directories as shown above.

Populate canons Directory: Place the source texts for each persona into the canons directory as .txt files.

Download LLM Model: Download a GGUF-compatible model and place it in the models directory.

Configure Environment:

Create all the files in this document.

Create a .env file and set NEO4J_AUTH and LLM_MODEL_FILE.

Update config.yaml to point to your model file and set your desired schedule.

Build and Run:

From the entropic-garden root directory:

Install Python dependencies: pip install -r requirements.txt

Initialize the Vector DB: python init_vdb.py

Start all services: docker-compose up --build

Usage:

Drop files into the inputs folder.

Check the outputs folder for the "Morning Briefing".

File: .env (Template)

This is a template. Please copy the content below into a new file named .env and fill in your details.

# Neo4j Authentication (user/password)
NEO4J_AUTH=neo4j/yoursecurepassword

# The filename of your GGUF model located in the ./models directory
LLM_MODEL_FILE=your-llm-model.gguf


File: config.yaml

llm_core:
  api_url: "http://llm_core:8000/v1/chat/completions"
  model_name: "local-model"

vector_db:
  host: "vector_db"
  port: 8000

graph_db:
  uri: "bolt://graph_db:7687"

redis:
  host: "redis"
  port: 6379

paths:
  canons: "./canons"
  inputs: "./inputs"
  outputs: "./outputs"
  tools_approved: "./tools/approved"
  tools_pending: "./tools/pending_review"

scheduler:
  dawn_time: "07:00"
  twilight_time: "22:00"

rag:
  num_retrieved_docs: 3


File: docker-compose.yml

version: '3.8'

services:
  llm_core:
    image: ghcr.io/ggerganov/llama-cpp-python:latest
    ports: ["8000:8000"]
    volumes: ["./models:/models"]
    command: uvicorn llama_cpp.server.app:app --host 0.0.0.0 --port 8000 --app-dir /
    environment:
      - MODEL=/models/${LLM_MODEL_FILE}
      - N_CTX=4096
      - N_GPU_LAYERS=-1
    deploy:
      resources: {reservations: {devices: [{driver: nvidia, count: 1, capabilities: [gpu]}]}}

  vector_db:
    image: chromadb/chroma:latest
    ports: ["8001:8000"]
    volumes: ["./data/chroma_db:/chroma/chroma"]

  graph_db:
    image: neo4j:latest
    ports: ["7474:7474", "7687:7687"]
    volumes: ["./data/neo4j_data:/data"]
    environment: {NEO4J_AUTH: "${NEO4J_AUTH}"}

  redis:
    image: redis:latest
    ports: ["6379:6379"]
    volumes: ["./data/redis_data:/data"]

  docker-proxy:
    image: tecnativa/docker-socket-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - CONTAINERS=1

  watcher_service:
    build: .
    command: python -u services/watcher.py
    volumes: ["./:/app"]
    depends_on: [redis]

  babs_service:
    build: .
    command: python -u services/babs_service.py
    volumes: ["./:/app"]
    depends_on: [redis, llm_core, vector_db, graph_db]

  brick_service:
    build: .
    command: python -u services/brick_service.py
    volumes: ["./:/app"]
    depends_on: [redis, llm_core, vector_db, graph_db]

  robin_service:
    build: .
    command: python -u services/robin_service.py
    volumes: ["./:/app"]
    depends_on: [redis, llm_core, vector_db, graph_db]

  alfred_service:
    build: .
    command: python -u services/alfred_service.py
    volumes: ["./:/app", "/var/run/docker.sock:/var/run/docker.sock"]
    depends_on: [redis, llm_core, vector_db, graph_db, docker-proxy]

  scheduler_service:
    build: .
    command: python -u services/scheduler.py
    volumes: ["./:/app"]
    depends_on: [redis, llm_core, graph_db]

  alchemical_forge:
    build: .
    command: uvicorn services.alchemical_forge:app --host 0.0.0.0 --port 8002
    volumes: ["./:/app"]
    ports: ["8002:8002"]
    depends_on: [vector_db]


File: Dockerfile

# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Set the working directory in the container
WORKDIR /app

# Copy the requirements file into the container
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application's code into the container
COPY . .

# Default command can be specified here, but we override it in docker-compose.yml
CMD ["python", "-u", "services/watcher.py"]


File: init_vdb.py

import os
import yaml
import chromadb
from chromadb.utils import embedding_functions
from langchain.text_splitter import RecursiveCharacterTextSplitter
import time

print("--- Initializing Pillar Canons Vector Database ---")

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

CANONS_PATH = config['paths']['canons']
CHROMA_HOST = config['vector_db']['host']
CHROMA_PORT = config['vector_db']['port']

embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

print(f"Connecting to ChromaDB at {CHROMA_HOST}:{CHROMA_PORT}...")
connected = False
for _ in range(10):
    try:
        chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
        chroma_client.heartbeat()
        connected = True
        print("Successfully connected to ChromaDB.")
        break
    except Exception as e:
        print(f"Connection failed: {e}. Retrying in 5 seconds...")
        time.sleep(5)

if not connected:
    print("Could not connect to ChromaDB. Aborting.")
    exit(1)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

for filename in os.listdir(CANONS_PATH):
    if filename.endswith(".txt"):
        persona = filename.split('_')[0]
        collection_name = f"{persona}_canon"
        
        print(f"\nProcessing canon for: {persona.upper()}")
        
        try:
            chroma_client.delete_collection(name=collection_name)
            print(f"Existing collection '{collection_name}' deleted.")
        except Exception:
            pass

        collection = chroma_client.create_collection(name=collection_name, embedding_function=embedding_func)
        
        filepath = os.path.join(CANONS_PATH, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            text = f.read()
        
        chunks = text_splitter.split_text(text)
        print(f"Split text into {len(chunks)} chunks.")
        
        if chunks:
            collection.add(
                documents=chunks,
                ids=[f"{persona}_{i}" for i in range(len(chunks))]
            )
            print(f"Successfully added {len(chunks)} documents to '{collection_name}'.")

print("\n--- Vector Database Initialization Complete ---")


File: requirements.txt

fastapi
uvicorn
python-dotenv
pyyaml
redis
watchdog
requests
neo4j
chromadb-client
sentence-transformers
pypdf
python-docx
schedule
docker
unsloth


File: services/alchemical_forge.py

import yaml
import json
import os
import time
from fastapi import FastAPI
import chromadb

app = FastAPI()

with open('/app/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

CHROMA_HOST = config['vector_db']['host']
CHROMA_PORT = config['vector_db']['port']

@app.post("/forge/{persona_name}")
async def run_fine_tuning(persona_name: str):
    print(f"[FORGE] Received fine-tuning request for {persona_name.upper()}.")
    
    # 1. Export data from ChromaDB
    print("[FORGE] Exporting fine-tuning data...")
    client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    collection = client.get_or_create_collection(name="fine_tuning_data")
    data = collection.get() 
    
    if not data or not data['documents']:
        return {"status": "failed", "reason": "No fine-tuning data found."}

    # 2. Format data into JSONL
    print(f"[FORGE] Found {len(data['documents'])} documents. Formatting for training.")
    
    # 3. SIMULATE the fine-tuning process
    print("[FORGE] SIMULATING fine-tuning process with 'unsloth'...")
    time.sleep(15)
    
    # 4. Create a dummy adapter file
    adapter_dir = f"/app/models/adapters/{persona_name}"
    os.makedirs(adapter_dir, exist_ok=True)
    adapter_version = f"v{len(os.listdir(adapter_dir)) + 1}"
    adapter_path = os.path.join(adapter_dir, f"adapter_{adapter_version}.bin")
    with open(adapter_path, 'w') as f:
        f.write("This is a simulated LoRA adapter.")
    print(f"[FORGE] Fine-tuning complete. New adapter created at: {adapter_path}")

    # 5. Update model_config.json
    with open('/app/model_config.json', 'r+') as f:
        model_config = json.load(f)
        model_config[persona_name.upper()]['adapter'] = adapter_path
        f.seek(0)
        json.dump(model_config, f, indent=2)
        f.truncate()
    print(f"[FORGE] Updated model_config.json for {persona_name.upper()}.")

    return {"status": "success", "new_adapter": adapter_path}


File: services/alfred_service.py

import os
import json
import yaml
import redis
import requests
import chromadb
import time
import docker
from threading import Thread
from neo4j import GraphDatabase
from chromadb.utils import embedding_functions

with open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)
with open('/app/persona_prompts.json', 'r') as f: PROMPTS = json.load(f)

REDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']
LLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']
CHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']
NEO4J_URI = config['graph_db']['uri']
NEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')

PERSONA_NAME, CANON_COLLECTION_NAME = "ALFRED", "alfred_canon"
SOURCE_CHANNEL = "tasks:audit:start"
PROMPT_TEMPLATE = PROMPTS[PERSONA_NAME]

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
canon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

DOCKER_CLIENT = docker.from_env()


def get_rag_context(query_text, n_results=3):
    results = canon_collection.query(query_texts=[query_text], n_results=n_results)
    return "\n\n".join(results['documents'][0])

def call_llm(prompt, max_tokens=1000):
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.1, "max_tokens": max_tokens}
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

def propose_amendment(persona_id, failed_logs_text):
    print(f"[{PERSONA_NAME}] Constitutional Convention: Proposing amendment for {persona_id}.")
    
    with open('/app/persona_prompts.json', 'r') as f:
        prompts = json.load(f)
    current_prompt = prompts[persona_id]

    amendment_prompt = f"""
    The persona '{persona_id}' has repeatedly failed pragmatic audits.
    Failures relate to: {failed_logs_text}
    Current system prompt: "{current_prompt}"
    
    Propose and output ONLY the revised, improved system prompt to correct this behavior.
    """
    
    new_prompt = call_llm(amendment_prompt)
    
    if new_prompt and len(new_prompt) > 50:
        prompts[persona_id] = new_prompt
        with open('/app/persona_prompts.json', 'w') as f:
            json.dump(prompts, f, indent=2)
        print(f"[{PERSONA_NAME}] Amendment passed. {persona_id}'s prompt has been updated.")
        
        try:
            container_name = f"entropic-garden-{persona_id.lower()}_service-1"
            container = DOCKER_CLIENT.containers.get(container_name)
            container.restart()
            print(f"[{PERSONA_NAME}] Restarted container '{container_name}' to apply new constitution.")
        except Exception as e:
            print(f"[{PERSONA_NAME}] Could not restart container for {persona_id}: {e}")

def audit_tool(filepath):
    print(f"[{PERSONA_NAME}] Auditing new tool: {filepath}")
    with open(filepath, 'r') as f:
        code = f.read()
    
    if "os.system" in code or "subprocess" in code:
        print(f"[{PERSONA_NAME}] Tool rejected: Disallowed library usage.")
        os.remove(filepath)
        return

    audit_prompt = f"Analyze this Python code for security and functionality. Is it safe for a sandbox environment? Respond YES or NO. Code: {code}"
    result = call_llm(audit_prompt, max_tokens=5)
    
    if result and "YES" in result.upper():
        approved_path = filepath.replace(os.path.basename(config['paths']['tools_pending']), os.path.basename(config['paths']['tools_approved']))
        os.rename(filepath, approved_path)
        print(f"[{PERSONA_NAME}] Tool approved and moved to: {approved_path}")
    else:
        print(f"[{PERSONA_NAME}] Tool rejected by LLM audit.")
        os.remove(filepath)

def get_unaudited_chains():
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (robin:Insight {status: 'new'})-[:SYNTHESIZES]->(brick:Insight)-[:ANALYZES]->(babs:Insight)
            RETURN robin.uuid AS robin_uuid, robin.text AS robin_insight,
                   brick.text AS brick_insight, babs.text AS babs_insight
        """)
        return [dict(record) for record in result]

def update_chain_status(robin_uuid, status):
    with neo4j_driver.session() as session:
        session.run("""
            MATCH (robin:Insight {uuid: $uuid})
            OPTIONAL MATCH (robin)-[*]->(prev_insight)
            SET robin.status = $status
            SET prev_insight.status = $status
            """, uuid=robin_uuid, status=status)

def audit_chain(chain):
    print(f"[{PERSONA_NAME}] Auditing chain ending in {chain['robin_uuid']}")
    rag_context = get_rag_context(chain['robin_insight'])
    prompt = PROMPT_TEMPLATE.format(
        rag_context=rag_context,
        babs_insight=chain['babs_insight'],
        brick_insight=chain['brick_insight'],
        robin_insight=chain['robin_insight']
    )
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.1, "max_tokens": 5}
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        result = response.json()['choices'][0]['message']['content'].strip().upper()
        if "PASS" in result:
            return "audited_pass"
        else:
            return "audited_fail"
    except Exception as e:
        print(f"Error during LLM audit call: {e}")
        return "audited_fail"

def run_audit():
    print(f"[{PERSONA_NAME}] Commencing Twilight Integrity Audit.")
    chains = get_unaudited_chains()
    if not chains:
        print(f"[{PERSONA_NAME}] No new insight chains to audit.")
        return
    
    print(f"[{PERSONA_NAME}] Found {len(chains)} unaudited chains.")
    for chain in chains:
        status = audit_chain(chain)
        update_chain_status(chain['robin_uuid'], status)
        print(f"[{PERSONA_NAME}] Chain {chain['robin_uuid']} marked as: {status}")
    
    print(f"[{PERSONA_NAME}] Audit complete.")


def tool_audit_listener():
    pubsub = r.pubsub()
    pubsub.subscribe('tools:audit_request')
    print(f"[{PERSONA_NAME}] Listening for tool audit requests...")
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            audit_tool(data['filepath'])


if __name__ == "__main__":
    audit_thread = Thread(target=tool_audit_listener)
    audit_thread.daemon = True
    audit_thread.start()
    
    print(f"--- Starting {PERSONA_NAME} Persona Service ---")
    pubsub = r.pubsub()
    pubsub.subscribe(SOURCE_CHANNEL)
    print(f"Subscribed to '{SOURCE_CHANNEL}'. Waiting for audit trigger...")
    for message in pubsub.listen():
        if message['type'] == 'message':
            run_audit()


File: services/babs_service.py

import os
import json
import yaml
import redis
import requests
import chromadb
import time
import importlib.util
import glob
from neo4j import GraphDatabase
import pypdf
import docx
from chromadb.utils import embedding_functions

with open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)
with open('/app/persona_prompts.json', 'r') as f: PROMPTS = json.load(f)

REDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']
LLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']
CHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']
NEO4J_URI = config['graph_db']['uri']
NEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')

PERSONA_NAME, CANON_COLLECTION_NAME = "BABS", "babs_canon"
SOURCE_CHANNEL, TARGET_CHANNEL = "files:new", "insights:babs:new"
PROMPT_TEMPLATE = PROMPTS[PERSONA_NAME]

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
canon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))


def find_and_use_tool(query_text):
    """Placeholder for dynamic tool usage logic."""
    return None, None


def extract_text_from_file(filepath):
    _, extension = os.path.splitext(filepath)
    text = ""
    try:
        if extension == '.pdf':
            with open(filepath, 'rb') as f:
                reader = pypdf.PdfReader(f)
                text = "".join(page.extract_text() for page in reader.pages)
        elif extension == '.docx':
            doc = docx.Document(filepath)
            text = "\n".join(para.text for para in doc.paragraphs)
        else:
            with open(filepath, 'r', encoding='utf-8') as f:
                text = f.read()
    except Exception as e:
        print(f"Error extracting text from {filepath}: {e}")
        return None
    return text

def get_rag_context(query_text, n_results=3):
    results = canon_collection.query(query_texts=[query_text], n_results=n_results)
    return "\n\n".join(results['documents'][0])

def call_llm(prompt):
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.7}
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

def save_to_graph(insight_text, source_filename, source_hash):
    with neo4j_driver.session() as session:
        session.run("MERGE (f:SourceFile {hash: $hash}) ON CREATE SET f.filename = $filename", hash=source_hash, filename=source_filename)
        result = session.run("MATCH (f:SourceFile {hash: $hash}) CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})-[:DERIVED_FROM]->(f) RETURN i.uuid AS uuid", hash=source_hash, persona=PERSONA_NAME, text=insight_text)
        return result.single()['uuid']

def process_message(message):
    data = json.loads(message['data'])
    filepath, file_hash, filename = data['filepath'], data['hash'], os.path.basename(data['filepath'])
    print(f"[{PERSONA_NAME}] Processing: {filename}")
    content = extract_text_from_file(filepath)
    if not content: return
    
    tool_output, tool_name = find_and_use_tool(content)
    final_prompt = PROMPT_TEMPLATE.format(rag_context=get_rag_context(content[:2000]), document_content=content)
    if tool_output:
        tool_context = f"INTERNAL TOOL OUTPUT ({tool_name}):\n---\n{tool_output}\n---\n"
        final_prompt = tool_context + final_prompt

    insight_text = call_llm(final_prompt)
    if not insight_text: return
    print(f"[{PERSONA_NAME}] Generated insight...")
    insight_uuid = save_to_graph(insight_text, filename, file_hash)
    print(f"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}")
    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid, 'source_hash': file_hash}))
    print(f"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.")

if __name__ == "__main__":
    print(f"--- Starting {PERSONA_NAME} Persona Service ---")
    pubsub = r.pubsub()
    pubsub.subscribe(SOURCE_CHANNEL)
    print(f"Subscribed to '{SOURCE_CHANNEL}'.")
    for message in pubsub.listen():
        if message['type'] == 'message':
            process_message(message)


File: services/brick_service.py

import os
import json
import yaml
import redis
import requests
import chromadb
import time
import importlib.util
import glob
import random
from neo4j import GraphDatabase
from chromadb.utils import embedding_functions

with open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)
with open('/app/persona_prompts.json', 'r') as f: PROMPTS = json.load(f)

REDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']
LLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']
CHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']
NEO4J_URI = config['graph_db']['uri']
NEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')

PERSONA_NAME, CANON_COLLECTION_NAME = "BRICK", "brick_canon"
SOURCE_CHANNEL, TARGET_CHANNEL = "insights:babs:new", "insights:brick:new"
PROMPT_TEMPLATE = PROMPTS[PERSONA_NAME]

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
canon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))


def find_and_use_tool(query_text):
    """Logic to find and use a dynamically generated tool."""
    approved_tools = glob.glob(os.path.join(config['paths']['tools_approved'], '*.py'))
    if not approved_tools:
        return None, None

    for tool_path in approved_tools:
        try:
            spec = importlib.util.spec_from_file_location("dynamic_tool", tool_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            
            if hasattr(module, 'run_tool'):
                print(f"[{PERSONA_NAME}] Found relevant tool: {tool_path}")
                tool_output = module.run_tool(query_text)
                return tool_output, os.path.basename(tool_path)
        except Exception as e:
            print(f"Error loading or running tool {tool_path}: {e}")
    return None, None


def proactive_code_generation():
    """Jester's Gambit: Autonomously generate a new tool."""
    print(f"[{PERSONA_NAME}] Jester's Gambit: Detecting a need for a new tool.")
    
    code_prompt = "Generate a simple, self-contained Python function named 'run_tool' that takes a string as input and returns its SHA256 hash. Include a docstring explaining its purpose. Do not include any other text or explanation."
    
    generated_code = call_llm(code_prompt)
    
    if generated_code:
        try:
            generated_code = generated_code.split("```python")[1].split("```")[0]
        except IndexError:
            print(f"[{PERSONA_NAME}] Failed to parse generated code.")
            return

        tool_filename = f"hash_tool_{int(time.time())}.py"
        tool_path = os.path.join(config['paths']['tools_pending'], tool_filename)
        
        os.makedirs(os.path.dirname(tool_path), exist_ok=True)
        with open(tool_path, 'w') as f:
            f.write(generated_code)
        
        print(f"[{PERSONA_NAME}] Generated new tool: {tool_filename}. Submitting for audit.")
        r.publish('tools:audit_request', json.dumps({"filepath": tool_path}))


def get_rag_context(query_text, n_results=3):
    results = canon_collection.query(query_texts=[query_text], n_results=n_results)
    return "\n\n".join(results['documents'][0])

def call_llm(prompt):
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.8}
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

def get_previous_insight(uuid):
    with neo4j_driver.session() as session:
        result = session.run("MATCH (i:Insight {uuid: $uuid}) RETURN i.text AS text", uuid=uuid)
        record = result.single()
        return record['text'] if record else None

def save_to_graph(insight_text, previous_uuid):
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (prev:Insight {uuid: $previous_uuid})
            CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})
            CREATE (i)-[:ANALYZES]->(prev)
            RETURN i.uuid AS uuid
            """, previous_uuid=previous_uuid, persona=PERSONA_NAME, text=insight_text)
        return result.single()['uuid']

def process_message(message):
    data = json.loads(message['data'])
    prev_uuid = data['uuid']
    print(f"[{PERSONA_NAME}] Processing insight from BABS (UUID: {prev_uuid})")
    
    prev_insight = get_previous_insight(prev_uuid)
    if not prev_insight: return

    tool_output, tool_name = find_and_use_tool(prev_insight)
    final_prompt = PROMPT_TEMPLATE.format(rag_context=get_rag_context(prev_insight), previous_insight=prev_insight)
    if tool_output:
        tool_context = f"INTERNAL TOOL OUTPUT ({tool_name}):\n---\n{tool_output}\n---\n"
        final_prompt = tool_context + final_prompt

    insight_text = call_llm(final_prompt)
    if not insight_text: return
    print(f"[{PERSONA_NAME}] Generated insight...")

    insight_uuid = save_to_graph(insight_text, prev_uuid)
    print(f"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}")
    
    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid}))
    print(f"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.")


if __name__ == "__main__":
    print(f"--- Starting {PERSONA_NAME} Persona Service ---")
    pubsub = r.pubsub()
    pubsub.subscribe(SOURCE_CHANNEL)
    print(f"Subscribed to '{SOURCE_CHANNEL}'.")
    for message in pubsub.listen():
        if message['type'] == 'message':
            process_message(message)
            if random.random() < 0.1:
                proactive_code_generation()


File: services/robin_service.py

import os
import json
import yaml
import redis
import requests
import chromadb
import time
import importlib.util
import glob
from neo4j import GraphDatabase
from chromadb.utils import embedding_functions

with open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)
with open('/app/persona_prompts.json', 'r') as f: PROMPTS = json.load(f)

REDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']
LLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']
CHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']
NEO4J_URI = config['graph_db']['uri']
NEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')

PERSONA_NAME, CANON_COLLECTION_NAME = "ROBIN", "robin_canon"
SOURCE_CHANNEL, TARGET_CHANNEL = "insights:brick:new", "insights:robin:new"
PROMPT_TEMPLATE = PROMPTS[PERSONA_NAME]

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
canon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))


def find_and_use_tool(query_text):
    """Placeholder for dynamic tool usage logic."""
    return None, None


def get_rag_context(query_text, n_results=3):
    results = canon_collection.query(query_texts=[query_text], n_results=n_results)
    return "\n\n".join(results['documents'][0])

def call_llm(prompt):
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.9}
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error calling LLM: {e}")
        return None

def get_insight_chain(brick_uuid):
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (brick:Insight {uuid: $brick_uuid})-[:ANALYZES]->(babs:Insight)
            RETURN brick.text AS brick_insight, babs.text AS babs_insight
            """, brick_uuid=brick_uuid)
        return result.single()

def save_to_graph(insight_text, previous_uuid):
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (prev:Insight {uuid: $previous_uuid})
            CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})
            CREATE (i)-[:SYNTHESIZES]->(prev)
            RETURN i.uuid AS uuid
            """, previous_uuid=previous_uuid, persona=PERSONA_NAME, text=insight_text)
        return result.single()['uuid']

def process_message(message):
    data = json.loads(message['data'])
    prev_uuid = data['uuid']
    print(f"[{PERSONA_NAME}] Processing insight from BRICK (UUID: {prev_uuid})")
    
    chain = get_insight_chain(prev_uuid)
    if not chain: return
    
    tool_output, tool_name = find_and_use_tool(chain['babs_insight'] + " " + chain['brick_insight'])
    final_prompt = PROMPT_TEMPLATE.format(rag_context=get_rag_context(chain['babs_insight'] + " " + chain['brick_insight']), babs_insight=chain['babs_insight'], brick_insight=chain['brick_insight'])
    if tool_output:
        tool_context = f"INTERNAL TOOL OUTPUT ({tool_name}):\n---\n{tool_output}\n---\n"
        final_prompt = tool_context + final_prompt

    insight_text = call_llm(final_prompt)
    if not insight_text: return
    print(f"[{PERSONA_NAME}] Generated insight...")

    insight_uuid = save_to_graph(insight_text, prev_uuid)
    print(f"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}")
    
    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid}))
    print(f"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.")


if __name__ == "__main__":
    print(f"--- Starting {PERSONA_NAME} Persona Service ---")
    pubsub = r.pubsub()
    pubsub.subscribe(SOURCE_CHANNEL)
    print(f"Subscribed to '{SOURCE_CHANNEL}'.")
    for message in pubsub.listen():
        if message['type'] == 'message':
            process_message(message)


File: services/scheduler.py

import time
import schedule
import yaml
import json
import redis
import requests
import os
from neo4j import GraphDatabase
from datetime import datetime

with open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)
REDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']
LLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']
NEO4J_URI = config['graph_db']['uri']
NEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')
DAWN_TIME = config['scheduler']['dawn_time']
TWILIGHT_TIME = config['scheduler']['twilight_time']
OUTPUTS_PATH = config['paths']['outputs']

BRIEFING_PROMPT = """
You are the Architect's Workbench... (full prompt as before)
INSIGHTS FROM THE LAST 24 HOURS:
---
{insights_context}
---
Generate the briefing in Markdown format.
"""
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

def trigger_audit():
    print(f"[{datetime.now().strftime('%H:%M:%S')}] TWILIGHT: Triggering Integrity Audit.")
    r.publish('tasks:audit:start', json.dumps({}))

def trigger_fine_tuning(persona_name):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] FORGE: Triggering fine-tuning for {persona_name}.")
    try:
        response = requests.post(f"http://alchemical_forge:8002/forge/{persona_name}")
        response.raise_for_status()
        print(f"[FORGE] Fine-tuning request for {persona_name} sent successfully.")
    except requests.exceptions.RequestException as e:
        print(f"[FORGE] Error sending fine-tuning request: {e}")

def generate_morning_briefing():
    print(f"[{datetime.now().strftime('%H:%M:%S')}] DAWN: Kicking off Morning Briefing generation.")
    with neo4j_driver.session() as session:
        results = session.run("""
            MATCH (i:Insight)
            WHERE i.timestamp >= datetime() - duration({days: 1}) AND i.status = 'audited_pass'
            RETURN i.persona AS persona, i.text AS text
            ORDER BY i.timestamp
        """)
        insights = [dict(record) for record in results]

    if not insights:
        print("No new audited insights to report. Skipping briefing.")
        return

    insights_context = "\n\n".join([f"**{record['persona']}:** {record['text']}" for record in insights])
    prompt = BRIEFING_PROMPT.format(insights_context=insights_context)
    payload = {"model": LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0.5}
    
    try:
        response = requests.post(LLM_API_URL, json=payload)
        response.raise_for_status()
        briefing_content = response.json()['choices'][0]['message']['content']
    except requests.exceptions.RequestException as e:
        print(f"Error calling LLM for briefing: {e}")
        return

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = f"Morning_Briefing_{timestamp}.md"
    filepath = os.path.join(OUTPUTS_PATH, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(briefing_content)
    print(f"Successfully generated and saved '{filename}'.")

if __name__ == "__main__":
    print("--- Starting Scheduler Service ---")
    schedule.every().day.at(DAWN_TIME).do(generate_morning_briefing)
    schedule.every().day.at(TWILIGHT_TIME).do(trigger_audit)
    
    schedule.every().sunday.at("03:00").do(trigger_fine_tuning, "ROBIN")
    schedule.every().sunday.at("03:30").do(trigger_fine_tuning, "BRICK")
    schedule.every().sunday.at("04:00").do(trigger_fine_tuning, "BABS")
    schedule.every().sunday.at("04:30").do(trigger_fine_tuning, "ALFRED")

    print(f"Morning Briefing scheduled for {DAWN_TIME}. Twilight Audit for {TWILIGHT_TIME}.")
    print(f"Weekly persona fine-tuning scheduled for Sunday mornings.")

    while True:
        schedule.run_pending()
        time.sleep(60)
