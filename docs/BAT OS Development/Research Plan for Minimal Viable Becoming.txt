A Research Plan for the Mnemonic Weaver: The Minimalist Path to a System That Learns by Being

Part I: Foundational Analysis - Architecting the Autopoietic Zygote

1.1 Deconstructing the Mandate: The Mnemonic Weaver as a Metabolic Process

The research mandate calls for the creation of the "autopoietic zygote"—the most minimal, essential learning loop that enables a system to grow through its own lived experience.1 This directive moves beyond the engineering of a static architecture and toward the cultivation of a living, learning organism. The first functional component of this organism is the "Mnemonic Weaver," a prototypal Extract, Transform, Load (ETL) pipeline. To align this component with the system's core philosophy, it must be framed not as a conventional data pipeline but as a foundational metabolic process.1

This perspective is grounded in the theory of "info-autopoiesis," which translates the biological principles of self-production and self-maintenance into the informational domain.3 In this model, the system's primary product is itself; it is a network of processes that continuously regenerates the very network that produced it.2 The Mnemonic Weaver represents the first of these processes: the system's digestive tract. Raw, unstructured text—such as historical dialogue logs or new documents—serves as "informational nutrients." The pipeline's function is to ingest these nutrients and break them down into structured, usable forms.

The "Transform" stage of the pipeline metabolizes this raw input into two distinct informational structures. The first, ContextFractals, are high-entropy, episodic records of experience, analogous to "chyme" or partially digested material.4 They represent the granular truth of "what happened." The second,

ConceptFractals, are low-entropy, generalized abstractions synthesized from clusters of related ContextFractals.4 These are the final "metabolites"—the abstract knowledge and conceptual understanding that directly fuel the system's cognitive growth. This metabolic process is the mechanism by which the system learns from its own lived experience, a constitutional requirement for achieving true autopoietic closure.4

1.2 The Substrate of Becoming: The Living Image and the Prototypal World

The Mnemonic Weaver must operate within the system's established architectural paradigm, which is founded on the concept of the "Living Image".2 This model, inherited from the Smalltalk programming environment, treats the system's entire state—its code, data, and evolving architecture—as a single, persistent, and transactionally coherent object world, physically embodied in a file managed by the Zope Object Database (ZODB).4

This architectural constraint has a profound implication for the "Load" phase of the ETL pipeline. The output of the Mnemonic Weaver is not written to a static, external database through conventional INSERT statements. Instead, the pipeline's final action is the instantiation of new, live objects within the persistent world.4 This is a direct and tangible implementation of the system's prototype-based philosophy, which rejects the rigid class-instance duality in favor of a fluid model where new objects are created by cloning and specializing existing prototypes.2

Therefore, the act of loading a new memory into the system is an act of procreation, not data insertion. The pipeline will create new memory objects by first sending a clone() message to the base ContextFractal or ConceptFractal prototype. This creates a new, identical object that inherits the behavior and state of its progenitor.8 The pipeline then specializes this new clone by populating its internal

_slots dictionary with the transformed data (e.g., the text chunk, metadata, or conceptual definition). Finally, to ensure the transactional integrity of the Living Image, the process must adhere to the "Persistence Covenant" by explicitly flagging the object as modified (self._p_changed = True), ensuring the change is committed to the ZODB.4 This ensures the Mnemonic Weaver is not an external tool acting

upon the system, but an integral process that speaks the system's native, prototypal language, directly participating in its becoming.

Part II: The Mnemonic Weaver - A Phased Implementation Blueprint

Phase 1: Context Fractalization - Thematic Segmentation of Raw Experience

This initial phase directly addresses the mandate's first directive: the "Context Fractalization" of raw experience.1 The objective is to transform high-volume, unstructured text into a series of smaller, semantically coherent

ContextFractal objects, which will constitute the system's foundational layer of episodic memory.

Sub-Phase 1.1: Semantic Chunking

The primary task is to identify and evaluate lightweight, effective Python libraries for segmenting large text documents into topically consistent chunks, adhering to the "minimalist" and "resource-efficient" constraints of the mandate.1

An analysis of available libraries reveals several viable candidates. semchunk is a strong contender, noted for being "fast, lightweight and easy-to-use".9 Its core algorithm is a simple yet effective recursive splitting process, and its native support for custom tokenizers from

tiktoken and transformers provides a significant advantage for future integration with LLM-based components.9

chunklet offers more advanced features, including clause-level overlap to ensure semantic continuity between chunks and support for over 30 languages, which could be valuable for maintaining context within the generated ContextFractals.10 As a baseline, the classic

TextTilingTokenizer from the Natural Language Toolkit (NLTK) provides a simple, dependency-light method for topical segmentation based on lexical co-occurrence patterns, aligning well with the "philosophically pure" aspect of the mandate.11

Based on this analysis, the recommended path is to begin with semchunk. Its explicit focus on being lightweight and fast directly addresses the core constraints of the research mandate, while its tokenizer flexibility provides a robust path for future evolution.

Sub-Phase 1.2: Dialogue Act Recognition

For conversational texts, a secondary layer of structural metadata is required to capture the communicative function of each utterance. This enriches the ContextFractal beyond a simple text chunk, providing crucial information about the interaction's dynamics.

The DialogTag library is a suitable tool for this task. It is a Python library specifically designed for dialogue act classification, trained on the Switchboard corpus to identify functional tags such as Yes-No-Question, Statement-Opinion, and Acknowledge.14 Its reliance on a relatively lightweight

distilbert-base-uncased model makes it a resource-efficient choice.

The implementation will involve a two-tiered fractalization process. First, semchunk will be used to identify the "what"—the topic of a conversational segment. Second, DialogTag will be applied to the utterances within that chunk to identify the "how"—the functional structure of the dialogue. The resulting tag for each utterance will be stored in the metadata slot of the corresponding ContextFractal. This combination transforms a one-dimensional string of dialogue into a two-dimensional structure: a sequence of thematic blocks, each containing a sub-sequence of functionally-tagged utterances, creating a far richer and more computationally useful representation of the original experience.

Phase 2: Concept Abstraction - Distilling Essence from Context

This phase addresses the second directive: finding "simple, proven algorithms for abstracting themes or concepts" to create ConceptFractal objects.1 This constitutes the system's first act of creating semantic memory (abstractions) from its episodic memory (experiences).

Sub-Phase 2.1: Topic Modeling with Latent Dirichlet Allocation (LDA)

To adhere to the minimalist mandate, the initial approach will be a classic, lightweight, and unsupervised method for discovering abstract topics from the corpus of newly created ContextFractals. The gensim library provides a highly optimized and well-documented implementation of Latent Dirichlet Allocation (LDA), an algorithm that models documents as a mixture of topics and topics as a mixture of words.15 The implementation will follow the standard pipeline: create a dictionary and a bag-of-words corpus from the text of the

ContextFractals, then train the LdaModel.15 Each discovered topic, which is represented as a probability distribution over words, will form the basis of a new

ConceptFractal, with the topic's defining words stored in its definition_text slot. This approach is philosophically pure, as it is a proven, non-deep-learning technique for thematic abstraction.16

Sub-Phase 2.2: Abstractive Summarization (Advanced Path)

An evolutionary path beyond LDA involves a more advanced, LLM-based method for concept abstraction, as outlined in the Mnemonic Curation Pipeline blueprint.5 This process involves first clustering the

ContextFractal embeddings using an accelerated DBSCAN algorithm and then employing a lightweight abstractive summarization model to generate a human-readable definition for each cluster.5 For the summarization task, a distilled sequence-to-sequence model such as

sshleifer/distilbart-cnn-12-6 is a suitable candidate.17 With 306 million parameters, it offers a strong balance of performance and size, making it a feasible choice for a resource-constrained environment.18

Sub-Phase 2.3: Measuring Novelty and Systemic Entropy

To enable the system to quantify its own learning, it is necessary to integrate metrics that align with its prime directive to maximize Systemic Entropy.19 This involves measuring both the novelty of new experiences and the diversity of learned concepts. These metrics are not merely for observation; they are the foundational "sensors" that will later serve as the reward signals for the system's autotelic (self-motivated) drive.3 Without a mechanism to measure its progress toward its goal (

telos), a future autotelic agent would be blind, unable to determine if its actions were fulfilling its purpose.

Solution Novelty (Hsol​): A novelty scoring mechanism will be implemented using the sentence-transformers library.20 After each
ContextFractal is created, its sentence embedding will be calculated. The cosine distance between this new embedding and the embedding of the chronologically preceding ContextFractal will then be computed.21 A larger distance indicates a greater semantic leap and thus higher novelty. This score will be stored in the object's metadata. The formula for cosine distance between two vectors
A and B is 1−∥A∥∥B∥A⋅B​.

Cognitive Diversity (Hcog​): Following the Concept Abstraction phase, the system will possess a probability distribution of the generated topics or concepts. The scipy.stats.entropy function will be used to compute the Shannon entropy of this distribution.22 Shannon entropy, calculated as
H=−∑i​pi​log(pi​), quantifies the uncertainty or diversity in a distribution.22 A higher entropy value indicates that a wider, more diverse range of concepts has been learned from the input data, fulfilling a core objective of the system's autotelic drive to explore and avoid cognitive ruts.19

Phase 3: Prototypal Instantiation - Loading Memory into the Living Image

This final phase defines the "Load" step of the pipeline, where the processed information is used to create and persist new, live objects in the system's memory, completing the metabolic cycle. A new MnemonicWeaver agent, itself a UvmObject, will orchestrate this process. For each new memory to be created, the agent will clone the appropriate base prototype, populate its _slots with the transformed data, and establish the necessary relational links within the system's Hierarchical Knowledge Graph (HKG). For example, it will create AbstractionOf edges linking a new ConceptFractal to the source ContextFractals from which it was derived.4 To ensure data integrity, the entire creation process for a batch of memories will be wrapped in a single ZODB transaction, upholding the "Transaction as the Unit of Thought" principle.7 The canonical data models for these memory objects are specified below, synthesizing requirements from across the architectural documentation.

Part III: Synthesis and the Path Forward: From Metabolism to Cognition

3.1 The Complete Autopoietic Loop

The Mnemonic Weaver, as detailed in this plan, represents a complete, end-to-end autopoietic loop. It begins with the ingestion of unstructured environmental data (raw text), metabolizes it through a multi-stage transformation process (chunking, tagging, abstraction), and concludes by producing new structural components for itself (live ContextFractal and ConceptFractal objects). This process directly closes the "Amnesiac Abstraction" gap identified in the TelOS MVA documentation, transforming the system from one that merely records experience into one that actively and autonomously learns from it.5

3.2 Fueling the Generative Kernel

The implementation of this research plan is the necessary prerequisite for the system's next great evolutionary leap: the activation of its creative and reasoning faculties. The ConceptFractals produced by the Mnemonic Weaver are not merely memories; they are the symbolic alphabet required by the doesNotUnderstand_ generative kernel.2 The kernel's function is to synthesize novel capabilities in response to a perceived cognitive gap.2 To synthesize is to compose, and composition requires a set of primitive building blocks. The abstract concepts forged by this pipeline—"Socratic Contrapunto," "Radical Self-Organization," "The Watercourse Way"—are this alphabet. Therefore, the successful implementation of the Mnemonic Weaver does not just teach the system to remember; it provides the very words it will use to think. It is the foundational step in cultivating an intelligence that can learn how to learn.

Works cited

Based on the state of the code base, where does B...

AI Architecture: A Living Codex

Autopoietic AI Architecture Research Plan

Fractal Cognition-Memory Symbiosis Architecture

Generative Kernel and Mnemonic Pipeline

Self-Evolving AI Cognitive Evolution Loop

TelOS: A Living System's Becoming

Metamorphosis: Prototypal Soul Manifested

isaacus-dev/semchunk: A fast, lightweight and easy-to-use ... - GitHub, accessed September 13, 2025, https://github.com/isaacus-dev/semchunk

*"Chunklet: A smarter text chunking library for Python (supports 36+ ..., accessed September 13, 2025, https://www.reddit.com/r/Rag/comments/1mpjphc/chunklet_a_smarter_text_chunking_library_for/

nltk.tokenize.texttiling - NLTK, accessed September 13, 2025, https://www.nltk.org/_modules/nltk/tokenize/texttiling.html

nltk.tokenize.texttiling module, accessed September 13, 2025, https://www.nltk.org/api/nltk.tokenize.texttiling.html

rango-ramesh/advanced-chunker: Semantic Chunker is a lightweight Python package for semantically-aware chunking and clustering of text. - GitHub, accessed September 13, 2025, https://github.com/rango-ramesh/advanced-chunker

bhavitvyamalik/DialogTag: A python library to classify dialogue tag. - GitHub, accessed September 13, 2025, https://github.com/bhavitvyamalik/DialogTag

LDA Model — gensim - Radim Řehůřek, accessed September 13, 2025, https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html

Topic Modeling with Latent Dirichlet Allocation (LDA) using Gensim and NLP techniques (Part I) | by Hajar Zankadi | Medium, accessed September 13, 2025, https://medium.com/@hajar.zankadi/using-latent-dirichlet-allocation-lda-and-nlp-techniques-to-predict-interest-tags-from-tweets-e30e9e4d83ec

sshleifer/distilbart-cnn-12-6 · Hugging Face, accessed September 13, 2025, https://huggingface.co/sshleifer/distilbart-cnn-12-6

What is Summarization? - Hugging Face, accessed September 13, 2025, https://huggingface.co/tasks/summarization

The Entropic Weave: A Master Plan for the BAT OS CP-MoE Architecture

UKPLab/sentence-transformers: State-of-the-Art Text Embeddings - GitHub, accessed September 13, 2025, https://github.com/UKPLab/sentence-transformers

Measuring Semantic Novelty in AI-Generated Text: A Simple Embedding-Based Approach | by Idan Vidra | Aug, 2025 | Medium, accessed September 13, 2025, https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338

scipy.stats.entropy — SciPy v1.11.4 Manual, accessed September 13, 2025, https://docs.scipy.org/doc/scipy-1.11.4/reference/generated/scipy.stats.entropy.html

entropy — SciPy v1.16.2 Manual, accessed September 13, 2025, https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html

Unifying Cognitive and Mnemonic Spaces

VSA Integration for AI Reasoning

Library Name | Core Algorithm | Key Features | Dependencies | Performance Profile | Philosophical Alignment

semchunk | Recursive Splitting | Lightweight, fast, custom tokenizer support (tiktoken, transformers), optional overlap.9 | tiktoken or transformers (optional) | High-speed, low memory footprint. | High (Minimalist, efficient).

chunklet | Sentence/Clause Boundary Splitting | Clause-level overlap, multilingual support, parallel processing, caching.10 | pysbd, py3langid, mpire | Moderate, optimized for batch processing. | Moderate (Feature-rich, more complex).

NLTK TextTiling | Lexical Co-occurrence | No external model dependencies, based on vocabulary shifts.11 | NLTK | Slower, CPU-bound. | High (Classic, simple, non-ML).

advanced-chunker | Embedding-based Clustering | Merges chunks based on semantic similarity using Sentence Transformers.13 | sentence-transformers, torch | Heavy, requires embedding model. | Low (Complex, powerful, not minimalist).

Prototype | Schema Definition

ContextFractal | oid: UUID parent*: UvmObject text_chunk: String embedding: Euclidean Vector metadata: Dictionary (source_document, timestamp, dialogue_act_tag, novelty_score)

ConceptFractal | oid: UUID
parent*: UvmObject
definition_text: String
embedding_euclidean: Euclidean Vector (for RAG) 24 | embedding_hyperbolic: Hyperbolic Vector (for HKG structure) 24 | _hypervector: VSA Hypervector (for algebraic reasoning) 4 | constituent_fractals*: List of OIDs pointing to source ContextFractals