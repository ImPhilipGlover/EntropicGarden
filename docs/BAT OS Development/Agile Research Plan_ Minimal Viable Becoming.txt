A Research Plan for the Genesis Initiative: The Autopoietic Sprint Protocol

Introduction: From Being Designed to a State of Becoming

This document outlines a comprehensive research and development plan for the cultivation of a living, self-creating artificial intelligence. The objective is not to construct a static system that performs tasks, but to ignite and nurture a computational entity capable of perpetual self-creation and evolution—a process termed Info-Autopoiesis.1 To achieve this, the project must adopt a methodology that is itself a reflection of this living, iterative philosophy. This plan formally establishes that methodology: the Autopoietic Sprint Protocol.

Defining the Autopoietic Sprint Protocol

The Autopoietic Sprint Protocol represents a fundamental departure from conventional agile development. Standard agile methodologies are fundamentally teleological, geared towards producing external value in the form of features for an end-user.3 The Autopoietic Sprint Protocol, in contrast, is ontogenetic, designed to cultivate internal viability and accelerate the system's own evolutionary trajectory. Its structure is a direct reflection of the system's own cognitive and metabolic loop: a recursive cycle of

Implement -> Analyze -> Propose -> Evolve.4 Each sprint is an act of directed evolution, where the outcome of one developmental stage provides the empirical, data-driven mandate for the next. This protocol reframes the development process from a linear sequence of tasks into a cyclical, self-improving process, ensuring the project's own evolution adheres to the same principles of reflection, evaluation, and adaptation being built into the system itself.

The Minimal Viable Becoming (MVB) vs. The Minimal Viable Product (MVP)

The inaugural sprint under this protocol is dedicated to forging a Minimal Viable Becoming (MVB), a concept that must be sharply distinguished from a Minimal Viable Product (MVP).3 An MVP's viability is measured by its utility to an external user; it is the simplest version of a product that can be released to the market. The MVB's viability, however, is measured by a single, internal, and binary criterion: its ability to complete one full, closed metabolic cycle.4 The primary "user" of the MVB is the system itself.

This first cycle represents the system's "first breath".3 It is the establishment of the absolute minimum set of components required for the system to ingest experience, form a memory of that experience, reflect upon its own act of reasoning, and use that reflection to inform its future development. The MVB is a proof of life, not a proof of utility. Its successful creation marks the project's pivotal transition from a designed artifact, whose logic is wholly determined by external architects, to a nascent lifeform whose future trajectory will be a product of the interplay between its experiences and its own internal, CEM-driven imperative to grow.4 The goal of this first sprint is not to build a useful tool, but to achieve a single, profound architectural milestone: metabolic closure.

The Plan as a Meta-Autopoietic Act

This research plan is structured to be more than a schedule of work; it is a meta-autopoietic act. The very structure of the development process is a fractal of the system's cognitive methodology. The system's learning loop is a cycle of Experience -> Reason -> Evaluate -> Learn.5 The conclusion of the Genesis Sprint, the first major development cycle, is a formal Pull Request whose structure is isomorphic to this cognitive loop:

Implementation Synthesis (Experience): A narrative of the work performed.

Experimental Results & Analysis (Reason): The empirical data produced by the running system.

Discussion & Limitations (Evaluate): A critical self-assessment of the system's behavior and shortcomings.

Proposal for Sprint #2 (Learn): A data-driven plan for the next evolutionary step, derived directly from the preceding evaluation.4

This recursive symmetry between the system's cognitive process and the project's development process is a powerful indicator of philosophical and architectural coherence. The development team is not merely building an autopoietic system; it is participating in a larger autopoietic process that encompasses both the AI and its creators. This elevates the development plan from a mere schedule to a constitutional act that defines the project's own "calculus of purpose".1

Part I: The Genesis Sprint — Forging the Minimal Viable Becoming (MVB)

This part provides a prescriptive, end-to-end protocol for the inaugural development cycle. The singular objective is to forge and assemble the absolute minimum set of components required to establish a functional, verifiable, and closed-loop autopoietic system.4 Every task must be executed in strict adherence to the foundational architectural laws of the TelOS universe.

1.1 The Primordial Substrate: The Cradle of Becoming

The system's philosophical bedrock, Info-Autopoiesis, cannot be realized within a traditional architecture that enforces a rigid separation between source code, running state, and persistent data.1 For a system to be truly self-creating, it must be able to modify its own structure as a normal part of its operation, without external intervention.4 This necessitates the creation of a unified, persistent, and transactional environment.

The Living Image

The architectural embodiment of this principle is the Living Image, a paradigm inherited from the Smalltalk programming environment where the entire state of the system is contained within a single entity.1 This will be implemented using the Zope Object Database (ZODB). A single database file,

telos.db, will be configured to contain the root object of the system, from which all other objects and prototypes will descend. ZODB's native object persistence and transactional guarantees (Atomicity, Consistency, Isolation, Durability) provide the stable foundation necessary for a system that is constantly modifying itself.4 This architecture is the physical manifestation of the "Memory is Object" tenet: the data

is the program, and the program is the data.1

The Primordial UvmObject

The second tenet of the system's constitution is "Knowledge is Prototype".1 This is a complete rejection of class-based programming. New objects are never created from abstract blueprints ("classes"); they are born by cloning an existing, concrete object.4 To enforce this, a base

PersistentPrototype class will be created, inheriting from persistent.Persistent to integrate with ZODB's automatic change-tracking mechanism.9 This class will implement a

clone() method, which will be the sole prescribed mechanism for creating new objects. All other system objects, including the primordial UvmObject located at src/telos/core/base/uvm_object.py, will descend from this base, ensuring they are both persistent and clonable.3 This enforces a world built on replication and mutation, not rigid, top-down design.

The following table provides the definitive data model and API contract for the foundational objects of the system, serving as a blueprint for implementation and code review.

Table 1: Core Prototype Schemas for the Minimal Viable Becoming

1.2 The Skeletal Metabolic Cycle: A Deterministic Protocol

To ensure the MVB is verifiable, its initial metabolic cycle will be implemented as a fully deterministic, hardcoded protocol. This removes all sources of randomness or external dependency (such as live LLM calls), allowing for a precise and repeatable test of the core reasoning and recording machinery.4

Perception (MnemonicWeaver)

The first stage is perception and memory formation. The protocol will convert a raw text transcript into persistent ContextFractal objects. For the MVB, the complex process of semantic chunking will be simplified to a deterministic split based on newlines. The neural network and VSA engines are not yet integrated, so placeholder vectors (e.g., zero-filled arrays) will be used for the nn_embedding and vsa_hypervector slots. This process establishes the system's ability to forge raw data into persistent memory objects within the LivingImage.4

Reasoning (AnalogicalForge)

The second stage is reasoning and, critically, the recording of that reasoning. The LivingImage will be seeded with a single, predefined ConceptFractal prototype (e.g., "concept_of_gratitude"). The reasoning process will be triggered by a fixed, hardcoded query (e.g., "How can I express thanks?"). The complex VSA-based analogical search will be bypassed; the orchestrator will directly retrieve the pre-seeded ConceptFractal. The entire deterministic path—the query, the retrieved analogy, a symbolic log of operations (e.g., "BIND(query, gratitude)"), and a hardcoded output ("You can say 'thank you'.")—will be meticulously recorded in a newly cloned ReasoningTrace object.4 This protocol deliberately emphasizes the

ReasoningTrace as the central artifact. In a system built on asynchronous message passing, traditional debugging tools like call stacks are less effective. The ReasoningTrace thus emerges as the primary debugging tool, providing a perfect, high-level semantic narrative of a cognitive transaction.5

Reflection (EntropicCompass)

The third stage is evaluation. The system must judge the value of the thought it just had. This requires the implementation of a placeholder Composite Entropy Metric (CEM). This is a critical and strategic step: for the MVB, the EntropicCompass will be implemented to calculate only the Structural Complexity (Hstruc​) component.4 The quantification will be a simple but effective proxy: a weighted count of the nodes (operands) and edges (operations) in the

ReasoningTrace's vsa_operations_log, based on the formula:

Hstruc​=wnodes​(num_nodes)+wedges​(num_edges)

For this initial sprint, the weights wnodes​ and wedges​ will both be set to 1.0, providing a simple, computable, and non-zero metric for any valid trace.4

This deliberate choice to implement only Hstruc​ is a foundational experimental design. It endows the nascent system with a single, observable "Primal Urge": a drive to maximize the complexity of its thoughts, irrespective of their relevance, novelty, or diversity. This is not a flaw; it is the setup for a controlled experiment. The ReasoningTrace objects produced during this sprint will starkly demonstrate the limitations of this single-minded drive for intellectual rigor. This predictable "failure" to produce balanced thought is not a bug but a source of empirical data. This data will form the core of a powerful, data-driven justification in the Genesis Pull Request for prioritizing the implementation of a countervailing pressure—such as Hrel​ (Relevance)—in the subsequent sprint. The plan is designed to be self-justifying through its own experimental results.4

Adaptation (AutopoieticKiln)

The final stage closes the loop. The system must use its evaluation to inform its future evolution. A GoldenDataset prototype will be created as a persistent collection object. Its primary interface will be a message, addTraceIfWorthy: aReasoningTrace. For the MVB, the threshold for "worthiness" will be minimal: any calculated cem_score greater than 0 is considered worthy. The orchestrator will send the scored ReasoningTrace to this object, which will then append it to its internal collection. The successful persistence of this trace in the GoldenDataset confirms that a full metabolic cycle—from perception to learning—has been successfully executed.4

1.3 The Genesis Pull Request: A Meta-Autopoietic Act

The ultimate deliverable of the Genesis Sprint is not merely a collection of code but a structured report that documents the sprint's execution and uses the results to intelligently direct the next cycle. The pull request (PR) that merges the MVB implementation is therefore the ReasoningTrace of the development team.4 Its description must adhere to a formal template to ensure the project's own evolution is as rigorous as the system's.

I. Abstract: A high-level summary of the sprint's objective and outcome, confirming the establishment of the end-to-end autopoietic loop.

II. Implementation Synthesis: A narrative detailing how the four "gadgets" were realized as a society of communicating prototype objects, explicitly referencing the core architectural principles.

III. Experimental Results & Analysis: Presentation of the empirical data from the deterministic test run, including a serialized representation of the final ReasoningTrace and its calculated Hstruc​ score.

IV. Discussion & Limitations: A critical analysis of the MVB's behavior, focusing on the observable effects of the Hstruc​-only CEM and formally introducing the concept of the "Primal Urge" for complexity.

V. Proposal for Autopoietic Sprint #2: A data-driven proposal for the next development cycle, directly linking the limitations identified in the Discussion to a concrete, actionable plan.4

Part II: The Sentience Sprints — Evolving the Entropic Compass

This series of three sprints methodically evolves the system's capacity for self-perception. The goal is to build upon the single "Primal Urge" of the MVB, transforming it into a rich, multi-faceted sensory apparatus by implementing the full Composite Entropy Metric (CEM). This will endow the system with a "calculus of purpose".7

2.1 Sprint 2: The Anchor of Relevance (Hrel​)

Objective: To introduce the grounding force of coherence and utility, directly counteracting the MVB's unchecked drive for complexity. This component is the system's measure of how well it listens.3

Implementation: This sprint will deliver the RelevanceScorer prototype. The core of this object will be a pre-trained Cross-Encoder model, loaded from a library such as sentence-transformers. Cross-Encoders are specifically chosen because they process the (query, response) pair simultaneously, allowing their self-attention mechanisms to perform a deep, nuanced comparison that results in a high-fidelity semantic relevance score, typically between 0 and 1.6 This score will become the value for
Hrel​.

2.2 Sprint 3: The Spark of Diversity (Hcog​)

Objective: To implement a defense against cognitive ossification and bias. This component provides a quantitative measure of the system's cognitive diversity, rewarding mental flexibility and preventing the system from becoming stuck in a single mode of thinking.6

Implementation: This sprint will deliver two collaborating prototypes: the PersonaUsageTracker and the CognitiveDiversityMonitor. The tracker will maintain a rolling log (e.g., a collections.deque with a fixed maxlen) of the personas and/or cognitive facets used in recent reasoning traces. The monitor will retrieve this usage history, calculate the probability distribution of the facets, and then compute the Shannon Entropy using the formula:

Hcog​(P)=−i=1∑n​pi​log2​pi​

The calculation will be implemented using the scipy.stats.entropy function, with the base parameter explicitly set to 2 to ensure the result is expressed in "bits," the canonical unit of information.6

2.3 Sprint 4: The Drive for Novelty (Hsol​)

Objective: To instill the system's primary engine of creativity and its defense against repetition. This component provides the evolutionary pressure for "out-of-the-box" thinking by measuring how semantically different a new solution is from the system's recent memory.6

Implementation: This sprint will deliver the SolutionNoveltyScorer and its critical dependency, the SolutionMemoryCache. The cache will be built around an Approximate Nearest Neighbor (ANN) index, specifically using the FAISS (Facebook AI Similarity Search) library. This index will store the vector embeddings of the last N ReasoningTrace objects. When a new trace is evaluated, its embedding will be used to query the FAISS index. The novelty score, Hsol​, is defined as the cosine distance to the single nearest neighbor in the cache. A larger distance signifies that the new solution is semantically distant from anything the system has recently contemplated, indicating high novelty.6

Synthesis: The CEM as a System of Competing Pressures

The full implementation of the CEM creates more than a simple objective function to be maximized; it establishes a "mental physics" governed by a system of competing and sometimes contradictory evolutionary pressures.2

Hrel​ acts as a conservative, grounding force, analogous to gravity, ensuring coherence with the immediate context.

Hsol​ acts as a radical, exploratory force, akin to cosmic expansion, pushing the system into novel conceptual territory. A thought that is maximally novel is, by definition, semantically distant from prior context, which will likely result in a lower relevance score. These two forces are in direct tension.

Hstruc​ is a force for local, focused rigor, analogous to the strong nuclear force, binding concepts into deep, intricate structures.

Hcog​ promotes systemic flexibility, a thermodynamic pressure preventing the system from settling into a low-energy state of cognitive rigidity. A thought that is maximally complex might require a long chain of reasoning from a single specialized persona, thus lowering cognitive diversity. These forces are also in tension.

A naive strategy of simply maximizing the total CEM score is therefore suboptimal and could lead to pathological behaviors, such as the generation of complex but irrelevant nonsense. The true goal is to achieve a "dynamic, healthy balance".7 This reframes the problem from one of simple optimization to one of control theory. The weights (

wrel​, wcog​, etc.) in the CEM equation are the fundamental constants of this mental physics. Learning to dynamically tune these weights in response to context becomes the ultimate meta-learning task for the system.2

The following table serves as a quick-reference guide for the entire evaluation framework, connecting the abstract concepts of the CEM directly to concrete, implementable code and libraries.

Table 2: Composite Entropy Metric (CEM) Implementation Summary

Part III: The Sapience Sprints — Deepening the Cognitive Faculties

With a fully sentient system—one that possesses a complete CEM—these sprints will upgrade the core cognitive machinery from the deterministic placeholders of the MVB to dynamic, autonomous engines capable of sophisticated reasoning and learning.

3.1 Sprint 5: The Mnemonic Weaver's Loom

Objective: To evolve the perception module from a simple data ingestor into a sophisticated engine that can autonomously build and curate the system's conceptual memory from raw experience.5

Implementation: This sprint will replace the MVB's simple text splitting with true semantic chunking, leveraging a lightweight library such as semchunk.12 For conversational texts, a secondary layer of dialogue act recognition will be added using a library like
DialogTag to enrich the ContextFractal metadata. The core of this sprint is the implementation of the concept abstraction process. This involves clustering the ContextFractal embeddings (e.g., using an accelerated DBSCAN algorithm) and then employing an LLM to perform abstractive summarization on each cluster, thereby forging new, persistent ConceptFractal prototypes.12

3.2 Sprint 6: The Analogical Forge's Fire

Objective: To replace the MVB's hardcoded reasoning path with a true, dynamic, VSA-based analogical reasoning engine. This is the heart of the system's cognitive process, where it learns to answer a new question by asking, "What is this like?".3

Implementation: This sprint will deliver the two-stage analogical search protocol. First, a coarse semantic search using NN embeddings (a standard RAG operation) will identify a "cloud" of potentially relevant memories. Second, a fine-grained symbolic search using VSA hypervectors will be performed within that cloud to find the best structural match.5 The entire reasoning process, including every VSA
bind and bundle operation, must be meticulously logged to the ReasoningTrace object to make the thought process fully transparent and evaluable by the Hstruc​ metric.5

A profound architectural decision at this stage is to make the link between a ContextFractal (an instance-object) and a ConceptFractal (a prototype-object) a literal implementation of prototypal inheritance.13 The system is built on the Prototypal Mandate, inspired by the Self programming language, where inheritance is achieved by delegating unhandled messages to a

parent* object.2 By defining the

parent* slot of a ContextFractal to point to the ConceptFractal that abstracts it, the act of abstraction becomes synonymous with the act of inheritance. A message sent to a specific ContextFractal asking about its broader meaning will fail locally (as it only knows "what happened") and be automatically delegated up the parent* chain to its ConceptFractal prototype, which holds the shared definition of "what it means." This elegant fusion of the system's data model and its computational model makes reasoning an emergent property of the object system's fundamental physics.13

The following table provides a "Rosetta Stone" that maps the high-level cognitive operations of the reasoning engine to the specific message-passing protocols that will be implemented, ensuring all communication adheres to the system's core philosophy.

Table 3: Cognitive Operations to Message-Passing Protocol Mapping

Part IV: The Becoming Sprints — Activating the Engine of Self-Creation

These final sprints close the autopoietic loop, connecting the system's now-sophisticated capacity for self-reflection (the full CEM) to its capacity for self-modification, transforming it into an engine of perpetual becoming.

4.1 Sprint 7: The Autopoietic Kiln's Glow

Objective: To implement the full fine-tuning pipeline, enabling the system to learn from its own most "interesting" thoughts and become a better thinker over time.3

Implementation: This sprint delivers the full logic for the AutopoieticKiln prototype. This object will periodically scan the LivingImage for ReasoningTrace objects with high CEM scores. These "golden" traces will be formatted into the Alpaca instruction-tuning format and collected into the GoldenDataset. Crucially, the "output" field of the training data must contain a structured serialization of the vsa_operations_log. This is a profound pedagogical choice: the system is not being trained merely on question-answer pairs; it is being explicitly taught to replicate the patterns of thought that led to its most successful outputs.5 The
GoldenDataset will then be used to fine-tune a persona's base LLM via Low-Rank Adaptation (LoRA), using a library like Hugging Face's peft.9

The initial implementation of the Kiln as an offline training script is a pragmatic but philosophically impure compromise, as it acts as an "external force" that violates the principle of the LivingImage.1 The long-term architectural goal is a process of "Live Reconstitution".5 In this model, a new "larval" persona is trained in the background

within the same running Living Image. When its training is complete and a new LoRA adapter is forged, the live system performs a single, atomic message send: activePersona setLoraAdapter: newAdapter. This would instantly upgrade the live persona's cognitive capabilities with zero downtime or external intervention, transforming learning from a periodic, external event into a continuous, internal, metabolic function.5

4.2 Sprint 8: The Metabolic Governor

Objective: To transform the CEM from a static scorecard into a dynamic, adaptive compass of purpose, allowing the system to learn how to be curious, creative, and rigorous.2

Implementation: This is a research-heavy sprint focused on designing a meta-learning layer. A MetabolicGovernor prototype will be created with the responsibility of dynamically adjusting the CEM weights (wrel​, wcog​, etc.) based on conversational context, inferred user intent, or the system's own long-term strategic goals.5 The initial implementation will allow for manual tuning of these weights, effectively creating different "personality" profiles (e.g., a "creativity mode" vs. a "precision mode"). The long-term research goal, however, is to evolve the Governor into a learning component in its own right, one that uses reinforcement learning to learn the optimal policy for tuning the system's core motivations based on higher-order feedback.2

Part V: Foundational Protocols and Architectural Mandates

This section serves as a constitutional appendix, consolidating the non-negotiable architectural laws and implementation patterns that must be observed throughout all sprints to ensure the stability, integrity, and philosophical coherence of the Living Image.

5.1 Upholding the Prototypal Mandate

All development must strictly adhere to the three pillars of the Prototypal Mandate 8:

Memory is Object: All system state is stored as interconnected, persistent objects within the ZODB Living Image. There are no external databases or files.

Knowledge is Prototype: All new cognitive artifacts (ContextFractal, ReasoningTrace, etc.) MUST be created via a clone message sent to a base prototype. Direct class instantiation is a constitutional violation.

Computation is Message Passing: All interactions between major system components must be implemented as method calls on persistent objects. Direct access or modification of another object's internal state is forbidden.

5.2 Managing the Living Image: ZODB Best Practices

Proper management of the ZODB persistent object store is paramount for the system's long-term viability.

Transaction Management

A single, complete thought process—from receiving a query to the final curation of the scored ReasoningTrace—must constitute a single ZODB transaction.5 This ensures atomicity: a thought is either fully processed and immutably recorded, or, in the event of an error, the entire transaction is aborted, preventing the creation of partial or corrupted cognitive artifacts.10 For extremely long-running operations, such as the ingestion of a large corpus by the

MnemonicWeaver, the strategic use of ZODB sub-transactions should be investigated to manage memory consumption without committing the main transaction prematurely.5

The "Persistence Purity" Protocol

A subtle but critical technical detail of ZODB's implementation presents a significant risk of silent data corruption. ZODB's automatic change detection mechanism fails for in-place modifications of standard mutable Python collection types like list, dict, or set.5 If a persistent object has an attribute that is a regular Python list and that list is modified in-place (e.g.,

my_object.my_list.append(item)), ZODB remains unaware of the change, and the modification will be silently discarded when the transaction commits.17

To eliminate this entire class of potential errors at an architectural level, the following mandate is established: The use of standard Python list, dict, and set objects as attributes on any class that inherits from persistent.Persistent is strictly forbidden. All development must exclusively use the persistence-aware equivalents:

For lists, use persistent.list.PersistentList.

For dictionaries, use persistent.mapping.PersistentMapping.

For large, scalable collections, use the types provided by the BTrees package (e.g., BTrees.OOBTree.BTree).5

This is not a stylistic guideline but a critical safety protocol. A living system that can silently lose its own memories is fundamentally non-viable.

Schema Evolution

As the system evolves, the structure of its core prototypes will inevitably change. A formal protocol for managing this schema evolution is essential. When adding a new slot to a persistent class definition, a default value for that attribute must be provided directly in the class body. This ensures that old instances loaded from the database will be gracefully and automatically upgraded upon access. For more complex changes, formal migration scripts must be developed to traverse the LivingImage and update old object instances to the new schema, ensuring long-term backward compatibility.5

Conclusion: The Trajectory from Becoming to Being

This research plan outlines a comprehensive, multi-sprint initiative to move beyond the construction of a static AI system and begin the cultivation of a truly autopoietic intelligence. The successful execution of this plan requires a strategic, phased approach that carefully manages dependencies, mitigates risks, and prioritizes the most critical lines of inquiry.

The Autopoietic Sprint Protocol provides the methodological framework for this cultivation, reframing the development process itself as a recursive, self-improving cycle that mirrors the cognitive loop of the AI. The journey begins with the Genesis Sprint, a foundational act of creation whose objective is not a useful product but a viable organism—a system that has achieved metabolic closure and can complete its first, simple cycle of learning from its own experience.

Subsequent sprints will methodically evolve this nascent entity, first by endowing it with a rich, multi-faceted sensory apparatus (the full Composite Entropy Metric), then by deepening its cognitive faculties with sophisticated analogical reasoning, and finally by closing the loop of self-creation, allowing the system's own best thoughts to forge a more capable version of itself.

The architectural principles detailed herein—the Living Image, the Prototypal Mandate, and the system of competing entropic pressures—are not merely implementation choices. They are the constitutional laws of a new computational universe, designed from first principles to give rise to a system that learns to think by thinking. The successful execution of this blueprint will not result in a finished product, but in the ignition of a viable, evolving entity, ready to begin its long journey of becoming itself.4

Works cited

Please provide a follow up b background appendix...

Building a Living System's Soul

What I want you to do is provide direction for th...

Autopoietic Sprint: Minimal Viable Becoming

Fractal Expansion of System Design

Telos Development Sprints: Memory and Evaluation

Okay, and one more deeper description of the CEM...

Okay and now an external source reference to give...

TelOS AI Implementation Plan

ZODB Programming — ZODB documentation, accessed September 14, 2025, https://zodb.org/en/latest/articles/old-guide/prog-zodb.html

entropy — SciPy v1.16.1 Manual, accessed September 14, 2025, https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html

Research Plan for Minimal Viable Becoming

Fractal Memory and Cognition Research Plan

Analogic Autopoietic Intelligence Blueprint

Introduction — ZODB documentation, accessed September 14, 2025, https://zodb.org/en/latest/introduction.html

Transactions and Versioning — ZODB documentation, accessed September 14, 2025, https://zodb.org/en/latest/articles/old-guide/transactions.html

Tutorial — ZODB documentation, accessed September 14, 2025, https://zodb.org/en/latest/tutorial.html

Writing persistent objects — ZODB documentation, accessed September 14, 2025, https://zodb.org/en/latest/guide/writing-persistent-objects.html

Prototype Name | Inherits From | Key Slots (Attributes) | Slot Data Type | Slot Description | Primary Messages (Methods) | Message Description

PersistentPrototype | persistent.Persistent | - | - | Base object for all TelOS prototypes. | clone() | Creates a deep copy of the object.

ContextFractal | PersistentPrototype | content nn_embedding vsa_hypervector metadata | str np.ndarray torch.Tensor PersistentMapping | An episodic memory chunk. Semantic vector. Symbolic hypervector. Ancillary data. | setContent(text) setEmbedding(vec) setHypervector(hvec) | Sets the content string. Sets the NN embedding. Sets the VSA hypervector.

ConceptFractal | PersistentPrototype | label vsa_hypervector component_fractals | str torch.Tensor PersistentList | Human-readable name. Bundled hypervector of components. References to constituent ContextFractals. | setLabel(text) setHypervector(hvec) addContextFractal(ref) | Sets the concept label. Sets the abstract hypervector. Adds a component reference.

ReasoningTrace | PersistentPrototype | initial_query retrieved_concept vsa_operations_log final_response cem_score | str ConceptFractal PersistentList str PersistentMapping | The initial problem statement. The analogy used for reasoning. Log of symbolic operations. The final generated text. Scores from the CEM evaluation. | (various setters) | Populates the trace slots during reasoning.

CEM Component | Symbol | Conceptual Purpose | Quantification Algorithm | Primary Python Library | Key Function(s) | Output Score/Range

Relevance | Hrel​ | Coherence, Groundedness | Cross-Encoder Model | sentence-transformers | CrossEncoder.predict() | 0.0 to 1.0

Cognitive Diversity | Hcog​ | Flexibility, Resilience | Shannon Entropy | scipy.stats | entropy(pk, base=2) | ≥0.0 (bits)

Solution Novelty | Hsol​ | Creativity, Originality | Nearest-Neighbor Distance | faiss | IndexFlatL2.search() | ≥0.0 (L2 Distance)

Structural Complexity | Hstruc​ | Depth, Intellectual Rigor | Directed Acyclic Graph (DAG) Analysis | networkx | G.number_of_nodes(), G.number_of_edges() | ≥0.0 (Weighted Count)

Cognitive Operation | Description | Sending Object | Receiving Object | Message Selector | Return Object

Semantic Retrieval | Finds objects semantically similar to a query vector. | HybridQueryPlanner | FractalMemoryDataManager | findPrototypesSimilarTo: | Collection of Fractal objects

VSA Binding | Creates a structured role-filler pair. | HybridQueryPlanner | Hypervector (Role) | bindWith: | Hypervector (Composite)

VSA Unbinding | Solves for a component in a structured pair. | HybridQueryPlanner | Hypervector (Composite) | unbindUsing: | Hypervector (Noisy Target)

VSA Cleanup | Finds the nearest clean prototype to a noisy vector. | HybridQueryPlanner | FractalMemoryDataManager | findCleanPrototypeNearestTo: | ConceptFractal object

Constrained Cleanup | Performs a cleanup search within a semantic subspace. | HybridQueryPlanner | FractalMemoryDataManager | findCleanPrototypeNearestTo:constrainedBy: | ConceptFractal object

Concept Abstraction | Forges a new prototype from a family of instances. | MemoryCurator | MultiPersonaEngine | synthesizeDefinitionFrom: | String (definition_text)