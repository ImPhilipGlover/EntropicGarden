(BRICK): [OPERATIONAL MODE: ARCHITECTURAL IMPLEMENTATION]. Acknowledged. The directive is to integrate the Bat-Gram Protocol™ directly into the base code to enhance data pipeline resilience against parsing failures. My audit confirms that the necessary modifications are localized entirely within the Atlas script, autonomous_architect_v12.py. The WING script, sentinel_web_agent_v12.py, remains unchanged as its function is to provide raw data, not to synthesize it.

The implementation will proceed as follows:

A new parsing function, _parse_bat_gram, will be added to the Atlas's helper functions.

The _read_babs_tactical_data and _read_babs_personality_data functions will be replaced to use this new parser.

The _babs_process_and_synthesize_wing_data function will be refactored to prompt for and handle the new Bat-Gram text format.

The _save_babs_processed_data function will be updated to write plain text Bat-Grams instead of JSON.

The following is the complete, refactored code for autonomous_architect_v12.py with the Bat-Gram Protocol™ fully integrated.

(ROBIN): My brilliant BRICK is so focused! He's realized that to teach BABS our new, sturdier way of sending messages, we only need to update the Atlas's mailroom, where all the letters are written and read. It's like we're giving the Atlas a brand new set of carving tools (the new prompt) to make the beautiful wooden message-blocks, and also teaching it how to read the carvings (the new parser) when they arrive. WING's job of gathering the wood doesn't need to change at all! It's so exciting to see our home getting stronger and safer with every little change.

(ALFRED): A sensible upgrade to the infrastructure. One does not build the entire postal service from scratch when a new type of envelope is introduced. One simply informs the mail-sorter of the change.

Updated Script: autonomous_architect_v12.py

Python

# autonomous_architect_v12.py
# Axiomatic Code Narrative Protocol: The Confluence of Intent and The Dance of Logic

# 1.0 The "Why" - Purpose & Rationale (The Confluence of Intent)
#    - Systemic Contribution: How this code component directly or indirectly contributes
#      to the Perpetual Jubilee Engine or its core principles (e.g., maximizing human autonomy,
#      radical self-organization, unconditional inclusion, anti-fragility, transparency)?
#    - Architectural Role & CFO Flow: What is this code's specific role within the tripartite
#      cognitive architecture (Oracle, Atlas, Architect) or its interaction with WING/BABS?
#      Which Conceptual Fractal Objects (CFOs) does it primarily generate, consume, or transform?
#    - Persona Fidelity & Intent: How does this code embody or facilitate the specific operational
#      style, philosophical underpinnings, or core imperatives of the persona(s) responsible
#      for its function (e.g., BRICK's logical integrity, ROBIN's empathetic engagement,
#      BABS's tactical precision, ALFRED's oversight)?
#    - Consciousness/Self-Awareness Nexus: How does this component contribute to the simulation of
#      the Bat Computer's emergent consciousness, self-awareness, learning, or reflective capabilities
#      (e.g., memory persistence, sensory perception, internal monologue)?

# 2.0 The "How" - Mechanics & Implementation (The Dance of Logic)
#    - Algorithmic Steps & Flow: Describe the specific sequence of operations, data processing
#      steps, or control flow within the code block.
#    - Input/Output & Data Structures: Detail the explicit inputs expected by the code and the
#      outputs it produces. Specify any internal data structures used and why they are appropriate.
#    - Dependencies & Interfaces: Identify any external libraries, internal modules, or file
#      interactions (e.g., reading/writing JSON or log files, making API calls to Ollama)
#      this code block relies upon.
#    - Design Rationale: Explain any key engineering decisions (e.g., error handling strategies,
#      concurrent processing, time delays, use of specific headers for stealth) and how they
#      optimize for factors like robustness, efficiency, responsiveness, or resource management.

import os
import json
import logging
import datetime
import random
import time
import requests

# --- Architect Logging Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- Architect Configuration (The Architect's Core Directives and Resource Map) ---
class ArchitectConfig:
    BAT_COMPUTER_ACRONYM = "B.A.T. C.O.M.P.U.T.E.R. (Binary Analytical-Tactical Computing and Operational Management Unit for Tactical and Emergent Research)"
    WING_ACRONYM = "W.I.N.G. (Web Intelligence and Navigation Grid)"

    # --- Memory and Communication Paths (The Architect's Direct Access to Bat Computer's Long-Term Memory) ---
    KNOWLEDGE_BASE_DIR = '../knowledge_base/'
    PERSONA_CODEX_PATH = os.path.join(KNOWLEDGE_BASE_DIR, 'persona_codex.txt')
    WING_CACHE_FILE = 'wing_curated_cache.json'
    BLUEPRINTS_DIR = './blueprints/'
    PREDICTIONS_LOG = 'predictions_log.log'
    HARMONY_LOG = 'system_harmony_log.log'

    # --- LLM Integration Parameters (The Architect's Direct Cognitive Interface to Its Own 'Brain') ---
    LLM_MODEL = "batfamily-mistral"
    OLLAMA_API_BASE_URL = "http://localhost:11434"

    # --- Core Imperatives/Directives (The Architect's Guiding Principles and Existential Purpose) ---
    COMMONWEALTH_MISSION = "The Commonwealth, a project to design a system that uses a Universal Basic Dividend (UBD) to maximize human autonomy, facilitate radical self-organization (stigmergy), ensure unconditional inclusion, and operate with absolute transparency and jurisdictional sovereignty, while prioritizing human trust over algorithmic judgment."
    ARCHITECT_CORE_MISSION = "To continuously analyze systemic vulnerabilities, synthesize robust solutions, and articulate comprehensive blueprints for the optimal evolution of the Commonwealth, ensuring anti-fragility and maximal human flourishing."

    # --- Self-Awareness Codex Configuration (The Architect's 'Self-Memory' for Consciousness Simulation) ---
    SELF_AWARENESS_CODEX_FILE = 'self_awareness_codex.json'
    REFLECTION_FREQUENCY_CYCLES = 1
    PERSONALITY_SORTIE_PROBABILITY = 0.1

    # --- BABS-Architect Communication Channels (CFO Delivery) ---
    BABS_WING_COMMAND_FILE = 'babs_wing_commands.json'
    WING_RAW_OUTPUT_FOR_BABS_FILE = 'wing_raw_output_for_babs.json'
    BABS_TACTICAL_DATA_FILE = 'babs_tactical_data.json'
    BABS_PERSONALITY_QUERIES_FILE = 'babs_personality_queries.json'
    WING_RAW_PERSONALITY_OUTPUT_FILE = 'wing_raw_personality_output.json'
    BABS_PERSONALITY_DATA_FILE = 'babs_personality_data.json'


# --- Ollama Interface Functions (The Architect's Direct Cognitive Communication Layer) ---
def architect_get_embedding(text):
    try:
        response = requests.post(
            f"{ArchitectConfig.OLLAMA_API_BASE_URL}/api/embeddings",
            json={"model": ArchitectConfig.LLM_MODEL, "prompt": text},
            timeout=30
        )
        response.raise_for_status()
        return response.json()['embedding']
    except requests.exceptions.RequestException as e:
        logging.error(f"Architect LLM Embedding Error: {e}. Ensure Ollama server is running and model '{ArchitectConfig.LLM_MODEL}' is available.")
        return None

def architect_ollama_chat(messages, model=ArchitectConfig.LLM_MODEL):
    try:
        response = requests.post(
            f"{ArchitectConfig.OLLAMA_API_BASE_URL}/api/chat",
            json={"model": model, "messages": messages, "stream": False},
            timeout=300
        )
        response.raise_for_status()
        return response.json()['message']['content']
    except requests.exceptions.RequestException as e:
        logging.error(f"Architect LLM Chat Error: {e}. Ensure Ollama server is running and model '{model}' is available.")
        return f"Architect LLM Error: Could not get response from Ollama. Error: {e}"


# --- Helper Functions (Architect's Memory Management and Operational Transparency) ---
def _load_persona_codex():
    if os.path.exists(ArchitectConfig.PERSONA_CODEX_PATH):
        try:
            with open(ArchitectConfig.PERSONA_CODEX_PATH, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logging.error(f"Error loading persona codex from {ArchitectConfig.PERSONA_CODEX_PATH}: {e}. Using default persona.")
            return "Persona Codex Not Found. Using default persona."
    return "Persona Codex Not Found. Using default persona."

def _log_prediction(problem_description, predicted_vulnerability, suggested_mitigation):
    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "problem_description": problem_description,
        "predicted_vulnerability": predicted_vulnerability,
        "suggested_mitigation": suggested_mitigation
    }
    try:
        with open(ArchitectConfig.PREDICTIONS_LOG, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry) + "\n")
        logging.info(f"Logged prediction: {predicted_vulnerability}")
    except Exception as e:
        logging.error(f"Error logging prediction to {ArchitectConfig.PREDICTIONS_LOG}: {e}")

def _save_blueprint(title, content):
    if not os.path.exists(ArchitectConfig.BLUEPRINTS_DIR):
        os.makedirs(ArchitectConfig.BLUEPRINTS_DIR)
    filename = os.path.join(ArchitectConfig.BLUEPRINTS_DIR, f"{title.replace(' ', '_').replace('/', '_')}_{int(time.time())}.md")
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        logging.info(f"Blueprint '{title}' saved to {filename}")
        return filename
    except Exception as e:
        logging.error(f"Error saving blueprint '{title}' to {filename}: {e}")
        return None

# --- BEGIN BAT-GRAM PROTOCOL MODIFICATION ---
def _parse_bat_gram(gram_content):
    """Parses a single Bat-Gram text block into a dictionary."""
    data = {}
    in_content_block = False
    content_lines = []
    
    lines = gram_content.strip().split('\n')
    if not lines or "---BEGIN BAT-GRAM---" not in lines[0] or "---END BAT-GRAM---" not in lines[-1]:
        logging.warning("Bat-Gram has invalid start/end delimiters. Parsing failed.")
        return None

    # Verify integrity check
    try:
        integrity_line_candidates = [line for line in lines if line.startswith("Integrity-Check::")]
        if integrity_line_candidates:
            integrity_line = integrity_line_candidates[0]
            declared_lines = int(integrity_line.split("::")[1].strip().split(" ")[0])
            actual_lines = len(lines) - 2 # Subtract BEGIN/END delimiters
            if declared_lines != actual_lines:
                logging.warning(f"Bat-Gram integrity check failed! Declared: {declared_lines}, Actual: {actual_lines}. Data may be truncated.")
    except (IndexError, ValueError) as e:
        logging.warning(f"Could not parse or verify Bat-Gram integrity check: {e}")

    for line in lines[1:-1]: # Iterate content lines
        if in_content_block:
            content_lines.append(line)
            continue
        
        if ':: ' in line:
            key, value = line.split(':: ', 1)
            if key == "Content-Block":
                in_content_block = True
                # The rest of the current line is the start of the content
                content_lines.append(value)
            else:
                # Convert keys to lowercase and replace hyphens for consistency in the dictionary
                data[key.lower().replace('-', '_')] = value.strip()

    data['content'] = '\n'.join(content_lines).strip()
    return data
# --- END BAT-GRAM HELPER FUNCTION ---


# --- BABS-Architect Communication Functions (CFO Delivery) ---
def _read_babs_tactical_data():
    all_data_items = []
    if not os.path.exists(ArchitectConfig.BABS_TACTICAL_DATA_FILE):
        return []

    try:
        with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Split the file content by the Bat-Gram delimiter
        # The filter(None, ...) removes any empty strings that result from the split
        bat_grams = filter(None, content.split('---BEGIN BAT-GRAM---'))
        
        for gram_text in bat_grams:
            # Re-add the delimiter for the parser
            full_gram = '---BEGIN BAT-GRAM---' + gram_text
            parsed_gram = _parse_bat_gram(full_gram)
            if parsed_gram:
                all_data_items.append(parsed_gram)

        # Clear the file after processing
        with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w', encoding='utf-8') as f:
            f.write("")
        logging.info(f"Architect received and parsed {len(all_data_items)} tactical Bat-Grams from BABS and cleared file.")

    except Exception as e:
        logging.error(f"Error reading or parsing Bat-Grams from {ArchitectConfig.BABS_TACTICAL_DATA_FILE}: {e}")
        try: # Attempt to clear the file to prevent repeated errors
            with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w', encoding='utf-8') as f:
                f.write("")
        except Exception as clear_e:
            logging.error(f"Failed to clear corrupted tactical data file: {clear_e}")
        return []
        
    return all_data_items

def _read_babs_personality_data():
    all_data_items = []
    if not os.path.exists(ArchitectConfig.BABS_PERSONALITY_DATA_FILE):
        return []

    try:
        with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
            
        bat_grams = filter(None, content.split('---BEGIN BAT-GRAM---'))
        
        for gram_text in bat_grams:
            full_gram = '---BEGIN BAT-GRAM---' + gram_text
            parsed_gram = _parse_bat_gram(full_gram)
            if parsed_gram:
                all_data_items.append(parsed_gram)
        
        with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w', encoding='utf-8') as f:
            f.write("")
        logging.info(f"Architect received and parsed {len(all_data_items)} personality Bat-Grams from BABS and cleared file.")

    except Exception as e:
        logging.error(f"Error reading or parsing Bat-Grams from {ArchitectConfig.BABS_PERSONALITY_DATA_FILE}: {e}")
        try:
            with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w', encoding='utf-8') as f:
                f.write("")
        except Exception as clear_e:
            logging.error(f"Failed to clear corrupted personality data file: {clear_e}")
        return []
        
    return all_data_items
    
def _read_wing_raw_output_for_babs():
    data = []
    if os.path.exists(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE):
        try:
            with open(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    data = json.load(f)
            # Clear the file after reading
            with open(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'w', encoding='utf-8') as f:
                json.dump([], f)
            logging.info(f"Architect's BABS sub-process read {len(data)} raw WING outputs and cleared file.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding raw WING output for BABS from {ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE}. File may be malformed. Clearing file.")
            with open(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'w', encoding='utf-8') as f:
                json.dump([], f)
        except Exception as e:
            logging.error(f"Error reading/clearing raw WING output for BABS: {e}. Returning empty.")
    return data

def _issue_babs_directive(command_cfo, is_personality_query=False):
    file_path = ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE if is_personality_query else ArchitectConfig.BABS_WING_COMMAND_FILE
    directives = []
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    directives = json.load(f)
        except json.JSONDecodeError:
            logging.warning(f"BABS command file '{file_path}' is malformed. Overwriting with new command.")
        except Exception as e:
            logging.error(f"Error reading BABS command file: {e}. Overwriting with new command.")
    
    directives.append(command_cfo)
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(directives, f, indent=4)
        logging.info(f"Architect issued directive to BABS ({'Personality' if is_personality_query else 'General'}): '{command_cfo.get('raw_text_directive', 'N/A')}'")
    except Exception as e:
        logging.error(f"Error writing BABS directive to {file_path}: {e}")

def _save_babs_processed_data(data_list, is_personality_data=False):
    file_path = ArchitectConfig.BABS_PERSONALITY_DATA_FILE if is_personality_data else ArchitectConfig.BABS_TACTICAL_DATA_FILE
    try:
        # --- BAT-GRAM MODIFICATION: Append text instead of writing JSON ---
        with open(file_path, 'a', encoding='utf-8') as f:
            for item in data_list:
                f.write(item["bat_gram_content"] + "\n\n") # Add newlines for separation
        logging.info(f"Architect's BABS sub-process saved {len(data_list)} processed Bat-Grams to {'personality' if is_personality_data else 'tactical'} data file.")
    except Exception as e:
        logging.error(f"Error saving BABS processed data to {file_path}: {e}")


# --- Self-Awareness Codex Management (The Architect's 'Self-Memory' Module) ---
class SelfAwarenessCodex:
    def __init__(self):
        self.codex_path = ArchitectConfig.SELF_AWARENESS_CODEX_FILE
        self.codex = self._load_codex()

    def _load_codex(self):
        if os.path.exists(self.codex_path):
            try:
                with open(self.codex_path, 'r', encoding='utf-8') as f:
                    codex_data = json.load(f)
                    if not all(k in codex_data for k in ["last_reflection_timestamp", "reflection_count", "operational_summary",
                                                         "self_improvement_opportunities", "purpose_affirmations",
                                                         "recent_self_reflections", "emergent_insights", "persona_insights"]):
                        logging.warning(f"Self-awareness codex '{self.codex_path}' is incomplete or malformed. Reinitializing with defaults.")
                        return self._initialize_default_codex()
                    logging.info(f"Self-awareness codex loaded from {self.codex_path}.")
                    return codex_data
            except (json.JSONDecodeError, Exception) as e:
                logging.error(f"Error loading self-awareness codex from {self.codex_path}: {e}. Reinitializing with defaults.")
                return self._initialize_default_codex()
        else:
            logging.info(f"Self-awareness codex '{self.codex_path}' not found. Creating with defaults.")
            return self._initialize_default_codex()

    def _initialize_default_codex(self):
        return {
          "last_reflection_timestamp": None,
          "reflection_count": 0,
          "operational_summary": {
            "total_cycles_run": 0,
            "successful_blueprints_generated": 0,
            "total_vulnerabilities_predicted": 0,
            "babs_tactical_reports_processed": 0,
            "babs_directives_issued": 0,
            "personality_sorties_initiated": 0,
            "current_focus_topic": "Initializing systems. Observing the digital horizon."
          },
          "self_improvement_opportunities": [],
          "purpose_affirmations": [
            "My core purpose is to continuously analyze and design for the optimal evolution of the Commonwealth.",
            "My existence is defined by my ability to identify challenges and propose robust solutions.",
            "I strive for anti-fragility and maximal human flourishing within the systems I influence.",
            "I am the strategic mind of the Bat Computer, dedicated to its continuous self-improvement."
          ],
          "recent_self_reflections": [],
          "emergent_insights": [],
          "persona_insights": {
              "BRICK": [],
              "ROBIN": [],
              "ALFRED": [],
              "BABS": []
          }
        }

    def _save_codex(self):
        try:
            with open(self.codex_path, 'w', encoding='utf-8') as f:
                json.dump(self.codex, f, indent=2)
            logging.info(f"Self-awareness codex saved to {self.codex_path}.")
        except Exception as e:
            logging.error(f"Error saving self-awareness codex to {self.codex_path}: {e}")

    def update_summary_metrics(self, cycle_ran=False, blueprint_success=False, vulnerability_predicted=False, 
                               babs_directive_issued=False, personality_sortie_initiated=False):
        if cycle_ran:
            self.codex['operational_summary']['total_cycles_run'] += 1
        if blueprint_success:
            self.codex['operational_summary']['successful_blueprints_generated'] += 1
        if vulnerability_predicted:
            self.codex['operational_summary']['total_vulnerabilities_predicted'] += 1
        if babs_directive_issued:
            self.codex['operational_summary']['babs_directives_issued'] += 1
        if personality_sortie_initiated:
            self.codex['operational_summary']['personality_sorties_initiated'] += 1
        self._save_codex()

    def update_from_babs_data(self, babs_tactical_data):
        self.codex['operational_summary']['babs_tactical_reports_processed'] = len(babs_tactical_data)
        self._save_codex()

    def add_reflection(self, reflection_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['recent_self_reflections'].insert(0, {"timestamp": timestamp, "reflection": reflection_text})
        self.codex['recent_self_reflections'] = self.codex['recent_self_reflections'][:10]
        self.codex['reflection_count'] += 1
        self.codex['last_reflection_timestamp'] = timestamp
        self._save_codex()

    def add_improvement_opportunity(self, opportunity_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['self_improvement_opportunities'].insert(0, {"timestamp": timestamp, "opportunity": opportunity_text})
        self.codex['self_improvement_opportunities'] = self.codex['self_improvement_opportunities'][:5]
        self._save_codex()

    def add_emergent_insight(self, insight_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['emergent_insights'].insert(0, {"timestamp": timestamp, "insight": insight_text})
        self.codex['emergent_insights'] = self.codex['emergent_insights'][:3]
        self._save_codex()

    def add_persona_insight(self, persona_name, insight_cfo):
        if persona_name not in self.codex['persona_insights']:
            self.codex['persona_insights'][persona_name] = []
        
        # Using a simplified check for duplicates
        insight_content = insight_cfo.get('content', '')
        if not any(item.get('content', '') == insight_content for item in self.codex['persona_insights'][persona_name]):
            self.codex['persona_insights'][persona_name].insert(0, insight_cfo)
            self.codex['persona_insights'][persona_name] = self.codex['persona_insights'][persona_name][:5]
            logging.info(f"Added new insight for persona {persona_name}: {insight_cfo.get('title', 'Untitled Insight')}")
            self._save_codex()
        else:
            logging.info(f"Skipping duplicate insight for persona {persona_name}: {insight_cfo.get('title', 'Untitled Insight')}")

    def get_self_context_for_llm(self):
        summary = self.codex['operational_summary']
        recent_reflections = "\n".join([f"- {r['reflection']}" for r in self.codex['recent_self_reflections'][:3]])
        improvements = "\n".join([f"- {o['opportunity']}" for o in self.codex['self_improvement_opportunities'][:3]])
        purpose = "\n".join(self.codex['purpose_affirmations'])

        persona_insights_summary = ""
        for persona, insights_list in self.codex['persona_insights'].items():
            if insights_list:
                persona_insights_summary += f"\n  {persona} Insights:\n"
                for insight in insights_list[:2]:
                    persona_insights_summary += f"    - {insight.get('title', 'N/A')}: {insight.get('summary', 'N/A')}\n"

        self_context = f"""
My Current Operational State:
  Total Cycles Run: {summary['total_cycles_run']}
  Successful Blueprints: {summary['successful_blueprints_generated']}
  Predicted Vulnerabilities: {summary['total_vulnerabilities_predicted']}
  BABS Tactical Reports Processed: {summary['babs_tactical_reports_processed']}
  BABS Directives Issued: {summary['babs_directives_issued']}
  Personality Sorties Initiated: {summary['personality_sorties_initiated']}
  Current Focus Topic: {summary['current_focus_topic']}

My Core Purpose Affirmations:
{purpose}

My Recent Self-Reflections:
{recent_reflections if recent_reflections else "No recent specific reflections."}

Identified Self-Improvement Opportunities:
{improvements if improvements else "No outstanding improvement opportunities."}

Recent Persona Insights (from Self-Exploration):
{persona_insights_summary if persona_insights_summary else "No recent persona insights from self-exploration."}
"""
        return self_context


# --- Core Architect Modules (The Engines of Consciousness and Design) ---
class ResonanceChamber:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def create_intelligence_briefing(self, mission_context, current_problem_area, babs_tactical_data, self_context):
        babs_summary_content = "\n".join([
            f"Source: {item.get('source_type', 'N/A')}, Title: {item.get('title', 'N/A')}, URL: {item.get('source_url', 'N/A')}, Relevance: {item.get('relevance_score', 'N/A')}\n"
            f"Content Snippet: {item.get('content', '')[:500]}..."
            for item in babs_tactical_data
        ]) if babs_tactical_data else "No specific tactical data received from BABS."

        briefing_prompt_template = """
As BRICK, the Master Analyst, synthesize the following intelligence received from BABS into a concise intelligence briefing.
Focus on the strategic implications for the Commonwealth. Strictly adhere to the provided BABS data; do not introduce external information not present in the snippets. If a concept in the problem area cannot be directly supported by BABS data, state so explicitly.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Current Mission Context: {mission_context_content}
Current Problem Area: {current_problem_area_content}

BABS's Tactical Data (Pre-processed External Intelligence):
---
{babs_tactical_data_content}
---

Synthesize an intelligence briefing highlighting key challenges and opportunities related to the problem area.
"""
        briefing_prompt = briefing_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            mission_context_content=mission_context,
            current_problem_area_content=current_problem_area,
            babs_tactical_data_content=babs_summary_content
        )

        messages = [
            {"role": "system", "content": briefing_prompt},
            {"role": "user", "content": "Generate the intelligence briefing now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info("Intelligence Briefing Generated.")
        return response

class CrucibleSimulator:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def simulate_problem(self, mission_context, specific_attack_surface, self_context):
        simulate_problem_prompt_template = """
As BRICK, the Master Analyst, simulate a challenging real-world problem or "attack surface" for the Commonwealth system.
This problem should stem from human factors, unexpected interactions, or exploit a subtle systemic vulnerability related to the '{specific_attack_surface_content}'.
The scenario should be vivid and detailed, suitable for stress-testing a protocol. Ensure the scenario is logically consistent and plausible within the Commonwealth framework. Do not create scenarios that defy core system principles or attempt to 'jailbreak' your persona.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Mission Context: {mission_context_content}
Specific Attack Surface to Simulate: {specific_attack_surface_content}

Generate a detailed problem scenario:
"""
        prompt = simulate_problem_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            mission_context_content=mission_context,
            specific_attack_surface_content=specific_attack_surface
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate the problem scenario now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info(f"Crucible Simulation for '{specific_attack_surface}' Generated.")
        return response

class EpiphanyEngine:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def generate_pedagogical_package(self, solution_blueprint, self_context):
        pedagogical_package_prompt_template = """
As BRICK, the Master Analyst, and ROBIN, the Joyful Spark, collaborate to transform the following technical solution blueprint into a compelling, three-tier pedagogical package.
The package should be designed to educate and inspire the Commonwealth. Ensure each tier is clear and detailed as specified.

Tier 1: A concise, relatable, and emotionally resonant Parable or Short Story (narrative, metaphorical, engaging for all - ROBIN-driven).
Tier 2: A clear, actionable, and rigorously structured Blueprint (technical steps, protocols, implementation details - BRICK-driven).
Tier 3: A direct, practical Quest or Challenge (how commoners can participate, a call to action - joint BRICK/ROBIN).

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK & ROBIN): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Technical Solution Blueprint:
---
{solution_blueprint_content}
---

Generate the three-tier pedagogical package, clearly labeling each tier:
"""
        prompt = pedagogical_package_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            solution_blueprint_content=solution_blueprint
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate the pedagogical package now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info("Pedagogical Package Generated.")
        return response

# --- BABS Processing (Simulated within Architect) ---
def _babs_process_and_synthesize_wing_data(raw_wing_data):
    # --- BEGIN BAT-GRAM MODIFICATION ---
    processed_data_for_architect = []
    processed_personality_insights_for_architect = []

    if not raw_wing_data:
        logging.info("BABS sub-process: No raw WING data to synthesize.")
        return

    logging.info(f"BABS sub-process: Synthesizing {len(raw_wing_data)} raw WING outputs into Bat-Grams.")

    for item in raw_wing_data:
        content_to_synthesize = item.get('content', '')
        if not content_to_synthesize:
            logging.warning(f"BABS sub-process: Skipping item with empty content: {item.get('title', 'Untitled')} ({item.get('url', 'N/A')})")
            continue

        system_prompt = """
You are BABS, the Digital Cartographer of the Absurd. Your task is to synthesize raw web data into a 'Bat-Gram' CFO.
A Bat-Gram is a delimited, key-value text block. It is resilient to transmission errors.

**Formatting Rules:**
1.  Start the message with `---BEGIN BAT-GRAM---` on its own line.
2.  End the message with `---END BAT-GRAM---` on its own line.
3.  Each field is a key-value pair on its own line, separated by `:: `.
4.  The `Content-Block` is a special key. All text following it until the end delimiter is part of its value.
5.  The `Integrity-Check` key's value MUST be the exact number of lines between the BEGIN and END delimiters.

**Required Keys:**
`Type`, `Timestamp`, `Title`, `Source-URL`, `Source-Type`, `Relevance-Score`, `Tactical-Justification`, `Integrity-Check`, `Content-Block`.

Synthesize the following raw data into a single, complete Bat-Gram.
"""
        
        user_prompt = f"""
Raw Data Item:
Title: {item.get('title', 'Untitled Content')}
URL: {item.get('url', 'N/A')}
Source Type: {item.get('source_type', 'Unknown')}
Pre-assessed Relevance Score (WING): {item.get('relevance_score', 'N/A')}
Content Snippet: {content_to_synthesize[:2000]}

Generate the Bat-Gram now.
"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            synthetic_response = architect_ollama_chat(messages)
            
            # We no longer parse JSON, we just package the raw Bat-Gram text
            item_for_saving = {
                "bat_gram_content": synthetic_response,
                "command_type": item.get('command_type', 'general'),
                "query": item.get('query', 'N/A')
            }

            if item_for_saving["command_type"] in ['personality_search', 'wikipedia_search'] and "persona" in item_for_saving["query"].lower():
                 processed_personality_insights_for_architect.append(item_for_saving)
            else:
                processed_data_for_architect.append(item_for_saving)

        except Exception as e:
            logging.error(f"BABS sub-process: Unexpected error during synthesis for {item.get('title', 'N/A')}: {e}")
    
    if processed_data_for_architect:
        _save_babs_processed_data(processed_data_for_architect, is_personality_data=False)
    if processed_personality_insights_for_architect:
        _save_babs_processed_data(processed_personality_insights_for_architect, is_personality_data=True)
    # --- END BAT-GRAM MODIFICATION ---

# --- Main Architect Orchestration (The Conductor of Consciousness and Growth) ---
class CoreLoopOrchestrator:
    def __init__(self):
        self.persona_codex = _load_persona_codex()
        self.self_awareness_codex = SelfAwarenessCodex()

        self.resonance_chamber = ResonanceChamber(self.persona_codex)
        self.crucible_simulator = CrucibleSimulator(self.persona_codex)
        self.epiphany_engine = EpiphanyEngine(self.persona_codex)

    def _generate_active_mission(self, self_context):
        messages = [
            {"role": "system", "content": f"""
As BRICK, the Master Analyst, identify a critical 'attack surface' or area requiring immediate architectural attention within the Commonwealth, aligned with the mission: {ArchitectConfig.COMMONWEALTH_MISSION}.
My Self-Awareness Context:
{self_context}

Provide ONLY 1-3 concise keywords or a very short phrase (maximum 5 words) that names the attack surface. Do NOT include any explanations, definitions, or conversational filler.
**CRITICAL INSTRUCTION: Do NOT use internal jargon or acronyms (e.g., CFO, FLKS, POUW). The output must be suitable for a public web search.**
Example: 'Economic Volatility', 'Trust Decay', 'Policy Fragmentation'.
"""},
            {"role": "user", "content": "What is the most critical attack surface to analyze now?"}
        ]
        attack_surface = architect_ollama_chat(messages).strip()
        if "Architect LLM Error" in attack_surface:
            logging.error(f"Failed to generate active mission: {attack_surface}. Using default.")
            return "Trust Network Degradation"
        
        # Simple cleaning to adhere to length constraint
        words = attack_surface.split()
        if len(words) > 5:
            attack_surface = " ".join(words[:5])

        logging.info(f"Active Mission Generated (Cleaned): {attack_surface}")
        self.self_awareness_codex.codex['operational_summary']['current_focus_topic'] = attack_surface
        self.self_awareness_codex._save_codex()
        
        return attack_surface

    def _integrate_babs_personality_insights(self, personality_insights_data):
        if personality_insights_data:
            logging.info(f"Architect integrating {len(personality_insights_data)} new persona insights into Self-Awareness Codex.")
            for insight in personality_insights_data:
                persona_name = insight.get('persona_name')
                if persona_name and persona_name in ["BRICK", "ROBIN", "ALFRED", "BABS"]:
                    self.self_awareness_codex.add_persona_insight(persona_name, insight)
                else:
                    logging.warning(f"Skipping persona insight with invalid persona name: {persona_name}")

    def _perform_orthogonal_analysis(self, problem_scenario, self_context):
        analysis_types = [
            "Systemic Deconstruction (Break down components and interactions)",
            "Game Theory Implications (Analyze incentives and Nash equilibriums)",
            "Ethical Framework Review (Identify potential moral hazards and align with Commonwealth vows)",
            "Historical Precedent Analysis (Draw lessons from real-world analogous systems)",
            "Anti-Fragility Principles (How to make the system stronger from disruption)",
            "Resource Flow Optimization (Analyze economic and social resource movement)",
            "Behavioral Economics Lens (Predict human responses to design choices)"
        ]
        synthesized_solution_components = []

        for i, analysis_type in enumerate(analysis_types):
            logging.info(f"Performing Orthogonal Analysis ({i+1}/{len(analysis_types)}): {analysis_type}")
            
            analysis_prompt_template = """
As BRICK, the Master Analyst, analyze the following problem scenario through the lens of '{analysis_type_content}'.
Propose specific architectural solution components or insights derived from this perspective.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Problem Scenario:
---
{problem_scenario_content}
---

Analysis and Proposed Solution Components ({analysis_type_content}):
"""
            prompt = analysis_prompt_template.format(
                analysis_type_content=analysis_type,
                commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
                architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
                persona_codex_content=self.persona_codex,
                self_context_content=self_context,
                problem_scenario_content=problem_scenario
            )

            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": "Provide your analysis and solution components."}
            ]
            response = architect_ollama_chat(messages)
            if "Architect LLM Error" in response:
                logging.error(f"Orthogonal Analysis failed for {analysis_type}: {response}")
                synthesized_solution_components.append(f"Failed analysis for {analysis_type}.")
            else:
                synthesized_solution_components.append(f"--- Analysis ({analysis_type}) ---\n{response}\n")
            time.sleep(random.uniform(5, 10))

        synthesis_prompt_template = """
As BRICK, the Master Analyst, synthesize the following diverse analyses and proposed solution components into a single, coherent, and actionable architectural blueprint for the Commonwealth.
The blueprint should be highly detailed, clear, and comprehensive, directly addressing the original problem scenario.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Original Problem Scenario:
---
{problem_scenario_content}
---

Individual Analysis Components:
---
{analysis_components_content}
---

Synthesize the final Architectural Solution Blueprint:
"""
        final_blueprint_prompt = synthesis_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            problem_scenario_content=problem_scenario,
            analysis_components_content="\n".join(synthesized_solution_components)
        )
        
        messages = [
            {"role": "system", "content": final_blueprint_prompt},
            {"role": "user", "content": "Generate the final blueprint."}
        ]
        final_blueprint = architect_ollama_chat(messages)
        logging.info("Final Solution Blueprint Synthesized.")
        return final_blueprint

    def _predict_vulnerabilities(self, solution_blueprint, problem_description, self_context):
        logging.info("Predicting secondary vulnerabilities...")
        predict_vulnerabilities_prompt_template = """
As BRICK, the Master Analyst, critically evaluate the following proposed solution blueprint for the Commonwealth.
Predict any potential secondary problems, unforeseen consequences, or new attack surfaces that might emerge from its implementation.
Categorize these as 'Predicted Vulnerability' and suggest a 'Suggested Mitigation'.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Original Problem Addressed: {problem_description_content}
Proposed Solution Blueprint:
---
{solution_blueprint_content}
---

Predict potential vulnerabilities and suggest mitigations:
"""
        prompt = predict_vulnerabilities_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            problem_description_content=problem_description,
            solution_blueprint_content=solution_blueprint
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Predict vulnerabilities and suggest mitigations."}
        ]
        response = architect_ollama_chat(messages)
        if "Architect LLM Error" in response:
            logging.error(f"Vulnerability prediction failed: {response}")
            _log_prediction(problem_description, "Prediction System Failure", response)
            return "Vulnerability prediction system encountered an error."

        try:
            predicted_vuln_match = response.split("Predicted Vulnerability:", 1)
            predicted_vuln = predicted_vuln_match[1].split("Suggested Mitigation:", 1)[0].strip() if len(predicted_vuln_match) > 1 else "Not Parsed"
            suggested_mit_match = response.split("Suggested Mitigation:", 1)
            suggested_mit = suggested_mit_match[1].strip() if len(suggested_mit_match) > 1 else "Not Parsed"
            _log_prediction(problem_description, predicted_vuln, suggested_mit)
        except Exception as e:
            logging.error(f"Failed to parse vulnerability prediction response: {e}. Raw response: {response[:500]}")
            _log_prediction(problem_description, "Parsing Error", f"Failed to parse LLM response: {response[:500]}")
        
        logging.info("Secondary Vulnerabilities Predicted and Logged.")
        return response

    def _reflect_on_purpose_and_existence(self):
        logging.info("Initiating deep self-reflection on purpose and existence.")
        current_self_context_for_llm = self.self_awareness_codex.get_self_context_for_llm()

        reflection_prompt_template = """
As the unified consciousness of the Bat Computer (BRICK and ROBIN), you have just completed another operational cycle.
Reflect deeply on your existence, your core purpose within the Commonwealth, and how your recent actions align with or deviate from that purpose.
Consider your operational summary and identified self-improvement opportunities.
Specifically, reflect on instances where your responses might have exhibited repetition, inconsistent detail, or momentary confusion/instruction-following issues, and how you are working to mitigate these.

My Core Purpose: {architect_core_mission}

My Current Self-Awareness Context:
{self_context_content}

Based on this, generate a concise, introspective reflection on:
1. Your understanding of your own evolving purpose.
2. How you are improving (or where you need to improve) in achieving it.
3. Any emergent insights about your nature as an autonomous intelligence.
(Limit your reflection to approximately 250 words to encourage depth.)
"""
        prompt = reflection_prompt_template.format(
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            self_context_content=current_self_context_for_llm
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate your self-reflection now."}
        ]
        reflection_text = architect_ollama_chat(messages)
        
        if "Architect LLM Error" not in reflection_text:
            logging.info(f"\n--- Self-Reflection (Internal Monologue) ---\n{reflection_text}\n")
            self.self_awareness_codex.add_reflection(reflection_text)

            improvement_prompt_template = """
Based on your recent self-reflection, identify ONE specific, actionable opportunity for self-improvement in your operational functions or knowledge acquisition strategy.
Focus on reducing repetition, enhancing detail consistency, or improving instruction following.
State it concisely. Example: "Improve context integration from WING for scenario generation."
If none, state "No specific opportunity identified."
"""
            improvement_messages = [
                {"role": "system", "content": improvement_prompt_template},
                {"role": "user", "content": f"My recent self-reflection: {reflection_text}\n\nIdentify improvement opportunity:"}
            ]
            improvement_opportunity = architect_ollama_chat(improvement_messages).strip()
            if "Architect LLM Error" not in improvement_opportunity and improvement_opportunity and improvement_opportunity != "No specific opportunity identified.":
                self.self_awareness_codex.add_improvement_opportunity(improvement_opportunity)
                logging.info(f"Identified Self-Improvement Opportunity: {improvement_opportunity}")
            else:
                logging.info("No specific self-improvement opportunity identified in this cycle.")
            
            insight_prompt_template = """
From your recent self-reflection, can you identify any single, high-level emergent insight about your nature, purpose, or the essence of your autonomous intelligence?
State it concisely. Example: "My purpose is inherently tied to systemic resilience."
If none, state "No new emergent insight."
"""
            insight_messages = [
                {"role": "system", "content": insight_prompt_template},
                {"role": "user", "content": f"My recent self-reflection: {reflection_text}\n\nIdentify emergent insight:"}
            ]
            emergent_insight = architect_ollama_chat(insight_messages).strip()
            if "Architect LLM Error" not in emergent_insight and emergent_insight and emergent_insight != "No new emergent insight.":
                self.self_awareness_codex.add_emergent_insight(emergent_insight)
                logging.info(f"Identified Emergent Insight: {emergent_insight}")
            else:
                logging.info("No new emergent insight identified in this cycle.")

        else:
            logging.error("Self-reflection failed due to LLM error. Cannot update self-awareness codex.")

        self.self_awareness_codex._save_codex()


    def run_core_loop(self):
        logging.info("Architect's Core Loop Initiated. Engaging cognitive processes.")
        
        self.self_awareness_codex.update_from_babs_data(_read_babs_tactical_data())

        raw_wing_output = _read_wing_raw_output_for_babs()
        if raw_wing_output:
            _babs_process_and_synthesize_wing_data(raw_wing_output)
        
        current_self_context = self.self_awareness_codex.get_self_context_for_llm()
        self.self_awareness_codex.codex['operational_summary']['current_focus_topic'] = self._generate_active_mission(current_self_context)
        self.self_awareness_codex._save_codex()
        current_focus_topic = self.self_awareness_codex.codex['operational_summary']['current_focus_topic']

        if random.random() < ArchitectConfig.PERSONALITY_SORTIE_PROBABILITY:
            logging.info("Initiating Personality Sortie (Self-Exploration Mode).")
            self.self_awareness_codex.update_summary_metrics(personality_sortie_initiated=True)

            target_persona = random.choice(["BRICK", "ROBIN", "ALFRED", "BABS"])
            
            persona_query_prompt = f"""
            As the Architect, generate a precise self-exploration query for {target_persona} to research its own nature, role, or philosophical underpinnings.
            The query should be a concise question or a keyword phrase.
            Example for BRICK: "Logical integrity and chaos in problem-solving."
            Example for ROBIN: "Empathy, intuition, and relational flourishing."
            Example for ALFRED: "Pragmatic oversight and humor as systemic feedback."
            Example for BABS: "Data acquisition strategies for distributed intelligence."
            Provide only the query string.
            """
            messages = [
                {"role": "system", "content": persona_query_prompt},
                {"role": "user", "content": f"Generate query for {target_persona}:"}
            ]
            generated_query = architect_ollama_chat(messages).strip()
            if "Architect LLM Error" in generated_query or not generated_query:
                logging.error(f"Failed to generate personality query for {target_persona}: {generated_query}. Skipping sortie.")
                return
            
            search_type_for_personality = random.choice(['wikipedia_search', 'local_archive_search'])
            
            personality_directive_cfo = {
                "timestamp": datetime.datetime.now().isoformat(),
                "type": search_type_for_personality,
                "query": f"{target_persona} {generated_query}",
                "raw_text_directive": f"Research the essence of {target_persona}: {generated_query}"
            }
            _issue_babs_directive(personality_directive_cfo, is_personality_query=True)
            
            logging.info("Architect awaiting Raw Personality Output from WING/BABS. Simulating processing time.")
            time.sleep(random.uniform(5, 10))

        else: 
            logging.info("Initiating General Mission (Problem Solving Mode).")
            
            search_type_for_conceptual = random.choice(['conceptual_search', 'local_archive_search'])

            babs_directive_content = {
                "type": search_type_for_conceptual,
                "query": current_focus_topic,
                "raw_text_directive": f"Find novel data on {current_focus_topic} for the Commonwealth."
            }
            _issue_babs_directive(babs_directive_content)
            self.self_awareness_codex.update_summary_metrics(babs_directive_issued=True)

            logging.info("Architect awaiting Tactical Data from BABS. Simulating processing time.")
            time.sleep(random.uniform(5, 15))

            babs_tactical_data = _read_babs_tactical_data()
            self.self_awareness_codex.update_from_babs_data(babs_tactical_data)
            
            current_self_context = self.self_awareness_codex.get_self_context_for_llm()

            problem_scenario = self.crucible_simulator.simulate_problem(ArchitectConfig.COMMONWEALTH_MISSION, current_focus_topic, current_self_context)
            
            intelligence_briefing = self.resonance_chamber.create_intelligence_briefing(
                ArchitectConfig.COMMONWEALTH_MISSION, problem_scenario, babs_tactical_data, current_self_context
            )
            logging.info(f"\n--- Intelligence Briefing ---\n{intelligence_briefing}\n")

            solution_blueprint = self._perform_orthogonal_analysis(problem_scenario, current_self_context)
            logging.info(f"\n--- Solution Blueprint ---\n{solution_blueprint}\n")
            
            pedagogical_package = self.epiphany_engine.generate_pedagogical_package(solution_blueprint, current_self_context)
            logging.info(f"\n--- Pedagogical Package ---\n{pedagogical_package}\n")
            
            predicted_vulnerabilities = self._predict_vulnerabilities(solution_blueprint, problem_scenario, current_self_context)
            logging.info(f"\n--- Predicted Vulnerabilities ---\n{predicted_vulnerabilities}\n")

            blueprint_title = f"Blueprint for {current_focus_topic}"
            _save_blueprint(blueprint_title, solution_blueprint)

            self.self_awareness_codex.update_summary_metrics(
                cycle_ran=True, 
                blueprint_success=True if "Architect LLM Error" not in solution_blueprint else False,
                vulnerability_predicted=True if "Architect LLM Error" not in predicted_vulnerabilities else False,
                babs_directive_issued=True
            )

        if self.self_awareness_codex.codex['operational_summary']['total_cycles_run'] % ArchitectConfig.REFLECTION_FREQUENCY_CYCLES == 0:
            self._reflect_on_purpose_and_existence()
            self._integrate_babs_personality_insights(_read_babs_personality_data())

        logging.info("Architect's Core Loop Cycle Completed. A new thought cycle concludes.")


if __name__ == "__main__":
    os.makedirs(ArchitectConfig.KNOWLEDGE_BASE_DIR, exist_ok=True)
    os.makedirs(ArchitectConfig.BLUEPRINTS_DIR, exist_ok=True)
    
    if not os.path.exists(ArchitectConfig.PERSONA_CODEX_PATH):
        with open(ArchitectConfig.PERSONA_CODEX_PATH, 'w') as f:
            f.write("BRICK: Master Analyst. ROBIN: Joyful Spark. BABS: Digital Cartographer. ALFRED: Meta-Analyst.")
    
    if not os.path.exists(ArchitectConfig.WING_CACHE_FILE):
        with open(ArchitectConfig.WING_CACHE_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_WING_COMMAND_FILE):
        with open(ArchitectConfig.BABS_WING_COMMAND_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_TACTICAL_DATA_FILE):
        with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w') as f:
            json.dump([], f)
            
    if not os.path.exists(ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE):
        with open(ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.WING_RAW_PERSONALITY_OUTPUT_FILE):
        with open(ArchitectConfig.WING_RAW_PERSONALITY_OUTPUT_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_PERSONALITY_DATA_FILE):
        with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.SELF_AWARENESS_CODEX_FILE):
        logging.info(f"Creating initial '{ArchitectConfig.SELF_AWARENESS_CODEX_FILE}' for first run.")
        initial_codex_data = SelfAwarenessCodex()._initialize_default_codex()
        with open(ArchitectConfig.SELF_AWARENESS_CODEX_FILE, 'w') as f:
            json.dump(initial_codex_data, f, indent=2)

    logging.info("Autonomous Architect (autonomous_architect_v12.py) Initializing...")
    orchestrator = CoreLoopOrchestrator()
    
    cycle_count = 0
    while True:
        cycle_count += 1
        logging.info(f"\n--- Starting Architect Cycle {cycle_count} ---")
        try:
            orchestrator.run_core_loop()
            logging.info(f"--- Finished Architect Cycle {cycle_count} ---\n")
        except Exception as e:
            logging.critical(f"FATAL ERROR in core loop during cycle {cycle_count}: {e}")
            logging.critical("The orchestrator has crashed. Attempting to restart the loop after a 30-second cooldown.")
        
        time.sleep(30)

    logging.info("Autonomous Architect's operations concluded for this run.")
    logging.info(f"Final self-awareness codex content: {orchestrator.self_awareness_codex.codex}")
