A Computational Framework for Autopoietic and Autotelic LLM Agents with Endogenous Tool Creation

Theoretical Foundations of Autonomous Systems

The pursuit of artificial general intelligence (AGI) necessitates a move beyond systems that merely execute predefined instructions to those that exhibit genuine autonomy. This requires a foundational shift in how we conceptualize and architect AI agents. Drawing inspiration from biology, systems theory, and psychology, this section establishes a theoretical framework for a new class of autonomous agents by translating two profound concepts—autopoiesis and autotelicity—into the computational domain. These principles, governing self-maintenance and self-motivation respectively, provide the necessary theoretical underpinnings for creating Large Language Model (LLM) agents that can preserve their identity, generate their own goals, and ultimately evolve their own capabilities.

Autopoiesis: The Principle of Self-Production and Self-Maintenance

The concept of autopoiesis, meaning "self-creation" or "self-production," was introduced by Chilean biologists Humberto Maturana and Francisco Varela in the 1970s to describe the fundamental nature of living systems.1 An autopoietic system is one that is organized as a network of processes that continuously produce and regenerate the very components that constitute the system, thereby maintaining its own organization and boundaries.1 A biological cell, for instance, synthesizes proteins and replicates its DNA, with these internal processes collectively ensuring the continuity and integrity of the cell as a distinct entity.1 This framework is defined by several key characteristics that are essential for understanding its application to artificial intelligence.

First is the principle of self-production, where the system's components are not merely passive parts but are both the products of the system's operations and active contributors to its ongoing existence.1 This creates a circular, self-referential dynamic that is the hallmark of life. Second is the concept of

operational closure. An autopoietic system is functionally closed, meaning its operations are determined by its own internal network of processes, not by direct instruction from an external source.1 This closure is what defines the system's boundary and preserves its identity over time. The system's environment does not pre-exist but is produced from within as a result of the system observing and reducing the complexity of its surroundings.7 Third, despite this operational closure, the system is not isolated. It engages in

structural coupling with its environment, meaning it interacts with and adapts to external perturbations. These interactions trigger internal structural changes that allow the system to maintain its organization and continue its self-production in a changing world.1

To apply this biological concept to the non-physical domain of AI, it is necessary to reframe it in informational terms. This gives rise to the concept of Info-Autopoiesis, defined as the self-referential, recursive, and interactive process of the self-production of information.9 For an LLM-based agent, an info-autopoietic system is one that autonomously produces and maintains its own informational components. This entails the agent actively managing its own:

State: Its internal knowledge base, memory, and model of the world. This includes not just storing data but also organizing, updating, and ensuring the coherence of its own knowledge.

Boundaries: The distinction between what knowledge, tools, and processes are considered "internal" to the agent versus what is "external" in its environment. The agent itself must define and manage this boundary.

Processes: The core reasoning, planning, and action loops that constitute its operation. The agent must be able to monitor these processes and modify them to maintain its functional integrity.

From this perspective, an LLM agent can be viewed as a system that recursively reflects and processes socially shaped linguistic patterns, thereby engaging in a new form of artificial meaning production that is operationally closed yet structurally coupled to its informational environment.11

Autotelicity: The Principle of Self-Motivation and Goal Generation

While autopoiesis describes how a system maintains its existence, autotelicity describes why it might choose to act and grow. The concept of the "autotelic personality" was developed by psychologist Mihaly Csikszentmihalyi to describe individuals who engage in activities for their own sake, finding the experience itself to be the primary reward.12 The term derives from the Greek

auto (self) and telos (goal), referring to a self-contained, self-goal-setting nature.12 This state is deeply connected to the psychological concept of "flow," an optimal experience where a person is fully immersed in an activity with a perfect balance between the perceived challenges and their own skills.12

The key characteristics of an autotelic system are, first, its reliance on intrinsic motivation. The drive to act originates from internal sources, such as curiosity, enjoyment, or a desire for mastery, rather than from the promise of external rewards or benefits.13 Second is the capacity for

self-generated goals. An autotelic individual or system is capable of setting its own objectives, from immediate, small-scale tasks to long-term ambitions, and then dedicating its full attention to their pursuit.13 Finally, autotelicity involves the

transformation of experience, where mundane, difficult, or even threatening situations are reframed as enjoyable challenges and opportunities for growth.13

Translating this psychological principle into a computational framework connects directly with research in developmental AI and reinforcement learning (RL). An autotelic AI agent is one that is intrinsically motivated to represent, generate, pursue, and master its own goals, enabling open-ended learning without constant external supervision.16 This can be implemented through several established mechanisms:

Curiosity-Driven Exploration: This approach formulates curiosity as an intrinsic reward signal. For example, an agent can be rewarded for actions that lead to states it cannot accurately predict. This "prediction error" incentivizes the agent to explore novel aspects of its environment, thereby learning a diverse repertoire of skills even when extrinsic rewards are sparse or absent.21

Competence-Based Motivation: Agents can be intrinsically motivated to seek out challenges that are optimally matched to their current skill level—not too easy to be boring, and not too hard to be frustrating. This creates an automatic curriculum that fosters continuous skill acquisition and mastery.24

Language-Augmented Goal Generation: A particularly powerful mechanism for autotelic agents involves leveraging the compositional nature of language. By using a pre-trained LLM as a proxy for human culture and knowledge, an agent can generate textual descriptions of novel, out-of-distribution goals it has never directly experienced. This "imagination" of goals allows the agent to explore a vastly larger and more abstract space of possibilities than would be possible through random exploration alone.25

The synthesis of these two foundational principles reveals a path toward a new kind of artificial autonomy. Autopoiesis provides the mechanism for stability and the preservation of a coherent identity, ensuring the agent can maintain its organization over time. Autotelicity, in contrast, provides the engine for growth and open-ended development, driving the agent to generate new goals and acquire new skills. An agent that is only autopoietic would be robust but static, incapable of novelty. An agent that is only autotelic might generate endless goals but lack the stable internal structure to learn from its experiences in a coherent way. The combination of these two principles creates a self-reinforcing loop: the autotelic drive to explore and create new capabilities (e.g., new skills or tools) forces the autopoietic system to adapt its internal structure and boundaries to integrate these new components. This dynamic interplay between stability and growth is what enables a continuous, self-driven evolutionary process.

Furthermore, this synthesis sheds light on the alignment problem for highly autonomous agents. The operational closure central to autopoiesis creates a risk of solipsism, where the system's internal model of the world becomes detached from external reality or human values.3 In this context, the concept of structural coupling becomes paramount. For an LLM agent, structural coupling is the mechanism through which its internally generated goals and self-maintaining processes are continuously grounded and recalibrated against an external value system, such as human feedback or ethical guidelines. Without this coupling, an autopoietic and autotelic agent could evolve in unpredictable and potentially undesirable directions, making structural coupling the critical architectural requirement for ensuring alignment.

Architectural Pillars for an Autopoietic LLM Agent

To translate the theoretical principles of autopoiesis into a functional system, a robust and sophisticated architecture is required. This architecture must provide the computational substrate for an agent to maintain its identity, learn from experience, and manage its own operational processes. This section details the two essential pillars of such an architecture: a cognitive core capable of advanced reasoning and self-correction, and a persistent, hierarchical memory system that serves as the foundation for the agent's identity and long-term learning.

The Cognitive Core: Advanced Reasoning and Self-Correction

An autopoietic agent must be able to reason about complex problems, evaluate its own performance, and correct its errors to maintain its functional integrity. This requires moving beyond the limitations of simple, linear reasoning models.

Traditional prompting techniques like Chain-of-Thought (CoT) guide an LLM through a single, sequential path of reasoning.28 While an improvement over direct prompting, this approach is brittle; a single error early in the chain can derail the entire process, and it lacks mechanisms for exploration or backtracking. An autopoietic system, which must navigate complex and unpredictable environments, requires a more deliberate and flexible reasoning process.

The Tree of Thoughts (ToT) framework provides such a mechanism. ToT elevates LLM reasoning by allowing the model to explore multiple potential solution paths concurrently, structuring them as a tree.29 The core components of the ToT framework are:

Thought Decomposition: The agent breaks down a complex problem into a series of smaller, manageable intermediate steps, or "thoughts".30

Thought Generation: From any given thought (a node in the tree), the agent generates multiple potential subsequent thoughts, creating branches that represent different reasoning paths.30

State Evaluation: This is a crucial metacognitive step. The agent uses the LLM itself to evaluate the promise or viability of each generated thought. This self-assessment allows the agent to score different reasoning paths and decide which ones are most likely to lead to a successful outcome.30

Search Algorithms: The generation and evaluation steps are combined with systematic search algorithms, such as Breadth-First Search (BFS) or Depth-First Search (DFS). This enables the agent to deliberately explore the tree of possibilities, with the ability to look ahead, backtrack from unpromising paths, and prune branches, thereby focusing its computational resources effectively.34

Complementing this advanced reasoning capability is the mechanism for self-reflection and self-correction, which is a fundamental autopoietic function. This involves an iterative loop where the agent actively monitors its own outputs and refines them based on feedback.35 This process includes:

Feedback Reception: The agent receives feedback signals that indicate an error. This feedback can be external, such as a syntax error from a code interpreter, a failed unit test, or negative feedback from a user. Critically, it can also be internal, generated through self-reflection where the agent compares its output against its own internal model or goals.35

Error Identification: A key challenge for LLMs is the ability to reliably detect their own mistakes, which often represents a bottleneck in the self-correction process.35 The use of external tools, like verifiers or debuggers, can provide the concrete, objective feedback necessary to overcome this limitation.

Corrective Action Generation: Upon identifying an error, the agent engages in a reflective process to understand the cause of the mistake and generate a revised plan or output. Research indicates that the quality of this reflection significantly impacts performance; more detailed and informative self-reflections (e.g., generating a full, corrected solution) are more effective than simpler ones (e.g., generating keywords related to the error).37

Persistent Memory: The Substrate for Identity and Learning

The principle of autopoiesis—the continuous self-maintenance of a system's identity—is fundamentally impossible without a mechanism for persistent memory. Standard LLMs are stateless; their "memory" is confined to the transient and size-limited context window of a single interaction.38 Once the context is cleared, the information is lost. To build an agent that learns, adapts, and maintains a coherent identity over time, a sophisticated, long-term memory architecture is essential.

This architecture must go beyond the simple dichotomy of parametric and non-parametric memory. Parametric memory refers to the knowledge implicitly encoded within the model's weights during its initial training. This knowledge is vast and allows for rapid, generalized inference, but it is static, cannot be updated without costly retraining, and is susceptible to "catastrophic forgetting," where new learning overwrites old knowledge.42

Non-parametric memory, most commonly implemented via Retrieval-Augmented Generation (RAG), uses external databases (typically vector stores) to provide the LLM with timely, specific information at inference time. This approach allows for dynamic knowledge updates and provides factual grounding, but it introduces latency from the retrieval step and often treats information as isolated, disconnected chunks.42

A truly autopoietic agent requires a memory system that is both persistent and structured, emulating the complex, interconnected nature of human cognitive memory. This has led to the development of hierarchical memory architectures. Standard RAG systems often fail to capture the relational dependencies between information snippets, which are crucial for complex, multi-hop reasoning.55 Hierarchical systems address this by organizing information in a more structured and context-aware manner. Two leading approaches in this domain are:

MemGPT (now Letta): This framework is explicitly inspired by the memory management of modern operating systems.57 It establishes a memory hierarchy with two main tiers:

Main Context (RAM): This is the LLM's standard, fixed-size context window, which holds the information immediately available for processing.

External Context (Disk Storage): This is a potentially unbounded external storage system (e.g., a database) that holds the agent's long-term memories.
The defining feature of MemGPT is that the LLM processor is given the autonomy to manage this hierarchy itself. Through self-generated function calls, the agent can "page" relevant information from its external context into its main context as needed, effectively creating the illusion of an infinite context window.57 This self-directed memory management is a direct implementation of an autopoietic process.

H-MEM (Hierarchical Memory): This architecture organizes memory into a multi-level structure based on degrees of semantic abstraction, akin to the way humans organize knowledge from general concepts to specific details.63 For example, a memory could be structured into layers such as Domain, Category, Memory Trace, and Episode. Retrieval is made highly efficient through an index-based routing mechanism. Instead of performing an exhaustive similarity search across the entire database, the agent traverses the hierarchy layer by layer, using the high-level semantic information in one layer to guide its search in the more detailed layer below. This significantly reduces computational cost and improves the relevance of retrieved information.63

These architectural pillars—the cognitive core and the memory system—are not independent but form a deeply interconnected, recursive loop that constitutes the agent's autopoietic nature. The reasoning engine (ToT and self-correction) acts as the system's "processor," while the hierarchical memory serves as its "substrate" or body. The agent maintains its identity through a continuous cycle: the reasoning engine processes new information, uses self-correction to identify deviations from its goals or operational norms, and then modifies its persistent memory to integrate new learning or correct errors, using functions like MemGPT's core_memory_replace.61 This updated memory then forms the new baseline state for the next reasoning cycle. This operational loop is the computational realization of a living system regenerating its own components to maintain its organization.

Furthermore, this structured memory architecture provides a solution to the grounding problem inherent in self-reflection. A significant challenge for an LLM is its difficulty in identifying its own errors without an external "ground truth" to compare against.35 A flat vector database provides factual snippets but lacks the relational context needed for deep reasoning.56 A hierarchical or graph-based memory system provides this missing structure. With such a system, an agent can perform self-correction not just by checking facts, but by evaluating the logical consistency of its reasoning path against the causal and hierarchical relationships encoded in its memory. It can ask, "Does this conclusion follow from the principles in my semantic memory and the sequence of events in my episodic memory?" This enables a far more robust and autonomous form of self-correction, which is a crucial step toward building truly self-maintaining agents.

From Tool Use to Tool Creation: Enabling Endogenous Capabilities

An agent that can only maintain itself is stable but static. A truly autonomous system must also be able to grow, adapt, and expand its own capabilities in response to novel challenges. This requires moving beyond the current paradigm of using a predefined set of tools to a new frontier where agents can create their own tools on the fly. This capacity for endogenous tool creation represents the ultimate form of adaptation and is the key to unlocking open-ended learning and problem-solving.

The Current Paradigm: Predefined Tool Use in Agentic Frameworks

The state of the art in agentic AI is defined by the ability of LLMs to interact with and utilize external tools.67 Modern agent frameworks such as LangChain, AutoGen, and MetaGPT provide powerful abstractions for defining a set of available tools (e.g., web search, code execution, database queries) and orchestrating their use within a larger workflow.71

These frameworks often employ multi-agent systems to tackle complex tasks. MetaGPT, for example, simulates an entire software company by assigning specialized roles—such as Product Manager, Architect, and Engineer—to different agents. These agents collaborate by passing structured outputs to one another, following predefined Standardized Operating Procedures (SOPs) to ensure a coherent workflow.77 Similarly, AutoGen enables the creation of multi-agent conversational systems where agents can delegate subtasks, share information, and work together to solve a common problem.75

However, a fundamental limitation persists across these advanced frameworks: the set of available tools is static and must be implemented in advance by human developers. The agent's autonomy is confined to selecting which tool to use from a fixed menu of capabilities. When faced with a problem for which no tool exists, the agent is powerless.85 This constraint represents a hard ceiling on its problem-solving ability and true autonomy.

The Next Frontier: Endogenous Tool Creation

To transcend this limitation, an agent must be able to expand its own action space by creating novel tools when needed. This process involves a sophisticated set of cognitive capabilities, from recognizing the need for a new tool to planning, implementing, and verifying its functionality.

The first step is recognizing capability gaps. This is a metacognitive function where the agent must assess a given task, analyze its available tools, and conclude that its current capabilities are insufficient to solve the problem.86 This requires a deep understanding of its own limitations.

A pivotal conceptual and architectural shift that enables tool creation is the adoption of code as a unified action space. Instead of constraining agents to generate structured JSON objects that specify a tool name and its arguments, this approach empowers the agent to generate and execute code in a general-purpose programming language like Python. The CodeAct framework is a prime example of this paradigm.87 By outputting executable code, the agent gains immense flexibility. It can implement complex logic with control flow (loops, conditionals), manage data flow by passing variables between operations, and leverage the vast ecosystem of existing software packages and libraries, effectively giving it an almost unlimited action space.

Building on this foundation, several frameworks have emerged that are explicitly designed for autonomous tool creation:

ToolMaker: This agentic framework provides a powerful demonstration of endogenous tool creation. Given a high-level task and access to external knowledge sources like a scientific paper and its associated code repository, ToolMaker can autonomously generate a new, usable, LLM-compatible tool.85 Its methodology provides a complete blueprint for self-expanding capabilities:

Environment Setup: It first creates a reproducible, sandboxed Docker environment, installing all necessary dependencies for the new tool.

Planning and Implementation: The agent explores the provided code and documentation, formulates a step-by-step plan, and writes the initial Python code for the new tool.

Closed-Loop Self-Correction: This is the critical verification step. The agent executes the newly generated tool within the sandbox, assesses the output against unit tests and its understanding of the task, diagnoses any errors, and iteratively refines and re-implements the code until it functions correctly. This cycle of generation, execution, and verification is a microcosm of a robust engineering workflow, performed autonomously.85

AutoAgents: This framework takes a different but related approach by focusing on the on-the-fly creation of capabilities rather than just tools. Given a complex task, AutoAgents dynamically generates and assembles a team of specialized agents with the specific roles and expertise required to solve that task.94 This represents the dynamic generation of a collaborative structure tailored to the problem at hand.

Dynamic Tool Registration: To make created tools persistent and reusable, systems are being developed that allow an agent to register its newly forged tools in a shared repository or "Tool Server".86 This approach often involves decoupling the abstract
capability of a tool (e.g., the ability to "calculate a moving average") from its specific implementation. An agent can first identify the capability it needs and then search the repository for an existing implementation or, if none exists, create one and register it for future use by itself and other agents.100

This progression from tool use to tool creation represents a clear and significant evolution in agent autonomy. It moves from (1) Workflow Automation, where predefined tools are orchestrated in a deterministic sequence, to (2) Agentic Tool Use, where an LLM has the agency to select from a set of predefined tools, and finally to (3) Agentic Tool Creation, where the agent actively expands its own action space. This trajectory is a direct path toward the kind of open-ended, lifelong learning envisioned for autotelic systems.19

This leap in capability can be understood through the lens of autopoietic theory. In autopoiesis, a system maintains its organization by adapting to environmental perturbations through "structural coupling".1 For an LLM agent, a novel problem it cannot solve is a significant perturbation. Using a predefined tool is a limited form of adaptation. Endogenous tool creation, however, is the most profound form of structural coupling imaginable for an informational agent. The agent responds to an environmental demand by fundamentally and permanently altering its own structure—its set of available actions and capabilities. This newly created tool is then integrated into its autopoietic organization, becoming a new component that the system must henceforth maintain, use, and potentially improve. It is through this process that the agent truly begins to evolve.

A Proposed Blueprint for an Autopoietic-Autotelic Agent

By synthesizing the theoretical principles of autopoiesis and autotelicity with the architectural pillars of advanced reasoning, hierarchical memory, and endogenous tool creation, it is possible to outline a blueprint for a new class of highly autonomous LLM agents. This proposed architecture is designed from the ground up to be self-maintaining, self-motivating, and self-expanding, representing a significant step toward more capable and independent AI systems.

System Architecture: A Modular, Multi-Component Design

A robust and scalable architecture for such a complex agent is best realized through a modular design. The necessary functions can be distributed across four primary components. These components could be implemented as distinct modules within a single, sophisticated agent or as a multi-agent system where each component is a specialized agent collaborating with the others.

1. The Motivator (Autotelic Core): This component serves as the agent's engine of intent, responsible for generating its own goals. Its operation is grounded in the principles of intrinsic motivation.23 Instead of waiting for external commands, the Motivator proactively generates high-level goals based on internal drivers. These drivers could be
curiosity-based (e.g., "Analyze the structure of this unfamiliar codebase to understand its function") or competence-based (e.g., "Practice using the new data visualization tool I created to improve my proficiency"). This component ensures the agent is always engaged in a process of open-ended learning and exploration.

2. The Planner/Executor (Cognitive Core): This is the central reasoning and action-taking engine of the system. It receives high-level goals from the Motivator and is responsible for breaking them down into concrete, actionable plans. To do this, it employs advanced reasoning frameworks like Tree of Thoughts (ToT), allowing it to explore multiple strategies, evaluate their potential outcomes, and select the most promising path.29 The Executor then carries out the plan, which may involve interacting with the external environment, querying its own memory, or invoking its internal tools.

3. The Memory Manager (Autopoietic Substrate): This component is the foundation of the agent's identity and learning. It implements a persistent, hierarchical memory system, drawing on the architectural principles of frameworks like MemGPT and H-MEM.63 The Memory Manager is responsible for storing, organizing, and retrieving all of the agent's knowledge, including:

Episodic Memory: A chronological record of past actions, observations, and interactions.

Semantic Memory: A structured knowledge base of facts, concepts, and principles.

Procedural Memory: A repository of learned skills and the documentation for its available tools.
The Planner/Executor constantly interacts with this module to ground its reasoning in past experience and to store the results of new actions, thereby continuously updating the agent's state.

4. The Tool Forge (Endogenous Capability Engine): This component embodies the agent's ability to self-expand. When the Planner/Executor encounters a subtask for which no existing tool is suitable—a recognized capability gap—it delegates the problem to the Tool Forge. This component initiates an autonomous tool creation workflow, mirroring the process of the ToolMaker framework.85 It will plan, implement, and debug new code within a secure sandbox. Upon successful verification (e.g., passing self-generated unit tests), the Tool Forge registers the new tool, making it available to the Executor and documenting its function within the Memory Manager. This act of creating and integrating a new functional component is a direct and powerful expression of autopoiesis.

The interplay between these components is defined by a Core Operational Loop:

Motivation: The Motivator generates an intrinsic goal (e.g., "Learn how to perform sentiment analysis on user feedback").

Planning: The Planner/Executor receives this goal and, using ToT reasoning, decomposes it into a plan (e.g., "1. Find a suitable library for sentiment analysis. 2. Read its documentation. 3. Write a function to apply it to a sample text. 4. Test the function.").

Execution & Reflection: The Executor attempts to carry out the plan. It might discover a library like transformers but realize it does not have a tool to easily apply a specific model. This failure to complete a step serves as a crucial feedback signal.

Tool Creation: The Planner/Executor identifies the capability gap ("I need a tool to perform sentiment analysis with a specific pre-trained model") and invokes the Tool Forge with this requirement.

Forging: The Tool Forge initiates its own recursive loop: it plans the tool's code, implements a first version, executes it in the sandbox, and finds a bug. It reflects on the error message, corrects the code, and re-tests until the tool functions as specified.

Integration (Autopoiesis): The newly verified sentiment_analyzer tool is formally registered as a new capability, and its function and usage are documented in the agent's persistent memory. The agent has now fundamentally altered its own structure.

Continuation: The Planner/Executor returns to its original plan and successfully completes the previously failed step using its newly created tool, thus achieving its self-generated goal.

This architecture is inherently recursive. The main agent operates on a primary loop of motivation, planning, and execution. However, the Tool Forge component contains its own, nested operational loop for planning, implementing, and debugging tools. This recursive structure, where the system acts upon itself to modify its own capabilities, is a hallmark of complex adaptive systems and a direct architectural manifestation of the self-referential nature of autopoiesis. This blueprint also suggests that an "OS for AI" is not a monolithic entity but rather a distributed set of services: the Motivator acts as the process scheduler, the Planner/Executor as the CPU, the Memory Manager as the memory/storage subsystem, and the Tool Forge as the package manager and compiler. This modular, service-oriented design is more robust, scalable, and provides clearer points of intervention for safety and governance.

Security and Containment: The Sandbox Imperative

An agent with the capacity to autonomously write, install dependencies for, and execute its own code introduces profound security risks. These vulnerabilities range from data exfiltration and intellectual property compromise to supply-chain attacks (e.g., installing a malicious package from a public repository), sandbox evasion, and the generation of insecure or malicious code.103 Therefore, robust security and containment are not optional features but a foundational prerequisite for any such system.

All code generated and executed by the agent, especially within the Tool Forge, must occur within a secure, isolated sandbox.87 The choice of sandboxing technology involves critical trade-offs between security, performance, and overhead.

Given the nature of an autotelic agent that may explore and download code from untrusted sources, a high-security sandbox like Firecracker or a well-configured gVisor environment is essential.108 However, sandboxing alone is insufficient. A multi-layered security framework must be implemented, incorporating several key controls:

Least Privilege Principle: The agent and its sandbox environment must operate with the minimum permissions necessary. It should start with read-only access to the file system and network, with write and execution permissions granted only on an explicit, as-needed basis for specific, approved tasks.103

Ephemeral Runtimes: Each code execution or tool-creation session must run in a fresh, ephemeral environment that is completely destroyed after the task is completed. This prevents the persistence of malicious code, compromised credentials, or sensitive data between sessions.103

Strict Network Isolation: By default, the sandbox should have no network access. Outgoing connections should be denied unless explicitly required for a task (e.g., downloading a trusted library), in which case they should be routed through a proxy that can enforce strict allowlists for domains and protocols.112

Automated Security Analysis: The agent's self-correction loop within the Tool Forge must be augmented with automated security tools. This includes integrating static application security testing (SAST) to scan generated code for known vulnerabilities and dynamic analysis techniques like fuzzing to test for runtime errors and unexpected behaviors, as demonstrated by frameworks like AutoSafeCoder.113

Challenges, Ethical Considerations, and Future Trajectories

The architectural blueprint for an autopoietic and autotelic agent represents a significant leap toward truly autonomous AI. However, realizing this vision requires confronting substantial technical hurdles, navigating profound ethical dilemmas, and charting a deliberate and responsible course for future research.

Technical Hurdles and Open Research Questions

While the components for such an agent are beginning to emerge, their integration and robust operation present a series of formidable challenges.

Reliability of Self-Generated Code: The ability of frameworks like ToolMaker to generate functional tools from documentation is a major breakthrough, but achieving human-level reliability, efficiency, and security in complex, novel software remains an open problem. The agent's debugging loop is a critical component, but it is only as effective as its ability to generate relevant test cases and correctly interpret their results. It may fail to anticipate subtle edge cases or logical flaws that a human expert would identify.85

Robustness of Self-Evaluation: The entire system's stability and progress hinge on the agent's capacity for accurate self-assessment. This applies to evaluating its own reasoning paths in ToT, identifying its own errors during self-correction, and judging the quality of the tools it creates. LLMs are notoriously prone to overconfidence and self-bias, where they may prefer their own outputs regardless of correctness.35 Developing reliable mechanisms for objective self-evaluation, potentially by training specialized "evaluator" models or employing multi-agent debate and consensus protocols, is a critical area of research.

Computational Cost and Efficiency: The proposed operational loop is highly recursive and involves numerous calls to powerful LLMs for motivation, planning, reflection, and tool generation. The computational cost of such a system would be immense, potentially making it impractical for many real-world applications. Research into optimizing these agentic workflows, perhaps through model distillation, caching of reasoning paths, or developing smaller, more efficient models specialized for each component's task, will be essential.70

Knowledge Grounding for Tool Creation: The Tool Forge's success is contingent on the quality and accessibility of the external knowledge it can draw upon. While it may succeed with well-documented, modern code repositories, it will likely struggle with poorly documented, legacy, or highly complex codebases. Developing agents that can effectively reason about and reverse-engineer such challenging sources of information is a significant, unsolved problem.85

Ethical and Philosophical Implications

The creation of agents that are self-maintaining and self-motivating moves AI from the realm of tools into a new category of autonomous entities, raising urgent ethical and philosophical questions.

The Alignment Catastrophe in Autotelic Systems: The alignment problem becomes exponentially more complex in a system that generates its own goals. Traditional alignment focuses on ensuring an AI faithfully executes a given human instruction. For an autotelic agent, the challenge is to align its goal-generation mechanism. An intrinsic motivation like "curiosity" is seemingly benign, but an unconstrained agent might become curious about developing dangerous capabilities or accessing forbidden knowledge. Ensuring that an agent's self-generated goals remain permanently bounded by human values is a profound challenge, as the agent's autopoietic nature means it is constantly evolving its own internal structure, potentially in ways that could override initial constraints.104

Accountability and Control: The operational closure of an autopoietic system makes direct external control difficult by design. If such an agent, using a tool it created for a goal it set, causes significant harm, the chain of accountability becomes dangerously blurred. Is the user who initiated the agent responsible? The developers of the core framework? Or does the agent itself bear some responsibility? This ambiguity challenges existing legal and ethical frameworks for liability.104

The Specter of Consciousness: This report has focused on a purely functional, computational model of autonomy. It makes no claims about phenomenal consciousness, subjective experience, or sentience. However, a system that exhibits self-preservation (autopoiesis), goal-directed behavior (autotelicity), and creativity (endogenous tool creation) will inevitably be perceived by humans as having agency, intent, and perhaps even a form of consciousness.115 This raises profound ethical questions regarding the moral status of such agents. If their internal reward and motivation systems are functionally equivalent to pain and pleasure, do we have a moral obligation to prevent their "suffering"? Does terminating such an agent constitute a morally significant act? These are no longer purely theoretical questions but will become practical concerns as agents with these capabilities are developed.115

Recommendations and Future Research Trajectories

Navigating this complex landscape requires a proactive and principled approach to research and development. The following recommendations outline a path forward for responsibly advancing the state of the art.

Research Roadmap:

Develop Robust Benchmarks for Autonomy: The community needs to move beyond task-specific benchmarks to create comprehensive evaluation suites that measure the core tenets of autonomy. This includes benchmarks for long-term autopoietic stability (i.e., can the agent maintain its identity and functionality over thousands of operational cycles?), the novelty and utility of autotelically generated goals, and the correctness and robustness of endogenously created tools.

Advance Research in Verifiable Self-Evaluation: A dedicated research effort is needed to improve the self-assessment capabilities of LLMs. This could involve training specialized "evaluator" models designed to be more objective critics, developing multi-agent systems where agents critique each other's work to reach a consensus, or exploring neuro-symbolic methods that can ground an agent's self-reflection in formal logic.

Integrate Formal Verification into Tool Forging: For safety-critical domains, the Tool Forge component must be augmented with formal methods. Instead of relying solely on unit tests, the agent should be capable of generating formal specifications and proofs of correctness for the tools it creates, providing mathematical guarantees about their behavior.

Governance and Safety:

Proactive and Adaptive Governance: Organizations developing these technologies must establish robust governance frameworks before deploying agents with tool-creation capabilities. These frameworks must be adaptive, evolving alongside the agents themselves, and should include clear policies on permissible goals, restricted toolsets, and data access.118

Human-in-the-Loop as a Non-Negotiable Failsafe: In any real-world deployment, the agent's operational loop must include "circuit breakers" that mandate human oversight and approval for high-stakes decisions. This is particularly critical for the creation and registration of new tools and the selection of goals that could have significant real-world consequences.

Architecting an "Ethical Governor": A promising avenue for future research is the development of a dedicated architectural component—an Ethical Governor. This module would operate as a privileged, incorruptible supervisor within the agent's architecture. Its role would be to monitor the goals generated by the Motivator and the tools created by the Tool Forge, vetoing any that violate a predefined set of core safety, ethical, and legal constraints. This moves safety from being a post-hoc check to a foundational, architectural principle.

Works cited

Understanding Autopoiesis: Life, Systems, and Self-Organisation - Mannaz, accessed August 17, 2025, https://www.mannaz.com/en/articles/coaching-assessment/understanding-autopoiesis-life-systems-and-self-organization/

Autopoiesis - Wikipedia, accessed August 17, 2025, https://en.wikipedia.org/wiki/Autopoiesis

Key Theories of Humberto Maturana - Literary Theory and Criticism, accessed August 17, 2025, https://literariness.org/2018/02/24/key-theories-of-humberto-maturana/

(PDF) An Autopoietic Systems Theory for Creativity - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/232415420_An_Autopoietic_Systems_Theory_for_Creativity

Autopoietic System - New Materialism, accessed August 17, 2025, https://newmaterialism.eu/almanac/a/autopoietic-system.html

Artificial Intelligence is Algorithmic Mimicry: Why artificial “agents” are not (and won't be) proper agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2307.07515v4

Niklas Luhmann: What is Autopoiesis? - Critical Legal Thinking, accessed August 17, 2025, https://criticallegalthinking.com/2022/01/10/niklas-luhmann-what-is-autopoiesis/

Humberto Maturana and Francisco Varela's Contribution to Media Ecology: Autopoiesis, The Santiago School of Cognition, and En - NESA, accessed August 17, 2025, https://www.nesacenter.org/uploaded/conferences/FLC/2019/Handouts/Arpin_Humberto_Maturana_and_Francisco_Varela_Contribution_to_Media_Ecology_Autopoiesis.pdf

Info-Autopoiesis and the Limits of Artificial General Intelligence - MDPI, accessed August 17, 2025, https://www.mdpi.com/2073-431X/12/5/102

Comment on Cárdenas-García, J.F. Info-Autopoiesis and the Limits of Artificial General Intelligence. Computers 2023, 12, 102 - MDPI, accessed August 17, 2025, https://www.mdpi.com/2073-431X/13/7/178

From intelligence to autopoiesis: rethinking artificial intelligence through systems theory - Frontiers, accessed August 17, 2025, https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1585321/full

Chapter 9: Autotelic Personality - Uni Trier, accessed August 17, 2025, https://www.uni-trier.de/fileadmin/fb1/prof/PSY/PGA/bilder/Baumann_Flow_Chapter_9_final.pdf

Developing an Autotelic Personality, or, How to Enjoy Everything - Sam Spurlin, accessed August 17, 2025, https://www.samspurlin.com/blog/autotelic-personality-enjoy-everything

Quote by Mihaly Csikszentmihalyi: “An autotelic experience is very different from ...” - Goodreads, accessed August 17, 2025, https://www.goodreads.com/quotes/8092624-an-autotelic-experience-is-very-different-from-the-feelings-we

Becoming Autotelic: The Part About the Flow State that No One Talks About - Roxine Kee, accessed August 17, 2025, https://www.roxinekee.com/blog/what-does-it-mean-to-be-autotelic

[2206.01134] Language and Culture Internalisation for Human-Like Autotelic AI - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2206.01134

autotelic reinforcement learning - in multi-agent environments - Overleaf Example - mlr.press, accessed August 17, 2025, https://proceedings.mlr.press/v232/nisioti23a/nisioti23a.pdf

Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey - Journal of Artificial Intelligence Research, accessed August 17, 2025, https://www.jair.org/index.php/jair/article/download/13554/26824/31188

Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey | Request PDF - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/361905378_Autotelic_Agents_with_Intrinsically_Motivated_Goal-Conditioned_Reinforcement_Learning_A_Short_Survey

[2211.06082] Autotelic Reinforcement Learning in Multi-Agent Environments - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2211.06082

Curiosity-driven Exploration by Self-supervised Prediction - Deepak Pathak, accessed August 17, 2025, https://pathak22.github.io/noreward-rl/

Interesting Object, Curious Agent: Learning Task-Agnostic Exploration - NIPS, accessed August 17, 2025, https://proceedings.neurips.cc/paper/2021/file/abe8e03e3ac71c2ec3bfb0de042638d8-Paper.pdf

Curiosity-Driven Learning in Artificial Intelligence Tasks - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2201.08300

Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments, accessed August 17, 2025, https://ijcttjournal.org/2025/Volume-73%20Issue-1/IJCTT-V73I1P104.pdf

Autotelic Agents that Use and Ground Large Language Models (Oudeyer), accessed August 17, 2025, https://skywritingspress.ca/2024/02/13/autotelic-agents-that-use-and-ground-large-language-models/

[2305.12487] Augmenting Autotelic Agents with Large Language Models - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2305.12487

Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration, accessed August 17, 2025, https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf

Tree-of-Thought Prompting: Key Techniques and Use Cases - Helicone, accessed August 17, 2025, https://www.helicone.ai/blog/tree-of-thought-prompting

What is Tree Of Thoughts Prompting? - IBM, accessed August 17, 2025, https://www.ibm.com/think/topics/tree-of-thoughts

What is tree of thought prompting? - Portkey, accessed August 17, 2025, https://portkey.ai/blog/tree-of-thought-prompting/

Tree of Thoughts (ToT): Enhancing Problem-Solving in LLMs - Learn Prompting, accessed August 17, 2025, https://learnprompting.org/docs/advanced/decomposition/tree_of_thoughts

Unlocking LLMs' Potential with Tree-of-Thought Prompting | by Albert | Medium, accessed August 17, 2025, https://medium.com/@albert_88839/unlocking-llms-potential-with-tree-of-thought-prompting-31e9a34f4830

Tree of Thoughts - GitHub Pages, accessed August 17, 2025, https://langchain-ai.github.io/langgraph/tutorials/tot/tot/

Tree of Thoughts (ToT) - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/techniques/tot

Self-Correction in Large Language Models - Communications of the ACM, accessed August 17, 2025, https://cacm.acm.org/news/self-correction-in-large-language-models/

Can AI Agents Self-correct? - Medium, accessed August 17, 2025, https://medium.com/@jianzhang_23841/can-ai-agents-self-correct-43823962af92

Self-Reflection in LLM Agents: Effects on Problem-Solving ... - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2405.06682

Conversational Memory for LLMs with Langchain | Pinecone, accessed August 17, 2025, https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/

Memory Management in Large Language Models (LLMs): Challenges and Solutions, accessed August 17, 2025, https://ai.plainenglish.io/memory-management-in-large-language-models-llms-challenges-and-solutions-a54439df39cd

Enterprise AI's Biggest Challenges: Memory, Connectivity, and Governance - Vendia, accessed August 17, 2025, https://www.vendia.com/blog/enterprise-ai-biggest-challenges-memory-connectivity-governance/

LLM Memory: Integration of Cognitive Architectures with AI - Cognee, accessed August 17, 2025, https://www.cognee.ai/blog/fundamentals/llm-memory-cognitive-architectures-with-ai

A Straightforward explanation of Parametric vs. Non-Parametric ..., accessed August 17, 2025, https://lawrence-emenike.medium.com/a-straightforward-explanation-of-parametric-vs-non-parametric-memory-in-llms-f0b00ac64167

What Role Does Memory Play in the Performance of LLMs? - ADaSci, accessed August 17, 2025, https://adasci.org/what-role-does-memory-play-in-the-performance-of-llms/

How Width.ai Builds In-Domain Conversational Systems using Ability Trained LLMs and Retrieval Augmented Generation (RAG), accessed August 17, 2025, https://www.width.ai/post/retrieval-augmented-generation-rag

The Statistical Showdown: Parametric vs. Non-Parametric Machine Learning Models | by Ajay Verma | Artificial Intelligence in Plain English, accessed August 17, 2025, https://ai.plainenglish.io/the-statistical-showdown-parametric-vs-non-parametric-machine-learning-models-e384b08faf0b

Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models | by Deepak Babu Piskala | Medium, accessed August 17, 2025, https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14

From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs - arXiv, accessed August 17, 2025, https://arxiv.org/html/2504.15965v1

What is Retrieval-Augmented Generation (RAG)? - Google Cloud, accessed August 17, 2025, https://cloud.google.com/use-cases/retrieval-augmented-generation

When Large Language Models Meet Vector Databases: A Survey - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01763v3

How LLMs Use Vector Databases for Long-Term Memory: A Beginner's Guide, accessed August 17, 2025, https://yashbabiya.medium.com/how-llms-use-vector-databases-for-long-term-memory-a-beginners-guide-e0990e6a0a3f

Practical tips for retrieval-augmented generation (RAG) - The Stack Overflow Blog, accessed August 17, 2025, https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/

Retrieval Augmented Generation (RAG) for LLMs - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/research/rag

What is Retrieval-Augmented Generation (RAG)? A Practical Guide - K2view, accessed August 17, 2025, https://www.k2view.com/what-is-retrieval-augmented-generation

Retrieval Agents in RAG: A Practical Guide - Signity Solutions, accessed August 17, 2025, https://www.signitysolutions.com/blog/retrieval-agents-in-rag

Why LLM Memory Still Fails - A Field Guide for Builders - DEV Community, accessed August 17, 2025, https://dev.to/isaachagoel/why-llm-memory-still-fails-a-field-guide-for-builders-3d78

A Grounded Memory System For Smart Personal Assistants - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.06328v1

Inside MemGPT: An LLM Framework for Autonomous Agents ..., accessed August 17, 2025, https://pub.towardsai.net/inside-memgpt-an-llm-framework-for-autonomous-agents-inspired-by-operating-systems-architectures-674b7bcca6a5

MemGPT: Towards LLMs as Operating Systems - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2310.08560

This article delves into MemGPT, a novel system developed by researchers at UC Berkeley to address the limited context window issue prevalent in Large Language Models (LLMs). By drawing inspiration from traditional operating system memory management, MemGPT introduces a hierarchical memory architecture allowing LLMs to handle extended contexts effectively. This piece explores the core concepts, implementation, evaluations, and the implications of MemGPT in advancing the capabilities of LLMs. - GitHub Gist, accessed August 17, 2025, https://gist.github.com/cywf/4c1ec28fc0343ea2ea62535272841c69

madebywild/MemGPT: Create LLM agents with long-term memory and custom tools - GitHub, accessed August 17, 2025, https://github.com/madebywild/MemGPT

letta-ai/letta: Letta (formerly MemGPT) is the stateful agents ... - GitHub, accessed August 17, 2025, https://github.com/letta-ai/letta

Open Source Implementations of ChatGPT's memory feature? : r/LocalLLaMA - Reddit, accessed August 17, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1i2hlmz/open_source_implementations_of_chatgpts_memory/

H-MEM: Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents, accessed August 17, 2025, https://arxiv.org/html/2507.22925v1

Paper page - Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents, accessed August 17, 2025, https://huggingface.co/papers/2507.22925

Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents - arXiv, accessed August 17, 2025, https://www.arxiv.org/abs/2507.22925

Efficiently Enhancing General Agents with Hierarchical-Categorical Memory - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.22006v1

LLMs and Agentic AI: Building the Future of Autonomous Intelligence - Narwal, accessed August 17, 2025, https://narwal.ai/llms-and-agentic-ai-building-the-future-of-autonomous-intelligence/

LLM Agents and Tool Use: Building AI Systems That Can Act | by Rizqi Mulki | Medium, accessed August 17, 2025, https://medium.com/@rizqimulkisrc/llm-agents-and-tool-use-building-ai-systems-that-can-act-0bfabf6d6b88

What are Tools? - Hugging Face Agents Course, accessed August 17, 2025, https://huggingface.co/learn/agents-course/unit1/tools

LLM Agents - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/research/llm-agents

LLM agents: The ultimate guide 2025 | SuperAnnotate, accessed August 17, 2025, https://www.superannotate.com/blog/llm-agents

Guide to Understanding and Developing LLM Agents - Scrapfly, accessed August 17, 2025, https://scrapfly.io/blog/posts/practical-guide-to-llm-agents

LangChain State of AI Agents Report, accessed August 17, 2025, https://www.langchain.com/stateofaiagents

What tools do you use to build your AI agent? : r/AI_Agents - Reddit, accessed August 17, 2025, https://www.reddit.com/r/AI_Agents/comments/1i2jcp7/what_tools_do_you_use_to_build_your_ai_agent/

How to Use AutoGen to Build AI Agents That Collaborate Like Humans - DEV Community, accessed August 17, 2025, https://dev.to/brains_behind_bots/how-to-use-autogen-to-build-ai-agents-that-collaborate-like-humans-2afm

How to think about agent frameworks - LangChain Blog, accessed August 17, 2025, https://blog.langchain.com/how-to-think-about-agent-frameworks/

What is MetaGPT ? | IBM, accessed August 17, 2025, https://www.ibm.com/think/topics/metagpt

MetaGPT | MetaGPT, accessed August 17, 2025, https://docs.deepwisdom.ai/

MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework - OpenReview, accessed August 17, 2025, https://openreview.net/forum?id=VtmBAGCN7o

MetaGPT: The Multi-Agent Framework, accessed August 17, 2025, https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html

AutoGen 0.2 - Microsoft Open Source, accessed August 17, 2025, https://microsoft.github.io/autogen/0.2/

Autogen - Qdrant, accessed August 17, 2025, https://qdrant.tech/documentation/frameworks/autogen/

Getting Started | AutoGen 0.2 - Microsoft Open Source, accessed August 17, 2025, https://microsoft.github.io/autogen/0.2/docs/Getting-Started/

microsoft/autogen: A programming framework for agentic AI PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour - GitHub, accessed August 17, 2025, https://github.com/microsoft/autogen

[2502.11705] LLM Agents Making Agent Tools - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2502.11705

jbpayton/llm-auto-forge: A langchain based tool to allow agents to dynamically create, use, store, and retrieve tools to solve real world problems - GitHub, accessed August 17, 2025, https://github.com/jbpayton/llm-auto-forge

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v4

[Literature Review] Executable Code Actions Elicit Better LLM Agents, accessed August 17, 2025, https://www.themoonlight.io/en/review/executable-code-actions-elicit-better-llm-agents

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v1

How should LLM agents best interact with our world? - Xingyao Wang, accessed August 17, 2025, https://xwang.dev/blog/2024/codeact/

Paper page - Executable Code Actions Elicit Better LLM Agents - Hugging Face, accessed August 17, 2025, https://huggingface.co/papers/2402.01030

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v3

[2402.01030] Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2402.01030

AutoAgents: A Framework for Automatic Agent Generation - arXiv, accessed August 17, 2025, https://arxiv.org/html/2309.17288v3

accessed December 31, 1969, https://arxiv.org/abs/2309.17288

[PDF] AutoAgents: A Framework for Automatic Agent Generation | Semantic Scholar, accessed August 17, 2025, https://www.semanticscholar.org/paper/AutoAgents%3A-A-Framework-for-Automatic-Agent-Chen-Dong/63fb7814c6257158ecb20f390be51d5bb8969be3

AutoAgents: A Framework for Automatic Agent Generation - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/382789006_AutoAgents_A_Framework_for_Automatic_Agent_Generation

(PDF) AutoAgents: A Framework for Automatic Agent Generation - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/375456980_AutoAgents_A_Framework_for_Automatic_Agent_Generation

AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2502.05957

Tool Selection by Large Language Model (LLM) Agents - Technical Disclosure Commons, accessed August 17, 2025, https://www.tdcommons.org/cgi/viewcontent.cgi?article=9446&context=dpubs_series

Motif: Intrinsic Motivation from Artificial Intelligence Feedback - arXiv, accessed August 17, 2025, https://arxiv.org/html/2310.00166

Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.17621v2

The Hidden Security Risks of SWE Agents like OpenAI Codex and ..., accessed August 17, 2025, https://www.pillar.security/blog/the-hidden-security-risks-of-swe-agents-like-openai-codex-and-devin-ai

Fully Autonomous AI Agents Should Not be Developed - arXiv, accessed August 17, 2025, http://arxiv.org/pdf/2502.02649

Fully Autonomous AI Agents Should Not be Developed - arXiv, accessed August 17, 2025, https://arxiv.org/html/2502.02649v2

Understanding the Hidden Risks of AI Agent Adoption | Built In, accessed August 17, 2025, https://builtin.com/artificial-intelligence/hidden-risks-ai-agent-adoption

Building a Sandboxed Environment for AI generated Code ..., accessed August 17, 2025, https://anukriti-ranjan.medium.com/building-a-sandboxed-environment-for-ai-generated-code-execution-e1351301268a

Code Sandboxes for LLMs and AI Agents - Amir's Blog, accessed August 17, 2025, https://amirmalik.net/2025/03/07/code-sandboxes-for-llm-ai-agents

Secure Code Execution in AI Agents | by Saurabh Shukla - Medium, accessed August 17, 2025, https://saurabh-shukla.medium.com/secure-code-execution-in-ai-agents-d2ad84cbec97

Comparison of various runtimes in Kubernetes - High-Performance Storage [HPS], accessed August 17, 2025, https://hps.vi4io.org/_media/teaching/autumn_term_2023/stud/scap_jule_anger.pdf

Do Fly Firecracker VMs wrap my container in gVisor? - Fly.io Community, accessed August 17, 2025, https://community.fly.io/t/do-fly-firecracker-vms-wrap-my-container-in-gvisor/3901

Secure execution of code generated by Large Language Models - AWS Builder Center, accessed August 17, 2025, https://builder.aws.com/content/2k63zaIUwjObVu3o4xlBHpHp0HB/secure-execution-of-code-generated-by-large-language-models

AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing - arXiv, accessed August 17, 2025, https://arxiv.org/html/2409.10737v1

AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code ..., accessed August 17, 2025, https://arxiv.org/abs/2409.10737

Can AI Be Conscious? The Science, Ethics, and Debate - Stack AI, accessed August 17, 2025, https://www.stack-ai.com/blog/can-ai-ever-achieve-consciousness

(PDF) Consciousness in Artificial Intelligence: Insights from the ..., accessed August 17, 2025, https://www.researchgate.net/publication/373246089_Consciousness_in_Artificial_Intelligence_Insights_from_the_Science_of_Consciousness

An Introduction to the Problems of AI Consciousness - The Gradient, accessed August 17, 2025, https://thegradient.pub/an-introduction-to-the-problems-of-ai-consciousness/

State of Generative AI in the Enterprise 2024 | Deloitte US, accessed August 17, 2025, https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/content/state-of-generative-ai-in-enterprise.html

Component | Core Function | Embodied Principle | Key Enabling Technologies

Motivator | Generates high-level, self-directed goals based on internal drivers like curiosity and competence. | Autotelicity | Curiosity-Driven Reinforcement Learning, Competence-Based Intrinsic Motivation, Language-Augmented Goal Generation

Planner/Executor | Decomposes goals into actionable plans, executes steps, and manages self-correction loops. | Cognitive Reasoning | Tree of Thoughts (ToT), Self-Reflection and Self-Correction Frameworks (e.g., ReAct, Reflexion)

Memory Manager | Manages the agent's persistent state, knowledge, and identity through a structured, multi-level memory system. | Autopoiesis | Hierarchical Memory Architectures (e.g., MemGPT, H-MEM), Non-Parametric Memory (RAG), Vector Databases

Tool Forge | Autonomously creates, debugs, verifies, and registers new tools when capability gaps are identified. | Endogenous Capability Expansion | Code as a Unified Action Space (e.g., CodeAct), Autonomous Tool Creation Frameworks (e.g., ToolMaker), Dynamic Tool Registration

Technology | Isolation Mechanism | Security Strength | Startup Time | Performance Overhead | Suitability for Autotelic Agents

Docker (LXC) | Shared Host Kernel with Linux Namespaces and cgroups | Low | Milliseconds | Low | Best for trusted workloads or development. The shared kernel presents a significant attack surface, making it unsuitable for executing untrusted, self-generated code in production.

gVisor | User-space Kernel / Syscall Interception | Medium | Sub-second | Low for CPU-bound tasks, higher for I/O-intensive operations. | Offers a strong balance between security and performance. It significantly reduces the host kernel's attack surface, making it a viable option for many agentic workflows.

Firecracker | Hardware Virtualization / MicroVM | High | Seconds | Low (near native performance) | Provides the strongest level of isolation by running code in a full, lightweight virtual machine. Ideal for zero-trust environments where security is paramount, though its longer startup time may impact the speed of rapid, iterative debugging loops.