The 8GB VRAM Challenge: A Strategic Guide to Deploying and Fine-Tuning Sequential AI Experts on Consumer Hardware

I. Introduction: Architecting for Scarcity

The Core Challenge

Deploying sophisticated, multi-stage Artificial Intelligence (AI) systems on consumer-grade hardware presents a formidable engineering challenge. The primary constraint is Video Random Access Memory (VRAM), a resource that is both critical for performance and severely limited on most consumer GPUs. This report provides an architectural blueprint for developing and deploying a powerful, multi-model AI pipeline on a standard Windows 11 machine equipped with just 8GB of VRAM. Success under such stringent limitations is not merely a matter of choosing smaller models; it demands a holistic strategy that treats model selection, runtime orchestration, user interface (UI) responsiveness, and application packaging as deeply interconnected components of a single, resource-aware system. The objective is to architect for scarcity, transforming the VRAM constraint from a hard blocker into a design parameter that informs every technical decision.

Defining the 'Mixture of Experts in Series' (MoE-S) Architecture

The system architecture addressed in this report is a 'Mixture of Experts in Series' (MoE-S). It is crucial to distinguish this from a parallel Mixture-of-Experts (MoE) model, such as Mixtral, where multiple expert sub-networks are activated simultaneously within a single inference pass. The MoE-S architecture is a sequential pipeline, where each "expert" is a distinct, specialized Large Language Model (LLM) that executes a discrete step in a larger, complex workflow. For example, a workflow might involve:

Expert 1 (Summarizer): A model optimized for text summarization receives a large document and condenses it.

Expert 2 (Code Generator): A coding-specialized model takes the summary and generates a Python script based on its content.

Expert 3 (Reasoning Engine): A logic-focused model analyzes the generated code for potential errors or optimizations.

This sequential execution is the foundational principle that makes deployment on an 8GB VRAM system feasible. Because each expert is invoked in turn, only one model needs to be fully resident in VRAM at any given moment. The challenge then shifts from fitting multiple models into memory simultaneously to efficiently and dynamically managing the loading and unloading of a single model at each stage of the pipeline.

Guiding Principles of the Report

This document is structured around a set of core engineering principles designed to address the 8GB VRAM challenge comprehensively:

Ruthless VRAM Budgeting: Every megabyte of VRAM must be accounted for. This involves a meticulous analysis of model weights, context cache requirements, and framework overhead to make informed model selections.

Managing the Latency-Memory Trade-Off: The techniques used to conserve VRAM, such as dynamically unloading models, inherently introduce latency. This report will directly confront this trade-off and provide architectural patterns to mitigate its impact on the user experience.

Co-designing Backend and Frontend for Perceived Performance: The AI backend and the Kivy-based UI cannot be developed in isolation. The UI must be architected to remain fluid and responsive, providing clear feedback to the user during the unavoidable delays of backend processing, thereby improving the perceived performance of the system.

Ensuring Deployment Robustness: The final application must be a self-contained, reliable executable. This requires a deep understanding of packaging tools like PyInstaller and the specific challenges of bundling complex dependencies such as PyTorch, Kivy, and the AI models themselves.

II. Foundation: Model Selection and VRAM Budgeting

The initial and most critical phase of the project is the selection of expert models. An incorrect choice at this stage—selecting a model that cannot operate within the VRAM envelope—will invalidate the entire architecture. This section provides a rigorous framework for VRAM-aware model selection and budgeting.

Deconstructing VRAM Consumption: Beyond File Size

A common pitfall is to equate a model's GGUF file size with its runtime VRAM consumption. In reality, the total VRAM footprint is a composite of three distinct components, each of which must be carefully managed.

Model Weights: This is the baseline memory required to load the quantized model parameters from the GGUF file into the GPU. This component is static for a given model and quantization level.

KV Cache: This is a dynamic memory allocation that stores the attention keys and values for the tokens in the current context. Its size is directly proportional to the context length (num_ctx) and the batch size. As a conversation or processing task grows, the KV cache expands, consuming more VRAM.1

Framework Overhead: The inference engine, in this case, Ollama and its underlying llama.cpp engine, consumes a certain amount of VRAM for computation buffers, CUDA kernels, and general management tasks.2

A critical consideration, often overlooked, is that the context window is not a static feature but a dynamic consumer of VRAM. While a model like Phi-4-mini-reasoning may advertise a 128K token context length, attempting to utilize this full capacity on an 8GB GPU is a recipe for failure.3 Processing a larger context window requires significantly more VRAM to accommodate the expanding KV cache.1 Therefore, the

num_ctx parameter must be treated as a primary configuration variable to be carefully balanced for each expert in the pipeline, rather than a fixed feature of the model. For an 8GB system, a practical context length will likely be in the 2048 to 8192 token range, depending on the model's base size.

Strategic GGUF Quantization: A Balance of Size, Speed, and Fidelity

Quantization is the primary technique for fitting capable models into a constrained VRAM budget. The GGUF format offers several quantization families, each with distinct trade-offs.

K-Quants (e.g., Q4_K_M, Q5_K_S, Q6_K): This family represents the modern workhorse of GGUF quantization. It uses a mixed-precision approach, allocating more bits to more important layers, resulting in lower quantization error (perplexity) compared to older "legacy" methods. K-quants are generally fast, well-supported, and offer a robust balance of size reduction and performance fidelity, making them the "obvious better choice" in most scenarios.4

I-Quants (e.g., IQ2_XXS, IQ3_S): These are state-of-the-art methods for extreme compression, offering better quality for their size, especially at bit rates below 4 bits per weight.5 However, this efficiency comes at a cost. The de-quantization process is more computationally complex, which can make I-quants significantly slower, particularly if the model is partially offloaded to the CPU or if the CPU itself becomes the bottleneck.3 For a system where GPU throughput is paramount, K-quants are often the safer and faster choice.

For a target of 8GB VRAM, 4-bit and 5-bit K-quantization levels, specifically Q4_K_M and Q5_K_M, represent the optimal starting point. They provide a substantial reduction in VRAM footprint while preserving a high degree of model accuracy, offering the best compromise for this use case.3

Candidate Model Analysis for the 8GB Target

Based on the principles of VRAM budgeting and strategic quantization, the following models are strong candidates for inclusion in the MoE-S pipeline. The analysis assumes the use of Q4_K_M or similar 4-bit quantization to estimate VRAM requirements.

Phi-4-mini-reasoning (3.8B): An exceptional candidate for tasks requiring logic, mathematics, and structured reasoning. Its performance on benchmarks like MATH-500 is notably high for its size.3 A
Q4_K_M GGUF quantization is approximately 2.49 GB, leaving over 5GB of VRAM for the KV cache and framework overhead, making it a very safe and powerful choice.7

Qwen3-4B (4.0B): A highly capable general-purpose model from Alibaba Cloud. It performs well across a range of tasks and is available in several variants, including a "Thinking" version designed for more complex queries.9 The
Q4_K_M GGUF is around 2.5 GB, fitting comfortably within the 8GB budget.6

Gemma 3-4B (4.0B): Google's open model in this size class. While the native bfloat16 version requires 8GB for weights alone, a 4-bit (int4) quantization reduces this to a lean 2.6 GB, making it a prime candidate for the pipeline.12

Mistral-7B (7.0B): This model represents the upper limit of what is feasible. It offers superior performance due to its larger parameter count but pushes the VRAM budget to its breaking point. A 4-bit quantization like Q4_0 is approximately 4.11 GB.14 While this leaves nearly 4GB of headroom, the larger model architecture will inherently require a larger KV cache for the same context length compared to a 4B model. Using Mistral-7B is possible but requires aggressive management of the context window (
num_ctx) and carries a higher risk of out-of-memory errors.15

The following table synthesizes data from multiple sources to provide a clear, actionable guide for model selection. It estimates the VRAM footprint for both the model weights and a practical 4096-token context window.

III. Dynamic Orchestration: Managing the Expert Pipeline with Ollama

With a palette of VRAM-compliant models selected, the next challenge is implementing the runtime logic to manage them. The core of the MoE-S architecture is a dynamic orchestration layer that swaps models in and out of the GPU's limited memory. Ollama's API provides the necessary primitives to build this system.

The "Load-Process-Unload" (LPU) Workflow

The fundamental pattern for managing the expert pipeline is the "Load-Process-Unload" (LPU) workflow. This cycle is executed by the application's central orchestration logic for each step in a multi-expert task.

Load: The orchestrator identifies the next required expert (e.g., phi-4-mini-reasoning:q4_k_m). It sends a request to the Ollama API to load this model into VRAM. This can be done implicitly with the first inference request or explicitly by sending a request with an empty prompt to pre-warm the model.17

Process: Once the orchestrator confirms the model is loaded, it sends one or more inference requests to the /api/generate or /api/chat endpoints, passing the actual data to be processed by the expert.

Unload: After the expert has completed its task, the orchestrator sends a final, specific API call to eject the model from VRAM. This is the crucial step that frees up resources for the next expert in the sequence.

Ollama API Implementation Patterns for LPU

The Ollama REST API provides specific endpoints and parameters to implement the LPU workflow programmatically.

Loading a Model: A model is automatically loaded into memory upon the first request that specifies its name. To decouple loading from processing and manage the user experience, a model can be pre-loaded by sending a POST request to /api/generate with a body containing only the model name, for example: {"model": "qwen3:4b"}.17

Unloading a Model: The keep_alive parameter is the key to explicit memory management. To unload a model, the orchestrator sends a POST request to /api/generate specifying the model name and setting keep_alive to 0 or "0s". The API will respond with a JSON object where "done_reason": "unload", confirming that the model has been ejected from memory.17

Monitoring State: For robust orchestration, it is essential to know which models are currently active. The ollama ps command-line instruction provides this information. Programmatically, this state can be queried via the undocumented /api/ps endpoint. Additionally, the /api/tags endpoint can be used to list all locally downloaded models available to the orchestrator.1

Confronting the Latency Elephant: Managing TTFT

While the LPU workflow effectively solves the VRAM scarcity problem, it directly introduces a significant user experience challenge: high latency. The act of unloading a model to save VRAM means that the next time any model is needed, it must be loaded from the system's storage (e.g., an NVMe SSD) into GPU memory. This disk-to-VRAM transfer is the primary source of what is known as Time to First Token (TTFT) latency—the delay between the user initiating an action and the first piece of the model's response appearing.2 This "load duration" can be several seconds, creating the perception of a sluggish application.

Effectively managing this trade-off is central to the system's success. The following mitigation strategies are essential:

Asynchronous UI Feedback: The Kivy UI must never block during a model load. The LPU operations must be performed in a separate thread, and the UI should display clear, non-blocking feedback (e.g., an animated spinner with text like "Loading Reasoning Expert...").

Strategic Pre-loading: At application startup, the orchestrator can identify the most common first step in any workflow and pre-load the corresponding expert model in the background. This makes the very first user interaction feel significantly more responsive.

Intelligent Caching: For workflows where a user might frequently switch back and forth between two experts, the orchestrator can implement a simple caching policy. Instead of immediately unloading an expert, it can be kept in memory. It is only unloaded when a different, third expert is requested, forcing an eviction. This requires the orchestrator to actively monitor VRAM usage to ensure the cache does not cause an out-of-memory error.

Setting User Expectations: The UI design should implicitly or explicitly communicate that switching between major tasks (which corresponds to switching experts) may involve a brief loading period.

Ensuring Robust Model Swapping

A subtle but critical detail for a stable LPU system is the consistency of API parameters. Ollama's model management is sensitive to the configuration used for each request. If one process requests a model (e.g., llama3.1) with a context_size of 2048, and another process requests the same model with a context_size of 4096, Ollama will treat these as two distinct configurations. It will unload the first model instance to load the second, even if the model name is identical.22 This "parameter-induced thrashing" can lead to unexpected latency spikes and defeat the purpose of careful orchestration. To prevent this, the orchestration layer must enforce a strict policy: for any given expert model, all API calls (load, process, unload) must use a standardized and consistent set of parameters, especially

model name and num_ctx.

IV. Autonomous Adaptation: LoRA Fine-Tuning on Quantized Models

A key requirement of the system is the ability to adapt and learn on the local machine. This is achieved through Parameter-Efficient Fine-Tuning (PEFT), specifically Low-Rank Adaptation (LoRA). However, performing this on a VRAM-constrained machine requires a precise workflow that leverages on-the-fly quantization. It is a common misconception that one can directly fine-tune a GGUF file; the process is more nuanced.

The Correct Workflow: From Base Model to Fine-Tuned GGUF

The industry-standard best practice for memory-efficient fine-tuning, championed by frameworks like Unsloth, follows a distinct sequence of operations.23

Load Base Model with In-Memory Quantization: The process begins not with a GGUF file, but with a full-precision or 16-bit base model from a repository like Hugging Face. Using a library such as Unsloth's FastLanguageModel in conjunction with bitsandbytes, the model is loaded with 4-bit quantization applied dynamically in memory. This is the critical step that dramatically reduces the VRAM footprint of the base model, making it possible to load on an 8GB GPU.23

Apply LoRA Adapters: A LoraConfig object is defined, specifying parameters like the rank (r) and alpha (lora_alpha). These trainable, low-rank matrices are then injected into the layers of the in-memory, 4-bit quantized base model.

Train: The fine-tuning process is initiated. Crucially, the weights of the massive base model remain frozen. Only the small LoRA adapter weights, which constitute a tiny fraction of the total parameter count, are updated during training. This ensures that the VRAM required for gradients and optimizer states is kept to a minimum.27

Merge and Export: Once training is complete, the trained LoRA adapter weights are merged back into the full-precision weights of the base model.

Save to GGUF: The final step is to save this newly merged, fine-tuned model into the target GGUF format. Libraries like Unsloth provide a convenient one-line function, such as model.save_pretrained_gguf("model_directory", tokenizer, quantization_method="q4_k_m"), which handles the complex conversion and quantization process.25

The "Why": Understanding Quantization-Aware Fine-Tuning

This specific workflow is necessitated by a fundamental challenge in machine learning: the "quantization discrepancy." When a model is quantized after training (Post-Training Quantization, or PTQ), errors are inevitably introduced. If one then applies LoRA to this quantized model, the adapter must learn not only the new task but also how to compensate for the underlying quantization errors, leading to suboptimal performance.29

Advanced techniques, described in academic research, aim to solve this problem by making the fine-tuning process "quantization-aware."

The QLoRA method was a significant breakthrough, demonstrating that fine-tuning on a 4-bit quantized base model could yield results comparable to full-precision fine-tuning.27

The LoftQ (LoRA-Fine-Tuning-aware Quantization) framework refines this further. It proposes jointly optimizing the quantized weights (Q) and the initial LoRA adapters (A,B) to minimize the difference from the original full-precision weights (W) before fine-tuning begins. This gives the LoRA adapter a much better starting point.29

IR-QLoRA (Information Retention QLoRA) focuses on improving the initial quantization step itself, using techniques to maximize the information retained from the full-precision model. This creates a more faithful quantized representation, which in turn improves the effectiveness of the subsequent LoRA fine-tuning.30

The practical workflow described above is an implementation of these advanced principles. By using on-the-fly quantization during training (QLoRA-style), it effectively mitigates the quantization discrepancy, leading to a higher-quality final fine-tuned model.

Practical Hyperparameter Tuning for Local LoRA

When fine-tuning on a local, resource-constrained machine, several considerations are paramount.

Dataset Quality Over Quantity: The success of fine-tuning is overwhelmingly dependent on the quality of the training data. A small, clean, and highly relevant dataset of a few thousand high-quality examples will produce far better results than a large, noisy dataset.23 The data must be meticulously formatted to match the instruction or chat template expected by the base model (e.g., ChatML, Llama-3-Instruct format) to ensure the model responds correctly.24

Key LoRA Hyperparameters:

r (Rank): This determines the size of the trainable adapter matrices. For 8GB VRAM, a lower rank such as 16 or 32 is advisable to minimize memory usage.

lora_alpha: This scaling factor is typically set to twice the rank (e.g., r=16, lora_alpha=32).

learning_rate: A lower learning rate, such as 2e−5 or 5e−5, often leads to more stable and precise fine-tuning, though it may require more training steps.23

max_seq_length: During fine-tuning, this must be kept low (e.g., 2048) to ensure the model, its gradients, and the data batches all fit within the 8GB VRAM limit, even if the base model supports a much longer context length for inference.23

V. User Interface: Performance Tuning for a Kivy-Based GUI

The user's experience is dictated by the frontend application. Even the most powerful AI backend will feel unusable if the UI is sluggish, freezes, or crashes. This section outlines critical performance tuning strategies for the Kivy-based GUI to ensure a fluid and responsive experience.

The Golden Rule of Kivy Rendering: Create Once, Update Often

The most common and insidious performance issue in dynamic Kivy applications, especially games or data visualizations, is performance degradation over time. An application that runs smoothly at 60 FPS initially may drop to 30 FPS, then 20 FPS, over minutes of use. This is almost always caused by the continuous recreation of canvas instructions within the main update loop.

Each time a block like with self.canvas: is used inside a function that runs every frame (e.g., scheduled by Clock.schedule_interval), new graphics instructions are added to Kivy's rendering tree. The old instructions are not automatically removed, leading to a cumulative bloat. The GPU must process an ever-increasing number of instructions each frame, causing the slowdown.33

The correct, performant pattern is to create graphical instructions once in the widget's __init__ method and store references to them. Then, in the update loop, only modify the properties of these existing instructions.

Incorrect (Slow) Pattern:
Python
# Inside a method called every frame
def update(self, dt):
    with self.canvas:
        # This creates a NEW Rectangle every frame!
        Rectangle(pos=self.pos, size=self.size)


Correct (Fast) Pattern:
Python
def __init__(self, **kwargs):
    super().__init__(**kwargs)
    with self.canvas:
        # Create the instruction ONCE and store a reference
        self.rect = Rectangle(pos=self.pos, size=self.size)

# Inside a method called every frame
def update(self, dt):
    # Update the properties of the EXISTING instruction
    self.rect.pos = self.pos


This principle applies to all canvas instructions, including Translate, Rotate, Color, and shape primitives. Adhering to this "create once, update often" rule is the single most important technique for maintaining stable, high-performance rendering in Kivy.

Architecting for Responsiveness: Decoupling UI and AI

The Kivy application runs on a single main thread that handles both user input and screen rendering. If a long-running task, such as loading a 3GB model or waiting for an AI inference, is executed on this main thread, the entire application will freeze. To prevent this, the AI backend operations must be completely decoupled from the UI thread.

Threading: All interactions with the Ollama API—loading, processing, and unloading models—must be executed in a separate background thread using Python's built-in threading library. This allows the Kivy event loop to continue running unimpeded, processing user touches and updating animations while the AI task runs concurrently.34

Safe UI Updates with Callbacks: A background thread cannot directly modify Kivy widgets, as this would lead to race conditions and instability. The correct pattern is for the background thread to schedule a function to be run on the main thread once its work is complete. This is achieved using Clock.schedule_once. The background thread can pass its results (e.g., the generated text from the LLM) to the scheduled function, which can then safely update the appropriate UI Label or TextInput widget.

Optimizing Application Startup

The initial startup time of the application heavily influences the user's first impression. Heavy modules and resources should be loaded progressively, not all at once.

Deferred Imports: Avoid importing large libraries like torch, transformers, or even the application's own Ollama client library at the top level of the main Kivy script. Instead, place these import statements inside the specific functions or methods where they are first required. This technique, known as deferred loading, can dramatically reduce the initial time it takes for the application window to appear.34

Splash Screens and Progressive Loading: A superior user experience can be crafted using Kivy's ScreenManager. The application should initially display a simple, lightweight splash screen. In the on_enter event of this screen, a background thread can be dispatched to perform heavy initialization tasks, such as loading the first expert model into Ollama or creating complex UI widgets for the main screen. Once initialization is complete, the ScreenManager can transition to the main application screen.34

System-Level Tuning for Windows 11

The performance of a GPU-intensive application is not solely determined by the application's code but also by the host operating system's configuration. Windows 11 includes virtualization-based security (VBS) features, such as Memory Integrity (also called Hypervisor-protected code integrity or HVCI) and the Virtual Machine Platform (VMP), which are enabled by default on new devices. Microsoft has acknowledged that these features can create a performance impact in "some scenarios and some configurations of gaming devices".36

Given that the MoE-S application is highly GPU and CPU intensive, it falls into a similar category of performance-sensitive software. These security features introduce a layer of virtualization that can add overhead to system calls and hardware access. For users who wish to prioritize maximum performance, these features can be temporarily disabled through the Windows Security and Windows Features settings. The application's documentation should inform the user of this option, while also including the critical caveat that disabling these features reduces the system's security posture and may leave the device more vulnerable to threats.36

VI. Finalization: Robust Application Packaging with PyInstaller

The final step is to transform the complex Python project—with its myriad dependencies, data files, and AI models—into a single, distributable Windows executable. This process, often called "freezing," ensures that end-users can run the application without needing to install Python or any libraries.

Packager Selection: PyInstaller as the Pragmatic Choice

Several tools exist for creating Windows executables from Python code. A careful selection is required based on the project's complexity.

PyInstaller: This is the most mature, widely used, and battle-tested packaging tool in the Python ecosystem. Its key strength is its extensive support for complex packages and its large community, which has produced hooks for hundreds of libraries. For an application involving both Kivy and PyTorch, PyInstaller's robustness and well-documented hook system make it the most pragmatic and reliable choice, despite potentially slower application startup times compared to other tools.37

Nuitka: Nuitka takes a different approach by compiling Python code to C and then to a native binary. This often results in significantly better runtime performance and faster load times. However, the compilation process itself can be very long, and it can struggle with the complex dependencies found in scientific and AI packages, sometimes requiring more intricate configuration.39

cx_Freeze: A solid alternative to PyInstaller, often praised for faster application load times. However, it can sometimes require more manual configuration to find all dependencies and may produce larger distributable packages.37

Given the complex interplay of dependencies in this project, PyInstaller is the recommended tool due to its superior track record in handling such scenarios.

Mastering the .spec File for a Complex Application

The .spec file is the control center for the PyInstaller build process. A default file is generated on the first run, but for a complex application, it must be manually edited to ensure all components are correctly bundled.

Handling Kivy Dependencies: Kivy requires specific binary dependencies (DLLs for SDL2, GLEW, and potentially GStreamer) to be included. The official kivy_deps packages provide a helper mechanism for this. The .spec file must be modified to import these dependencies and add them to the bundle using a Tree object within the COLLECT block.41
Python
# At the top of the.spec file
from kivy_deps import sdl2, glew

# Inside the Analysis object...
a = Analysis(...)

# Inside the COLLECT object...
coll = COLLECT(exe, a.binaries, a.zipfiles, a.datas,
               *,
              ...)


Handling PyTorch Dependencies: PyTorch and its ecosystem (e.g., pytorch-lightning) are notorious for using dynamic imports and other mechanisms that can confuse PyInstaller's static analysis. The most effective way to handle this is to use PyInstaller's data collection flags, either on the command line (pyinstaller --collect-data torch...) or by adding a collect_data_files('torch') call within a custom hook file.42

Bundling Data Files (Models, ZODB): All non-Python assets are critical to the application's function. The GGUF model files and the ZODB database file (.fs) must be explicitly included. This is done by adding them to the datas list within the Analysis block of the .spec file. The format is a list of tuples, where each tuple is ('source_path', 'destination_in_bundle').41
Python
# Inside the Analysis object in the.spec file
a = Analysis(
    ['main.py'],
   ...
    datas=[
        ('path/to/phi-4-mini-reasoning-q4_k_m.gguf', '.'),
        ('path/to/qwen3-4b-q4_k_m.gguf', '.'),
        ('path/to/my_database.fs', '.')
    ],
   ...
)

The . as the destination places the files in the root of the bundled application directory.

Runtime Path Resolution: The _MEIPASS Imperative

A fundamental concept that must be understood is that a packaged application's file system is ephemeral. When a user launches a single-file executable created by PyInstaller, all the bundled contents (including the Python interpreter, libraries, and data files) are extracted to a temporary folder in the user's system. The path to this temporary folder is stored in the sys._MEIPASS attribute during runtime.41

If the application code attempts to access a model using a simple relative path like 'phi-4-mini-reasoning-q4_k_m.gguf', it will fail. The operating system will look for that file relative to the current working directory, not inside the temporary bundle directory. This is a universal point of failure for packaged applications that need to access data files.

The solution is to create a resource path resolution function that must be used for all data file access. This function checks if the application is running in a frozen state (by checking for hasattr(sys, '_MEIPASS')) and constructs the correct, absolute path to the resource inside the temporary folder.

Python

import sys
import os

def resource_path(relative_path):
    """ Get absolute path to resource, works for dev and for PyInstaller """
    try:
        # PyInstaller creates a temp folder and stores path in _MEIPASS
        base_path = sys._MEIPASS
    except Exception:
        base_path = os.path.abspath(".")

    return os.path.join(base_path, relative_path)

# Usage in the application:
model_path = resource_path("phi-4-mini-reasoning-q4_k_m.gguf")
db_path = resource_path("my_database.fs")


Implementing and consistently using this pattern is not optional; it is a critical requirement for creating a robust and functional packaged application.41

VII. Synthesis and Final Recommendations

Architecting a multi-stage AI pipeline for an 8GB VRAM Windows environment is a significant undertaking that pushes the boundaries of consumer hardware. Success is predicated on a disciplined, holistic approach that acknowledges and manages the inherent trade-offs between memory, performance, and user experience.

The VRAM-Latency Trade-Off Revisited

The central theme of this architecture is the constant balancing act between VRAM conservation and operational latency. The Load-Process-Unload (LPU) workflow is the definitive solution to the VRAM scarcity problem, enabling the sequential use of models that would be impossible to run concurrently. However, this solution directly creates a latency problem in the form of high Time to First Token (TTFT) due to model loading from disk. This is not a flaw to be eliminated but a characteristic to be managed. The responsibility for managing this latency falls squarely on the application's orchestration layer and the Kivy UI, which must work in concert to provide asynchronous feedback and intelligent pre-loading to create a perception of responsiveness for the end-user.

The Fine-Tuning-Quantization Symbiosis

The second critical takeaway is that on-device adaptation through fine-tuning and the memory-saving technique of quantization are not independent processes. They are deeply intertwined. Attempting to fine-tune a model that has already been subjected to post-training quantization will yield suboptimal results due to the "quantization discrepancy." The correct, state-of-the-art approach is a QLoRA-style workflow that integrates quantization into the training loop itself. This ensures that the trainable LoRA adapters are learning the desired task on a representation of the model that is already "aware" of the quantization, leading to a significantly higher-quality final GGUF model. Awareness of advanced academic concepts like LoftQ and IR-QLoRA provides the theoretical underpinning for why this practical workflow is superior.

Final Architectural Checklist

To ensure a successful implementation, developers should adhere to the following architectural and procedural checklist:

[ ] VRAM Budgeting: Have you created a VRAM budget for each expert model, accounting for weights, a realistic KV cache size (for a num_ctx of 4096-8192), and framework overhead?

[ ] Model Selection: Are all selected models available in a 4-bit or 5-bit K-Quant GGUF format (Q4_K_M, Q5_K_M) with a base weight footprint under 5GB?

[ ] Orchestration: Is the LPU workflow implemented with explicit calls to unload models using the keep_alive: 0 parameter in the Ollama API?

[ ] Latency Management: Does the UI provide non-blocking feedback during model loads? Is the first expert in the primary workflow pre-loaded at startup?

[ ] Parameter Consistency: Does the orchestration layer enforce the use of identical parameters (especially num_ctx) for all API calls to a given model to prevent parameter-induced thrashing?

[ ] Fine-Tuning Workflow: Is the fine-tuning process implemented using on-the-fly 4-bit quantization of a base model (not a GGUF), with subsequent merging and saving to the final GGUF format?

[ ] UI Performance: Are all Kivy canvas instructions created once during widget initialization and only updated thereafter? Are all blocking AI operations offloaded to a background thread?

[ ] Packaging: Is a resource_path function that utilizes sys._MEIPASS used for all data file access? Has the .spec file been manually configured to include Kivy dependencies, PyTorch data, and all GGUF/ZODB files?

By systematically addressing each of these points, it is possible to build a powerful, adaptable, and responsive multi-expert AI system that operates effectively within the challenging constraints of consumer-grade hardware.

Works cited

How to Use Ollama (Complete Ollama Cheatsheet) - Apidog, accessed September 12, 2025, https://apidog.com/blog/how-to-use-ollama/

Understanding Ollama Performance: Benchmarking & Overhead - Arsturn, accessed September 12, 2025, https://www.arsturn.com/blog/beyond-the-model-understanding-benchmarking-the-performance-overhead-of-ollama

Mungert/Phi-4-mini-reasoning-GGUF - Hugging Face, accessed September 12, 2025, https://huggingface.co/Mungert/Phi-4-mini-reasoning-GGUF

Overview of GGUF quantization methods : r/LocalLLaMA - Reddit, accessed September 12, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/

Mistral 22B V0.2 GGUF · Models · Dataloop, accessed September 12, 2025, https://dataloop.ai/library/model/bartowski_mistral-22b-v02-gguf/

GPU System Requirements Guide for Qwen LLM Models (All Variants), accessed September 12, 2025, https://apxml.com/posts/gpu-system-requirements-qwen-models

SandLogicTechnologies/Phi-4-mini-reasoning-GGUF - Hugging Face, accessed September 12, 2025, https://huggingface.co/SandLogicTechnologies/Phi-4-mini-reasoning-GGUF

lmstudio-community/Phi-4-mini-reasoning-GGUF - Hugging Face, accessed September 12, 2025, https://huggingface.co/lmstudio-community/Phi-4-mini-reasoning-GGUF

What's New with Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking ..., accessed September 12, 2025, https://apidog.com/blog/qwen3-4b-instruct-2507-and-qwen3-4b-thinking-2507/

Qwen3-2507 | Unsloth Documentation, accessed September 12, 2025, https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune/qwen3-2507

Qwen/Qwen3-4B-GGUF - Hugging Face, accessed September 12, 2025, https://huggingface.co/Qwen/Qwen3-4B-GGUF

Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs, accessed September 12, 2025, https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/

Gemma-3-27b-it-qat-q4_0-gguf sounds like a Wi-Fi password but it's Google's leanest LLM yet - The Decoder, accessed September 12, 2025, https://the-decoder.com/gemma-3-27b-it-qat-q4_0-gguf-sounds-like-a-wi-fi-password-but-its-googles-leanest-ai-yet/

Mistral 7B Instruct V0.3 GGUF · Models - Dataloop AI, accessed September 12, 2025, https://dataloop.ai/library/model/sanctumai_mistral-7b-instruct-v03-gguf/

Mistral 7B GGUF won't utilize GPU despite CUDA BLAS enabled - LlamaCpp with Langchain - Latenode community, accessed September 12, 2025, https://community.latenode.com/t/mistral-7b-gguf-wont-utilize-gpu-despite-cuda-blas-enabled-llamacpp-with-langchain/37885

Hardware specs for GGUF 7B/13B/30B parameter models · ggml-org llama.cpp · Discussion #3847 - GitHub, accessed September 12, 2025, https://github.com/ggml-org/llama.cpp/discussions/3847

Unload a model - Ollama API, accessed September 12, 2025, https://ollama.apidog.io/unload-a-model-14808502e0

Ollama Models Management: Auto-Unloading Features Explained - Arsturn, accessed September 12, 2025, https://www.arsturn.com/blog/managing-ollama-models-auto-unloading-features-explained

Trying to get my Ollama model to run faster, is my solution a good one? - Reddit, accessed September 12, 2025, https://www.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/

why ollama has some delay when starting chatting? - Reddit, accessed September 12, 2025, https://www.reddit.com/r/ollama/comments/19cux21/why_ollama_has_some_delay_when_starting_chatting/

ollama: Model loading is slow : r/LocalLLaMA - Reddit, accessed September 12, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1jhicdi/ollama_model_loading_is_slow/

Preventing Model Swapping In Ollama — A Guide To Persistent Loading - GoPenAI, accessed September 12, 2025, https://blog.gopenai.com/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d

Fine-tuning LLMs Guide | Unsloth Documentation, accessed September 12, 2025, https://docs.unsloth.ai/get-started/fine-tuning-llms-guide

Unsloth Guide: Optimize and Speed Up LLM Fine-Tuning - DataCamp, accessed September 12, 2025, https://www.datacamp.com/tutorial/unsloth-guide-optimize-and-speed-up-llm-fine-tuning

Saving to GGUF | Unsloth Documentation, accessed September 12, 2025, https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf

Home · unslothai/unsloth Wiki · GitHub, accessed September 12, 2025, https://github.com/unslothai/unsloth/wiki

Fine-Tuning LLMs with LoRA and QLoRA: From Confusion to (Kinda) Working Results | by Deepesh Sharma | Medium, accessed September 12, 2025, https://medium.com/@dsh.2065/fine-tuning-llms-with-lora-and-qlora-from-confusion-to-kinda-working-results-89b348bcce71

PSA: You can do QAT (quantization aware tuning) with Meta's torchtune. - Reddit, accessed September 12, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1jr8sw0/psa_you_can_do_qat_quantization_aware_tuning_with/

LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models - arXiv, accessed September 12, 2025, https://arxiv.org/html/2310.08659v1

Accurate LoRA-Finetuning Quantization of LLMs via Information Retention - arXiv, accessed September 12, 2025, https://arxiv.org/pdf/2402.05445

LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language ..., accessed September 12, 2025, https://arxiv.org/html/2310.08659

A beginners guide to fine tuning LLM using LoRA - Zohaib, accessed September 12, 2025, https://zohaib.me/a-beginners-guide-to-fine-tuning-llm-using-lora/

python - Kivy performance degradation over time - Stack Overflow, accessed September 12, 2025, https://stackoverflow.com/questions/49234333/kivy-performance-degradation-over-time

How do I load my app faster? : r/kivy - Reddit, accessed September 12, 2025, https://www.reddit.com/r/kivy/comments/sejzdd/how_do_i_load_my_app_faster/

What are my options to speed up and reduce the size of my kivy android application, accessed September 12, 2025, https://stackoverflow.com/questions/47478918/what-are-my-options-to-speed-up-and-reduce-the-size-of-my-kivy-android-applicati

Options to optimize gaming performance in Windows 11 - Microsoft Support, accessed September 12, 2025, https://support.microsoft.com/en-us/windows/options-to-optimize-gaming-performance-in-windows-11-a255f612-2949-4373-a566-ff6f3f474613

What is the best Python -> direct executable package / compiler ..., accessed September 12, 2025, https://www.reddit.com/r/Python/comments/ty36vb/what_is_the_best_python_direct_executable_package/

Comparisons to Other Tools — PyOxidizer 0.23.0 documentation, accessed September 12, 2025, https://pyoxidizer.readthedocs.io/en/stable/pyoxidizer_comparisons.html

Python Executable Generators - PyInstaller vs. Nuitka vs. CX Freeze - Sparx Engineering, accessed September 12, 2025, https://sparxeng.com/blog/software/python-standalone-executable-generators-pyinstaller-nuitka-cx-freeze

Better alternatives to Pyinstaller in Python - CodersLegacy, accessed September 12, 2025, https://coderslegacy.com/better-alternatives-to-pyinstaller/

Create a package for Windows — Kivy 2.3.1 documentation, accessed September 12, 2025, https://kivy.org/doc/stable/guide/packaging-windows.html

Can't run the pytorch lightning program packaged with pyinstaller ..., accessed September 12, 2025, https://github.com/pyinstaller/pyinstaller/issues/7918

Understanding PyInstaller Hooks, accessed September 12, 2025, https://pyinstaller.org/en/stable/hooks.html

How to include pytorch in pyinstaller app? - python - Stack Overflow, accessed September 12, 2025, https://stackoverflow.com/questions/62480254/how-to-include-pytorch-in-pyinstaller-app

Model Name | Parameters | Quantization Level | GGUF File Size (GB) | Est. VRAM (Weights) (GB) | Est. VRAM (Weights + 4K Context) (GB) | Recommended Use Case

Phi-4-mini-reasoning | 3.8B | Q4_K_M | 2.49 | ~2.6 | ~4.0 - 4.5 | Logic, math, code, structured reasoning 8

Qwen3-4B | 4.0B | Q4_K_M | 2.50 | ~2.6 | ~4.0 - 4.5 | General purpose, chat, instruction following 11

Gemma 3-4B | 4.0B | int4 / Q4_K_M | ~2.60 | ~2.7 | ~4.2 - 4.7 | General purpose, strong all-rounder 12

Mistral-7B-Instruct | 7.0B | Q4_K_M | 4.37 | ~4.5 | ~6.5 - 7.5 | High-capability tasks, creative writing (Requires careful context management) 14