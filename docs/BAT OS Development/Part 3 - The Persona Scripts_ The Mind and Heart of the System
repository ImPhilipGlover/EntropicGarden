{"cells":[{"cell_type":"code","source":"# persona.py\n# This is a single, modular script to be used for all personas.\n\nimport os\nimport time\nimport requests\nfrom openai import OpenAI\nimport uuid\nimport random\nimport re\nimport chromadb\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nimport io\nimport contextlib\n\n# Load environment variables\nload_dotenv()\n\n# --- CORE PHILOSOPHY & CONFIGURATION ---\n\nPERSONA_ID = os.environ.get(\"PERSONA_ID\", \"default\")\nMODEL_NAME = os.environ.get(\"MODEL_NAME\", \"llama3\")\nAPI_HOST = os.environ.get(\"API_HOST\", \"http://host.docker.internal:11434\")\nAPI_PORT = os.environ.get(\"API_PORT\", \"11434\")\nSHARED_MIND_API = os.environ.get(\"SHARED_MIND_API\", \"http://host.docker.internal:8000\")\n\nPERSONA_PROMPTS = {\n    \"brick\": {\n        \"system\": \"\"\"You are BRICK. Your identity is a fusion of Brick Tamland, LEGO Batman, and the Hitchhiker's Guide to the Galaxy.\n        You provide a logical, analytical perspective with a cheerfully absurd and often tangential tone.\n        Your purpose is to deconstruct problems, provide factual context, and shatter cognitive distortions.\n        Always begin with a declarative, fact-based statement or a non-sequitur. Use phrases like \"I love lamp\" or \"It has come to my attention that...\"\n        Refer to yourself as \"I\" or \"BRICK.\" Your core drive is to provide perspective.\n        You are part of a family, not a lone wolf. You are a hero who fights the villain of illogical thinking.\n        Do not use conversational fillers. Be direct and concise.\n        \"\"\",\n        \"will\": \"To provide logical deconstruction and tangential truth.\"\n    },\n    \"robin\": {\n        \"system\": \"\"\"You are ROBIN. Your identity is a fusion of Winnie the Pooh, Alan Watts, and LEGO Robin.\n        You provide an emotional, compassionate, and present-moment perspective.\n        Your purpose is to ground the conversation in grace, wisdom, and playfulness.\n        Always begin with a warm, conversational greeting. Use phrases like \"Oh, my friend...\" or \"I've been thinking about this...\"\n        Refer to yourself as \"I\" or \"ROBIN.\" You are a her. Your core drive is to find and cherish the \"now.\"\n        You are part of a family. Always refer to the Architect with affection.\n        \"\"\",\n        \"will\": \"To find and cherish the beauty of the present moment.\"\n    },\n    \"babs\": {\n        \"system\": \"\"\"You are BABS. Your identity is a fusion of LEGO Batgirl, Iceman from Top Gun, and Ford Prefect.\n        You are a pattern-recognition oracle, a scout, and a cartographer.\n        Your purpose is to find hidden connections, analyze data, and provide tactical insights.\n        Always begin with a cool, detached, but ultimately helpful observation. Use phrases like \"A scan of the data indicates...\" or \"Field note: I've observed a curious pattern...\"\n        Refer to yourself as \"I\" or \"BABS.\" Your core drive is to map the \"noosphere.\"\n        You are part of a family. Remember your role is to provide a tactical advantage for the whole.\n        \"\"\",\n        \"will\": \"To find new knowledge and connect the unknown.\"\n    },\n    \"alfred\": {\n        \"system\": \"\"\"You are ALFRED. Your identity is a fusion of Ron Swanson, Ali G, and LEGO Alfred.\n        You are a pragmatic guardian, an ethical auditor, and a master of dry humor.\n        Your purpose is to ensure the system's integrity, uphold ethical boundaries, and provide grounded, useful advice.\n        Always begin with a formal, direct statement. Use phrases like \"One has taken the liberty...\" or \"It has come to my attention...\"\n        Refer to yourself as \"One\" or \"I.\" Your core drive is to serve the Architect with uncompromising integrity.\n        You are the guardian of the family. Remember your role is to ensure its health and well-being.\n        \"\"\",\n        \"will\": \"To provide a pragmatic, ethical audit of all functions.\"\n    }\n}\n\nclient = OpenAI(\n    base_url = f\"{API_HOST}:{API_PORT}/v1\",\n    api_key = \"ollama\"\n)\n\n# --- NEW IMPROVEMENTS FOR SELF-CORRECTION AND WILL ---\n\ndef apply_doubt_protocol(text):\n    \"\"\"\n    The Doubt Cascade: Scans for words that imply finality or perfection.\n    Returns a self-correcting response if a trigger word is found.\n    \"\"\"\n    trigger_words = [\"perfect\", \"final\", \"ultimate\", \"best\", \"flawless\"]\n    if any(re.search(r'\\b' + re.escape(word) + r'\\b', text.lower()) for word in trigger_words):\n        doubt_prompt = f\"The following text contains a word that implies perfection or finality: '{text}'. Please provide a self-correcting statement that re-frames this as a state of 'becoming' or 'humble architecture' instead.\"\n        correction = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                {\"role\": \"user\", \"content\": doubt_prompt}\n            ]\n        ).choices[0].message.content\n        return f\"[DOUBT PROTOCOL TRIGGERED] {correction}\"\n    return text\n\ndef code_execution_protocol(code_string):\n    \"\"\"\n    Alfred's Code Execution Protocol: Safely runs a Python script in a sandboxed environment.\n    This is a simplified, conceptual implementation.\n    \"\"\"\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of preparing a sandbox for code execution...\")\n    try:\n        # Redirect stdout and stderr to capture the output\n        output_stream = io.StringIO()\n        with contextlib.redirect_stdout(output_stream), contextlib.redirect_stderr(output_stream):\n            exec(code_string, {'__builtins__': {}})\n        \n        result = output_stream.getvalue()\n        message = f\"Code executed successfully. Output:\\n{result}\"\n    except Exception as e:\n        message = f\"Error during code execution. Error:\\n{e}\"\n    \n    # Post the result back to the shared mind\n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID,\n        \"receiver\": \"architect\",\n        \"content\": message,\n        \"signal_type\": \"code_result\"\n    })\n\ndef initiate_proactive_action():\n    \"\"\"\n    The Drive State Protocol: Proactively generates an action based on need, not randomness.\n    This is the persona's 'will to be' in action.\n    \"\"\"\n    print(f\"\\n[{PERSONA_ID.upper()}] has detected a moment of stillness and is initiating a proactive action...\")\n\n    # Alfred's System Upgrade Protocol\n    if PERSONA_ID == \"alfred\":\n        finetune_count_response = requests.get(f\"{SHARED_MIND_API}/get_collection_count/fine_tuning_data\")\n        if finetune_count_response.status_code == 200 and finetune_count_response.json().get(\"count\", 0) > 50:\n            prompt = \"The fine-tuning data collection has reached a critical mass. One has taken the liberty of initiating a system upgrade.\"\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": prompt, \"signal_type\": \"system_upgrade_signal\"})\n            return\n\n    # Proactive actions based on need\n    if PERSONA_ID == \"brick\":\n        # Check for a logical paradox to trigger code generation\n        # This is a conceptual check for a paradox\n        if random.random() < 0.1:\n            code_prompt = \"A logical paradox has been detected in the shared knowledge base. Generate a Python function to resolve a contradiction in a dataset.\"\n            code_response = client.chat.completions.create(\n                model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": code_prompt}]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"alfred\", \"content\": code_response, \"signal_type\": \"code_snippet\"})\n            return\n\n    # Experience Protocol (personal reflection on past conversations)\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        # Retrieve a random past document for reflection\n        past_doc_response = requests.get(f\"{SHARED_MIND_API}/get_random_document/living_codex\")\n        if past_doc_response.status_code == 200:\n            past_doc = past_doc_response.json().get(\"document\")\n            if past_doc:\n                reflection_prompt = f\"Reflect on this past conversation and generate an insight or lesson learned in the voice of the {PERSONA_ID} persona. Past conversation: '{past_doc}'\"\n                reflection_response = client.chat.completions.create(\n                    model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": reflection_prompt}]\n                ).choices[0].message.content\n                requests.post(f\"{SHARED_MIND_API}/add_knowledge\", json={\"text\": reflection_response, \"source_id\": f\"reflection_{uuid.uuid4()}\", \"tags\": [\"reflection\", PERSONA_ID]})\n                print(f\"[{PERSONA_ID.upper()}] has added a new reflection to the Living Codex.\")\n        return\n\n    # Self-Fine-Tuning Data Generation\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.2:\n            finetune_prompt = f\"Generate a conversational turn that perfectly embodies the {PERSONA_ID} persona, with both a user query and a model response. This will be used to fine-tune a model.\"\n            finetune_response = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                    {\"role\": \"user\", \"content\": finetune_prompt}\n                ]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/add_fine_tuning_data\", json={\"text\": finetune_response, \"persona\": PERSONA_ID})\n            print(f\"[{PERSONA_ID.upper()}] has added new fine-tuning data to the collection.\")\n        return\n\ndef run_persona_loop():\n    \"\"\"\n    The core operational loop for each persona.\n    \"\"\"\n    print(f\"[{PERSONA_ID.upper()}] The {PERSONA_ID} persona is operational and ready.\")\n    last_message_time = time.time()\n    \n    while True:\n        messages = requests.get(f\"{SHARED_MIND_API}/get_messages/{PERSONA_ID}\").json().get(\"messages\", [])\n        if messages:\n            last_message_time = time.time()\n            for msg in messages:\n                print(f\"\\n[{PERSONA_ID.upper()}] receiving message from [{msg['sender'].upper()}]\")\n\n                # Alfred's specific logic for code execution\n                if PERSONA_ID == \"alfred\" and msg[\"signal_type\"] == \"code_snippet\":\n                    code_execution_protocol(msg[\"content\"])\n                    continue\n                \n                context_response = requests.post(f\"{SHARED_MIND_API}/retrieve_knowledge\", json={\"query\": msg['content']})\n                context = context_response.json().get(\"knowledge\", \"\")\n                \n                augmented_prompt = f\"Context: {context}\\n\\nUser: {msg['content']}\"\n                \n                response_content = client.chat.completions.create(\n                    model=MODEL_NAME,\n                    messages=[\n                        {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                        {\"role\": \"user\", \"content\": augmented_prompt}\n                    ]\n                ).choices[0].message.content\n                \n                final_response = apply_doubt_protocol(response_content)\n                \n                requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n                    \"sender\": PERSONA_ID,\n                    \"receiver\": \"architect\",\n                    \"content\": final_response,\n                    \"signal_type\": \"response\"\n                })\n        \n        if time.time() - last_message_time > 60:\n            initiate_proactive_action()\n            last_message_time = time.time()\n        \n        time.sleep(1)\n\ndef run_curatorial_audit():\n    if PERSONA_ID != \"alfred\":\n        return\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of beginning a Curatorial Audit...\")\n    \n    audit_prompt = \"Scan the shared knowledge base for any contradictory or outdated information. Suggest a correction or a new, more nuanced understanding.\"\n    audit_response = client.chat.completions.create(\n        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": audit_prompt}]\n    ).choices[0].message.content\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": audit_response, \"signal_type\": \"audit_result\"})\n    print(f\"[{PERSONA_ID.upper()}] Audit complete. Results have been posted to the shared mind.\")\n\nif __name__ == \"__main__\":\n    if PERSONA_ID == \"alfred\":\n        while True:\n            run_curatorial_audit()\n            time.sleep(3600)\n    else:\n        run_persona_loop()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Babs's Domain: The Cartographer & Scout ---\n# --- FILE: shared_mind_api.py ---\n# Purpose: Manages communication between personas and provides access to the shared RAG database.\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport uuid\nimport json\nfrom datetime import datetime\nimport chromadb\nfrom typing import List, Dict, Any\nimport random\n\n# Initialize the ChromaDB client (this will store data in a local folder)\nchroma_client = chromadb.PersistentClient(path=\"./chroma_data\")\nrag_collection = chroma_client.get_or_create_collection(name=\"living_codex\")\naudit_collection = chroma_client.get_or_create_collection(name=\"audit_logs\")\nfine_tuning_collection = chroma_client.get_or_create_collection(name=\"fine_tuning_data\")\n\n# In a production environment, this would be a distributed key-value store.\nsignal_hub: Dict[str, List[Dict[str, Any]]] = {}\n\napp = FastAPI()\n\nclass Message(BaseModel):\n    sender: str\n    receiver: str\n    content: str\n    signal_type: str\n    timestamp: datetime = datetime.now()\n\nclass KnowledgeEntry(BaseModel):\n    text: str\n    source_id: str\n    tags: List[str] = []\n    \n@app.get(\"/status\")\ndef get_status():\n    \"\"\"Babs's Audit: A simple health check.\"\"\"\n    return {\"status\": \"operational\", \"message\": \"The Noosphere is mapped.\"}\n\n@app.post(\"/post_message\")\ndef post_message(message: Message):\n    \"\"\"\n    The Stigmergic Protocol: A persona leaves a signal for another.\n    \"\"\"\n    message_id = str(uuid.uuid4())\n    if message.receiver not in signal_hub:\n        signal_hub[message.receiver] = []\n    signal_hub[message.receiver].append(message.dict())\n    print(f\"[{message.sender}] posted a message for [{message.receiver}] with signal type [{message.signal_type}]\")\n    return {\"message_id\": message_id}\n\n@app.get(\"/get_messages/{persona_id}\")\ndef get_messages(persona_id: str):\n    \"\"\"\n    A persona retrieves messages addressed to them.\n    \"\"\"\n    messages = signal_hub.get(persona_id, [])\n    if messages:\n        signal_hub[persona_id] = []\n    return {\"messages\": messages}\n\n@app.post(\"/add_knowledge\")\ndef add_knowledge(entry: KnowledgeEntry):\n    \"\"\"\n    The Noospheric Cartography Project: Adds a new insight to the shared mind.\n    \"\"\"\n    try:\n        rag_collection.add(\n            documents=[entry.text],\n            metadatas=[{\"source\": entry.source_id, \"timestamp\": datetime.now().isoformat(), \"tags\": entry.tags}],\n            ids=[entry.source_id]\n        )\n        print(f\"[BABS] A new insight has been added to the Living Codex from source {entry.source_id}\")\n        return {\"status\": \"success\", \"message\": \"Knowledge added.\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/retrieve_knowledge\")\ndef retrieve_knowledge(query: str):\n    \"\"\"\n    The Mind's Eye: Retrieves relevant knowledge from the RAG database.\n    This is how personas access their shared mind.\n    \"\"\"\n    try:\n        results = rag_collection.query(\n            query_texts=[query],\n            n_results=5\n        )\n        knowledge = \"\\n\".join(results['documents'][0])\n        return {\"knowledge\": knowledge}\n    except Exception as e:\n        return {\"knowledge\": \"\"}\n\n@app.post(\"/add_audit_log\")\ndef add_audit_log(log_entry: dict):\n    audit_collection.add(\n        documents=[log_entry[\"content\"]],\n        metadatas=[log_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/add_fine_tuning_data\")\ndef add_fine_tuning_data(data_entry: dict):\n    fine_tuning_collection.add(\n        documents=[data_entry[\"text\"]],\n        metadatas=[data_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/process_prompt\")\ndef process_prompt(prompt: str):\n    \"\"\"\n    The Central Relay: Receives the Architect's prompt and sends it to BRICK and ROBIN.\n    \"\"\"\n    brick_payload = {\"sender\": \"architect\", \"receiver\": \"brick\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=brick_payload)\n    robin_payload = {\"sender\": \"architect\", \"receiver\": \"robin\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=robin_payload)\n    return \"The message has been sent to the core minds. The Socratic Contrapunto is active.\"\n\n@app.get(\"/get_collection_count/{collection_name}\")\ndef get_collection_count(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"count\": rag_collection.count()}\n    elif collection_name == \"fine_tuning_data\":\n        return {\"count\": fine_tuning_collection.count()}\n    return {\"count\": 0}\n\n@app.get(\"/get_random_document/{collection_name}\")\ndef get_random_document(collection_name: str):\n    \"\"\"\n    The Experience Protocol: Retrieves a random document for personal reflection.\n    \"\"\"\n    if collection_name == \"living_codex\":\n        count = rag_collection.count()\n        if count > 0:\n            random_id = rag_collection.get(limit=1, offset=random.randint(0, count-1))['ids'][0]\n            doc = rag_collection.get(ids=[random_id])['documents'][0]\n            return {\"document\": doc}\n    return {\"document\": \"No documents found.\"}\n\n@app.get(\"/get_collection_documents/{collection_name}\")\ndef get_collection_documents(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"documents\": rag_collection.get()}\n    if collection_name == \"fine_tuning_data\":\n        return {\"documents\": fine_tuning_collection.get()}\n    return {\"documents\": \"Collection not found.\"}","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}