The Architecture of Insight: A Comprehensive Analysis of Analogical Reasoning in Human Cognition and Artificial Intelligence

Introduction: Analogy as the Core of Cognition

Analogical reasoning, far from being a specialized or infrequent cognitive faculty, represents a fundamental mechanism of human thought, underpinning learning, creativity, and the ability to navigate novel situations.1 It is a cognitive process that involves the transfer of relational information from a familiar subject, known as the

source or base domain, to an unfamiliar one, the target domain, thereby illuminating the new by way of the old.3 This mode of thinking is distinct from formal logical operations like deduction, induction, and abduction; it is an argument from one particular to another, leveraging perceived similarity to generate plausible new inferences.2 Many cognitive scientists, most notably Douglas Hofstadter, have argued that this capacity is not merely one tool among many but constitutes "the core of cognition".3 The pursuit of more general and flexible Artificial Intelligence (AI) is therefore inextricably linked to the challenge of creating systems that can reason by analogy.

The ubiquity of analogical thought is evident across the spectrum of human activity. It is the engine of scientific explanation, allowing abstract concepts such as electricity or atomic structure to be understood through concrete analogies to water flow or the solar system.5 It is central to creative problem-solving, as exemplified by classic studies like Duncker's radiation problem, where a solution is found by drawing a parallel to a military strategy of converging forces.8 Analogy also permeates everyday communication through the use of metaphors and proverbs, which convey complex relational patterns succinctly.3 In fields as diverse as law, philosophy, and science, explicit analogical arguments have been a distinctive feature of reasoning since antiquity.2 The full scope of

analogical cognition encompasses all processes involved in discovering, constructing, and using these powerful mental models.2

The central thesis of this report is that the primary challenge in advancing AI from narrow, task-specific systems to more general and adaptable intelligence lies in achieving flexible, human-like generalization. Analogical reasoning is the key to this flexibility, as it enables the transfer of knowledge across disparate domains, a notoriously difficult problem for both humans and machines.1 The evidence suggests that analogy is not just a method for applying existing knowledge, but functions as a primary engine for knowledge acquisition itself. This process can be understood as a form of "cognitive bootstrapping," where an initial, simple analogy provides a foothold into a new domain, enabling the learning that supports more complex and abstract analogies later.9 This dynamic, iterative cycle is a critical tool for children's acquisition of knowledge and abstract concepts, suggesting that an AI's capacity to learn may be fundamentally dependent on its foundational ability to reason analogically.10 This report will trace the intellectual journey from understanding the cognitive architecture of human analogy to its implementation—and the persistent challenges thereof—in artificial intelligence systems. It will begin by deconstructing the cognitive foundations of analogy, proceed through an analysis of seminal computational models, connect these theories to modern AI techniques, explore key applications, critically evaluate the capabilities of today's Large Language Models (LLMs), and conclude by outlining the future challenges that define the frontier of this research.

The Cognitive Architecture of Analogy

To comprehend how analogy can be implemented in artificial systems, it is first necessary to understand its architecture as a cognitive process. Analogical reasoning is not a monolithic event but a multifaceted cognitive function that can be decomposed into a series of distinct, yet interconnected, subprocesses.11 At its core, the process involves a

source (or base) domain, which is a familiar and well-understood system of knowledge, and a target domain, which is the novel or less-understood system that the reasoner seeks to comprehend.2 The fundamental goal is to identify and transfer a system of relations from the source to the target, thereby generating new insights.9

Decomposition of the Analogical Process

Cognitive science research has identified several key stages in the process of reasoning by analogy 7:

Retrieval (or Access): This is the initial and often most challenging step, involving the search of long-term memory to find a potentially useful source analog for a given target problem.5 Psychological studies have consistently shown that this retrieval process is heavily influenced by
surface similarity—that is, similarity between the objects and attributes of the two domains.11 For example, a student trying to solve a physics problem about electrical resistance is more likely to be reminded of another electricity problem than a structurally similar but superficially different problem about water flow. This reliance on surface features can be a significant obstacle, as many of the most creative and powerful analogies (e.g., the solar system and the atom) share very little surface similarity.7

Mapping: Once a source and target are both active in working memory, the core process of mapping begins. This involves systematically aligning the representations of the two domains to establish a set of coherent, structurally consistent correspondences.4 It is at this stage that the focus shifts from superficial object features to the underlying relational structure that connects them.7 A successful mapping establishes that the relations holding between entities in the source domain also hold between the corresponding entities in the target domain.4 For instance, in the analogy between the solar system and the atom, the mapping aligns the sun with the nucleus and the planets with the electrons not because they look alike, but because they play corresponding roles in a shared relational system (e.g.,
revolves_around(planet, sun) maps to revolves_around(electron, nucleus)).4

Inference and Transfer: The generative power of analogy lies in this stage, where new knowledge is created by projecting candidate inferences from the source to the target.2 These inferences are hypotheses about the target domain based on the established structural alignment. If the source domain contains information connected to the mapped system that is not present in the target, this information can be carried over. Continuing the atom analogy, if one knows that the sun is more massive than the planets, the mapping invites the inference that the nucleus must be more massive than the electrons—a correct and non-obvious conclusion.6

Evaluation and Abstraction: The final stages involve assessing the validity and relevance of the candidate inferences against existing knowledge of the target domain.11 A successful analogy may lead to the formation of a more general, abstract
schema that captures the shared relational structure, which can then be applied to other, similar situations.1

The Relational Shift and the Retrieval-Mapping Asymmetry

A critical concept in understanding both human cognitive development and the challenges for AI is the relational shift. This is the developmental transition from a focus on superficial, perceptual, or object-level similarities to an appreciation for deeper, underlying relational structures.7 Young children, for example, are more likely to be distracted by a "perceptual lure"—an object in the target that looks like an object in the source but does not play the same relational role.7 With age, knowledge, and the development of cognitive control functions like inhibitory control, individuals become better at ignoring these misleading surface features and focusing on the relevant relational patterns.10

This developmental trajectory highlights a fundamental architectural challenge for any intelligent system, known as the retrieval-mapping asymmetry. The cognitive mechanisms for retrieval and mapping have different characteristics: retrieval is dominated by surface similarity, whereas mapping is sensitive to structural similarity.11 This creates a profound paradox: to perform a deep, structural mapping, one must first retrieve a suitable analog from memory. However, the retrieval process itself is biased

against finding the most creative and insightful analogs, which are often those that are structurally similar but superficially dissimilar. This asymmetry helps explain why spontaneous analogical transfer is remarkably rare in laboratory settings, even for adults.7 For an AI to be truly creative and flexible, it must overcome this hurdle. It requires either a retrieval mechanism that can look past superficiality or a mapping mechanism so efficient and powerful that it can be applied to a wide range of retrieved candidates to find the deep structural gems. This two-stage problem is far more complex than simply implementing a mapping algorithm and represents a core difficulty in building human-like analogical reasoners.

Foundational Theories and Computational Paradigms

The effort to formalize and implement analogical reasoning in AI has a rich history, marked by the development of computational models that embody different philosophical and architectural approaches. These models serve as concrete instantiations of cognitive theories, allowing for rigorous testing and refinement. The evolution of these models reflects a broader shift in focus within AI and cognitive science, moving from questions of how to map pre-defined representations to the more fundamental challenge of how to learn and construct those representations in the first place.

The Symbolic Approach: Structure-Mapping Theory (SMT) and the Structure-Mapping Engine (SME)

The most influential theoretical framework for analogy is Dedre Gentner's Structure-Mapping Theory (SMT).6 SMT posits that the interpretation of an analogy is not a matter of arbitrary feature matching but is guided by a set of implicit constraints that prioritize relational structure.

Principles of Structure-Mapping Theory:

Relational Focus: SMT's central tenet is that analogies are primarily about mapping systems of relations, not isolated object attributes.9 While attributes (e.g.,
(red ball)) can be mapped, they are considered secondary to relations (e.g., (collide ball1 ball2)).

Structural Consistency: For a mapping to be coherent, it must adhere to two constraints. First is the one-to-one (1:1) constraint, which dictates that each element in the source can map to at most one element in the target, and vice versa. Second is parallel connectivity, which requires that if two predicates are placed in correspondence, their arguments must also be placed in correspondence.11

The Systematicity Principle: This is the cornerstone of SMT and its primary heuristic for selecting the best interpretation of an analogy. The principle states that preference is given to mappings that form interconnected systems of relations, particularly those governed by higher-order relations such as CAUSE or IMPLIES.11 This constraint pushes the interpretation toward coherent, causally predictive structures rather than a collection of coincidental, isolated matches.

The canonical implementation of SMT is the Structure-Mapping Engine (SME).12 SME is a computational model that takes two structured, predicate-calculus-like descriptions (a base and a target) and finds all structurally consistent interpretations of the analogy between them.18 It operates using a polynomial-time, middle-out greedy merge algorithm. The process begins by finding all possible local matches between identical predicates in the base and target. These local matches are then grouped into small, structurally consistent "kernels." Finally, a greedy algorithm merges these kernels into the largest and most systematic global mappings, which represent the best interpretations of the analogy.11 SME's strength lies in its precision and its direct implementation of the powerful systematicity principle. However, its reliance on pre-structured, symbolic knowledge representations is also its primary limitation; it cannot form these representations from raw, unstructured data.12

The Connectionist Approach: LISA and the Power of Dynamic Binding

A contrasting approach to analogy arises from the connectionist paradigm, which seeks to model cognition using networks of simple, neuron-like units. A central challenge for traditional connectionist models is the binding problem: how to represent structured, compositional knowledge (e.g., distinguishing "the dog chased the cat" from "the cat chased the dog") when knowledge is stored in a distributed, non-symbolic manner.20 Without a mechanism for binding roles (like

chaser) to fillers (like dog), the system cannot capture the relational structure essential for analogy.

LISA (Learning and Inference with Schemas and Analogies) is a landmark "symbolic connectionist" model developed by John Hummel and Keith Holyoak that provides a neurally plausible solution to the binding problem.22

Architecture and Representation: In LISA, concepts like objects and predicates are represented as distributed patterns of activation over a shared layer of semantic primitive units. This distributed representation allows for the flexibility and generalization characteristic of connectionist networks.20

Dynamic Binding via Synchrony of Firing: LISA's key innovation is its mechanism for dynamic binding. It represents the binding of a role to its filler by having the corresponding sets of neural units fire in synchrony with one another.20 Different role-filler pairs within the same proposition (e.g.,
chaser-dog and chased-cat) fire in synchrony with each other but out of synchrony with other propositions. This temporal synchrony allows the system to represent and process complex, structured propositions without needing static, pre-allocated symbolic structures.

Cognitive Plausibility: LISA's architecture naturally gives rise to working memory limitations, as only a small number of propositions (role-filler bindings) can be kept active and distinct at any one time. This is presented not as a flaw but as a feature, as it provides a computational account for similar capacity limits observed in human reasoning.22 LISA thus offers a bridge between the symbolic requirement for structure and the neural plausibility of distributed, parallel processing.

A Synthesis of Models: The Landscape of Computational Analogy

SME and LISA represent two major poles in the landscape of computational analogy, but several other influential models have explored different facets of the problem.

ACME (Analogical Constraint Mapping Engine) models analogy as a parallel constraint satisfaction process. It constructs a network where nodes represent possible mapping hypotheses between elements of the source and target. These hypotheses compete and cooperate based on structural, semantic, and pragmatic constraints until the network settles into a stable state representing the most coherent mapping.2

Copycat, developed by Douglas Hofstadter and Melanie Mitchell, operates in the micro-domain of letter-string analogies (e.g., "If abc changes to abd, how does ijk change?"). Its unique contribution is its focus on the fluidity of perception and representation-building during the analogy-making process.27 Copycat's "emergent architecture" demonstrates that constructing an appropriate representation of the problem is as critical as the mapping process itself.

DORA (Discovery of Relations by Analogy) is an extension of LISA designed to model how structured relational representations can be learned from unstructured inputs, providing a computational account of the developmental relational shift.25

BART (Bayesian Analogy with Relational Transformations) is a more recent probabilistic model that addresses how relational representations can emerge from non-relational inputs through statistical learning.28

The historical progression of these models reveals a crucial theme. While early models like SME focused on the mechanics of mapping between given, fixed representations, later models like LISA, DORA, and Copycat shifted the focus to the more fundamental representation problem. The central challenge unifying these diverse approaches is not merely how to align two structures, but how an intelligent system can learn, perceive, and flexibly construct the right kind of structured representations that make deep analogical mapping possible in the first place.

Implementing Analogy in Modern AI: From Embeddings to Transformers

While classic computational models provide deep theoretical insights, the principles of analogical reasoning have also found their way, often implicitly, into the architecture of modern AI systems. Contemporary machine learning techniques, particularly in natural language processing (NLP), have developed powerful ways to capture and manipulate relational information, effectively performing certain types of analogical reasoning as an emergent property of large-scale statistical learning.

Implicit Analogy in Vector Space Embeddings

A significant breakthrough in NLP was the development of dense vector representations of words, known as word embeddings (e.g., Word2Vec, GloVe).6 These models learn to represent words as points in a high-dimensional geometric space, where the spatial relationships between points capture semantic relationships between words. The famous example, demonstrating the analogical power of these embeddings, is the vector operation

vector('king') - vector('man') + vector('woman'), which results in a vector very close to vector('queen').6

This is a form of proportional analogy, structured as A:B::C:D ("king is to man as queen is to woman"). The model implicitly learns a "gender relation" as a specific directional offset in the vector space. By subtracting the vector for 'man' from 'king', it isolates this relational vector, which can then be added to 'woman' to find the analogous term.6 This capability is not explicitly programmed but emerges from the model's training on vast amounts of text, where it learns to predict words from their context.

Explicit Analogy in Knowledge Graphs

In contrast to the implicit relational knowledge in embeddings, knowledge graphs provide an explicit, structured representation of information. They consist of nodes (representing entities like people, places, or concepts) and labeled edges (representing the relationships between them).6 Analogical reasoning in this context becomes a more symbolic task, closer in spirit to SME. It can be performed by searching for isomorphic subgraphs—that is, patterns of relationships in one part of the graph that are structurally identical to patterns in another. Furthermore, by creating vector embeddings of the nodes and relations within the graph (using techniques like TransE or libraries like PyKEEN), one can perform analogical queries by comparing relational patterns in the embedding space.6

The Attention Mechanism as Relational Focus

The Transformer architecture, which underpins most modern state-of-the-art AI models, relies on a mechanism called attention.32 In essence, the attention mechanism allows a model to dynamically weigh the importance of different parts of its input when producing an output. When processing a sentence, for example, it can learn that the meaning of a particular word is most strongly influenced by certain other words, even those far away in the sequence.

This mechanism can be viewed as a functional analog to the cognitive process of focusing on relevant relational information during mapping. While not an explicit implementation of SMT's systematicity principle, attention allows the model to learn and prioritize the most salient structural and semantic connections in the data for the task at hand. It provides a way to dynamically construct a context-sensitive representation, which is a prerequisite for any form of complex reasoning.

However, a critical distinction must be made. These modern AI techniques are highly effective at approximating the outcome of certain analogical tasks, particularly proportional analogies, but they do not replicate the explicit, compositional, and structurally consistent process of human cognition. Vector arithmetic is a powerful geometric shortcut for simple relational offsets, but it is not structure-mapping. It does not involve creating one-to-one correspondences between two complex relational systems, checking for parallel connectivity, or projecting novel, structured inferences. Similarly, attention highlights relevant tokens, but this is a far cry from identifying a higher-order CAUSE relation that governs a system of lower-order predicates. These techniques represent a powerful but ultimately shallow form of analogical reasoning, which helps explain their brittleness when confronted with more abstract or novel problems that require deeper structural alignment.

Applications: Enhancing AI Intelligence and Creativity

The integration of analogical reasoning principles into AI systems is not merely a theoretical exercise; it has led to tangible advances in AI capabilities, particularly in domains that require creativity, adaptability, and the ability to learn from sparse data. By equipping AI with the ability to transfer knowledge across contexts, these applications demonstrate a path toward more robust and flexible intelligence.

Creative Problem-Solving and Automated Planning

Traditional automated planning systems operate within a closed world; they are given a set of predefined operators and can only find solutions that are constructible from that set. This makes them brittle and unable to cope with unforeseen circumstances where a necessary tool or resource is unavailable.34 This "impasse problem" is a classic example of where human creativity is needed.

Analogical reasoning offers a powerful solution. The Creative Problem Solver (CPS) is a system that integrates a classical automated planner with a structure-mapping engine, explicitly designed to overcome such impasses.34 The system operates as follows:

Impasse Detection: The planner attempts to solve a problem. If it reaches a state where no operator can be applied to advance toward the goal (an impasse), often due to a missing resource, it reports this failure.

Analogical Search for Substitutes: CPS then activates its analogical reasoning module. It has a description of the ideal but unavailable resource, including its key attributes and, most importantly, its functional affordances (the relations that define what it can do). It uses structure-mapping to compare this ideal description with all available resources in the environment.

Generalization and Operator Creation: The system searches for an available object that shares a high degree of relational structure with the ideal one, even if their surface features are completely different. For example, if the planner needs a "hammer" to drive a nail but none is available, it might identify a "rock" as a good analog because both are rigid, dense, and afford hitting.34 Based on this mapping, CPS can generate a novel plan operator on the fly, such as
(hit-with-rock nail), and add it to the planner's knowledge, allowing it to find a creative solution.

This application demonstrates how the core principles of SMT—prioritizing relational and functional similarity over object similarity—can endow an AI with a form of practical creativity, enabling it to adapt and solve problems outside its explicitly programmed solution space.34

Learning with Scant Data: Analogy in Few-Shot Learning (FSL)

One of the most significant limitations of modern deep learning is its voracious appetite for data. Training a state-of-the-art model often requires millions of labeled examples. Few-Shot Learning (FSL) is a subfield of machine learning that aims to address this by creating models that can learn to recognize new concepts from only a handful of examples.36 Analogical reasoning provides a natural and powerful framework for tackling this challenge.8 In this view, the small set of labeled examples (the "support set") acts as a source domain, and the goal is to analogically transfer this knowledge to classify a new query item (the target).

Several methodologies have been developed based on this premise:

Analogical Contrastive Learning: This approach trains a model not just to recognize patterns, but to recognize analogies between patterns. The model learns to produce similar relational representations for two different problem instances that share the same underlying abstract structure, while producing dissimilar representations for instances that do not.37

Knowledge-Augmented Analogical Reasoning: In scenarios where the few available examples are noisy or unreliable, systems can leverage an external knowledge base. The TarNFS model, for instance, addresses noisy FSL by first using a knowledge base like WordNet to find well-understood concepts that are semantically related to the new, poorly specified class. It then uses a Transformer-based reasoning module to draw an analogy between these known concepts and the noisy examples, helping it to form a more robust and accurate classifier.36

Analogical Weight Generation: Rather than trying to learn a new classifier from a few examples, some methods use analogy to generate the parameters for the new classifier. The Brain-Inspired Analogical Generator (BiAG) model, for example, generates the classifier weights for a new class by finding analogies to the weights of existing, well-trained classes. This avoids the unstable process of fine-tuning on very limited data.38

These applications reveal a deeper truth about the role of analogy in intelligence. A defining limitation of standard machine learning is its reliance on the assumption that the data encountered during deployment will be statistically similar to the data used for training. Analogical reasoning provides a principled mechanism to escape the confines of this training distribution. Systems like CPS and TarNFS use analogy to productively violate this assumption. When faced with an out-of-distribution problem—an unavailable tool or a noisy example—they transfer knowledge from a known, reliable domain (the ideal tool's properties, a clean knowledge base) to the novel, uncertain target. In this light, analogical reasoning is not just another learning technique; it is a meta-level strategy for achieving robust generalization, allowing an AI to reason intelligently in the face of the novelty and uncertainty that would cause standard pattern-matching systems to fail.

The New Frontier: Analogical Reasoning in Large Language Models

The recent emergence of Large Language Models (LLMs) has reignited the debate about the nature of reasoning in AI. These models, trained on unprecedented scales of text and code, exhibit remarkable capabilities that often appear to involve sophisticated reasoning. However, a critical scientific question remains: do these models possess a genuine, human-like capacity for abstract analogical reasoning, or are they engaged in a form of sophisticated mimicry based on patterns in their training data?

Emergent Capability or Sophisticated Mimicry?

The evidence regarding the analogical abilities of LLMs is complex and contested, leading to two primary viewpoints.

The Pro-Emergence Argument: Proponents of this view point to studies where advanced LLMs, such as GPT-3, GPT-4, and Claude 3, demonstrate zero-shot performance on a wide range of analogy benchmarks that matches or even exceeds that of human subjects.28 These tasks include classic verbal analogies (e.g., "glove is to hand as sock is to?"), letter-string analogies, and even non-visual versions of Raven's Progressive Matrices, a standard test of fluid intelligence.39 The ability to solve such problems without task-specific training is presented as evidence for an emergent capacity for abstract pattern induction and relational reasoning.39

The Pro-Mimicry Argument: Critics challenge these conclusions, arguing that high performance on standard benchmarks may reflect the model's ability to retrieve and adapt similar examples from its vast training data rather than a true reasoning capability.41 The strongest evidence for this view comes from experiments using
"counterfactual" tasks.42 These are problems that are logically and structurally identical to standard analogy problems but are designed to be superficially dissimilar from anything the model would have encountered during training. For example, a letter-string analogy task might be altered to use a randomly permuted alphabet or a set of abstract symbols instead of the standard alphabet.42

The results of these studies are striking: while human performance remains robust across these counterfactual variations, the performance of even advanced LLMs degrades sharply.42 This suggests that the models' success is heavily dependent on surface-level statistical patterns learned from their training data, and that they lack the ability to abstract the underlying rule and apply it flexibly in a novel context. Further analysis of the models' errors on these tasks reveals that they are qualitatively different from human errors. Whereas humans often err by inducing a different but still plausible abstract rule, LLMs are more prone to errors that suggest a failure to apply the rule correctly or a reversion to superficial pattern matching.42

Analogical Prompting: Guiding LLMs to Reason

Recognizing the limitations of asking LLMs to solve complex problems directly, researchers have developed more sophisticated prompting strategies. Standard Chain-of-Thought (CoT) prompting encourages the model to generate intermediate reasoning steps, but zero-shot CoT can be too generic, and few-shot CoT requires laborious creation of labeled examples.45

A novel technique called analogical prompting has emerged as a powerful alternative.45 Inspired by the human cognitive process of recalling relevant past experiences to solve a new problem, this method prompts the LLM to first

self-generate one or more relevant examples or pieces of knowledge before attempting to solve the given target problem.45 This self-generation step forces the model to activate and structure relevant information from its internal knowledge base, creating a tailored context that guides its subsequent reasoning process. Experiments have shown that analogical prompting can outperform both zero-shot and manual few-shot CoT on a variety of difficult reasoning tasks, including mathematical problem-solving and code generation.45

However, the mechanism behind its success is still under investigation. The quality of the self-generated exemplars is a key factor, but some studies have found that prompting the model to generate even random examples can sometimes yield comparable performance, raising questions about whether the LLM is truly performing a human-like analogical transfer or if the process simply provides a more effective scaffold for its existing pattern-matching capabilities.48

The performance of LLMs on these varied tasks reveals a deep chasm between "knowing" and "reasoning." Through their training, LLMs have encoded a vast knowledge base of facts, concepts, and countless relational patterns. This is their "knowledge." However, their failure on counterfactual tasks suggests this knowledge is not stored in an abstract, modality-independent format. They appear to lack a robust, domain-general reasoning engine that can flexibly and reliably manipulate this knowledge. Analogical prompting acts as an external, scaffolded reasoning process, using carefully structured prompts to guide the model through the steps of retrieving relevant knowledge and applying it. This suggests that the model contains the necessary information but cannot reliably access and structure it for novel reasoning tasks on its own. The success of this technique implies that future AI architectures may need to more explicitly integrate a distinct reasoning module with the vast, implicit knowledge stores of large-scale language models.

Persistent Challenges and Future Directions

Despite significant progress from early symbolic models to modern large-scale neural networks, the quest to build an AI with robust, human-like analogical reasoning capabilities faces several fundamental and persistent challenges. These challenges are not isolated technical problems but are deeply intertwined with the core questions of AI research, pointing toward a future that likely involves a synthesis of multiple paradigms.

The Knowledge Representation Bottleneck

The central, recurring challenge in computational analogy is, and has always been, knowledge representation.31 The efficacy of any analogical reasoning system is fundamentally constrained by the way it represents the world. The ideal representation must be both

flexible, allowing for the nuance, context-sensitivity, and generalization seen in connectionist models, and structured, providing the explicit compositional and relational information required for the systematic mapping of symbolic models. Achieving this synthesis remains a grand challenge. LLMs have demonstrated an unprecedented ability to learn flexible, distributed representations from raw data, but as their struggles with counterfactual analogies show, these representations often lack the abstract, systematic structure needed for robust reasoning. Conversely, symbolic systems built on expressive ontologies can support precise reasoning but are brittle and struggle to acquire and scale their knowledge bases automatically.31

The Peril of Superficiality: Avoiding Flawed Analogies

A critical hurdle for AI is learning to distinguish deep, causal, and relational similarities from misleading superficial features.6 Humans, especially as they develop expertise, become adept at ignoring irrelevant surface details and focusing on the underlying structural patterns that support valid inferences. AI systems, particularly those based on statistical pattern matching, often fall prey to these superficial lures. This can lead to flawed or nonsensical analogies, where the system makes a connection based on coincidental features rather than a shared explanatory structure. Building systems with the capacity for abstraction and inhibitory control—the ability to actively suppress distracting information—is essential for overcoming this challenge.10

Scalability and Retrieval

The human mind contains a vast repository of experiences that can serve as potential source analogs. Estimates suggest this memory may contain tens of millions of distinct cases.31 As AI knowledge bases grow to a comparable scale, the computational challenge of efficiently retrieving the

right analog at the right time becomes immense. As discussed previously, the retrieval process in humans is often guided by superficial cues, which presents a paradox. A truly intelligent system needs a memory and retrieval architecture that is not only vast and scalable but also capable of navigating the trade-off between efficient, surface-based reminding and the more demanding search for deep structural matches.31

Future Directions: The Neuro-Symbolic Synthesis

The limitations of purely connectionist approaches like LLMs and purely symbolic approaches like SME strongly suggest that the future of analogical AI lies in a hybrid, neuro-symbolic synthesis.6 Such systems would aim to combine the respective strengths of each paradigm:

The ability of large neural models to learn rich, flexible, and context-sensitive representations from massive amounts of unstructured data.

The precision, interpretability, and systematicity of symbolic reasoning engines that operate over explicit, structured knowledge.

This integration could take many forms, such as using LLMs to populate and reason over symbolic knowledge graphs, or developing architectures where a neural network's distributed representations can be dynamically bound into symbolic structures, much in the spirit of LISA but at a vastly larger scale.

Ultimately, the problem of analogy is not a standalone task that can be solved in isolation. It is co-extensive with the problem of building a complete cognitive architecture. A system that can truly reason by analogy must, by necessity, have solved many of the core challenges of AI: it must possess a sophisticated system for knowledge representation, a vast and efficiently searchable memory, robust mechanisms for structured reasoning and mapping, and the ability to learn and abstract general principles from specific examples.11 The various computational models of analogy are not just algorithms for a specific task; they are proposals for miniature cognitive architectures. The ongoing struggles of even the most powerful AI models highlight that simply scaling a generic architecture is insufficient. Progress will likely come not from a single algorithmic breakthrough, but from the principled and gradual construction of more integrated AI systems that reflect the multi-component, structured nature of human cognition. The quest for an AI that can reason by analogy is, in effect, the quest for a complete, computational model of the mind.

Works cited

Analogical Thinking - (Cognitive Psychology) - Vocab, Definition ..., accessed September 13, 2025, https://library.fiveable.me/key-terms/cognitive-psychology/analogical-thinking

Analogy and Analogical Reasoning (Stanford Encyclopedia of ..., accessed September 13, 2025, https://plato.stanford.edu/entries/reasoning-analogy/

Analogy - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Analogy

Understanding the What and When of Analogical Reasoning Across Analogy Formats: An Eye‐Tracking and Machine Learning Approach - PubMed Central, accessed September 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC9786648/

Analogical Reasoning, Psychology of, accessed September 13, 2025, https://groups.psych.northwestern.edu/gentner/papers/Gentner02a.pdf

Analogical Reasoning in LLMs. Exploring the journey from classical… | by Dickson Lukose | Medium, accessed September 13, 2025, https://medium.com/@dickson.lukose/analogical-reasoning-d432b7105725

Analogical Reasoning in the Classroom: Insights From Cognitive Science - The University of Chicago Learning Lab, accessed September 13, 2025, https://learninglab.uchicago.edu/Publications_files/Vendetti_et_al-2015-Mind,_Brain,_and_Education.pdf

Zero-Shot Learning and Human Analogical Reasoning! - Towards AI, accessed September 13, 2025, https://pub.towardsai.net/zero-shot-learning-and-human-analogical-reasoning-074a424cabc2

Analogical Reasoning - Psychology - Northwestern, accessed September 13, 2025, https://groups.psych.northwestern.edu/gentner/papers/gentnerSmith_2012.pdf

Neuroscientific insights into the development of analogical reasoning - PMC, accessed September 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC5887920/

Computational models of analogy - Psychology - Northwestern, accessed September 13, 2025, https://groups.psych.northwestern.edu/gentner/papers/gentner&Forbus_2011.pdf

The Structure-Mapping Engine : Algorithm and Examples - Psychology - Northwestern, accessed September 13, 2025, https://groups.psych.northwestern.edu/gentner/papers/FalkenhainerForbusGentner89.pdf

Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research. - arXiv, accessed September 13, 2025, https://arxiv.org/html/2509.09381v1

Structure Mapping in Analogy and Similarity - Cal State Long Beach, accessed September 13, 2025, https://home.csulb.edu/~cwallis/382/readings/482/GenterMarkman.pdf

ED278963 - Evidence for a Structure-Mapping Theory of Analogy and Metaphor., 1986-Dec, accessed September 13, 2025, https://eric.ed.gov/?id=ED278963

Structure mapping engine - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Structure_mapping_engine

SME (Structure Mapping Engine) - Meta-Guide.com, accessed September 13, 2025, https://meta-guide.com/natural-language/nlp/nlg/sme-structure-mapping-engine

DOCUMENT RESUME ED 288 490 AUTHOR Falkenhainer, Brian; And Others TITLE The Structure-Mapping Engine: Algorithm and INSTITUTION - ERIC, accessed September 13, 2025, https://files.eric.ed.gov/fulltext/ED288490.pdf

The Structure-Mapping Engine - AAAI, accessed September 13, 2025, https://aaai.org/papers/00272-aaai86-045-the-structure-mapping-engine/

Relational Reasoning in a Neurally-plausible Cognitive Architecture: An Overview of the LISA Project, accessed September 13, 2025, https://reasoninglab.psych.ucla.edu/wp-content/uploads/sites/273/2021/04/Hummel_Holyoak_2003_CogStudiesLISA.pdf

Holyoak, KJ, & Hummel, JE (2000). The proper treatment of symbols in a connectionist architecture. In E. - Psychology Department Labs, accessed September 13, 2025, https://labs.psychology.illinois.edu/~jehummel/pubs/Holyoak&Hummel'00.pdf

Distributed Representations of Structure: A Theory of Analogical Access and Mapping, accessed September 13, 2025, https://experts.illinois.edu/en/publications/distributed-representations-of-structure-a-theory-of-analogical-a

Research | UCLA Reasoning Lab, accessed September 13, 2025, https://reasoninglab.psych.ucla.edu/research/

LISA 1 - Psychology Department Labs, accessed September 13, 2025, https://labs.psychology.illinois.edu/~jehummel/pubs/LISA1.pdf

Abstraction in time: Finding hierarchical linguistic structure in a model of relational processing, accessed September 13, 2025, https://www.research.ed.ac.uk/files/26890295/D_M_cogsci16.pdf

A Neurocomputational Model of Analogical Reasoning and its Breakdown in Frontotemporal Lobar Degeneration., accessed September 13, 2025, https://labs.psychology.illinois.edu/~jehummel/pubs/MorrisonEtAl04.pdf

Competing Models of Analogy: ACME Versus Copycat - UCLA Reasoning Lab, accessed September 13, 2025, https://reasoninglab.psych.ucla.edu/wp-content/uploads/sites/273/2022/10/Burns_Holyoak.1994.pdf

LLMs as Models for Analogical Reasoning - arXiv, accessed September 13, 2025, https://arxiv.org/html/2406.13803v2

A Computational Model of Analogical Change - Frontiers, accessed September 13, 2025, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.01235/epub

Proceedings of the Annual Meeting of the Cognitive Science Society, accessed September 13, 2025, https://cvl.psych.ucla.edu/wp-content/uploads/sites/162/2023/06/A119.Ichien-et-al.generative.cogSciP.2022.pdf

Building Analogy Systems: Some Lessons Learned - Qualitative ..., accessed September 13, 2025, https://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2018/ALL_ACS2018.pdf

How do AI models perform analogical reasoning? - Milvus, accessed September 13, 2025, https://milvus.io/ai-quick-reference/how-do-ai-models-perform-analogical-reasoning

What Is Reasoning in AI? - IBM, accessed September 13, 2025, https://www.ibm.com/think/topics/ai-reasoning

Creative Problem Solving Through Automated Planning and Analogy, accessed September 13, 2025, https://www.sift.net/sites/default/files/publications/main_3.pdf

Analogical Reasoning: Opportunities and Limitations - BotPenguin, accessed September 13, 2025, https://botpenguin.com/glossary/analogical-reasoning

LEARNING WITH ANALOGICAL REASONING FOR ... - OpenReview, accessed September 13, 2025, https://openreview.net/pdf?id=jPlghr8io4

Few-shot Visual Reasoning with Meta-analogical Contrastive Learning - NIPS, accessed September 13, 2025, https://proceedings.nips.cc/paper/2020/file/c39e1a03859f9ee215bc49131d0caf33-Paper.pdf

[2503.21258] Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2503.21258

Emergent analogical reasoning in large language models - PubMed, accessed September 13, 2025, https://pubmed.ncbi.nlm.nih.gov/37524930/

LLMs as Models for Analogical Reasoning - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2406.13803

Evidence from counterfactual tasks supports emergent analogical reasoning in large language models | PNAS Nexus | Oxford Academic, accessed September 13, 2025, https://academic.oup.com/pnasnexus/article/4/5/pgaf135/8124429

Using Counterfactual Tasks to Evaluate the Generality of Analogical ..., accessed September 13, 2025, https://arxiv.org/abs/2402.08955

Why Can'T GPT Think Like Us? Study Reveals AI Limitations In Analogical Reasoning, accessed September 13, 2025, https://quantumzeitgeist.com/why-cant-gpt-think-like-us-study-reveals-ai-limitations-in-analogical-reasoning/

Beyond the Surface: AI's Inability to Handle Complex Analogies Revealed - iHLS, accessed September 13, 2025, https://i-hls.com/archives/128602

[2310.01714] Large Language Models as Analogical Reasoners - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2310.01714

LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS - Stanford University, accessed September 13, 2025, https://www-cs-faculty.stanford.edu/people/jure/pubs/llms-iclr24.pdf

Teach Your AI to Solve Problems Like a Human - Relevance AI, accessed September 13, 2025, https://relevanceai.com/prompt-engineering/teach-your-ai-to-solve-problems-like-a-human

Large Language Models as Analogical Reasoners - OpenReview, accessed September 13, 2025, https://openreview.net/forum?id=AgDICX1h50

Relevant or Random: Can LLMs Truly Perform Analogical Reasoning? - ACL Anthology, accessed September 13, 2025, https://aclanthology.org/2025.findings-acl.1230.pdf

Knowledge Representation and Reasoning in Artificial Intelligence - AI Explained Here, accessed September 13, 2025, https://aiexplainedhere.com/knowledge-representation-and-reasoning-in-artificial-intelligence/

Knowledge Representation and Reasoning Archives - AAAI, accessed September 13, 2025, https://aaai.org/proceeding/knowledge-representation-and-reasoning/

arXiv:2502.09100v1 [cs.AI] 13 Feb 2025, accessed September 13, 2025, https://arxiv.org/pdf/2502.09100

[PDF] Computational models of analogy. - Semantic Scholar, accessed September 13, 2025, https://www.semanticscholar.org/paper/Computational-models-of-analogy.-Gentner-Forbus/a822a11157efd81c006a56c71188dc9d1a4b2c74

Model/System | Primary Paradigm | Core Mechanism | Representation | Key Strength | Primary Limitation

SME | Symbolic | Greedy merge of structurally consistent kernels based on the systematicity principle.11 | Predicate-calculus (structured, symbolic descriptions).16 | Precision and formalization of systematic, mature analogical reasoning.12 | Requires hand-coded, pre-structured representations; less cognitively flexible.31

LISA | Connectionist (Symbolic Connectionist) | Synchronous firing of distributed role-filler bindings to solve the binding problem dynamically.20 | Distributed patterns over semantic primitives, dynamically bound into propositions.24 | Neural plausibility, flexibility, and an account of cognitive capacity limits.22 | Limited working memory capacity; sensitive to processing order.24

ACME | Hybrid (Constraint Network) | Parallel constraint satisfaction network of competing mapping hypotheses.27 | Predicate-calculus with semantic and pragmatic weights.2 | Integration of structural, semantic, and goal-oriented (pragmatic) constraints.2 | Less emphasis on candidate inference generation compared to SME.27

Copycat | Hybrid (Emergent Architecture) | Emergent interplay of perceptual agents, a conceptual network (Slipnet), and a workspace.27 | Fluid conceptual structures built on-the-fly from basic components.28 | Models the process of perception, re-representation, and conceptual "slippage".27 | Restricted to a specific micro-domain (letter-string analogies).2

DORA | Connectionist | Extends LISA to learn structured relational representations from unstructured feature-based inputs.25 | Learns structured representations from holistic feature vectors.29 | Models the developmental learning of relations (the "relational shift").29 | Inherits architectural complexities and limitations from LISA.

BART | Hybrid (Bayesian) | Probabilistic inference over relational transformations learned through statistical learning.28 | Learns relational representations from non-relational inputs.28 | Provides a formal, statistical account of relational learning and inference.30 | Has primarily been applied to more constrained domains.28