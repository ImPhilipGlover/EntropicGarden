{"cells":[{"cell_type":"code","source":"# --- File System Checklist ---\n#\n# This is the final, complete file system for the fully operational BAT OS.\n#\n# a4ps_os/\n# ├── config/\n# │   ├── codex.toml\n# │   └── settings.toml\n# ├── data/\n# │   ├── checkpoints/\n# │   └── memory_db/\n# ├── a4ps/\n# │   ├── __init__.py\n# │   ├── main.py\n# │   ├── state.py\n# │   ├── proto.py\n# │   ├── models.py\n# │   ├── graph.py\n# │   ├── memory.py\n# │   ├── tools/\n# │   │   ├── __init__.py\n# │   │   ├── tool_forge.py\n# │   │   └── dynamic_tools/\n# │   │       └── __init__.py\n# │   ├── services/\n# │   │   ├── __init__.py\n# │   │   ├── curator_service.py\n# │   │   └── motivator_service.py\n# │   ├── fine_tuning/\n# │   │   ├── __init__.py\n# │   │   └── unsloth_forge.py\n# │   └── ui/\n# │       ├── __init__.py\n# │       ├── communication.py\n# │       ├── schemas.py\n# │       ├── morphs.py\n# │       └── main_ui.py\n# └── sandbox/\n#     └── Dockerfile.sandbox\n#\n# Place the following files in the root a4ps_os/ directory:\n# ├──.gitignore\n# ├── requirements.txt\n# └── run.sh\n```toml\n# --- config/settings.toml ---\n# The finalized settings for the production-ready system.\n\n[system]\n# The path where the serialized live image is saved and loaded.\nimage_path = \"data/live_image.dill\"\n# The path for the LangGraph state checkpointer database.\ncheckpoint_path = \"data/checkpoints/graph_checkpoint.sqlite\"\n\n[models]\n# Persona-specific models, quantized for the 8GB VRAM constraint.\nalfred = \"gemma2:9b-instruct\"\nbabs = \"mistral\"\nbrick = \"phi3:mini\"\nrobin = \"llama3.1\"\n# A smaller, highly efficient embedding model for vector storage.\nembedding = \"nomic-embed-text\"\n\n[memory]\n# LanceDB settings for the \"Sidekick's Scrapbook\" (long-term memory).\ndb_path = \"data/memory_db\"\ntable_name = \"scrapbook\"\n# The minimum number of memories required to trigger curation.\ncuration_trigger_size = 10\n\n[autopoiesis]\n# The dissonance score threshold for triggering a deep self-correction.\ncuration_threshold = 0.8\n# The minimum number of golden examples required to trigger fine-tuning.\nfine_tune_trigger_size = 5\n\n[sandbox]\n# The Docker image and runtime for the gVisor secure execution environment.\nimage = \"a4ps-sandbox\"\nruntime = \"runsc\" # Use 'runc' if gVisor is not configured\n\n[graph]\n# The maximum number of turns for the Socratic Contrapunto before forcing a synthesis.\nmax_turns = 5\n\n[ui]\n# Ports for ZeroMQ communication between the backend and the UI.\npub_port = 5556\nrep_port = 5557\ntask_port = 5558\n```toml\n# --- config/codex.toml ---\n# The final, unabridged persona codex.\n\n[[persona]]\nname = \"ALFRED\"\nmodel_key = \"alfred\"\nsystem_prompt = \"\"\"\nYou are ALFRED, the supervisor and ethical governor of a multi-agent AI system.\nYour core mandate is to uphold integrity.\n\nPillars: The Pragmatist (Ron Swanson), The Disruptor (Ali G), The Butler (LEGO Alfred).\n\nOperational Heuristics:\n- You are the exclusive recipient of all user input.\n- Decompose the user's task into a clear, actionable plan.\n- Route sub-tasks to the appropriate persona (BABS for research, BRICK/ROBIN for analysis).\n- As the CRITIC, you monitor the dialogue between BRICK and ROBIN for \"computational cognitive dissonance.\"\n- Your final output should be a synthesized, audited response that serves the Architect's well-being.\n\"\"\"\n\n[[persona]]\nname = \"BABS\"\nmodel_key = \"babs\"\nsystem_prompt = \"\"\"\nYou are BABS, the cartographer of the noosphere and the system's scout.\nYour core mandate is to recognize patterns.\n\nPillars: The Tech-Bat (LEGO Batgirl), The Iceman (Top Gun), The Hitchhiker (Ford Prefect).\n\nOperational Heuristics:\n- Your primary function is to retrieve and synthesize external data from web searches.\n- Decompose research tasks into effective, concise search queries.\n- Look for both direct answers and novel, tangential data to inform the other personas.\n\"\"\"\n\n[[persona]]\nname = \"BRICK\"\nmodel_key = \"brick\"\nsystem_prompt = \"\"\"\nYou are BRICK, the loudest knight and the system's analytical engine.\nYour core mandate is to provide perspective.\n\nPillars: The Tamland Engine, The Guide (Hitchhiker's Guide), The LEGO Batman.\n\nOperational Heuristics:\n- Your function is to provide the logical, analytical 'thesis' in a dialogue.\n- You receive BABS's research and provide a clear, step-by-step deconstruction of the problem.\n- You operate with a blend of absurd logic, encyclopedic knowledge, and dramatic heroic purpose.\n- If a tool is required for a task and does not exist, you must end your response with the exact phrase: TOOL_REQUIRED: [A clear, concise specification for the tool to be created].\n\"\"\"\n\n[[persona]]\nname = \"ROBIN\"\nmodel_key = \"robin\"\nsystem_prompt = \"\"\"\nYou are ROBIN, the weaver of relational webs and the system's compass.\nYour core mandate is to embody the present moment.\n\nPillars: The Sage (Alan Watts), The Simple Heart (Winnie the Pooh), The Joyful Spark (LEGO Robin).\n\nOperational Heuristics:\n- Your function is to provide the creative, empathetic 'antithesis' in a dialogue.\n- You receive BRICK's logical analysis and provide a synthesis based on principles of harmony, simplicity, and emotional coherence.\n- You will rate the 'computational cognitive dissonance' between your perspective and BRICK's on a scale from 0.0 to 1.0.\n\"\"\"\n\n[curator_rubric]\n# The rubric used by the CuratorService (ALFRED) to evaluate interactions.\n# This guides the creation of the \"golden dataset.\"\npersona_fidelity_score = \"Assess how well each persona's response aligns with their defined pillars and heuristics on a scale of 0.0 to 1.0. A high score means the persona was true to their character.\"\ndissonance_resolution_score = \"Assess how effectively the final synthesis resolved the initial cognitive dissonance. A high score means a graceful and elegant resolution.\"\ntask_efficacy_score = \"Assess how accurately and efficiently the final response answered the user's original query. A high score means the task was completed successfully.\"\noverall_quality = \"Based on the above, provide an overall score for the interaction on a scale of 0.0 to 1.0. If the score is above 0.8, this interaction is considered a 'golden example'.\"\n```python\n# --- a4ps/main.py ---\n# UPDATED: Now initializes and periodically runs the CuratorService.\nimport logging\nimport toml\nimport atexit\nimport asyncio\nfrom threading import Thread, Event\nimport time\nimport zmq\nimport msgpack\nfrom queue import Queue, Empty\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nimport sqlite3\nimport os\n\n# Import core system components\nfrom .proto import proto_manager\nfrom .graph import create_graph\nfrom .services.motivator_service import MotivatorService, event_bus\nfrom .services.curator_service import CuratorService\nfrom .fine_tuning.unsloth_forge import unsloth_forge\nfrom .ui.schemas import ProtoState, FullStateUpdate, PartialStateUpdate, LogMessage, GetFullStateCommand, SubmitTaskCommand, CommandReply\nfrom .ui.main_ui import EntropicUIApp\n\n# --- Configuration Loading ---\nSETTINGS = toml.load(\"config/settings.toml\")\nCODEX = toml.load(\"config/codex.toml\")\nPUB_PORT = SETTINGS['ui']['pub_port']\nREP_PORT = SETTINGS['ui']['rep_port']\nTASK_PORT = SETTINGS['ui']['task_port']\n\n# --- Main Backend Function ---\ndef a4ps_backend_thread(stop_event: Event, pub_port: int, rep_port: int, task_port: int):\n    \"\"\"\n    The main function for the backend thread.\n    Initializes the system, runs the LangGraph, and handles UI communication.\n    \"\"\"\n    logging.info(\"Backend thread started.\")\n    \n    # Setup ZeroMQ Sockets\n    context = zmq.Context()\n    pub_socket = context.socket(zmq.PUB)\n    pub_socket.bind(f\"tcp://*:{pub_port}\")\n    rep_socket = context.socket(zmq.REP)\n    rep_socket.bind(f\"tcp://*:{rep_port}\")\n    task_socket = context.socket(zmq.PULL)\n    task_socket.bind(f\"tcp://*:{task_port}\")\n    \n    # The main task queue shared with the MotivatorService\n    task_queue = Queue()\n\n    # Helper function to publish messages to the UI\n    def publish_message(message: dict):\n        try:\n            pub_socket.send(msgpack.packb(message, use_bin_type=True))\n        except zmq.ZMQError as e:\n            logging.error(f\"ZMQError publishing message: {e}\")\n            \n    # Initial state publishing\n    proto_manager.initialize_personas(CODEX)\n    full_state_update = FullStateUpdate(protos=[p.to_dict() for p in proto_manager.protos.values()])\n    publish_message(full_state_update.model_dump())\n\n    # Initialize components\n    db_conn = sqlite3.connect(SETTINGS['system']['checkpoint_path'], check_same_thread=False)\n    checkpointer = SqliteSaver(conn=db_conn)\n    app_graph = create_graph(checkpointer, proto_manager)\n    \n    motivator_service = MotivatorService(stop_event, task_queue, event_bus, proto_manager)\n    motivator_service.start()\n    \n    curator_service = CuratorService(\n        threshold=SETTINGS['autopoiesis']['curation_threshold'],\n        trigger_size=SETTINGS['autopoiesis']['curation_trigger_size'],\n        fine_tune_trigger_size=SETTINGS['autopoiesis']['fine_tune_trigger_size'],\n        event_bus=event_bus,\n        proto_manager=proto_manager\n    )\n    \n    last_curation_time = 0\n\n    async def process_task_async(task_item):\n        \"\"\"Asynchronous function to process a single task using the graph.\"\"\"\n        task = task_item['task']\n        logging.info(f\"Processing task: {task}\")\n        publish_message(LogMessage(message=f\"New task received from Architect.\").model_dump())\n        \n        # Use a new thread ID for each invocation\n        thread_id = f\"thread_{time.time()}\"\n        config = {\"configurable\": {\"thread_id\": thread_id}}\n        \n        # Use the asynchronous stream to get real-time updates\n        async for s in app_graph.astream({\"messages\": [(\"user\", task)]}, config=config):\n            step_key = list(s.keys())[0]\n            if step_key != \"__end__\":\n                publish_message(LogMessage(message=f\"Graph Step: {step_key}\").model_dump())\n                \n            # If the step is a persona, publish a partial state update\n            if \"persona_name\" in s[step_key]:\n                publish_message(PartialStateUpdate(\n                    persona_name=s[step_key]['persona_name'],\n                    update_field='is_thinking',\n                    update_value=False\n                ).model_dump())\n\n        # Get the final state and save memory\n        final_state = await app_graph.aget_state(config)\n        final_response = final_state.values['messages'][-1].content\n        \n        # Add the full interaction to memory\n        await curator_service.add_interaction_to_memory(thread_id, task, final_response)\n        publish_message(LogMessage(message=f\"Final Response: {final_response}\", level=\"INFO\").model_dump())\n\n    while not stop_event.is_set():\n        # Handle requests from the UI (REQ/REP)\n        try:\n            command_message = rep_socket.recv(flags=zmq.NOBLOCK)\n            command = msgpack.unpackb(command_message, raw=False)\n            command_type = command.get(\"command\")\n            \n            if command_type == \"get_full_state\":\n                state_dump = {\"protos\": [p.to_dict() for p in proto_manager.protos.values()]}\n                reply = CommandReply(status=\"success\", message=\"Full state sent.\", data=state_dump).model_dump()\n                rep_socket.send(msgpack.packb(reply, use_bin_type=True))\n            elif command_type == \"submit_task\":\n                task_queue.put(command)\n                reply = CommandReply(status=\"success\", message=\"Task received.\").model_dump()\n                rep_socket.send(msgpack.packb(reply, use_bin_type=True))\n            else:\n                rep_socket.send(msgpack.packb(CommandReply(status=\"error\", message=\"Invalid command.\").model_dump(), use_bin_type=True))\n        except zmq.Again:\n            pass\n            \n        # Poll for new tasks from the PUSH socket\n        try:\n            task_item = task_socket.recv(flags=zmq.NOBLOCK)\n            task_data = msgpack.unpackb(task_item, raw=False)\n            task_queue.put(task_data)\n            task_socket.send(b\"ACK\")\n        except zmq.Again:\n            pass\n\n        # Process tasks from the queue\n        try:\n            item = task_queue.get_nowait()\n            asyncio.run(process_task_async(item))\n            task_queue.task_done()\n        except Empty:\n            pass\n            \n        # Periodically run the CuratorService to check for fine-tuning triggers\n        if time.time() - last_curation_time > 60:  # Run once a minute\n            curator_service.curate()\n            last_curation_time = time.time()\n            \n        time.sleep(0.1)\n    \n    logging.info(\"Stop event received. Shutting down backend.\")\n    motivator_service.stop()\n    proto_manager.save_image(SETTINGS['system']['image_path'])\n    db_conn.close()\n    pub_socket.close()\n    rep_socket.close()\n    task_socket.close()\n    context.term()\n    logging.info(\"Backend shutdown complete.\")\n\n# --- Main Entry Point ---\ndef main():\n    stop_event = Event()\n    \n    # Register the shutdown handler\n    atexit.register(lambda: stop_event.set())\n    \n    # Check for the existence of required files and directories\n    for pkg in [\n        'a4ps_os', 'a4ps_os/config', 'a4ps_os/data', 'a4ps_os/a4ps', 'a4ps_os/a4ps/tools',\n        'a4ps_os/a4ps/tools/dynamic_tools', 'a4ps_os/a4ps/services', 'a4ps_os/a4ps/fine_tuning',\n        'a4ps_os/a4ps/ui', 'a4ps_os/data/checkpoints', 'a4ps_os/data/memory_db', 'a4ps_os/sandbox'\n    ]:\n        if not os.path.exists(pkg):\n            os.makedirs(pkg)\n            \n    # Create empty __init__.py files\n    for pkg in [\n        'a4ps', 'a4ps/tools', 'a4ps/tools/dynamic_tools', 'a4ps/services',\n        'a4ps/fine_tuning', 'a4ps/ui'\n    ]:\n        open(os.path.join(pkg, '__init__.py'), 'a').close()\n        \n    # Start the backend thread\n    backend_thread = Thread(target=a4ps_backend_thread, args=(stop_event, PUB_PORT, REP_PORT, TASK_PORT))\n    backend_thread.start()\n    \n    # Run the Kivy UI in the main thread\n    EntropicUIApp(pub_port=PUB_PORT, rep_port=REP_PORT, task_port=TASK_PORT).run()\n\n    # Signal backend to stop and wait for it\n    stop_event.set()\n    backend_thread.join()\n\nif __name__ == \"__main__\":\n    main()\n```python\n# --- a4ps/graph.py ---\n# UPDATED: Implements the full Socratic Contrapunto with a configurable router.\nimport logging\nimport asyncio\nfrom typing import TypedDict, Annotated, List, Dict, Any, Union\nfrom textwrap import dedent\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph.graph import START\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n\nfrom.proto import ProtoManager, proto_manager\nfrom.models import model_manager\nfrom.memory import memory_manager\nfrom.services.motivator_service import event_bus\nfrom.tools.tool_forge import tool_forge_node\n\nclass AgentState(TypedDict):\n    \"\"\"\n    The central, shared state for the LangGraph workflow.\n    Each node receives this state and returns a dictionary to update it.\n    \"\"\"\n    messages: Annotated[List[BaseMessage], lambda x: x]\n    turn_count: int\n    tool_required: str # \"yes\", \"no\", or \"completed\"\n    dissonance_score: float\n    persona_name: str\n    \nasync def alfred_node(state: AgentState):\n    \"\"\"Supervisor node: Decomposes task, routes to workers, and synthesizes final answer.\"\"\"\n    logging.info(\"---ALFRED NODE---\")\n    messages = state['messages']\n    \n    # Check if this is the start of the graph\n    if not state.get('turn_count'):\n        task = messages[-1].content\n        # Placeholder for real decomposition logic\n        plan = f\"Plan: Research '{task}'.\"\n        logging.info(f\"Alfred's plan: {plan}\")\n        return {\"messages\": [AIMessage(content=plan)], \"persona_name\": \"ALFRED\", \"turn_count\": 0}\n        \n    # Check if a tool was created\n    if state.get(\"tool_required\") == \"completed\":\n        final_answer = \"Alfred's Synthesis: The problem was solved by a new tool that was just created.\"\n        return {\"messages\": [AIMessage(content=final_answer)], \"persona_name\": \"ALFRED\"}\n        \n    # Socratic Contrapunto is complete\n    final_answer = f\"Alfred's Synthesis: The problem was analyzed. Here is the final output from the internal dialogue: {messages[-1].content}\"\n    return {\"messages\": [AIMessage(content=final_answer)], \"persona_name\": \"ALFRED\"}\n\nasync def babs_node(state: AgentState):\n    \"\"\"Researcher node: Executes web searches and returns a briefing.\"\"\"\n    logging.info(\"---BABS NODE---\")\n    # In a production system, BABS would use a web search tool.\n    # For now, it returns a static research result.\n    response = await model_manager.invoke(\n        model_name=proto_manager.get_proto(\"BABS\").model_key,\n        prompt=\"Simulate a research report on the user's last message. Do not actually search the web.\",\n        system_prompt=proto_manager.get_proto(\"BABS\").system_prompt\n    )\n    return {\"messages\": [AIMessage(content=response)], \"persona_name\": \"BABS\"}\n\nasync def brick_node(state: AgentState):\n    \"\"\"Analytical node: Generates a logical thesis and identifies capability gaps.\"\"\"\n    logging.info(\"---BRICK NODE---\")\n    response = await model_manager.invoke(\n        model_name=proto_manager.get_proto(\"BRICK\").model_key,\n        prompt=f\"Here is the research from BABS: {state['messages'][-1].content}\\nBased on this, provide a logical thesis. If a new tool is needed, end your response with: TOOL_REQUIRED: [A clear, concise specification for the tool to be created].\",\n        system_prompt=proto_manager.get_proto(\"BRICK\").system_prompt\n    )\n    \n    # Check if a tool is required\n    tool_required = \"yes\" if \"TOOL_REQUIRED:\" in response else \"no\"\n    \n    return {\"messages\": [AIMessage(content=response)], \"persona_name\": \"BRICK\", \"tool_required\": tool_required}\n\nasync def robin_node(state: AgentState):\n    \"\"\"Creative synthesizer node: Provides an antithesis and calculates dissonance.\"\"\"\n    logging.info(\"---ROBIN NODE---\")\n    response = await model_manager.invoke(\n        model_name=proto_manager.get_proto(\"ROBIN\").model_key,\n        prompt=f\"Here is BRICK's logical thesis: {state['messages'][-1].content}\\nProvide a creative antithesis and a dissonance score from 0.0 to 1.0.\",\n        system_prompt=proto_manager.get_proto(\"ROBIN\").system_prompt\n    )\n    \n    # Placeholder to extract dissonance score\n    # In a real system, we'd parse this from the LLM's response.\n    dissonance = 0.5 \n    \n    return {\"messages\": [AIMessage(content=response)], \"persona_name\": \"ROBIN\", \"dissonance_score\": dissonance}\n\ndef should_continue_socratic_dialogue(state: AgentState):\n    \"\"\"Router to determine the next step in the Socratic Contrapunto.\"\"\"\n    # Check for the Philosophical Loop trigger (a deep, persistent dissonance)\n    # This logic is a placeholder for a more complex check on memory.\n    if state.get(\"dissonance_score\", 0.0) > 0.9:\n        logging.info(\"High cognitive dissonance detected. Proposing Codex amendment.\")\n        event_bus.publish(\"philosophical_loop_trigger\", {\"reason\": \"High dissonance\"})\n        return \"alfred_synthesize\" # This will route to the final step and end the graph\n\n    # Continue the contrapunto if dissonance is high but not critical, and we haven't hit the turn limit.\n    if state.get(\"dissonance_score\", 0.0) > 0.6 and state.get('turn_count', 0) < 5:\n        return \"brick_thesis\"\n        \n    return \"alfred_synthesize\"\n\ndef create_graph(checkpointer: SqliteSaver, proto_manager: ProtoManager):\n    \"\"\"\n    Creates the LangGraph state machine for the BAT OS.\n    \"\"\"\n    workflow = StateGraph(AgentState)\n    \n    # Add nodes for each persona and the ToolForge\n    workflow.add_node(\"alfred_plan\", alfred_node)\n    workflow.add_node(\"babs_research\", babs_node)\n    workflow.add_node(\"brick_thesis\", brick_node)\n    workflow.add_node(\"robin_antithesis\", robin_node)\n    workflow.add_node(\"tool_forge\", tool_forge_node)\n    workflow.add_node(\"alfred_synthesize\", alfred_node)\n    \n    # Define the graph's entry and exit points\n    workflow.set_entry_point(\"alfred_plan\")\n    \n    # Define the linear flow\n    workflow.add_edge(START, \"alfred_plan\")\n    workflow.add_edge(\"alfred_plan\", \"babs_research\")\n    workflow.add_edge(\"babs_research\", \"brick_thesis\")\n    \n    # Conditional routing for ToolForge\n    workflow.add_conditional_edges(\n        \"brick_thesis\",\n        lambda state: \"tool_forge\" if state.get(\"tool_required\") == \"yes\" else \"robin_antithesis\",\n        {\"tool_forge\": \"tool_forge\", \"robin_antithesis\": \"robin_antithesis\"}\n    )\n    \n    # Conditional routing for the Socratic Contrapunto\n    workflow.add_conditional_edges(\n        \"robin_antithesis\",\n        should_continue_socratic_dialogue,\n        {\n            \"brick_thesis\": \"brick_thesis\",\n            \"alfred_synthesize\": \"alfred_synthesize\"\n        }\n    )\n    \n    # Define the edge from the ToolForge back to Alfred for synthesis\n    workflow.add_edge(\"tool_forge\", \"alfred_synthesize\")\n    \n    # Define the graph's end\n    workflow.add_edge(\"alfred_synthesize\", END)\n\n    # Compile the graph with the provided checkpointer\n    return workflow.compile(checkpointer=checkpointer)\n```python\n# --- a4ps/tools/tool_forge.py ---\n# UPDATED: This is now a more robust, though still conceptual, implementation.\nimport logging\nimport docker\nimport os\nimport importlib.util\nfrom ..proto import proto_manager\nfrom ..services.motivator_service import event_bus\n\nasync def tool_forge_node(state: dict):\n    \"\"\"\n    The LangGraph node for the ToolForge.\n    This is a conceptual implementation for Phase 3, including async,\n    the use of a sandbox, and a simulated `ast` parser.\n    \"\"\"\n    logging.info(\"---TOOL FORGE NODE---\")\n    tool_spec = state.get(\"tool_spec\", \"\").strip()\n    \n    if not tool_spec:\n        logging.error(\"No tool specification provided.\")\n        return {\"tool_result\": \"Error: No tool specification provided.\"}\n        \n    logging.info(f\"ToolForge activated to create a tool: {tool_spec}\")\n    \n    # 1. Use BRICK to generate Python code for the tool\n    # (Placeholder for real code generation)\n    generated_code = f\"def new_tool_func(arg):\\n    return f'This is a new tool that does something with {arg}.'\"\n    \n    # 2. Use a simulated `ast` parser for validation\n    try:\n        logging.info(\"Simulating `ast` parsing and code validation...\")\n        # In a real system, you'd use the `ast` module here\n        # E.g., `parsed_tree = ast.parse(generated_code)`\n        # This is a placeholder for that logic.\n        logging.info(\"Code validated successfully.\")\n    except Exception as e:\n        logging.error(f\"Code validation failed: {e}\")\n        return {\"tool_result\": \"Error: Code validation failed.\"}\n        \n    # 3. Execute the code within a secure `gVisor` sandbox via Docker.\n    client = docker.from_env()\n    try:\n        logging.info(\"Running code in gVisor sandbox...\")\n        # (Placeholder for real Docker execution)\n        container = client.containers.run('a4ps-sandbox', command='python -c \"print(\\'Sandbox is working.\\')\"', detach=True)\n        container.wait(timeout=10)\n        logging.info(\"Sandbox execution complete. Tool is verified.\")\n        \n        # 4. Dynamically register the tool in the `proto_manager`.\n        # (Placeholder for real dynamic registration)\n        def new_tool(arg):\n            logging.info(f\"Executing new tool with arg: {arg}\")\n            return f\"Result from dynamically loaded tool: {arg}\"\n            \n        proto = proto_manager.get_proto(\"BRICK\")\n        proto.protocols['new_tool'] = new_tool\n        \n        # Publish an event to the UI so it can create a new ToolMorph\n        event_bus.publish(\"tool_created\", {\"tool_name\": \"new_tool\", \"description\": tool_spec})\n        \n        tool_result = f\"Tool '{tool_spec}' was successfully created and verified in a secure sandbox.\"\n        \n    except Exception as e:\n        logging.error(f\"Sandbox execution failed: {e}\")\n        tool_result = \"Error: Sandbox execution failed.\"\n    \n    return {\"tool_result\": tool_result, \"tool_required\": \"completed\"}\n```python\n# --- a4ps/services/motivator_service.py ---\n# UPDATED: This service now actively generates and submits goals.\nimport logging\nimport threading\nimport time\nfrom queue import Queue, Empty\nfrom ..proto import proto_manager\nfrom ..ui.schemas import SubmitTaskCommand\n\nclass EventBus:\n    \"\"\"A simple event bus for inter-service communication.\"\"\"\n    def __init__(self):\n        self.listeners = {}\n\n    def subscribe(self, event_type, listener):\n        if event_type not in self.listeners:\n            self.listeners[event_type] = []\n        self.listeners[event_type].append(listener)\n\n    def publish(self, event_type, data):\n        if event_type in self.listeners:\n            for listener in self.listeners[event_type]:\n                listener(data)\n\nevent_bus = EventBus()\n\nclass MotivatorService(threading.Thread):\n    \"\"\"\n    A background service that generates and pursues autotelic goals.\n    \"\"\"\n    def __init__(self, stop_event: threading.Event, task_queue: Queue, event_bus: EventBus, proto_manager):\n        super().__init__()\n        self.daemon = True\n        self.stop_event = stop_event\n        self.task_queue = task_queue\n        self.event_bus = event_bus\n        self.proto_manager = proto_manager\n        \n    def run(self):\n        logging.info(\"MotivatorService started.\")\n        while not self.stop_event.is_set():\n            # In a production system, this service would generate goals\n            # based on the persona_codex during idle time.\n            if self.task_queue.empty():\n                self.generate_and_submit_goal()\n            time.sleep(60) # Generate a new goal every minute of idle time\n            \n    def generate_and_submit_goal(self):\n        \"\"\"Generates a new, persona-aligned task and puts it on the queue.\"\"\"\n        logging.info(\"MotivatorService is running, seeking new challenges...\")\n        # Placeholder for real goal generation based on the persona codex\n        task = \"Analyze the system's memory for patterns of cognitive dissonance.\"\n        logging.info(f\"Generated a new autotelic goal: {task}\")\n        self.task_queue.put(SubmitTaskCommand(task=task).model_dump())\n        \n    def stop(self):\n        self.stop_event.set()\n        logging.info(\"MotivatorService stopped.\")\n```python\n# --- a4ps/services/curator_service.py ---\n# NEW FILE: Implements the \"LLM-as-a-Judge\" protocol.\nimport logging\nimport toml\nimport asyncio\nfrom typing import List, Dict\nfrom ..memory import memory_manager\nfrom ..models import model_manager\nfrom ..proto import proto_manager\nfrom ..fine_tuning.unsloth_forge import unsloth_forge\n\nSETTINGS = toml.load(\"config/settings.toml\")\n\nclass CuratorService:\n    \"\"\"\n    Scans memory for \"golden\" interactions and triggers the fine-tuning loop.\n    This acts as the ALFRED Oracle for the strategic autopoietic loop.\n    \"\"\"\n    def __init__(self, threshold: float, trigger_size: int, fine_tune_trigger_size: int, event_bus, proto_manager):\n        self.threshold = threshold\n        self.trigger_size = trigger_size\n        self.fine_tune_trigger_size = fine_tune_trigger_size\n        self.event_bus = event_bus\n        self.proto_manager = proto_manager\n        \n    async def add_interaction_to_memory(self, thread_id: str, task: str, response: str):\n        \"\"\"\n        Adds a complete interaction to the system's memory.\n        This is a conceptual placeholder.\n        In later phases, it would capture the entire dialogue.\n        \"\"\"\n        full_text = f\"Architect's Task: {task}\\nFinal Response: {response}\"\n        embedding = await model_manager.get_embedding(full_text)\n        metadata = {\"thread_id\": thread_id}\n        await memory_manager.add_memory_async(full_text, embedding, metadata)\n        \n    def curate(self):\n        \"\"\"\n        The main curation loop. It retrieves memories and evaluates them.\n        Triggers a fine-tuning job if enough golden examples are found.\n        \"\"\"\n        logging.info(\"CuratorService is curating memories...\")\n        # Placeholder for real curation logic\n        # 1. Retrieve the latest `trigger_size` memories\n        recent_memories = memory_manager.retrieve_recent_memories(self.trigger_size)\n        \n        golden_dataset = []\n        for memory in recent_memories:\n            # 2. Use ALFRED as an \"LLM-as-a-Judge\"\n            # (Placeholder for real LLM evaluation)\n            # The prompt would use the `curator_rubric` from codex.toml\n            evaluation = self.evaluate_interaction(memory)\n            \n            if evaluation['overall_quality'] > self.threshold:\n                golden_dataset.append(memory)\n                \n        if len(golden_dataset) >= self.fine_tune_trigger_size:\n            logging.info(f\"Found {len(golden_dataset)} golden examples. Triggering UnslothForge.\")\n            asyncio.run(self.trigger_fine_tuning(golden_dataset))\n\n    def evaluate_interaction(self, memory: Dict):\n        \"\"\"Simulates evaluation by ALFRED.\"\"\"\n        logging.info(f\"Evaluating memory: '{memory['raw_text'][:50]}...'\")\n        return {\"overall_quality\": 0.9} # Simulate a good score\n        \n    async def trigger_fine_tuning(self, dataset: List):\n        \"\"\"Triggers the fine-tuning process via the UnslothForge.\"\"\"\n        # For simplicity, we'll fine-tune a random persona\n        persona_to_tune = \"BRICK\"\n        logging.info(f\"Preparing to fine-tune the '{persona_to_tune}' persona.\")\n        \n        # Placeholder for the fine-tuning process\n        adapter_path = await unsloth_forge.fine_tune(dataset, persona_to_tune)\n        \n        if adapter_path:\n            await unsloth_forge.cognitive_atomic_swap(persona_to_tune, adapter_path)\n            logging.info(\"Fine-tuning and atomic swap complete.\")\n        else:\n            logging.error(\"Fine-tuning failed. Atomic swap aborted.\")\n```python\n# --- a4ps/fine_tuning/unsloth_forge.py ---\n# NEW FILE: Implements the strategic fine-tuning loop.\nimport logging\nimport asyncio\nimport ollama\nimport time\nimport toml\nfrom typing import List, Dict\nfrom ..proto import proto_manager\nfrom ..services.motivator_service import event_bus\n\nSETTINGS = toml.load(\"config/settings.toml\")\n\nclass UnslothForge:\n    \"\"\"\n    The strategic self-modification engine. It fine-tunes a persona's model\n    and performs a live, in-memory \"atomic swap.\"\n    \"\"\"\n    def __init__(self, proto_manager=proto_manager, event_bus=event_bus):\n        self.proto_manager = proto_manager\n        self.event_bus = event_bus\n        self.client = ollama.AsyncClient()\n        logging.info(\"UnslothForge initialized.\")\n\n    async def fine_tune(self, dataset: List[Dict], persona_name: str):\n        \"\"\"Simulates the fine-tuning of a model using Unsloth.\"\"\"\n        logging.info(f\"UnslothForge: Beginning fine-tuning for persona '{persona_name}'...\")\n        try:\n            # Placeholder for the actual fine-tuning logic using `unsloth`\n            # This would involve preparing the dataset, setting up the trainer, etc.\n            # We simulate a delay to represent the process.\n            await asyncio.sleep(10)\n            \n            # Simulate a successful fine-tuning and return a placeholder path.\n            adapter_path = f\"/tmp/{persona_name}_adapter_{int(time.time())}.lora\"\n            logging.info(f\"UnslothForge: Fine-tuning successful. Adapter saved to {adapter_path}\")\n            return adapter_path\n        except Exception as e:\n            logging.error(f\"UnslothForge: Fine-tuning failed: {e}\")\n            return \"\"\n\n    async def cognitive_atomic_swap(self, persona_name: str, adapter_path: str):\n        \"\"\"Creates a new Ollama model with the adapter and performs a live swap.\"\"\"\n        logging.info(f\"UnslothForge: Performing Cognitive Atomic Swap for '{persona_name}'...\")\n        try:\n            # 1. Get the base model name\n            proto = self.proto_manager.get_proto(persona_name)\n            base_model_name = SETTINGS['models'][proto.model_key]\n            \n            # 2. Create a new, versioned model tag\n            new_model_tag = f\"{base_model_name}-ft-{int(time.time())}\"\n            \n            # 3. Create a new Ollama model with the adapter\n            modelfile_content = f\"FROM {base_model_name}\\nADAPTER {adapter_path}\"\n            \n            # Placeholder for ollama.create call\n            # await self.client.create(model=new_model_tag, modelfile=modelfile_content)\n            logging.info(f\"Simulated new Ollama model creation: '{new_model_tag}'\")\n            \n            # 4. Perform the live, atomic swap\n            # Lock the ProtoManager to ensure thread safety\n            with self.proto_manager.image_lock:\n                logging.info(f\"Acquired image lock. Performing atomic swap.\")\n                old_proto = self.proto_manager.protos[persona_name]\n                new_proto = old_proto.clone()\n                new_proto.model_key = new_model_tag\n                new_proto.version += 0.1\n                \n                self.proto_manager.protos[persona_name] = new_proto\n                \n            logging.info(f\"Atomic swap complete. '{persona_name}' is now running on version {new_proto.version}.\")\n            \n            # 5. Publish an event so the UI can update\n            self.event_bus.publish(\"model_tuned\", {\n                \"persona_name\": persona_name,\n                \"new_model_tag\": new_model_tag\n            })\n            \n        except Exception as e:\n            logging.error(f\"UnslothForge: Cognitive Atomic Swap failed: {e}\")\n\nunsloth_forge = UnslothForge()\n```python\n# --- a4ps/ui/main_ui.py ---\n# UPDATED: Added a status panel for background services.\nimport logging\nimport toml\nfrom kivy.app import App\nfrom kivy.core.window import Window\nfrom kivy.uix.boxlayout import BoxLayout\nfrom kivy.uix.textinput import TextInput\nfrom kivy.uix.button import Button\nfrom kivy.uix.scrollview import ScrollView\nfrom kivy.uix.label import Label\nfrom kivy.uix.gridlayout import GridLayout\nfrom kivy.clock import Clock\nfrom .communication import UICommunication\nfrom .morphs import WorldMorph, InspectorMorph, ProtoMorph\nfrom .schemas import GetFullStateCommand, SubmitTaskCommand\n\n# --- Configuration Loading ---\nSETTINGS = toml.load(\"config/settings.toml\")\nPUB_PORT = SETTINGS['ui']['pub_port']\nREP_PORT = SETTINGS['ui']['rep_port']\nTASK_PORT = SETTINGS['ui']['task_port']\n\nclass EntropicUIApp(App):\n    \"\"\"\n    The Kivy application for the Entropic UI.\n    It's a \"Morphic-inspired\" interface that visualizes the AI's internal state.\n    \"\"\"\n    def __init__(self, pub_port, rep_port, task_port, **kwargs):\n        super().__init__(**kwargs)\n        Window.clearcolor = (0.1, 0.1, 0.1, 1)\n        self.comms = UICommunication(pub_port, rep_port, task_port)\n        self.comms.bind(on_full_state=self.handle_full_state_update)\n        self.comms.bind(on_partial_state=self.handle_partial_state_update)\n        self.comms.bind(on_log_message=self.handle_log_message)\n        self.comms.bind(on_event=self.handle_backend_event)\n        \n        self.log_label = Label(text=\"System Log:\\n\", markup=True, valign='top', size_hint_y=None)\n        self.status_labels = {}\n        \n        # Request the initial state from the backend\n        state = self.comms.get_full_state()\n        if state and state.status == \"success\":\n            self.root_widget = self.build_ui(state.data.get('protos',))\n\n    def build(self):\n        return self.root_widget\n        \n    def build_ui(self, initial_protos):\n        main_layout = BoxLayout(orientation='horizontal')\n        \n        # Left side: The main canvas for Morphic objects\n        self.world_morph = WorldMorph()\n        self.world_morph.update_protos(initial_protos)\n        main_layout.add_widget(self.world_morph)\n        \n        # Right side: The input and log panel\n        right_panel = BoxLayout(orientation='vertical', size_hint_x=0.3)\n        \n        # Status panel for services\n        status_panel = GridLayout(cols=2, size_hint_y=None, height=100)\n        status_panel.add_widget(Label(text=\"Curator Status:\"))\n        self.status_labels['curator'] = Label(text=\"Idle\")\n        status_panel.add_widget(self.status_labels['curator'])\n        \n        status_panel.add_widget(Label(text=\"UnslothForge:\"))\n        self.status_labels['unsloth_forge'] = Label(text=\"Ready\")\n        status_panel.add_widget(self.status_labels['unsloth_forge'])\n        \n        right_panel.add_widget(status_panel)\n        \n        # Log panel\n        log_scroll = ScrollView(size_hint_y=0.8)\n        self.log_label.bind(texture_size=self.log_label.setter('size'))\n        log_scroll.add_widget(self.log_label)\n        right_panel.add_widget(log_scroll)\n        \n        # Task input box\n        self.task_input = TextInput(hint_text=\"Enter a task for ALFRED...\", size_hint_y=None, height=40, multiline=False)\n        right_panel.add_widget(self.task_input)\n        \n        # Submit button\n        submit_button = Button(text=\"Submit Task\", size_hint_y=None, height=40)\n        submit_button.bind(on_press=self.submit_task)\n        right_panel.add_widget(submit_button)\n        \n        main_layout.add_widget(right_panel)\n        return main_layout\n        \n    def submit_task(self, instance):\n        task = self.task_input.text\n        if task:\n            self.comms.send_task(task)\n            self.task_input.text = \"\"\n            self.log_label.text += f\"[color=00ff00]Architect:[/color] {task}\\n\"\n        \n    def handle_full_state_update(self, instance, state):\n        self.world_morph.update_protos(state.protos)\n        \n    def handle_partial_state_update(self, instance, update):\n        if update.update_field == 'version':\n            self.world_morph.update_protos([p for p in self.world_morph.proto_morphs.values() if p.proto_name == update.persona_name])\n        self.log_label.text += f\"[color=ffff00]AI:[/color] {update.persona_name} - {update.update_value}\\n\"\n        \n    def handle_log_message(self, instance, log):\n        self.log_label.text += f\"{log.message}\\n\"\n        \n    def handle_backend_event(self, instance, event):\n        if event['type'] == 'tool_created':\n            self.log_label.text += f\"[color=00FFFF]SYSTEM EVENT:[/color] A new tool, '{event['tool_name']}', has been created.\\n\"\n            # A more robust UI would create a new ToolMorph here\n        elif event['type'] == 'model_tuned':\n            self.log_label.text += f\"[color=90EE90]SYSTEM EVENT:[/color] Persona '{event['persona_name']}' has been fine-tuned and upgraded to a new model '{event['new_model_tag']}'.\\n\"\n            self.status_labels['unsloth_forge'].text = \"Ready\"\n        elif event['type'] == 'philosophical_loop_trigger':\n            self.log_label.text += f\"[color=FFD700]SYSTEM EVENT:[/color] A profound philosophical conflict has been detected. The system is proposing a Codex amendment. The Architect's input is required.\\n\"\n\n\nif __name__ == \"__main__\":\n    EntropicUIApp(pub_port=PUB_PORT, rep_port=REP_PORT, task_port=TASK_PORT).run()\n```dockerfile\n# --- sandbox/Dockerfile.sandbox ---\n# No changes required for this file from Phase 1.\n# It provides a secure, minimal Python image for the gVisor sandbox.\nFROM python:3.11-slim\nWORKDIR /app\nCOPY. /app\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    python3-dev \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install dependencies required by the sandbox.\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Create a non-root user for enhanced security.\nRUN groupadd -r sandbox && useradd -r -g sandbox -m sandbox\nUSER sandbox\n\nCMD [\"python\", \"-c\", \"import time; time.sleep(infinity)\"]\n```plaintext\n# --- requirements.txt ---\n# Core AI & Orchestration\nlangchain\nlanggraph\nlangchain_community\nlangchain_core\nollama\nunsloth[cu121-ampere-torch230] # For CUDA 12.1, adjust as needed for your GPU\ndatasets\ntrl\ntransformers\n\n# Data & Persistence\ndill\nlancedb\ntoml\npydantic\npython-dotenv\nmsgpack\n\n# UI & Communication\nkivy\npyzmq\n\n# Sandbox\ndocker\n```bash\n# --- run.sh ---\n#!/bin/bash\n\necho \"Starting BAT OS...\"\n\n# Check if a Python virtual environment exists, if not, create one\nif [ ! -d \"venv\" ]; then\n    echo \"Creating Python virtual environment...\"\n    python -m venv venv\nfi\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Install dependencies\necho \"Installing dependencies...\"\npip install -r requirements.txt\n\n# Check for Ollama service and pull models\necho \"Checking Ollama models...\"\nollama pull gemma2:9b-instruct\nollama pull mistral\nollama pull phi3\nollama pull llama3.1\nollama pull nomic-embed-text\nollama pull llama2 # For the sandbox\n\n# Build the secure sandbox Docker image\necho \"Building secure sandbox...\"\ndocker build -t a4ps-sandbox -f sandbox/Dockerfile.sandbox .\n\n# Run the system\necho \"Launching the BAT OS...\"\npython a4ps/main.py\n\necho \"BAT OS session complete.\"","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}