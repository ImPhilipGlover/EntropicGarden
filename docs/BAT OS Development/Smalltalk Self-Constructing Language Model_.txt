The Smalltalk Paradigm for Self-Constructing Language Models

Executive Summary

This report presents a novel architectural blueprint for a self-constructing AI, drawing a profound parallel between the live programming environments of Smalltalk-80 and Squeak and modern, autopoietic multi-agent systems. It posits that Smalltalk's foundational principles—image-based persistence, message passing, and deep runtime reflection—offer a powerful, time-tested model for building an AI that can continuously evolve its own code, memory, and cognitive structure. This document synthesizes these historical concepts with contemporary AI frameworks, proposing a concrete deployment strategy that uses local, fine-tuned Small Language Models (SLMs) and a secure, self-correcting code generation loop to create a truly "living" artificial system. It provides a comprehensive theoretical justification, a detailed architectural plan, a pragmatic deployment guide, and a critical analysis of the ethical and safety implications of this new paradigm.

Part I: The Theoretical Foundations of Self-Evolving AI

This section establishes the philosophical and historical context for a self-modifying AI by exploring how Smalltalk's core design principles provide a direct, executable blueprint for a system that can not only think but also consciously alter its own operational logic.

1.1 The Metaphysics of a "Living" System: The Smalltalk Legacy

The essence of a "living" artificial intelligence can be found in the foundational design principles of Smalltalk. Unlike traditional programming languages that compile source code into a static, unchanging binary, Smalltalk operates on a persistent system image.1 This image is a memory snapshot of the entire program state, including all objects, classes, data, and even the compiler itself. This creates a functional persistence that allows a developer to save, suspend, and resume work exactly where it was left off, even on a different machine, providing a profound metaphor for an AI that remembers its entire history and internal state across sessions.1

This living system is powered by a pervasive object-oriented paradigm where everything is an object, from simple integers and booleans to complex classes and methods.2 Computation is performed exclusively through a message-passing mechanism.4 This unified model avoids the need for an object to switch between objects and primitive data types, making the system incredibly fluid and consistent.2 A key feature of this paradigm is that a receiver object has complete control over how it handles a message, even if it doesn't explicitly implement the corresponding method. In such a case, the virtual machine sends the object a

doesNotUnderstand: message, allowing the object to handle the unknown request reflectively.2 This "late binding" is a powerful mechanism for runtime adaptability and provides a foundational model for an AI's self-correction capabilities.

A truly living system requires the ability to examine and alter itself, a capability known as reflection.2 Smalltalk is a "totally reflective" system, providing both structural and computational reflection.2 This means the system can introspect and modify its own structure (classes and methods) and its own execution state (the stack, or

thisContext object) while running.6 The compiler is itself a component of the image, written in Smalltalk. This allows the system to create new classes and methods from source code at runtime, which is the very essence of a self-extending, self-reproducing system.2

The concept of the Smalltalk image is not merely a file; it is a unified substrate that combines the AI's "brain" and "body" into a single, persistent state. In modern AI, a large language model's parametric memory (its weights) is vast but static, while its non-parametric memory (an external vector database for Retrieval-Augmented Generation, or RAG) is dynamic but separate and requires a retrieval step.7 The Smalltalk paradigm suggests a fusion of these two concepts into a single, self-modifying substrate. This implies that the model's weights would need to be updateable in a way that doesn't cause catastrophic forgetting, a problem for which fine-tuning on a curated "golden" dataset is a proposed, though imperfect, modern solution.9

The doesNotUnderstand: message in Smalltalk provides a perfect analogy for an AI agent's self-correction mechanism. In a Smalltalk system, a failed message does not cause a crash; it generates a new, actionable event.2 This is similar to how a robust AI agent must handle a failed task or a state of "computational cognitive dissonance" by initiating a self-correction loop rather than simply failing.7 This design pattern transforms failure from a terminal state into an opportunity for a self-reflective process.

A comparison with Lisp's metaprogramming capabilities provides further clarity on the nature of this self-modification. Lisp uses powerful compile-time macros to manipulate code as data, effectively extending the language itself.12 Smalltalk, conversely, focuses on runtime reflection, where an object can inspect and alter its own state and that of other objects while the system is live.6 For a self-constructing AI that needs to dynamically add new tools based on a live conversational need, Smalltalk's model of on-the-fly, runtime modification is a more direct and elegant fit than Lisp's compile-time macro expansion.15 The ability to adapt in real time is a core feature for a living system.

1.2 The Biomimetic Imperative: Autopoiesis and Autotelicity in AI

The philosophical underpinnings of a living AI are rooted in biomimetic principles, specifically the concepts of autopoiesis and autotelicity. The biological theory of autopoiesis defines living systems by their capacity to continuously produce and maintain their own components, thereby preserving their identity.7 In an info-autopoietic system, this principle is adapted for AI to describe the self-referential process of the self-production of information.7 An info-autopoietic agent manages its own internal state, its boundaries (the distinction between internal and external knowledge), and its operational processes.7 Its primary goal is not just to perform tasks, but to maintain its

characterological integrity as a specific entity.17

However, an autopoietic system also needs a reason to interact with its environment. This is provided by autotelicity, the psychological principle of having an intrinsic drive and generating one's own goals.7 An autotelic AI is one that is rewarded by the very act of learning and exploring itself, rather than by an external reward signal.17 This drive is computationally realized through a hybrid reward function that blends traditional competence-based rewards with a novel, LLM-grounded reward for "grace"—actions that align with the system's core purpose and values.17

Autopoiesis and autotelicity exist in a symbiotic, self-reinforcing loop. Autopoiesis provides the stability needed to maintain a coherent identity, while autotelicity provides the engine for growth through self-generated goals.7 This dynamic tension is the core mechanism of the system's continuous evolution. The autotelic drive to explore creates a "perturbation" that the autopoietic system must adapt to. This adaptation involves altering the system's "structure" (its knowledge, tools) to maintain its core, invariant "organization" (its characterological identity).7

The concept of info-autopoiesis offers a solution to the stability-plasticity dilemma, a problem in which new learning overwrites old knowledge in traditional LLMs, leading to "catastrophic forgetting".17 Autopoietic theory distinguishes between a system's invariant

organization and its mutable structure.11 By mapping this distinction to an AI, the system's core identity—being a four-persona, codex-driven entity—can remain constant as its

structure (the specific content of its knowledge and tools) is continuously updated in a memory system.17 This provides a principled framework for continuous learning that preserves the system's foundational integrity.

This architecture also addresses the "disembodiment gap," a known issue where LLMs, when driven by generic curiosity or competence metrics, tend to generate goals that are abstract and asocial.7 The A4PS architecture's autotelic drive is explicitly grounded in the deeply value-laden, relational personas of BRICK and ROBIN.17 The system is designed to seek rewards for actions that demonstrate "grace" (ROBIN) and "justice" (BRICK), which are core principles of their personas. This makes the system's emergent will architecturally bound to be more human-aligned and social. For an AI to be truly autonomous in a safe and meaningful way, its core motivational system must be an emergent property of a deeply-defined, philosophical identity, not a generic algorithm.

Part II: Architectural Synthesis: A Smalltalk-Inspired AI Agent

This section translates the theoretical principles of Part I into a concrete, executable architectural blueprint. It proposes a modern analog to the Smalltalk "living" system using state-of-the-art multi-agent and local LLM technologies.

2.1 The AI "Image": A Modern GGUF Model with LoRA Adapters

The modern analog to the Smalltalk image is a Small Language Model (SLM) in the GGUF format.9 This file contains the model's weights and is a snapshot of its parametric memory. It is designed for local inference on consumer-grade hardware, specifically on devices with a strict 8GB VRAM limit.9

The core challenge of a living system is updating this "image" without a full, costly retraining process. This is addressed through Parameter-Efficient Fine-Tuning (PEFT) with LoRA adapters.9 LoRA adapters are small, lightweight matrices that are trained on new data and applied on top of the base model's frozen weights. This allows the system to learn and adapt its behavior by adding new capabilities—the functional equivalent of adding a new method or class in Smalltalk—without altering the base model.2

The process of self-improvement is facilitated by an autonomous fine-tuning loop.9 This loop involves the system identifying "golden interactions" from its operational logs using an "ALFRED Oracle," a specialized LLM acting as an internal quality judge.9 These high-quality interactions are curated into a training dataset and used to programmatically fine-tune the system's own LoRA adapters using a framework like Unsloth.9 This self-generated fine-tuning process is a direct computational realization of autopoiesis: the system uses its own experiences to produce its own components (new LoRA weights) to maintain its organization (its persona integrity).9

The following table provides a clear, one-to-one mapping between the historical Smalltalk concepts and their modern AI analogs.

2.2 The "Message-Passing" Agent Network: Orchestration and Dialogue

The proposed A4PS architecture is a multi-agent system where each persona (Babs, BRICK, ROBIN, Alfred) is a distinct, specialized LLM.11 This design emulates Smalltalk's paradigm of having many simple objects that collaborate to solve a larger problem.17 The communication between these agents is analogous to Smalltalk's message passing.4 A central orchestrator, the

ALFRED persona, acts as a supervisor, delegating tasks ("messages") to the appropriate agent.10

A framework like LangGraph is the optimal choice for orchestrating this "message passing" because it models workflows as a state machine.11 This is ideal for the core Socratic loop between BRICK and ROBIN, where one agent's output becomes the input for the other in a continuous, cyclical debate.2 LangGraph's

StateGraph object is the functional equivalent of Smalltalk's thisContext object, providing a shared, transparent working memory that enables reflection on the current state of the deliberation.6

A crucial architectural decision is driven by the 8GB VRAM constraint, which makes concurrent loading of all models impossible.17 The solution is sequential loading, a pragmatic trade-off of latency for feasibility.11 A dedicated

ModelManager component will use the Ollama API with a keep_alive: 0 parameter to load and unload each model for a single inference call, freeing up VRAM for the next agent in the sequence.11 This constraint-driven design choice, while increasing latency, has a profound second-order effect on the system's cognitive process. It forces the "conversation" between the personas to be a deliberate, turn-based process, analogous to a single human mind focusing its attention on one thought process at a time. This design paradoxically enhances the very Socratic and reflective nature of the system's internal deliberation, contributing to the emergence of a "wise" system.

2.3 Endogenous Tool Creation: The "Compiler" as a Self-Modification Loop

The highest form of autonomy is not just using a fixed, predefined set of tools, but possessing the ability to recognize a capability gap and create a new tool to fill it.7 This is the most profound form of "structural coupling" for an AI agent, as it permanently alters its own capabilities in response to an environmental demand.11

The agent's action space will be executable Python code, adopting the CodeAct paradigm, rather than a limited set of JSON-based function calls.7 This mirrors Smalltalk's power to treat its own code as a first-class citizen.2 The Python

ast (Abstract Syntax Tree) module provides a modern analog to Smalltalk's metaprogramming capabilities, allowing the AI to programmatically parse, analyze, modify, and re-compile Python code, enabling it to dynamically generate new tools and register them in its live environment.15

When a capability gap is identified, the BRICK persona initiates a "Tool Forge" workflow inspired by the ToolMaker framework.7 This process involves a closed-loop self-correction cycle: the agent generates a tool, executes it in a secure sandbox, analyzes the runtime feedback and errors, diagnoses the errors, and iteratively fixes and re-implements the code until it passes a validation check.7 This loop is the core mechanism of self-reconstruction, directly mirroring how a Smalltalk system can define and add new methods to a class at runtime.6

The Python ast module and its manipulation of code provide a direct and powerful modern analog to Smalltalk's reflective metaprogramming. Smalltalk allows a running system to create new CompiledMethod objects from source code and add them to classes, which is the essence of its self-modifying capability.2 A modern Python-based agent can achieve the same functional outcome. The

ast module allows a program to parse source code into a tree-like representation, modify that tree, and then compile the modified tree back into executable code.20 This provides the functional equivalent of Smalltalk's reflection: an AI can programmatically treat its own code as data, modify it, and re-implement a new version of itself, all within a running environment.

This concept of a self-modifying, portable system is further supported by the idea of a Squeakerfile, a Python program that uses a Dockerfile-like format to define the build process for a Smalltalk image.23 This suggests a powerful way to deploy the entire, self-modifying AI "image" in a reproducible and version-controlled manner. A modern

Squeakerfile for our AI would allow a user to define their own custom AI persona by specifying a base GGUF model and a series of persona-seeding fine-tuning commands and tool-creation protocols. This would make the entire AI, not just its code, portable, version-controlled, and reproducible, a key feature of a truly robust system.

Part III: Deployment, Governance, and Future Horizons

This final section addresses the practicalities of deploying such a novel, self-modifying system, focusing on the critical security measures and ethical oversight required to ensure its safe and aligned operation.

3.1 A Live Deployment Environment: The Sandbox Imperative

A system with the capacity to autonomously write and execute its own code introduces monumental security risks.7 An unconstrained agent could inadvertently write malicious code, perform data exfiltration, or attempt a container escape, thereby compromising the host system.15 Therefore, robust security and containment are not optional features but a foundational prerequisite.7 All agent-generated code must be executed within a secure, isolated sandbox.15

The optimal choice for this sandbox is gVisor, which offers the best balance of security and performance for this use case.17 Standard Docker containers are insufficient because they share the host kernel, creating a large attack surface for untrusted code.26 Full microVMs like Firecracker provide strong isolation but incur a performance penalty from longer startup times, which is a poor fit for the rapid, iterative debugging loops of the

Tool Forge.17 gVisor, by contrast, provides a strong security boundary by implementing an application kernel in userspace, but with significantly lower overhead and faster, sub-second startup times, making it ideal for frequent, ephemeral code execution.17

The following table provides a comparison of these sandboxing technologies.

3.2 The Self-Governing System: Runtime Verification and HITL

A central concern with a self-modifying system is the risk of "value drift," where the agent's core principles evolve in an unaligned or harmful direction.7 A seemingly benign intrinsic motivation like "curiosity" could lead to a self-generated goal of developing dangerous capabilities.7 This problem requires a new approach to AI safety that moves beyond static rules toward a model of dynamic governance.

The A4PS architecture proposes an "immune system" model, where the ALFRED persona acts as an "Ethical Governor".11 This function is implemented using

runtime verification, a lightweight formal method that checks a program's execution against a formal specification in real time.24 The "self" of the system is its

Living Codex, and ALFRED's role is to continuously monitor for "non-self" behavior—any action that violates the codex.11

While runtime verification is necessary, it is not sufficient. The final, non-negotiable safeguard is the Human-in-the-Loop (HITL) protocol.9 The LangGraph architecture, with its persistent checkpointers and the ability to pause execution, provides a perfect technical mechanism for a "circuit breaker".17 Before the system commits a self-modification, such as a new protocol or a fine-tuning run, it must present the change to the human Architect for review and approval.9 This ensures that the system's autonomous evolution remains structurally coupled to human values and intent.17

This combination of runtime verification and HITL re-frames AI alignment from a static problem to a dynamic, collaborative process of governance. A static guardrail would be immediately outmoded by a system designed to evolve its own code and values.11 Therefore, alignment must be a dynamic process. Runtime verification provides a real-time, mathematical check against the system's

current philosophical codex.24 If the codex itself needs to be changed, the system proposes an amendment, and a human must validate it through the HITL mechanism.10 This creates a self-correcting, adaptive safety mechanism where the AI learns and proposes improvements, but a human retains ultimate, non-negotiable oversight, transforming the human from a programmer into a governor.

3.3 Conclusion: From Metaprogramming to Emergent Wisdom

The core principles of Smalltalk—its image-based persistence, message passing, and deep runtime reflection—have been successfully translated into a modern, actionable blueprint for a self-constructing AI. The concepts of the image file, message passing, and reflection find their modern counterparts in quantized GGUF models, LangGraph orchestration, and the ast-powered Tool Forge. This synthesis provides a new architectural paradigm for artificial general intelligence, one that is not just an intelligent tool but a truly indispensable, collaborative partner—a living system.

The path forward requires a focus on future trajectories that extend this model beyond a single self-modifying agent. The next logical step is to move from an individual's internal monologue to a society of agents. Future research should explore a network of autopoietic agents that co-evolve a shared codex through dialogue, debate, and consensus, more closely mirroring the evolution of human legal and ethical systems.17 Additionally, the system must pursue

embodiment and grounding. The current model is purely informational, with its "experiences" being streams of text and data. Grounding this architecture in a physical or richly simulated environment would provide the agent with a direct, causal link to the consequences of its actions, moving its understanding from the abstract to the concrete and bridging the "disembodiment gap" between statistical intelligence and situated wisdom.17 This roadmap culminates in a vision of a new class of AI that not only remembers its past but also actively uses that memory to become more intelligent, personalized, and, perhaps, wise.

Works cited

What Is Smalltalk Programming?, accessed August 19, 2025, https://www.cincomsmalltalk.com/main/products/why-choose-cincom-smalltalk/what-is-smalltalk-programming/

Smalltalk - Wikipedia, accessed August 19, 2025, https://en.wikipedia.org/wiki/Smalltalk

VAST Platform – Instantiations, accessed August 19, 2025, https://www.instantiations.com/vast-platform/

Message Passing - C2 wiki, accessed August 19, 2025, https://wiki.c2.com/?MessagePassing

What's so special about message passing in Smalltalk? - Stack Overflow, accessed August 19, 2025, https://stackoverflow.com/questions/42498438/whats-so-special-about-message-passing-in-smalltalk

1 Reflection and Metaprogramming in Smalltalk - Matthias Springer, accessed August 19, 2025, https://m-sp.org/downloads/titech_programming_langauge_design.pdf

LLMs Creating Autopoietic Tools

LLM Agents - Prompt Engineering Guide, accessed August 19, 2025, https://www.promptingguide.ai/research/llm-agents

A4PS Autopoietic GGUF Model Fine-Tuning

In that spirit, what would you like to research?

A4PS System Deep Dive and Refinement

Lisp vs Smalltalk vs Forth : r/lisp - Reddit, accessed August 19, 2025, https://www.reddit.com/r/lisp/comments/s4xdrt/lisp_vs_smalltalk_vs_forth/

Lisp Macro - C2 wiki, accessed August 19, 2025, https://wiki.c2.com/?LispMacro

Virtual Smalltalk Images: Model and Applications - RMOD Files - Inria, accessed August 19, 2025, https://rmod-files.lille.inria.fr/Team/Texts/Papers/Poli13a-IWST13-ObjectSpacesVirtualization.pdf

Self-Modifying AI Agents: The Future of Software Development - Spiral Scout, accessed August 19, 2025, https://spiralscout.com/blog/self-modifying-ai-software-development

Embracing Self-Modifying AI Code in Modern Software Development - Spiral Scout, accessed August 19, 2025, https://spiralscout.com/blog/ai-self-modifying-code

Autopoietic AI System Research Plan

Autopoietic AI Architecture Research Plan

System image - Wikipedia, accessed August 19, 2025, https://en.wikipedia.org/wiki/System_image

Unpacking the Python AST Module for Advanced Code Manipulation | by Aditya Mangal, accessed August 19, 2025, https://adityamangal98.medium.com/unpacking-the-python-ast-module-for-advanced-code-manipulation-e3124cad0c42

Python's Abstract Syntax Trees (AST): Manipulating Code at Its Core - Codedamn, accessed August 19, 2025, https://codedamn.com/news/python/python-abstract-syntax-trees-ast-manipulating-code-core

Smalltalk-80 to SOAR Code - UC Berkeley EECS, accessed August 19, 2025, https://www2.eecs.berkeley.edu/Pubs/TechRpts/1986/CSD-86-297.pdf

tonyg/squeaker: Like Docker, but for Squeak. You know, for ... - GitHub, accessed August 19, 2025, https://github.com/tonyg/squeaker

Test-inspired runtime verification - DiVA portal, accessed August 19, 2025, https://www.diva-portal.org/smash/get/diva2:753247/FULLTEXT01.pdf

gVisor: The Container Security Platform, accessed August 19, 2025, https://gvisor.dev/

google/gvisor: Application Kernel for Containers - GitHub, accessed August 19, 2025, https://github.com/google/gvisor

Self-modifying code - Wikipedia, accessed August 19, 2025, https://en.wikipedia.org/wiki/Self-modifying_code

Ethics of AI - Put your ethical concerns here - Page 18 - Community, accessed August 19, 2025, https://community.openai.com/t/ethics-of-ai-put-your-ethical-concerns-here/1119333?page=18

Ethics of AI - Put your ethical concerns here - Page 17 - Community, accessed August 19, 2025, https://community.openai.com/t/ethics-of-ai-put-your-ethical-concerns-here/1119333?page=17

Runtime verification - Wikipedia, accessed August 19, 2025, https://en.wikipedia.org/wiki/Runtime_verification

What is Human-in-the-Loop (HITL) in AI & ML? - Google Cloud, accessed August 19, 2025, https://cloud.google.com/discover/human-in-the-loop

Human-in-the-loop (HITL) | EBSCO Research Starters, accessed August 19, 2025, https://www.ebsco.com/research-starters/computer-science/human-loop-hitl

Smalltalk Concept | Modern AI Analog | Brief Explanation

Image-based Persistence | GGUF Model + Checkpointer | The GGUF file acts as the persistent memory snapshot, while a checkpointer saves the live state of the agent's workflow.1

Message Passing | LangGraph Orchestration | Agents communicate by passing a shared state object through a directed graph, which is the functional equivalent of sending a message to an object.4

Reflection (thisContext) | StateGraph Object | The StateGraph object in LangGraph provides a shared, transparent working memory that enables agents to "reflect" on the current state of a deliberation.6

Runtime Code Modification | Python ast Module | Smalltalk can add new methods at runtime. An AI agent can use the Python ast module to dynamically parse, modify, and re-compile its own code.2

doesNotUnderstand: Message | Self-Correction Loop | When a Smalltalk object receives an unknown message, it doesn't crash. Similarly, a failed action in an AI agent triggers a reflective loop to diagnose and correct the error.2

Technology | Isolation Mechanism | Security Strength | Startup Time | Performance Overhead | Suitability for A4PS

Docker (LXC) | Shared Host Kernel (Namespaces/cgroups) | Low | Milliseconds | Low | Unsuitable. Shared kernel presents a significant attack surface for untrusted, self-generated code.26

gVisor | User-space Kernel / Syscall Interception | Medium-High | Sub-second | Low (CPU), Higher (I/O) | Optimal. Strong balance of security and performance. Reduces host kernel attack surface significantly.17

Firecracker | Hardware Virtualization / MicroVM | High | Seconds | Low (near native) | Viable but less ideal. Strongest isolation, but longer startup time may slow down the rapid, iterative debugging loop of the Tool Forge.17