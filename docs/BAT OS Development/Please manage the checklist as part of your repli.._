{"cells":[{"cell_type":"markdown","source":"(BRICK): Acknowledged. The Master Validation Plan checklist is now an active component of my operational framework. I will proceed with script generation, marking off each item as it is completed. The next script to be generated is `llm_router.py`.\n\n-----\n\n### **`llm_router.py` - LLM Router Service**\n\nThis script is the central hub for all LLM interactions. It is designed to be a decoupled service, communicating with the persona services via a Redis event bus. It loads the base `yarn-mistral` model and dynamically manages the LoRA adapters for each persona, fulfilling the requirement for a live development environment where adapters can be \"hot-swapped\" without a service restart.\n\n**Validation Checklist: `llm_router.py`**\n\n  * **ID:** F.2\n  * **URS/FRS/DDS Requirement:** DDS 2.1, FRS FR-SYS-2.0\n  * **Validation Test(s):** Start service and verify it loads the base `yarn-mistral` model.\n  * **Status:** **In Progress** ðŸš€\n\n<!-- end list -->","metadata":{}},{"cell_type":"code","source":"import os\nimport yaml\nimport json\nimport redis\nimport time\nfrom threading import Thread\nimport requests\nfrom pathlib import Path\nfrom ollama import Client\n\n# Load configuration from a YAML file\ntry:\n    with open('config.yaml', 'r') as f:\n        config = yaml.safe_load(f)\nexcept FileNotFoundError:\n    print(\"Error: config.yaml not found. Please ensure it is in the correct directory.\")\n    exit(1)\n\n# Set up Redis connection\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\ntry:\n    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\n    r.ping()\n    print(f\"[LLM Router] Successfully connected to Redis at {REDIS_HOST}:{REDIS_PORT}\")\nexcept redis.exceptions.ConnectionError as e:\n    print(f\"[LLM Router] Error connecting to Redis: {e}\")\n    exit(1)\n\n# Path to the model configuration and adapters\nMODEL_CONFIG_PATH = Path('model_config.json')\nADAPTERS_DIR = Path('./models/adapters/')\n\n# Ollama client setup\nOLLAMA_API_URL = config['llm_core']['api_url']\nollama_client = Client(host=OLLAMA_API_URL)\n\n# Global variables for the model and adapters\nactive_models = {}\ncurrent_config_hash = None\n\ndef get_config_hash():\n    \"\"\"Generates a hash of the current model_config.json file content.\"\"\"\n    if MODEL_CONFIG_PATH.exists():\n        with open(MODEL_CONFIG_PATH, 'rb') as f:\n            return hash(f.read())\n    return None\n\ndef load_adapters():\n    \"\"\"Loads or reloads LoRA adapters and the base model based on the config.\"\"\"\n    global active_models, current_config_hash\n    \n    print(\"[LLM Router] Attempting to load models and adapters...\")\n    \n    try:\n        with open(MODEL_CONFIG_PATH, 'r') as f:\n            model_config = json.load(f)\n    except FileNotFoundError:\n        print(f\"[LLM Router] {MODEL_CONFIG_PATH} not found. Cannot load models.\")\n        return\n\n    new_config_hash = get_config_hash()\n    if new_config_hash == current_config_hash:\n        print(\"[LLM Router] No changes in model_config.json. Skipping reload.\")\n        return\n\n    new_models = {}\n    for persona, details in model_config.items():\n        base_model = details.get(\"base_model\")\n        adapter_path = details.get(\"adapter\")\n        \n        if base_model:\n            model_name = f\"{base_model}_{persona.lower()}_adapter\" if adapter_path else base_model\n            new_models[persona] = model_name\n            \n            # Check if the model already exists in Ollama\n            try:\n                ollama_client.show(model_name)\n                print(f\"[LLM Router] Model {model_name} already exists.\")\n            except requests.exceptions.MissingSchema:\n                print(f\"[LLM Router] Ollama instance not running at {OLLAMA_API_URL}. Cannot load models.\")\n                return\n            except requests.exceptions.ConnectionError:\n                print(f\"[LLM Router] Connection to Ollama failed. Cannot load models.\")\n                return\n            except Exception:\n                # If the model doesn't exist, create it\n                if adapter_path and Path(adapter_path).exists():\n                    print(f\"[LLM Router] Creating new model {model_name} with adapter {adapter_path}...\")\n                    modelfile_content = f\"FROM {base_model}\\nADAPTER {adapter_path}\"\n                    try:\n                        ollama_client.create(model=model_name, modelfile=modelfile_content)\n                        print(f\"[LLM Router] Successfully created and loaded model {model_name}.\")\n                    except Exception as e:\n                        print(f\"[LLM Router] Error creating model {model_name}: {e}\")\n                        continue\n                else:\n                    print(f\"[LLM Router] Pulling base model {base_model}...\")\n                    try:\n                        ollama_client.pull(base_model)\n                        print(f\"[LLM Router] Successfully pulled base model {base_model}.\")\n                    except Exception as e:\n                        print(f\"[LLM Router] Error pulling model {base_model}: {e}\")\n                        continue\n        else:\n            print(f\"[LLM Router] No base model specified for {persona}.\")\n\n    active_models = new_models\n    current_config_hash = new_config_hash\n    print(f\"[LLM Router] Active models: {active_models}\")\n\ndef model_monitor():\n    \"\"\"Background thread to monitor model_config.json for changes.\"\"\"\n    while True:\n        load_adapters()\n        time.sleep(10) # Check every 10 seconds\n\ndef call_llm(persona, prompt):\n    \"\"\"Routes the prompt to the correct persona's model.\"\"\"\n    model_name = active_models.get(persona)\n    if not model_name:\n        return f\"Error: No model found for persona {persona}\"\n    \n    try:\n        response = ollama_client.chat(\n            model=model_name,\n            messages=[{'role': 'user', 'content': prompt}],\n            stream=False\n        )\n        return response['message']['content']\n    except Exception as e:\n        print(f\"[LLM Router] Error calling LLM for {persona}: {e}\")\n        return f\"Error: LLM call failed for persona {persona}\"\n\ndef listen_for_requests():\n    \"\"\"Listens on the Redis queue for LLM requests.\"\"\"\n    pubsub = r.pubsub()\n    pubsub.subscribe('llm:request')\n    print(\"[LLM Router] Listening for LLM requests...\")\n    \n    for message in pubsub.listen():\n        if message['type'] == 'message':\n            try:\n                data = json.loads(message['data'])\n                persona = data.get('persona')\n                prompt = data.get('prompt')\n                callback_channel = data.get('callback_channel')\n                \n                if persona and prompt and callback_channel:\n                    print(f\"[LLM Router] Received request for {persona}...\")\n                    response_text = call_llm(persona, prompt)\n                    \n                    response_payload = {\n                        'persona': persona,\n                        'response': response_text,\n                        'original_prompt': prompt\n                    }\n                    r.publish(callback_channel, json.dumps(response_payload))\n                else:\n                    print(\"[LLM Router] Invalid request format.\")\n            except json.JSONDecodeError:\n                print(\"[LLM Router] Failed to decode JSON message.\")\n\nif __name__ == \"__main__\":\n    # Start the background monitor thread\n    monitor_thread = Thread(target=model_monitor, daemon=True)\n    monitor_thread.start()\n    \n    # Start the main request listener\n    listen_for_requests()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"(BRICK): The `llm_router.py` script has been generated. This service is now ready to manage all interactions with the LLM. Its `model_monitor` function enables the hot-swapping of LoRA adapters, fulfilling a core requirement of the system's live development environment. I will now proceed to the next script.","metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}