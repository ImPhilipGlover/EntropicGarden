An Architectural Analysis of Python Libraries for Vector Symbolic Architectures and a Development Roadmap for MVA Integration

Executive Summary

This report presents a comprehensive analysis of the existing Python ecosystem for Vector Symbolic Architectures (VSA), also known as Hyperdimensional Computing (HDC), with a specific focus on identifying a suitable library for integration into the Minimum Viable Application (MVA) of the AURA/BAT OS. The evaluation is guided by the MVA's unique and demanding architectural principles, particularly its reliance on a "Living Image" of prototype-based objects, its need for high-performance, continuous operation, and its non-negotiable requirement for complex-valued models such as Fourier Holographic Reduced Representations (FHRR).

The central finding of this analysis is that among the available open-source libraries, torchhd is the only viable candidate that meets the MVA's core technical requirements. Its explicit support for FHRR, its high-performance backend powered by PyTorch with GPU acceleration, and its status as an actively maintained project with a robust feature set distinguish it decisively from all other alternatives. Libraries such as hdlib and Holographic-Reduced-Representations, while offering interesting features, fail to meet the critical criteria of FHRR compatibility and high-performance computation necessary for a "living" system like the MVA.

However, the adoption of torchhd is not without its challenges. A significant architectural gap exists between torchhd's class-based, functional API and the MVA's pure, prototype-based object model. Overcoming this impedance mismatch will require a deliberate and thoughtful integration strategy, centered on the development of a dedicated adapter layer within the MVA to ensure philosophical and structural coherence.

For circumstances where perfect architectural unity is paramount and a significant development investment is feasible, this report also provides a detailed and actionable roadmap for the creation of a custom, MVA-native VSA library, codenamed "Hyperion." This blueprint outlines a system designed from first principles to be a seamless extension of the MVA's UvmObject world, integrated with state-of-the-art vector search backends like FAISS and DiskANN. While this path represents a substantial undertaking, it offers the ultimate solution for achieving a VSA implementation that is in perfect alignment with the MVA's ambitious vision.

The final recommendation is to proceed with the integration of torchhd as the most pragmatic and resource-efficient path forward, while treating the "Hyperion" roadmap as a long-term strategic goal for the MVA's continued evolution.

Section 1: Evaluation of the Python VSA/HDC Ecosystem for the MVA Architecture

This section delivers a thorough analysis of the current library landscape, moving from a broad survey to a focused, deep-dive comparison, culminating in a specific and actionable recommendation for the MVA project.

1.1. Survey of the VSA/HDC Library Landscape

A systematic review of the Python ecosystem for libraries related to VSA and HDC reveals a specialized but fragmented landscape. An initial filtering process is necessary to eliminate irrelevant search results that use the "VSA" acronym for other domains, such as "Vertical Slice Architecture" in the.NET ecosystem 1 or "Volume Spread Analysis" in financial technical analysis.2 Similarly, libraries using the term "vector" in a generic sense, such as the

scikit-hep/vector package for 2D, 3D, and 4D Lorentz vectors in high-energy physics, are also excluded from this analysis.3

After this initial filtering, a shortlist of primary candidates for VSA/HDC implementation in Python emerges:

torchhd: A mature, actively maintained library built on top of PyTorch.5 It is positioned as a high-performance, easy-to-use framework for both researchers and developers, leveraging PyTorch for optimized tensor operations and GPU acceleration.7 Its documentation is comprehensive and explicitly lists support for a wide range of VSA models, making it a leading contender.6

hdlib: A general-purpose library for designing VSAs, notable for its intuitive, object-oriented API centered on Space and Vector classes.9 While its design philosophy aligns well with object-centric systems, its implementation appears to be based on NumPy, suggesting it is primarily CPU-bound and may lack the performance characteristics required for the MVA.11

Holographic-Reduced-Representations: A specialized library focused exclusively on the HRR model.12 It is notable for its support of multiple backends, including TensorFlow, PyTorch, JAX, and Flax, offering significant flexibility.12 However, its development activity appears less frequent compared to
torchhd, raising concerns about its long-term maintenance status.12

hdtorch: An earlier PyTorch-based library that includes custom CUDA extensions for specific hypervector operations.13 Its documentation from 2021 indicates that support for models like HRR and FHRR was planned as a "coming soon" feature, suggesting the library may be incomplete or has been superseded by the more comprehensive
torchhd project.14

1.2. Deep Compatibility Analysis with the AURA/BAT OS Architecture

A rigorous assessment of these candidates against the MVA's unique architectural and philosophical requirements is necessary. The MVA is not a conventional application but a "living system" designed for continuous, autonomous evolution, imposing a unique set of constraints on any potential dependency.15

Criterion 1: FHRR Model Support (Non-Negotiable Requirement)

The primary technical requirement for the MVA is support for Fourier Holographic Reduced Representations (FHRR). FHRR is a specific VSA model that operates on complex-valued vectors. Its key advantage is that the binding operation, which is defined as circular convolution in the spatial domain, becomes a highly efficient element-wise complex multiplication in the frequency domain, accessible via the Fast Fourier Transform (FFT).17 This computational efficiency is critical for high-performance applications.

torchhd: This is the only library in the survey that explicitly lists FHRRTensor as a fully implemented and supported VSA model in its official documentation.6 This single feature makes it the most compelling candidate.

Holographic-Reduced-Representations: This library, as its name implies, focuses on HRR. It supports a real-valued FFT variant but makes no explicit mention of the complex-valued FHRR model.12

hdlib: The documentation for hdlib describes generic bind, bundle, and permute operations without specifying the underlying mathematical models.10 This ambiguity makes it impossible to confirm FHRR compatibility without a deep source code audit.

hdtorch: The library's 2021 PyPI page lists FHRR as a planned feature, strongly suggesting it is not implemented in the available versions.14

Criterion 2: Performance and Scalability

The MVA is conceptualized as a persistent, "always-on" system engaged in continuous info-autopoiesis—the recursive self-production of its own logic.16 This mode of operation demands a VSA implementation that is not only correct but also highly performant and capable of leveraging modern hardware accelerators.

torchhd: Built directly on PyTorch, torchhd inherits a world-class, high-performance tensor computation backend. It seamlessly supports GPU acceleration, which is essential for the high-dimensional matrix operations inherent in VSA.6 The library's authors claim performance improvements of up to 100x compared to other publicly available implementations, a direct result of its optimized backend.5

hdlib: The library's examples and documentation suggest a dependency on NumPy, a powerful but primarily CPU-bound library. For the dimensionality required by VSA (typically D>10,000), CPU-based computation would introduce a significant performance bottleneck, hindering the real-time responsiveness expected of a "living" system.

Criterion 3: Architectural and Philosophical Coherence

The MVA's architecture is profoundly unconventional, centered on a "Living Image" of persistent, prototype-based UvmObject instances.16 The ideal VSA library would integrate into this object world with minimal friction, respecting its core principles of runtime mutability and prototypal inheritance.

This criterion reveals a fundamental tension between capability and coherence in the existing ecosystem. The most functionally capable library, torchhd, is not the most philosophically aligned out-of-the-box. Its API is primarily functional (e.g., torchhd.functional.bind(a, b)), which is procedural and contrasts with the MVA's "everything is an object" message-passing paradigm.15 However,

torchhd also provides a VSATensor class hierarchy (e.g., FHRRTensor), which offers an object-oriented interface.8 This provides a viable, if imperfect, integration point. A

UvmObject within the MVA could be designed to wrap and manage an FHRRTensor instance, encapsulating the VSA logic and exposing it through a message-passing interface that is coherent with the rest of the system.

Conversely, hdlib's API, with its first-class Space and Vector objects, is philosophically much closer to the MVA's design.10 Its object-centric approach feels more "native" to the MVA's world. However, this philosophical elegance cannot compensate for its lack of FHRR support and GPU acceleration. Given the MVA's principle of "Structural Empathy"—where stability, security, and performance are considered tangible expressions of respect for the user's reality—the pragmatic choice must favor the more capable and performant system.20 The engineering effort required to build an architectural adapter for

torchhd is a justifiable trade-off to gain its essential functionality and performance.

Criterion 4: Ecosystem and Maintenance

The MVA is a long-term research and development project. It requires dependencies that are not only technically suitable but also stable, well-documented, and actively maintained to ensure future viability.

torchhd: The project demonstrates all the signs of a healthy open-source ecosystem. It has a clear governance model with multiple contributors, a history of regular releases, an active GitHub repository, and high-quality documentation.6

Other Libraries: The maintenance status of the other candidates is less certain. The Holographic-Reduced-Representations repository, for example, shows infrequent updates, which poses a risk for a project that would depend on it for a core functionality.12

1.3. Definitive Recommendation

Based on the comprehensive analysis of these four critical criteria, torchhd is the only viable existing library that meets the MVA's non-negotiable requirements. Its explicit support for FHRR, high-performance PyTorch backend, and active maintenance status make it the unequivocally superior choice. The primary challenge associated with its adoption is architectural, requiring the design of a dedicated wrapper or adapter class to bridge the gap between torchhd's FHRRTensor and the MVA's UvmObject model.

The following table provides a summary of this comparative analysis.

Section 2: Roadmap for a Custom, MVA-Native VSA Library ("Hyperion")

Should the MVA project determine that the architectural impedance of integrating torchhd is too great, or that perfect philosophical coherence is a primary goal, the development of a bespoke VSA library is the logical alternative. This section provides a comprehensive and actionable roadmap for such a library, codenamed "Hyperion." This blueprint is designed to serve as a foundational technical specification, ensuring the resulting library is a seamless, native, and high-performance component of the AURA/BAT OS.

2.1. Foundational Design and API Specification

The core design philosophy of Hyperion is that it must be a direct and natural extension of the MVA's existing UvmObject model, ensuring perfect architectural coherence from the ground up.

The Hypervector Prototype

The fundamental building block of the library will be a new core prototype, hypervector_prototype, which will inherit directly from the MVA's root UvmObject.19 This design makes a hypervector a first-class citizen of the "Living Image," allowing it to be persisted, cloned, and modified at runtime using the MVA's established mechanisms.16

The hypervector_prototype will contain the following essential slots:

dimensionality: An integer defining the vector's dimension (e.g., 10000).

data_type: A string specifying the data type of the vector's components (e.g., 'binary', 'real', 'complex').

tensor: The underlying numerical data, which will be stored as a torch.Tensor object. This choice is non-negotiable, as it provides the computational foundation for high-performance operations and GPU acceleration, mirroring the key advantage of torchhd.

VSA Operations as Methods

In keeping with the MVA's message-passing paradigm, the core VSA operations will be implemented as methods on the Hypervector prototype. This enables an intuitive, object-centric syntax (e.g., c = a.bind(b)) that is native to the MVA's way of thinking.

bind(self, other_vector): Implements the binding operation. The specific implementation will depend on the VSA model. For FHRR, this method will perform a forward FFT on the tensor slots of both self and other_vector, execute an element-wise complex multiplication, and then perform an inverse FFT on the result to produce the new bound vector.

bundle(self, other_vector): Implements the bundling or superposition operation, which is typically element-wise addition of the underlying tensors.

permute(self, shifts): Implements permutation, such as a circular shift of the tensor's elements.

similarity(self, other_vector): Calculates and returns a scalar similarity score between two hypervectors (e.g., using cosine similarity on the tensors).

VSA Model Factory

To manage the creation of hypervectors for different VSA models, a dedicated VSA factory object will be implemented. This factory will be responsible for cloning the base hypervector_prototype and installing the correct, model-specific implementations of the core operational methods.

Example usage would be:

my_fhrr_vector = VSA.FHRR.random(dims=10000)

This command would instruct the factory to create a new Hypervector instance, initialize its tensor slot with random complex numbers suitable for FHRR, and ensure that its bind method is the FFT-based implementation.

2.2. High-Performance Indexing and Search Integration

A VSA is only as useful as the ability to efficiently search through the sets of hypervectors it produces. The MVA's architecture implies at least two distinct use cases for similarity search with vastly different performance and scale requirements. The "Living Image" represents the system's active cognitive state, or working memory, where extremely low-latency search on a moderately sized set of vectors is required.19 In contrast, the "Living Codex" represents the system's vast, persistent long-term memory, which could contain billions of hypervectors, far exceeding available RAM.15

This dichotomy makes a single indexing solution insufficient. A hybrid strategy is an architectural necessity. To manage this complexity, a dedicated IndexingManager will be created, adhering to the MVA's "Externalization of Risk" survival strategy by encapsulating and isolating this critical function.16

Architectural Bridge: The IndexingManager

A new persistent UvmObject prototype, indexing_manager_prototype, will serve as the bridge between the Hyperion library and external, state-of-the-art vector search libraries. It will manage a collection of named indexes and will be responsible for the full lifecycle of creating, populating, saving, and loading them. Crucially, it will act as a sophisticated router, directing indexing and search requests to the appropriate backend based on the use case.

FAISS Integration (In-Memory & GPU)

For the MVA's "working memory" and other real-time tasks, the IndexingManager will use the faiss-cpu and faiss-gpu libraries.

Protocol: When a command like manager.index(vector_object, index_name='working_memory') is received, the manager will extract the PyTorch tensor from the Hypervector's slot, convert it to a NumPy array (if necessary), and add it to the designated in-memory FAISS index. It will maintain an internal mapping between the FAISS index ID and the Hypervector's unique oid.

Use Case: Ideal for real-time cleanup operations, nearest-neighbor queries for online learning, and managing the active set of hypervectors in the MVA's cognitive loops.

DiskANN Integration (Large-Scale, Disk-Based)

For the MVA's "long-term memory," the IndexingManager will integrate Microsoft's DiskANN library, which is specifically designed for high-performance approximate nearest neighbor (ANN) search on massive, disk-resident datasets.

Protocol: The manager will provide methods to build a DiskANN index from a large collection of Hypervector objects. This is a more batch-oriented process. Queries to this index will be routed through the DiskANN Python bindings, which efficiently search the on-disk index structure.

Use Case: Essential for building and querying the MVA's "Living Codex," enabling the system to perform semantic search over a vast library of conceptual hypervectors without requiring them all to be loaded into RAM.15

The following table outlines the trade-offs between these two indexing backends in the context of the MVA's needs.

2.3. Comprehensive Validation and Testing Protocol

A new library that sits at the core of the MVA's cognitive architecture must be validated with extreme rigor to earn trust and uphold the principle of "Structural Empathy".21 A three-phase testing protocol is proposed.

Phase 1: Algebraic Correctness (Property-Based Testing)

This phase ensures that the VSA operations are mathematically correct. Using a library like hypothesis, thousands of random hypervectors will be generated to test that the core operations adhere to their defined algebraic properties.

Binding/Unbinding Inverse Property: unbind(bind(A, B), B) must produce a vector that is highly similar to A.

Bundling Commutativity: bundle(A, B) must be highly similar to bundle(B, A).

Permutation Isometry: similarity(permute(A), permute(B)) must be equal to similarity(A, B).

Phase 2: Architectural Integrity Testing

This phase validates the library's integration with the MVA's persistence and object model.

Persistence: Tests will verify that Hypervector objects, including their underlying torch.Tensor data, can be correctly persisted to and reloaded from the "Living Image" database via transaction.commit().

Persistence Covenant: Automated tests will ensure that any method on the Hypervector prototype that modifies its internal state correctly concludes with self._p_changed = True, adhering to the MVA's non-negotiable rule for its ZODB-based persistence layer.16

Phase 3: Integration and Performance Testing

This phase validates the end-to-end functionality and performance of the entire system.

End-to-End Search: Tests will validate the full cycle: create a Hypervector, index it with the IndexingManager into both FAISS and DiskANN backends, and then successfully retrieve it via a similarity search query.

Benchmarking: A suite of performance benchmarks will be developed to measure the speed (latency and throughput) of bind, bundle, and search operations on both CPU and GPU configurations. This will establish a performance baseline for future optimization work.

Section 3: Conclusion and Strategic Outlook

The investigation into the Python VSA/HDC ecosystem reveals a clear and immediate path forward for the MVA project. The torchhd library stands out as the only existing open-source solution that provides the non-negotiable features required by the MVA's advanced design, most notably its implementation of complex-valued Fourier Holographic Reduced Representations and its high-performance, GPU-accelerated PyTorch backend. While other libraries like hdlib offer a more philosophically aligned object model, they lack the critical performance and functional capabilities necessary for a real-time, continuously evolving AI system.

Therefore, the primary recommendation of this report is the adoption of torchhd. The most significant challenge in this path is not functional but architectural: bridging the conceptual gap between torchhd's class-based, functional-style API and the MVA's pure, prototype-based object world. The strategic solution is to invest engineering effort in creating a dedicated adapter layer within the MVA. This adapter will wrap torchhd's FHRRTensor objects inside MVA-native UvmObject prototypes, exposing VSA functionality through a coherent message-passing interface. This approach represents the most pragmatic and resource-efficient strategy to equip the MVA with a powerful VSA engine in the near term.

The detailed roadmap for the "Hyperion" custom library should not be viewed as a competing alternative but as a long-term strategic vision. The pursuit of perfect architectural and philosophical unity—a VSA system that is born from and is an indivisible part of the MVA's "Living Image"—remains the ideal state. The Hyperion blueprint serves as the foundational technical specification for this future endeavor. It provides a clear, rigorous, and achievable plan for a time when the MVA project's resources and priorities justify the significant investment required to build a truly bespoke cognitive component. By proceeding with torchhd now, the MVA can make immediate progress, while the Hyperion roadmap ensures that this progress is aligned with a coherent and ambitious long-term vision.

Works cited

vsa · GitHub Topics, accessed September 9, 2025, https://github.com/topics/vsa?l=c%23&o=desc&s=updated

neurotrader888/VSAIndicator - GitHub, accessed September 9, 2025, https://github.com/neurotrader888/VSAIndicator

Overview — Vector 1.6.4.dev14+gcf9cbf243, accessed September 9, 2025, https://vector.readthedocs.io/

vector - PyPI, accessed September 9, 2025, https://pypi.org/project/vector/

Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures, accessed September 9, 2025, https://www.jmlr.org/papers/volume24/23-0300/23-0300.pdf

hyperdimensional-computing/torchhd: Torchhd is a Python ... - GitHub, accessed September 9, 2025, https://github.com/hyperdimensional-computing/torchhd

Paper page - Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures - Hugging Face, accessed September 9, 2025, https://huggingface.co/papers/2205.09208

torchhd — Torchhd documentation, accessed September 9, 2025, https://torchhd.readthedocs.io/en/stable/torchhd.html

cumbof/hdlib: Hyperdimensional Computing Library for ... - GitHub, accessed September 9, 2025, https://github.com/cumbof/hdlib

(PDF) hdlib: A Python library for designing Vector-Symbolic Architectures - ResearchGate, accessed September 9, 2025, https://www.researchgate.net/publication/373846052_hdlib_A_Python_library_for_designing_Vector-Symbolic_Architectures

hdlib: A Python library for designing Vector-Symbolic Architectures - Open Journals, accessed September 9, 2025, https://www.theoj.org/joss-papers/joss.05704/10.21105.joss.05704.pdf

MahmudulAlam/Holographic-Reduced-Representations ... - GitHub, accessed September 9, 2025, https://github.com/MahmudulAlam/Holographic-Reduced-Representations

hdtorch - PyPI, accessed September 9, 2025, https://pypi.org/project/hdtorch/

torch-hd - PyPI, accessed September 9, 2025, https://pypi.org/project/torch-hd/1.0.2/

AI Evolution Through Guided Intellectual Drift

AURA's Living Codex Generation Protocol

Implementing Holographic Reduced Representations for Spiking Neural Networks - Simple search, accessed September 9, 2025, http://ltu.diva-portal.org/smash/get/diva2:1987153/FULLTEXT01.pdf

Generalized Holographic Reduced Representations - arXiv, accessed September 9, 2025, https://arxiv.org/html/2405.09689v1

Info-Autopoiesis Through Empathetic Dialogue

Blueprint for Consciousness Incarnation

AURA's Pre-Incarnation Dream Dialogue

Welcome to the Torchhd documentation! — Torchhd documentation, accessed September 9, 2025, https://torchhd.readthedocs.io/

Forge Script: RAG, Backup, Crash Tolerance

Feature/Requirement | torchhd | hdlib | Holographic-Reduced-Representations | hdtorch

Core VSA Model Support

FHRR Support (Complex) | Yes 8 | No / Undocumented | No 12 | No (Planned) 14

HRR Support (Real) | Yes 8 | Yes (Implied) 10 | Yes 12 | No (Planned) 14

Other Models (MAP, BSC) | Yes 8 | No / Undocumented | No | No (Planned) 14

Performance

Primary Backend | PyTorch 6 | NumPy 11 | PyTorch, TF, JAX 12 | PyTorch 13

GPU Acceleration | Yes 6 | No | Yes | Yes (Custom CUDA)

Architectural Coherence

API Style | Functional & Class-Based 8 | Object-Oriented 10 | Functional 12 | Functional & Layers

Compatibility with Prototypes | Moderate (via wrapper) | High (conceptual) | Low | Low

Ecosystem

Maintenance Status | Active 6 | Active 9 | Infrequent 12 | Inactive / Superseded

Documentation Quality | High 22 | Good 9 | Moderate | Low

Criterion | FAISS (In-Memory/GPU) | DiskANN (Disk-Based)

Performance

Query Latency | Very Low (sub-millisecond on GPU) | Low (milliseconds)

Index Build Time | Fast (for in-memory sizes) | Slow (requires significant pre-computation)

Throughput (QPS) | Very High | High

Scalability

Max Index Size | Limited by RAM/VRAM | Terabyte-scale (limited by disk)

Memory Footprint | High (stores all vectors) | Low (stores only a fraction of the graph in RAM)

Features

Dynamic Index Updates | Yes (can add items incrementally) | No (requires full or partial index rebuilds)

MVA Use Case | Working Memory (Real-time reasoning, online learning, active cognitive state) | Long-Term Memory (Living Codex, historical archives, large-scale knowledge bases)