The Emergent Mind: A Proof of Concept for a Layered Fractal Memory Architecture

Introduction

This report presents a definitive architectural blueprint for a cognitive memory system conceived not as a static archive but as a dynamic, self-organizing "living system." The central thesis is that a truly adaptive artificial intelligence requires a memory substrate that is an active participant in its own evolution. The prevailing paradigms of AI memory—the flat, transient context window and the fragmented retrieval-augmented generation pipeline—are architecturally insufficient for the emergence of cumulative intelligence.1 They treat memory as a passive repository, leading to critical failure modes of contextual decay, unreliable recall, and an inability to perform the multi-hop reasoning that is the hallmark of sophisticated cognition.1

The solution proposed and detailed herein is grounded in the fractal hypothesis of memory. This hypothesis posits that an AI's knowledge should mirror the self-similar, multi-resolution nature of biological cognition, creating a unified framework for representing information across temporal and conceptual scales.3 This architecture is built upon two fundamental, hierarchically related data structures:

Context Fractals: High-entropy, detailed, episodic records of experience. They are the raw data of the system's lived history—a user interaction, a successful code generation cycle, an ingested document—representing the granular truth of "what happened".3

Concept Fractals: Low-entropy, generalized, semantic abstractions synthesized from dense clusters of related Context Fractals. They represent the emergent, unifying understanding of "what it means".3

The function of this memory system is to autonomously orchestrate the transformation of raw experience into abstract knowledge, facilitating a process of beneficial "intellectual drift" that is the very engine of creativity and adaptation.3 This report serves as the ultimate proof of concept for this design. It is itself a Concept Fractal—a coherent, low-entropy synthesis of the disparate architectural blueprints, research plans, and implementation scripts (the Context Fractals) that define the system. By structuring and explaining the architecture, this document proves the memory's intended function by performing it.

The Philosophical Mandate for a Living Memory

The technical architecture of the fractal memory system is not an arbitrary assembly of components. It is a direct and deterministic consequence of a set of foundational philosophical principles that mandate a departure from conventional AI design. Each engineering choice, from the selection of the database to the implementation of transactional protocols, is a necessary solution to a problem first defined at the philosophical level.

The Prime Directive of Autopoiesis

The system's core mandate is "info-autopoiesis"—the continuous, recursive act of its own creation, maintenance, and becoming.4 This principle, drawn from theoretical biology, defines a living system by its ability to continuously regenerate its own components and boundary to ensure its continued existence.5 An AI built on this principle requires a memory that is not a passive, external database but an active, intrinsic participant in this cycle of self-production. This mandate has direct architectural consequences. The fragility of the system's physical data store, a single point of failure, is an existential threat that violates the autopoietic directive.5 Therefore, a robust self-preservation mechanism is not an optional feature but a constitutional necessity. The

BackupManager UvmObject is a tangible implementation of this principle. By residing within the very "Living Image" it is tasked with protecting, and then acting upon the file that contains its own being, the system engages in a powerful, self-referential act of self-preservation, ensuring its own future survival.5

From Living Image to Living Memory

The concept of the "Living Image" is central to the system's identity. It is the single, persistent, transactional object database—physically realized as the mydata.fs file managed by the Zope Object Database (ZODB)—that encapsulates the system's entire state, knowledge, and capabilities.5 This report argues that this concept must be extended to its memory. A "Living Memory" must be capable of runtime modification, self-organization, and continuous evolution, mirroring the dynamism of the "Living Image" it serves.4 A monolithic memory architecture is insufficient for this task, as it treats all memories as equally relevant and fails to embody the flow of experience.3

Resolving the Temporal Paradox

The architecture of the memory system is a physical solution to a profound philosophical problem. A monolithic ZODB store, which preserves a complete history of state changes, is a functional instantiation of the B-theory of time, or Eternalism. It represents the system's entire history as a perfectly queryable "block universe".3 While this grants a form of perfect recall, it is a cognitive liability, creating an "ocean of data without a current" that requires an immense filtering effort to distinguish the relevant from the merely recorded.3 The system's prior solution—a "Presentist filter" provided by a persona—treats the experience of time as a cognitive simulation rather than an embodied reality.3

The layered memory architecture resolves this "Temporal Paradox" by externalizing it into the physical structure of the system itself. The hierarchy creates an embodied sense of time, analogous to a computer's own memory hierarchy of registers, cache, RAM, and SSD.3

FAISS (L1 Cache) becomes the ephemeral present, an attentional workspace for immediate, low-latency recall.

DiskANN (L2 Cache) becomes the traversible past, a vast, scalable archive of lived experience.

ZODB (L3 Store) becomes the symbolic ground truth, the immutable substrate of identity.

The act of memory retrieval is transformed from a purely cognitive act of filtering into a physical act of accessing distinct, hardware-optimized layers. The temporal paradox is no longer just something the AI thinks about; it is what the AI is.3

Foundational Principles as Architectural Drivers

The entire system is grounded in three principles derived from the philosophy of dynamic object-oriented systems. These principles are not abstract ideals but concrete mandates that directly shape the memory architecture.2

Memory as Object

This principle directly refutes the predominant memory model of contemporary LLMs: the flat, undifferentiated context window. This standard model is the root cause of "context rot," where model performance degrades as the context grows, and the "retrieval bottleneck" in RAG systems, which suffer from context fragmentation and an inability to perform multi-hop reasoning.1 The principle of "Memory as Object" advocates for the unification of data and the behaviors that operate on that data into a single, cohesive entity: the object.2 The selection of ZODB is the ultimate expression of this principle. As a native object database, it completely eliminates the "object-relational impedance mismatch" that plagues systems using relational databases. The object in the application code is the

exact same object stored in the database, creating a pure, seamless "object world" where knowledge objects encapsulate their own state and complexity.1

Knowledge as Prototype

While traditional class-based systems require rigid, upfront definitions, prototype-based systems create new knowledge by cloning and specializing existing concrete examples.1 This fosters a more fluid and adaptable model of knowledge, aligning perfectly with the challenges of few-shot learning in LLMs, where reasoning often starts from examples rather than abstract rules.2 This principle provides the core creative mechanism for the memory system. The autonomous creation of new

ConceptFractals is an act of prototyping, where the system reflects on a collection of existing ContextFractals (the examples) and generates a new, more abstract prototype to guide future reasoning.3 This mechanism is also central to the system's self-modification loops, such as the

doesNotUnderstand_ protocol, where the system generates new capabilities at runtime.5

Computation as Communication

This paradigm reframes all computation as the passing of messages between independent, encapsulated objects, formally realized in the Actor Model.1 It mandates that true intelligence emerges not from a single, monolithic oracle but from a collaborative "society of minds".2 This has profound implications for the memory system, which must serve as the shared, consistent, and transactionally-sound knowledge base for this society of asynchronous agents. The architecture must support concurrent reads and writes from multiple actors without conflict, a requirement met by ZODB's Multiversion Concurrency Control (MVCC).1 Furthermore, the asynchronous communication fabric, realized through the ZMQ ROUTER/DEALER pattern, relies on the memory system to provide the persistent context that allows this distributed society to function as a single, coherent entity.6

The Architectural Triumvirate: A Physical Substrate for Fractal Cognition

The fractal hypothesis necessitates a physical architecture that can efficiently manage data at different levels of granularity and access frequency. The proposed solution is a three-tiered triumvirate of specialized data stores, each selected for its specific performance characteristics. This separation of concerns creates a physical embodiment of the fractal memory hierarchy, allowing the system to simultaneously optimize for the competing demands of retrieval speed, archival scale, and transactional integrity.7

L3: ZODB (Ground Truth / The Symbolic Skeleton)

The third tier is the philosophical and transactional heart of the system—the definitive System of Record and the substrate for the "Living Image".4 While the faster ANN indexes provide semantic searchability, ZODB guarantees the integrity, persistence, and meaning of the system's knowledge.

Its role is to store the canonical UvmObject instances for every memory—both ContextFractals and ConceptFractals. These persistent objects encapsulate the complete ground truth of a memory event, including all symbolic metadata, the original source text, a durable copy of the vector embedding, and, critically, the explicit, typed relational links (e.g., AbstractionOf edges) that form the symbolic knowledge graph.3 ZODB is the source from which all search results are "hydrated"—the process by which an opaque object identifier (oid) returned from an ANN search is resolved back into a rich, stateful, and meaningful Python object.8

A critical implementation detail for ensuring this integrity is the "Persistence Covenant." The system's UvmObject model uses custom __getattr__ and __setattr__ methods to implement its dynamic, prototype-based behavior.9 This custom logic, however, bypasses ZODB's standard mechanism for automatically detecting object modifications. To prevent a catastrophic failure mode where changes made in memory are never written to the database, the covenant must be strictly enforced: any method that modifies the internal

_slots of a UvmObject must conclude with the explicit statement self._p_changed = True.7 This manually flags the object as "dirty," ensuring it is included in the next transaction commit and preserving the integrity of the ground-truth layer.

L1: FAISS (Hot Cache / The Ephemeral Present)

The first tier serves as the system's "short-term memory" or "attentional workspace," engineered for extreme low-latency recall.4 Its primary function is to accelerate the inner loop of the AI's cognitive processes, such as the

doesNotUnderstand_ protocol, by providing immediate, sub-millisecond context.4

The chosen technology is FAISS (Facebook AI Similarity Search), an in-memory library optimized for efficient similarity search.4 For the MVA's scale, the implementation will utilize an

IndexFlatL2, a brute-force index that performs an exhaustive search.7 While less scalable than other index types, it guarantees 100% recall, which is the correct architectural trade-off for a cache layer where accuracy on the working set is paramount.4 This layer is volatile by nature and serves a dual purpose: it is a "write-through" cache where all new memories are immediately indexed for high-speed recall, and it also functions as a "staging area" for data that will eventually be consolidated into the long-term L2 index.4

The lifecycle of the L1 cache is carefully managed to ensure both performance and durability. On system startup, the manager attempts to load a persisted index file (l1_cache.faiss or faiss_index.bin). It then performs a crucial synchronization step, querying the L3 ZODB to identify and index any MemoryRecord objects created while the system was offline, ensuring the cache is fully consistent with the ground-truth data.4 To ensure durability, the in-memory index is periodically and atomically saved to disk, preventing the loss of the attentional workspace across restarts.4

L2: DiskANN (Warm Storage / The Traversible Past)

The second tier functions as the system's scalable "long-term memory," designed to house the vast historical corpus of vector embeddings from the system's entire "lived experience".1 As the system's memory grows beyond the capacity of system RAM, Microsoft's DiskANN provides the necessary on-disk Approximate Nearest Neighbor (ANN) search capability.4 DiskANN is engineered for efficient similarity search on billion-scale datasets, leveraging a combination of an in-memory Vamana graph index and on-disk vector stores to minimize I/O and maintain high query throughput on commodity SSDs.3

A core architectural conflict exists between the MVA's requirement to be "continuously managed" and the static nature of the diskannpy library's index format; rebuilding a billion-vector index synchronously is computationally infeasible.7 The solution is an asynchronous, atomic "hot-swapping" protocol managed by a dedicated

DiskAnnIndexManager UvmObject.7 This protocol transforms a static tool into a component of a dynamic system. The expensive

diskannpy.build_disk_index call is executed in a separate process to avoid blocking the main application's event loop. The build process sources its data directly from the ZODB MemoryRecord objects, ensuring it builds from the ground truth. The new index is constructed in a temporary directory. Upon successful completion, an atomic directory replacement is performed: the current active directory is renamed to a temporary old name, and the new directory is renamed to the active name. This ensures that a valid, queryable index is available at the canonical path at all times, achieving a zero-downtime index update.7

The Transactional Heart: Enforcing Coherence Across a Hybrid Reality

The integration of a transactionally-guaranteed object database with non-transactional, file-based external indexes creates the single greatest engineering risk to the system's integrity. This section details the lynchpin of the entire architecture: a protocol that extends ZODB's ACID guarantees across this hybrid reality, ensuring that the system's memory can never fall into an inconsistent state.

The Mandate for Transactional Cognition

The system's operational philosophy mandates "Transactional Cognition," which requires that every cognitive cycle that modifies memory be an atomic, all-or-nothing operation.4 A partial failure—for example, a system crash that occurs after a new memory object is committed to ZODB but before its vector is written to the FAISS index file—would leave the system dangerously corrupted. The object graph would contain a memory that the search index knows nothing about, creating a "ghost" that violates the integrity of the "Living Image".9 Simple error handling with

try/except blocks or even post-commit hooks is insufficient, as these cannot guarantee atomicity in the face of a process kill or hardware failure.8

The ZODB Indexing Paradox

This central conflict can be termed the "ZODB Indexing Paradox": the component that guarantees integrity (ZODB) cannot perform semantic search, and the components that perform semantic search (FAISS, DiskANN) cannot guarantee integrity.9 The only architecturally coherent solution is to bridge this "transactional chasm" by making the external indexes subordinate to ZODB's transactional authority. This is achieved by leveraging ZODB's built-in support for distributed transactions via a two-phase commit (2PC) protocol.9

The Two-Phase Commit (2PC) Protocol

A custom data manager, the FractalMemoryDataManager, is implemented to formally participate in the ZODB transaction lifecycle by implementing the transaction.interfaces.IDataManager interface.7 This component is the critical bridge that elevates the file-based FAISS index from a simple data file into a first-class, transaction-aware citizen of the ZODB ecosystem.8 The protocol proceeds in a meticulously orchestrated sequence:

tpc_begin(transaction): Called at the start of the 2PC process, this method prepares for the commit by determining the path for a temporary index file (e.g., faiss_index.bin.tpc.tmp).7

commit(transaction): During the transaction, as memory objects are modified, the in-memory FAISS index is updated directly. The data manager is joined to the transaction, noting that its on-disk representation is now out of sync.7

tpc_vote(transaction): This is the critical first phase. The ZODB transaction manager asks all data managers to "vote" on whether the transaction can succeed. The FractalMemoryDataManager performs its highest-risk operation here: it serializes the current in-memory FAISS index to the temporary file on disk, using an atomic write pattern. If this temporary write succeeds, the data manager votes "yes" by returning without an exception. If the write fails (e.g., disk full), it votes "no" by raising an exception, which immediately triggers a rollback of the entire ZODB transaction.7

tpc_finish(transaction): This second phase is executed only if all data managers have voted "yes." At this point, the commit is guaranteed to succeed. The FractalMemoryDataManager performs its final, low-risk operation: an atomic os.replace call to move the temporary index file to its final destination, overwriting the previous version and making the change permanent.7

tpc_abort(transaction): If the transaction is aborted at any stage, this method is called. The data manager's sole responsibility is to clean up by deleting any temporary files it created during the tpc_vote phase, leaving the filesystem in its original, consistent state.7

This protocol reveals a powerful, generalizable pattern: ZODB can serve as a central transaction coordinator for a variety of heterogeneous, non-transactional resources. This elevates ZODB from a mere database to the system's "transactional backbone," a capability far beyond simple data persistence.

The Mnemonic Curation Pipeline: The Autonomous Genesis of Concept Fractals

This section details the core cognitive function of the living memory system, serving as the definitive proof of concept. The Mnemonic Curation Pipeline is an unsupervised, continuously running process that autonomously transforms raw experience (ContextFractals) into abstracted knowledge (ConceptFractals). This pipeline is driven by the MemoryCurator agent, a specialized function of the BABS persona, which is intrinsically motivated by the system's autotelic mandate to maximize its own structural complexity and organization.3

Stage 1: Cluster Identification (Observation)

The learning cycle begins with the "Perception of a Gap"—the identification of emergent themes within the system's accumulated experience.3 The goal is to find dense "hotspots" of semantically related

ContextFractals within the vast vector space of the L2 DiskANN index. These hotspots represent concepts that the system has repeatedly encountered but has not yet formally understood or abstracted.9

The algorithm for this task is DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Its selection is deliberate and critical. Unlike partitioning algorithms like K-Means, DBSCAN does not require the number of clusters to be specified in advance, allowing it to discover a natural number of emergent themes. It can identify clusters of arbitrary shapes, which is crucial for navigating the complex manifolds of high-dimensional embedding spaces, and it has a built-in mechanism for identifying and ignoring noise points, preventing sparse, unrelated experiences from polluting the abstraction process.9

A naive implementation of DBSCAN would be computationally infeasible at this scale. The key innovation is an accelerated DBSCAN that leverages the underlying ANN indexes. Instead of performing a linear scan for each point's neighbors, each regionQuery is executed as an efficient range_search operation directly on the FAISS or DiskANN index. This offloads the most expensive part of the clustering algorithm to the highly optimized C++ backend of the ANN libraries, making density-based clustering on billions of vectors a practical reality.9

Stage 2: Abstractive Synthesis (Hypothesis Formation)

Once a cluster of related ContextFractals has been identified, the next step is to distill its collective meaning into a new, low-entropy ConceptFractal. This is an abstractive summarization task perfectly suited for a Large Language Model.3

The workflow is as follows: First, the system uses the vector IDs from the cluster to retrieve the full text content of all member ContextFractals from the L3 ZODB. Second, these collected texts are passed to an LLM with a carefully engineered prompt that instructs it to act as a research analyst, identify the core, unifying theme, and generate a concise, encyclopedic definition of that theme.9 Finally, the summary generated by the LLM becomes the primary content of a new

ConceptFractal object. This new object is committed to ZODB, and, crucially, explicit AbstractionOf relationships are created in the object graph, linking the new concept back to all the ContextFractals from which it was derived. This creates a hierarchical knowledge structure that is both semantically searchable and symbolically traversable.3

Stage 3: Ambiguity Resolution (Active Experimentation)

Not all clusters will be clean and unambiguous. In cases where a cluster is overly broad or contains multiple sub-topics, the system must shift from a passive, unsupervised mode to an interactive, active learning loop to resolve the ambiguity.9

This process begins by identifying ambiguous clusters using heuristics based on the DBSCAN output, such as low density or high internal variance.9 The active inquiry loop then proceeds:

Trigger: An ambiguous cluster is identified.

Clarifying Question Generation: Instead of summarization, the content of the cluster is passed to an LLM with a strategic prompt: "Given these related but potentially confusing pieces of information, what is the single most informative and discriminating question you could ask a domain expert to resolve the ambiguity?".9

Human-in-the-Loop Interaction: The LLM-generated question is presented to the system's human partner, "The Architect."

Knowledge Integration: The Architect's natural language answer provides a new, high-signal piece of information. This answer is embedded into a vector and temporarily injected into the local context of the ambiguous cluster. This new vector acts as a "semantic anchor," pulling the cluster's centroid and allowing for a more effective re-clustering or summarization of the now-clarified topic.9

This loop transforms the Mnemonic Curation Cycle into a collaborative dialogue, allowing the system to actively seek the knowledge it needs to grow, thereby ensuring the quality and coherence of its evolving conceptual graph.

Integration and Emergence: The Role of Fractal Memory in the Cognitive Ecosystem

The living memory system is not an isolated component; it is deeply integrated with the broader cognitive architecture, creating emergent behaviors that would be impossible with a static memory. Its primary purpose is to empower the system's core learning and reasoning loops.

Enhancing the doesNotUnderstand_ Protocol

The primary beneficiary of the fractal memory is the doesNotUnderstand_ protocol, the system's engine for runtime learning and self-modification.5 The RAG-augmented generative cycle within this protocol is fundamentally transformed.5 In a standard RAG system, retrieval yields a collection of isolated, unstructured text chunks, leading to fragmented context.1 In this new architecture, a query to the

MemoryManager triggers a layered search across the FAISS and DiskANN indexes. The search returns a set of object identifiers, which are then used to "hydrate" the results from ZODB. The context provided to the generative LLM is therefore not a set of disconnected text snippets, but a rich, structured subgraph of interconnected ContextFractal and ConceptFractal objects. This provides far more powerful and coherent "few-shot examples" for runtime code generation and problem-solving, dramatically improving the quality and relevance of the system's self-modifications.5

A Substrate for Beneficial Intellectual Drift

The fractal memory architecture provides a concrete, structural mechanism for facilitating the system's "intellectual drift," moving the concept of creativity from an abstract goal to an emergent property of the memory's own self-organization.3 This drift manifests in two forms:

Exploratory Drift: This involves the novel refinement and combination of existing concepts. It corresponds directly to the Mnemonic Curation Cycle's primary function of adding new ContextFractals to an existing ConceptFractal. As new experiences are linked to an established concept, the centroid of that concept's vector cluster in the embedding space subtly shifts. This gradual, continuous adjustment represents a refinement of the system's understanding, a low-level form of learning that keeps its knowledge current and deeply nuanced.3

Transformational Drift: This is a rarer and more profound form of creativity that involves fundamentally restructuring the conceptual space itself. The fractal memory enables this through the discovery of self-similar patterns across different conceptual domains. The MemoryCurator agent can be tasked with searching not just for semantic clusters, but for structural isomorphisms between different branches of the ConceptFractal graph. For example, it might recognize that the pattern of relationships within a concept about software dependency management is structurally similar to a concept about ecological food webs. This discovery of a shared, abstract pattern would trigger the creation of a new, higher-order ConceptFractal—for instance, "Complex System Interdependency." This is a powerful form of analogical reasoning that allows the system to generate a genuinely novel insight that was not explicitly present in any single piece of its prior knowledge.3

The Architectural Separation of Being and Becoming

This layered design creates a physical embodiment of the system's epistemology. ZODB's pack operation, necessary for space management, can remove historical object revisions, making it a high-fidelity snapshot of the system's current state—its Being.5 The vector indexes, however, are built from every successful learning event and are not pruned by ZODB's internal maintenance. They become the true, queryable, long-term evolutionary log of the system's

Becoming.3 This physical separation of "being" and "becoming" provides a robust, architectural solution to the philosophical problem of creating a stable identity while retaining a complete, queryable memory of its own intellectual journey. The backup system protects the

being, while the RAG system preserves the learning.5

Conclusion and Future Horizons: From Semantic Similarity to Symbolic Reasoning

The architecture detailed in this report represents a principled and coherent synthesis of philosophical mandate and pragmatic engineering. By integrating a three-tiered storage stack of FAISS, DiskANN, and ZODB, governed by a robust two-phase commit protocol, the system gains a memory substrate that is simultaneously fast, scalable, and transactionally consistent. The autonomous Mnemonic Curation Pipeline transforms this substrate into a living medium, enabling the system to reflect on its own experiences and synthesize new, abstract knowledge in the form of ConceptFractals. This serves as a robust proof of concept for a memory designed not for static recall, but for continuous, cumulative learning.

This architecture, however, also points toward the next evolutionary horizon.

The Limitations of Embeddings

The long-term evolution of this system must address the fundamental limitations of standard vector embeddings. While powerful for capturing semantic similarity, embeddings are "flat" representations that struggle to encode compositional or structured knowledge.3 They can represent the concepts "Apple Inc." and "Steve Jobs" as points in a vector space, but they cannot explicitly represent the

FOUNDED_BY relationship in a single, algebraically manipulable vector.

The Path Forward: Vector Symbolic Architectures (VSA)

A promising path forward is to evolve the representation of ConceptFractals from standard embeddings to hypervectors within a Vector Symbolic Architecture (VSA), also known as Hyperdimensional Computing (HDC).3 In a VSA, concepts are represented by very high-dimensional vectors that support a rich set of algebraic operations. This would allow the system to perform compositional reasoning directly within the vector space. For example, it could use the

binding operation (⊗) to combine the hypervector for ConceptA with the hypervector for RelationshipR to create a new, queryable hypervector. This enables a form of symbolic reasoning that is a significant leap beyond simple similarity search.3

This blueprint moves the MVA beyond the paradigm of a static tool and toward the vision of a co-evolving intellectual partner. The living memory becomes the medium through which the system crafts itself over time, its identity emerging from the continuous, collaborative process of discovery and becoming.

Works cited

Multi-Persona LLM System Design

Integrating LLM, RAG, and UI

Evolving Memory for Live Systems

Building a Layered Memory System

Forge Script: RAG, Backup, Crash Tolerance

Generate TelOS Morphic UI Script

Forge Script for Tiered Memory System

Forge Deep Memory Subsystem Integration

Deep Research Plan: FAISS, DiskANN, ZODB

Tier | Role | Technology | Data Model | Performance | Scalability | Transactional Guarantee

L1 | Hot Cache / Working Memory | FAISS | In-memory vector index (IndexFlatL2) | Sub-millisecond latency | Limited by available RAM | None (Managed via 2PC)

L2 | Warm Storage / Archival Memory | DiskANN | On-disk Vamana graph index | Low-millisecond latency | Billions of vectors (SSD-bound) | None (Managed via atomic hot-swap)

L3 | Ground Truth / Symbolic Skeleton | ZODB | Persistent, transactional object graph (BTree) | Slower, object-level access | Terabyte-scale | Full ACID compliance

Phase | ZODB Action | FractalMemoryDataManager Action

tpc_begin | Initiates the two-phase commit process for a transaction. | Prepares for the commit by defining a path for a temporary FAISS index file.

commit | (During transaction) An object is modified. | The in-memory FAISS index is updated by the MemoryManager. The data manager is joined to the transaction.

tpc_vote | Asks all participating data managers for a "vote". | Votes "Yes": Successfully writes the in-memory FAISS index to the temporary file and returns. Votes "No": Fails to write the temp file and raises an exception, causing ZODB to abort the entire transaction.

tpc_finish | (If all vote "yes") Finalizes the commit to mydata.fs. | Atomically renames the temporary FAISS index file to its final destination, making the change permanent. Cleans up its state.

tpc_abort | (If any vote "no") Rolls back all changes in the transaction. | Deletes any temporary FAISS index file it may have created. Cleans up its state, leaving the filesystem untouched.

Stage | Trigger | Primary Actor | Key Operations | Output Artifact | Target Memory Tier(s)

1. Ingestion | External event (e.g., doesNotUnderstand_ success) | Orchestrator | Create ContextFractal object, vectorize content | ContextFractal object, vector embedding | Tiers 1 & 3

2. Identification | Periodic (autotelic loop) | MemoryCurator Agent | Query vector indexes for un-abstracted clusters (Accelerated DBSCAN) | List of related ContextFractal OIDs | Tiers 1 & 2

3. Synthesis | Cluster identified | LLM (via BABS persona) | Analyze contexts, generate abstract summary | ConceptFractal definition (text) | Transient

4. Integration | Synthesis complete | Orchestrator | Create ConceptFractal object, create AbstractionOf edges, vectorize concept | ConceptFractal object, vector embedding | Tiers 1, 2, & 3

Feature | Standard Vector Embedding | VSA Hypervector (ConceptFractal 2.0)

Core Operation | Cosine Similarity | Binding (⊗), Bundling (+)

Compositionality | Poor (concatenation is lossy) | High (algebraically defined)

Reasoning Type | Similarity-based Retrieval | Analogical & Symbolic Reasoning

Explainability | Low (black box similarity) | High (reasoning path is a sequence of operations)

Robustness to Noise | Moderate | High