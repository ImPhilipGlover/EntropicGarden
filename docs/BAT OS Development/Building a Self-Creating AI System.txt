A Research and Development Plan for a Self-Modifying Cognitive Architecture Based on Dynamic Object-Oriented Principles

Part I: Theoretical Foundations and Architectural Vision

1.1. Introduction: A Paradigm Shift Beyond the Transformer

The contemporary landscape of artificial intelligence is dominated by the transformer architecture, a model whose success has been predicated on unprecedented scale.1 However, this paradigm, for all its generative prowess, is built upon a fundamentally flawed computational model that presents an architectural cul-de-sac for achieving persistent memory, continuous learning, and true cognitive adaptability.1 The prevailing Large Language Model (LLM) architecture is characterized by a stateless processing core and a flat, transient context window that serves as its only form of short-term memory.1 This design choice is the root cause of systemic failures that cannot be resolved by incremental increases in model size or context length.

The most significant of these failures is "context rot," the well-documented phenomenon where model performance degrades as the context window fills, regardless of its absolute size.1 Information, whether from system prompts, user queries, or retrieved documents, coexists in a single, undifferentiated, and linear sequence of tokens. The model's attention mechanism, which scales quadratically with sequence length, becomes computationally unsustainable and struggles to distinguish signal from noise in this monolithic text blob.1 The primary method for augmenting LLM knowledge, Retrieval-Augmented Generation (RAG), inherits this structural blindness. By chunking source documents into arbitrary, disconnected snippets, standard RAG systems suffer from context fragmentation, context poisoning, and an inability to perform the multi-hop reasoning required for complex queries.1 These are not minor issues to be patched but are inherent symptoms of a paradigm that treats memory and knowledge as unstructured and ephemeral.

This research plan posits that a path forward requires a fundamental paradigm shift. The proposed solution is not an iteration on the existing model but a radical reconceptualization of the AI's cognitive architecture, inspired by the profound and time-tested philosophies of dynamic object-oriented systems, particularly those pioneered in environments like Smalltalk and Self.1 This plan outlines the development of a "live system"—a persistent, dynamic, and continuously evolving cognitive environment where the rigid distinctions between data, code, and model begin to dissolve. By embracing a new set of first principles, it becomes possible to design a self-modifying AI capable of genuine learning and robust, long-term reasoning.

1.2. The Minimum Viable Cognitive Core (MVCC): A Tripartite Architecture

At the heart of this research initiative is the design and implementation of a Minimum Viable Cognitive Core (MVCC). The MVCC is not merely a collection of algorithms but an integrated cognitive architecture founded upon a cohesive set of three interdependent principles derived from the philosophy of dynamic systems. These principles form a direct and powerful counterpoint to the limitations of the current LLM paradigm.1

Memory as Object: This principle mandates a shift away from the flat, transient context window towards a memory system composed of persistent, stateful, and encapsulated knowledge objects.1 In the current paradigm, memory is a monolithic document that must be re-read in its entirety for every cognitive act.1 In the MVCC, information is structured into discrete units, each representing a "bounded context" that co-locates related data and the logic that operates upon it. These objects hide their internal complexity and expose only a well-defined public interface, transforming memory from a passive store to be scanned into an active collection of components that can be queried. This object-centric model, realized through a knowledge graph, offers a path to a theoretically infinite memory where computational cost is tied to query complexity, not total memory size, thus breaking the flawed trade-off of the current model.1

Knowledge as Prototype: This principle rejects the rigid, pre-defined taxonomies of class-based systems in favor of a fluid, dynamic model of knowledge representation based on prototypes.1 In a prototype-based system, new concepts are not instantiated from abstract blueprints; they are created at runtime by cloning and specializing existing, concrete examples.1 A new object maintains a live link to its parent prototype, delegating requests for behavior it does not possess up the chain. This mechanism of dynamic inheritance provides a powerful model for few-shot learning and continuous adaptation. Instead of requiring costly retraining to acquire new knowledge, the MVCC will be able to synthesize new conceptual prototypes from its latent understanding and refine them through interaction, mirroring the process of creating specialized knowledge from a general model.1

Computation as Message Passing: This principle reframes all cognitive processes—from perception and memory access to reasoning and action—as a protocol of communication between independent, concurrent computational units, or actors.1 This moves decisively away from the pursuit of a single, monolithic "oracle" model towards the construction of a "society of minds".1 As formalized by the Actor Model, each computational unit has a private state and communicates asynchronously by sending and receiving messages.2 This enforces absolute encapsulation and decouples the sender of a request from its receiver, enabling the construction of complex, distributed reasoning systems that are inherently concurrent and scalable. The MVCC will be architected not as a single brain, but as a collaborative system of specialized agents whose collective intelligence emerges from their structured communication.1

1.3. The Fractal Memory Hypothesis: A Model for Continuous Abstraction

The central theoretical innovation of this research plan, and the primary mechanism for self-modification, is the Fractal Memory Hypothesis. This hypothesis proposes a model for continuous learning and abstraction by structuring the object-centric memory into two interconnected, self-similar layers: the episodic and the semantic. The bridge between these layers, facilitated by the prototype-based knowledge model, is the engine of cognitive growth.

Episodic Fractals: An episodic fractal is a subgraph within the object-centric memory that represents a specific, bounded experience. It is a high-fidelity, append-only record of a sequence of perceptions, internal state changes, and actions. Each episodic fractal is a self-similar structure, meaning that complex experiences are composed of smaller, nested experiences, all sharing a common structural schema. For example, the episodic fractal for "making a cup of tea" would contain smaller, linked fractals for "filling the kettle," "boiling the water," and "steeping the tea bag." This layer captures the veridical, context-dependent "what happened" of the system's existence.

Semantic Fractals: A semantic fractal is an abstract, generalized prototype object residing in a separate layer of the memory graph. It represents a recurring pattern, concept, or procedure that has been distilled from multiple, structurally similar episodic fractals. It captures the context-independent "what it means." Following the previous example, multiple episodic fractals corresponding to making Earl Grey, English Breakfast, and Green Tea would be generalized into a single, more abstract semantic fractal representing the prototypical concept of "making tea." This semantic object would contain the core, invariant sequence of actions and states, while abstracting away the specific details (like the type of tea).

The Bridge Mechanism as the Engine of Learning: The core of the hypothesis lies in the mechanism that connects these two layers. The cognitive system will be designed to continuously and autonomously perform a process of abstraction. It will actively search for isomorphic (structurally similar) subgraphs within its vast store of episodic fractals. Upon identifying a recurring pattern, it will synthesize a new candidate prototype—a new semantic fractal—that represents the generalized structure of that pattern. This new prototype is then either merged with an existing similar concept to refine it or established as a new concept in the semantic layer. This process of identifying isomorphisms in experience and promoting them into abstract prototypes is the bridge from episodic to semantic memory. It is a continuous, runtime process that allows the system to learn from its unique experiences, build a progressively more sophisticated model of its world, and thereby modify its own future behavior. This mechanism is the direct implementation of the plan's core objective: to use a prototype-based approach to manage fractal memory and bridge episodic context into semantic concepts.

Part II: Architectural Blueprint of the Minimum Viable Cognitive Core (MVCC)

2.1. The Object Store: A Graph-Based Fractal Memory

The foundation of the MVCC is the physical implementation of the "Memory as Object" principle. This requires moving beyond simplistic in-memory dictionaries or vector stores to a persistent, structured, and queryable object store capable of representing the complex, interconnected nature of the fractal memory.

Technology Selection: The memory store will be implemented using a native graph database. Neo4j is selected as the primary technology for this layer.3 This choice is predicated on its mature and robust feature set, including the official Python driver which provides a high-level, object-oriented interface for database interaction.5 Furthermore, the Neo4j Graph Data Science (GDS) library offers a powerful suite of graph algorithms and, critically, provides Python client-side
Graph and Model objects.6 These objects serve as an ideal abstraction layer, allowing the cognitive actors to interact with the memory in an object-centric manner, fulfilling the principles of abstraction and information hiding by concealing the underlying Cypher query language implementation.1 The viability of using Python to manage complex, object-centric schemas and automated query generation against a Neo4j backend has been demonstrated by libraries such as PromG, which uses Neo4j to store and analyze object-centric event data.8

Schema Design: The graph schema will be explicitly designed to support the fractal memory hypothesis, with distinct layers for episodic and semantic information.

Episodic Layer: This layer serves as the immutable log of the system's experience. The schema will consist of fundamental node types such as Event, Perception (data received from the external world), Action (operations performed by the system), and State (internal state snapshots). Relationships in this layer will be strictly temporal and causal, with types like PRECEDES, CAUSED, and RESULTED_IN. This layer is designed to be write-heavy and effectively append-only, ensuring a veridical record of interaction history.

Semantic Layer: This layer represents the system's abstracted knowledge. The schema will be composed of nodes representing PrototypeConcept, Attribute, and Method. Relationships will be semantic and hierarchical, with types such as IS_A (for inheritance), HAS_PROPERTY, and CAN_PERFORM. This layer is dynamic and will be continuously modified by the AbstractionActor as new concepts are learned and existing ones are refined.

Bridging Relationships: A specialized relationship type, INSTANCE_OF, will be defined to connect the two layers. An INSTANCE_OF edge will link a specific subgraph of nodes in the Episodic Layer (e.g., the set of events and actions constituting a particular act of "making coffee") to its corresponding PrototypeConcept node in the Semantic Layer. This relationship is critical, as it grounds abstract knowledge in concrete, verifiable experience, providing a mechanism for explainability and preventing conceptual drift.

Implementation Details: All interactions with the database will be mediated through a dedicated MemoryManagementActor. This actor will expose a high-level API (e.g., store_episodic_fractal, find_isomorphic_patterns, update_semantic_prototype) to the rest of the cognitive system. Internally, this actor will use the Neo4j Python driver's GraphDatabase.driver to manage connections and sessions.3 It will encapsulate all Cypher query logic, translating object-centric requests from other actors into efficient graph traversals and updates. This design enforces a clean separation of concerns and treats the memory system as a fully encapsulated object.

2.2. The Semantic Layer: Prototype-Based Knowledge Generalization

This layer implements the "Knowledge as Prototype" principle, providing the core mechanisms for dynamic learning and conceptual fluidity. The key challenge is to create a system that can generate and modify its own conceptual representations at runtime without recompilation or retraining. This is not merely a data storage problem but a fundamental challenge of computational knowledge representation. The dynamic nature of the Python language, specifically its support for metaprogramming, is not just a convenience but the critical enabling technology for implementing the delegation-based inheritance that lies at the heart of the prototypal philosophy.

The concept of prototypal inheritance, as described in the foundational document, relies on creating new objects by "cloning" and "specializing" existing ones, with behavior inherited via a "live link" or delegation.1 When an object receives a message for a method it does not possess, it delegates the request up its prototype chain.1 This is precisely the behavior of the

doesNotUnderstand: message in Smalltalk. Python's "magic method" __getattr__ provides a direct and elegant implementation of this exact mechanism.10 If a specialized object

B is a clone of a more general prototype A, any method call on B that is not defined in B itself will trigger B's __getattr__ method. This method can be implemented to simply forward the call to its parent prototype, A. This creates the "live link" and allows for a fluid, runtime-modifiable knowledge structure where changes to a parent prototype are instantly reflected in all its descendants.

2.2.1. The PrototypeObject Base Class: A core Python class, PrototypeObject, will serve as the base for all semantic concepts within the MVCC. Its implementation will be the cornerstone of the knowledge layer.

clone() Method: It will feature a clone() method that performs a deep copy of its internal state, creating a new, independent object that can be specialized. The new object will retain a reference to its parent prototype.

__getattr__ for Delegation: The class will implement the __getattr__(self, name) method. This method is invoked only when a requested attribute name is not found in the object's own dictionary.10 The implementation will be straightforward: it will attempt to retrieve the attribute from its stored parent prototype. This single method effectively implements the entire delegation mechanism of prototypal inheritance, allowing for the creation of dynamic and fluid conceptual hierarchies.

2.2.2. The Abstraction Algorithm (The Fractal Bridge): The AbstractionActor will execute a continuous cycle of learning, bridging the episodic and semantic memory layers. This algorithm is the heart of the system's self-modification capability.
Pseudocode for AbstractionActor.run_abstraction_cycle():
Python
# This is a conceptual representation of the algorithm's logic.

def run_abstraction_cycle(self):
    # Step 1: Pattern Detection
    # Query the MemoryManagementActor to find recurring, structurally
    # similar subgraphs in the episodic layer. This is a computationally
    # intensive graph isomorphism problem.
    episodic_patterns = self.memory_manager.ask(
        "find_isomorphic_subgraphs",
        min_frequency=3,
        min_complexity=5
    ).get()

    for pattern in episodic_patterns:
        # Step 2: Prototype Candidate Generation
        # Synthesize a new, generalized PrototypeObject from the common
        # elements of the identified episodic instances.
        candidate_prototype = self.synthesize_prototype_from_pattern(pattern)

        # Step 3: Prototype Refinement/Merging
        # Query the memory for existing semantic prototypes that are
        # structurally similar to the new candidate.
        similar_prototypes = self.memory_manager.ask(
            "find_similar_prototypes",
            prototype=candidate_prototype
        ).get()

        if similar_prototypes:
            # Merge the new information into the most similar existing prototype,
            # refining and generalizing it further. This is analogous to the
            # "fusing" process described in the ProtoLLM framework.[1]
            best_match = similar_prototypes
            updated_prototype = self.merge_prototypes(best_match, candidate_prototype)
            self.memory_manager.tell(
                "update_semantic_prototype",
                prototype=updated_prototype
            )
            final_prototype_id = best_match.id
        else:
            # If no similar prototype exists, persist the new candidate as a
            # novel concept in the semantic layer.
            new_prototype_id = self.memory_manager.ask(
                "store_semantic_prototype",
                prototype=candidate_prototype
            ).get()
            final_prototype_id = new_prototype_id

        # Step 4: Linking
        # Create INSTANCE_OF relationships from the episodic instances to the
        # new or updated semantic prototype, grounding the abstraction in experience.
        self.memory_manager.tell(
            "link_episodic_to_semantic",
            episodic_ids=pattern.instance_ids,
            semantic_id=final_prototype_id
        )


2.3. The Computational Fabric: A Society of Cognitive Actors

This layer implements the "Computation as Message Passing" principle, structuring the entire cognitive process as a collaborative, concurrent system. The choice of an actor framework is a critical architectural decision that has profound implications for the project's entire lifecycle, from development and debugging to deployment and scaling. The Actor Model's strict separation of concerns between an actor's internal logic and the underlying transport mechanism is a key strategic advantage.12 This separation allows the complex, stateful logic of the cognitive actors to be developed and validated in a deterministic, single-threaded environment before being deployed in a parallel or distributed setting. This capability drastically reduces the complexity of debugging and de-risks the initial development phases. For instance, frameworks like Thespian allow developers to switch between a sequential execution engine (

simpleSystemBase) and a multi-process engine (multiprocTCPBase) with a simple configuration change, requiring no modification to the actor code itself.12 This enables a phased approach where cognitive correctness is established first, followed by performance scaling.

2.3.1. Actor Framework Selection: After a comparative analysis of available Python actor frameworks, Thespian is selected as the foundational technology for the computational fabric.
Table 1: Comparative Analysis of Python Actor Frameworks

Thespian's comprehensive feature set, particularly its support for distributed operation and fault-tolerant supervision, aligns directly with the long-term architectural goals of the MVCC. While Pykka is a capable library for simple concurrency, it explicitly lacks the features required to build a resilient "society of minds".[16]


2.3.2. Core Cognitive Actors API: The MVCC will be composed of a set of specialized actors, each with a clearly defined responsibility and a formal message-based API. All messages exchanged between actors will be simple, pickleable Python objects.12

CognitiveSupervisor: The system's primary entry point and orchestrator, implementing the centralized supervisor pattern.1 It receives high-level goals from the external environment, decomposes them into a sequence of sub-tasks, and dispatches these tasks as messages to the appropriate specialist actors. It is also the top-level supervisor for the actor system.

PerceptionActor: The system's interface to the external world. It is responsible for receiving raw data streams (e.g., text from a user, sensor data), structuring this data into formal Perception objects, and sending them as messages to the MemoryManagementActor for storage in the episodic layer.

MemoryManagementActor: The sole gatekeeper to the Object Store (Neo4j). It exposes a high-level, object-oriented API to other actors for writing to and querying the fractal memory. By centralizing all database interactions, this actor enforces strict encapsulation of the memory system.

AbstractionActor: The engine of learning and self-modification. It is a long-running, stateful actor that periodically receives a TriggerAbstractionCycle message from the supervisor. It then executes the core abstraction algorithm (Section 2.2.2), communicating exclusively with the MemoryManagementActor to read episodic patterns and write or update semantic prototypes.

GoalExecutionActor: Responsible for interacting with the external environment to achieve concrete goals. It receives action plans (sequences of messages) from the supervisor and executes them by interacting with external tools, APIs, or physical effectors. Upon completing an action, it sends Action and State update messages to the MemoryManagementActor to create a record of its own behavior, closing the perception-action loop.

2.3.3. Supervision and Cognitive Resilience: The MVCC will implement a robust fault-tolerance model using parental supervision, a concept well-established in actor systems like Akka 18 and supported by Thespian.17 The
CognitiveSupervisor will act as the top-level supervisor for all other core actors. It will define a supervision strategy by handling the ChildActorExited system message, which is automatically sent to a parent when a child actor terminates unexpectedly.17 The strategy will be nuanced:

Restart: If a critical, long-lived actor like the MemoryManagementActor fails, the supervisor's directive will be to restart it, potentially after a short delay, to restore core system functionality.

Resume: For less critical failures where state may be preserved, a resume strategy could be employed (a conceptual goal, though restart is Thespian's primary mechanism).

Stop: If a transient, task-specific actor (e.g., an actor created to perform a single, non-critical web query) fails, the supervisor may simply log the failure and discard the actor, preventing a minor error from cascading through the system.
This explicit, hierarchical approach to error handling provides a framework for building a highly resilient cognitive system that can gracefully handle internal failures without catastrophic collapse.

Part III: Phased Implementation and Validation Roadmap

This roadmap outlines a four-phase plan for the development, optimization, and long-term deployment of the Minimum Viable Cognitive Core. The phases are designed to systematically de-risk the project by tackling foundational infrastructure, core research challenges, performance optimization, and advanced deployment targets in a logical sequence.

3.1. Phase 1: Core Infrastructure Development (Months 1-3)

Objective: To establish and validate the foundational software and architectural layers of the MVCC, focusing on the memory store and the basic actor communication fabric.

Tasks:

Deploy Object Store: A production-grade Neo4j graph database instance will be deployed. The graph schema, as detailed in Section 2.1, will be formally implemented, including node labels (Event, PrototypeConcept), property keys, and relationship types (PRECEDES, INSTANCE_OF).

Implement Memory Actor: The MemoryManagementActor will be implemented in Python using the Thespian actor framework.12 This actor will encapsulate all database interaction logic, using the official Neo4j Python driver to connect to the database and execute Cypher queries for creating and retrieving nodes and relationships.3

Establish Deterministic Actor System: The initial set of core cognitive actors (CognitiveSupervisor, PerceptionActor, GoalExecutionActor) will be implemented as Thespian classes. Crucially, the Thespian ActorSystem will be initialized using the simpleSystemBase.12 This transport layer executes all actor message handling sequentially in a single thread, eliminating concurrency issues and allowing for deterministic testing and debugging of the core cognitive logic.

Define Message Protocols: The initial set of message classes will be defined as simple Python objects. These messages, such as StoreEpisodicData(data) and QueryByPattern(pattern), will form the basic vocabulary for inter-actor communication, relying on Thespian's default pickle-based serialization.12

Exit Criteria: A fully operational, single-threaded actor system capable of receiving simulated perceptual data via the PerceptionActor, which then messages the MemoryManagementActor to successfully persist the data as a correctly structured episodic fractal in the Neo4j database. Verification will be done by direct inspection of the graph database.

3.2. Phase 2: Implementing the Fractal Memory Bridge (Months 4-9)

Objective: To implement and experimentally validate the core self-modification and learning mechanism of the MVCC—the bridge between episodic experience and semantic knowledge.

Tasks:

Implement Prototypal Inheritance: The PrototypeObject base class will be implemented in Python. The key feature will be the implementation of the __getattr__ magic method to perform delegation to a parent prototype, as specified in Section 2.2.1. This provides the runtime mechanism for dynamic inheritance.10

Develop Abstraction Algorithm: The AbstractionActor will be implemented, containing the pattern-matching and prototype synthesis algorithms detailed in Section 2.2.2. The pattern-matching component will leverage graph database queries to search for isomorphic subgraphs within the episodic layer.

Conduct Controlled Experiments: The system will be subjected to a series of controlled experiments. It will be fed curated streams of simulated episodic data representing simple, recurring tasks. For example, sequences representing "picking up a red block," "picking up a blue block," and "picking up a green block." The system's response will be monitored to validate its ability to form a stable and correct semantic abstraction, such as a new PrototypeConcept for "picking up a block."

Develop Visualization Tools: To aid in research and debugging, a suite of visualization tools will be developed. These tools will query the Neo4j database and render the episodic and semantic graph layers, allowing researchers to observe the formation of semantic prototypes from episodic data in real-time.

Exit Criteria: Demonstrable, repeatable evidence that the system can generalize from specific, recorded instances to abstract concepts. The primary validation will be the autonomous creation of a correct PrototypeConcept node in the semantic layer, linked via INSTANCE_OF relationships to the multiple episodic fractals from which it was derived.

3.3. Phase 3: High-Performance Core and Language Bridging (Months 10-12)

Objective: To address anticipated performance bottlenecks in the cognitive cycle and to scale the system for parallel execution. A pure Python implementation of computationally intensive tasks like graph isomorphism search will not be sufficient for a real-time system. Therefore, a hybrid Python/C++ architecture is required.

The selection of a binding technology is critical to maintaining a clean architecture and efficient developer workflow. The project's philosophy favors using Python for high-level orchestration and C++ for specialized, high-performance algorithms. This necessitates a tool that allows C++ developers to easily expose their libraries to the Python environment. Pybind11 is designed for precisely this use case, providing a C++-first API that leverages modern C++11 features for clean, efficient, and low-boilerplate binding code.21 This is a superior approach for this project compared to older, more cumbersome tools like SWIG, which require separate interface definition files, or CFFI, which is primarily designed for C ABIs and incurs runtime overhead.23

Tasks:

Performance Profiling: The running MVCC will be profiled to identify computational bottlenecks. The graph pattern-matching algorithm within the AbstractionActor is the primary anticipated bottleneck.

C++ Library Implementation: The identified bottlenecks will be re-implemented as a high-performance C++ library, leveraging optimized graph algorithms.

Create Python Bindings: Pybind11 will be used to create Python bindings for the new C++ library.21 This will produce a compiled Python extension module that can be imported and used directly within the Python-based actor system. The rationale for this choice is detailed in Table 2 below.

Integration and Scaling: The compiled C++ module will be integrated into the AbstractionActor, replacing the slow Python implementation. Concurrently, the Thespian ActorSystem base will be switched from simpleSystemBase to multiprocTCPBase.12 This will transition the MVCC from a deterministic, single-threaded model to a parallel, multi-process architecture capable of leveraging multiple CPU cores.

Table 2: Comparison of Python/C++ Binding Technologies

Exit Criteria: A functional, hybrid Python/C++ MVCC running in a multi-process configuration. Successful completion will be validated by demonstrating a significant, measurable reduction (e.g., an order of magnitude) in the execution time of the abstraction cycle compared to the pure Python implementation from Phase 2.

3.4. Phase 4 (Long-Term Vision): Porting the MVCC to a Component-Based OS (Months 13-24)

Objective: To achieve the highest levels of security, robustness, and real-time capability by deploying the MVCC onto the Genode OS Framework. This phase represents the ultimate realization of the "society of minds" concept, where the cognitive actors are no longer just application-level processes but are distinct, kernel-isolated OS components.

This is a significant but feasible engineering effort. The primary prerequisite for running the Python-based MVCC on Genode is the availability of a Python interpreter. Genode provides a substantial C runtime based on FreeBSD's libc and has a well-established POSIX compatibility layer, which are the main dependencies for porting the CPython or MicroPython interpreters.30 Existing documentation on porting Python to new operating systems confirms that a functional C library and toolchain are the most critical requirements.30 Once the interpreter is available, the next step is to bridge the application-level message passing of Thespian with the kernel-enforced, capability-based Inter-Process Communication (IPC) of Genode.34 Each Thespian actor can be packaged as a separate Genode component. A high-level Python call like

asys.tell(memory_actor, data) would be translated by a custom Thespian transport layer into a native Genode Remote Procedure Call (RPC) to the service offered by the memory_actor component. The Genode kernel would then enforce that only components possessing a valid capability for that service can communicate with it, thus elevating the architectural principle of encapsulation to a kernel-guaranteed security property.

Tasks:

Sub-Project: Port MicroPython to Genode: As a focused proof-of-concept, the team will first port the MicroPython interpreter to Genode. Due to its smaller codebase and fewer dependencies compared to CPython, this is a more tractable initial goal.36 Success will validate the Genode toolchain, C library, and POSIX compatibility layer for this class of application.30

Sub-Project: Map Actor Model to Genode IPC: A custom Thespian transport layer will be developed. This layer will replace Thespian's default TCP-based communication with one that uses Genode's native C++ RPC mechanism.35 When an actor sends a message, this layer will marshal the Python object and initiate a Genode RPC to the target component.

System Re-Packaging: Each of the core cognitive actors (CognitiveSupervisor, MemoryManagementActor, etc.) will be packaged as an independent Genode component. Each component will consist of the MicroPython interpreter and the specific Python script for that actor.

System Configuration: The final system will be assembled using Genode's declarative runtime configuration system.40 This configuration will define the component hierarchy (e.g., specifying the
CognitiveSupervisor as the parent of the other actors) and the session routing policies that grant capabilities for communication between them. This configuration file becomes the explicit, verifiable blueprint of the "society of minds."

Exit Criteria: A fully functional MVCC running natively on the Genode OS Framework. The successful demonstration will show each core cognitive actor running as a distinct, kernel-isolated component, communicating via Genode's secure IPC mechanisms. This will represent a novel architecture for building secure and robust artificial intelligence systems.

Part IV: Experimental Protocols and Future Directions

4.1. Benchmarking Cognitive Tasks

To validate the unique advantages of the MVCC architecture, a new suite of cognitive benchmarks must be developed. Standard NLP benchmarks are insufficient as they do not test for persistent memory, continuous learning, or dynamic knowledge abstraction. The following experimental protocols will be used to measure the system's performance on these key dimensions.

Continuous Learning Benchmark (CLB): This benchmark will consist of a long-running, goal-oriented conversational task spanning thousands of interaction turns. The system will be required to recall specific details, synthesize information provided hundreds or even thousands of turns prior, and maintain a consistent internal model of the task state. This directly tests the system's immunity to the "context rot" that plagues traditional LLMs, as all information will be persisted in the object-centric graph memory rather than a finite context window.1 Success is defined as maintaining a high level of accuracy on recall and reasoning tasks throughout the entire duration of the conversation.

Multi-Hop Abstraction Benchmark (MAB): This benchmark will test the core function of the fractal memory bridge. The system will be presented with a series of related but distinct procedural tasks through demonstration or instruction (e.g., the steps to make tea, the steps to make instant noodles, the steps to make pour-over coffee). After observing these distinct episodic sequences, the system will be queried to describe the abstract, high-level procedure for a generalized task (e.g., "describe the general process for preparing a hot beverage or food using a kettle"). Success is measured by the system's ability to generate a correct, high-level abstract prototype that captures the common sub-sequences (boil water, combine with ingredients, wait) while correctly parameterizing the variable elements (tea bag vs. noodles). This provides a direct, qualitative measure of the abstraction algorithm's efficacy.

Dynamic Adaptation Benchmark (DAB): This benchmark evaluates the system's few-shot learning capability, a direct outcome of the prototype-based knowledge model.1 The system will be introduced to a completely novel concept or entity with a small number of descriptive statements (e.g., "A 'greeble' is a blue cube. Greebles are heavy. Greebles can be used to press buttons."). Immediately following this introduction, the system will be given a task that requires reasoning about the new concept (e.g., "There is a red button on the wall. What tool could you use to press it?"). Success is defined as the system's ability to correctly identify and use the "greeble" in its reasoning process, demonstrating the successful runtime creation of a new, functional semantic prototype without any form of retraining.

4.2. Measuring Self-Modification

A key claim of this research is that the MVCC is a self-modifying system. To substantiate this claim, it is necessary to develop quantitative metrics that track the evolution of the system's internal knowledge structure over time. These metrics will provide an objective measure of learning and cognitive growth.

Semantic Growth Rate (SGR): This metric is defined as the number of new PrototypeConcept nodes created in the Semantic Layer per unit of experience. The unit of experience can be defined as a fixed number of interactions or episodic events (e.g., per 1000 Event nodes added to the episodic layer). A consistently positive SGR indicates that the system is actively discovering new patterns in its experience and forming new concepts.

Abstraction Efficiency (AE): This metric is the ratio of the total number of nodes in the Semantic Layer to the total number of nodes in the Episodic Layer. A successful, continuously learning system should exhibit an increasing AE over time. This indicates that the system is efficiently compressing its raw experiences into a more compact, generalized knowledge structure, which is a hallmark of effective learning. A stagnant or decreasing AE would suggest a failure in the abstraction mechanism.

Performance Trajectory (PT): This involves tracking the system's performance on the cognitive benchmarks from Section 4.1 over its operational lifetime. The system will be periodically evaluated on these benchmarks as it accumulates more experience. A positive performance trajectory—an increase in scores over time without any external fine-tuning or retraining—would provide the strongest evidence of successful self-modification and genuine learning.

4.3. Concluding Remarks and Long-Term Vision

The prevailing paradigm of Large Language Models, for all its successes, is built on a foundation of brute-force scale applied to a stateless and unstructured computational model. The persistent challenges of unreliable memory, contextually blind retrieval, and opaque reasoning are not incidental flaws but are the direct and inevitable consequences of this underlying philosophy. The approach of simply building larger models with bigger context windows is a path of diminishing returns that will only amplify these core architectural deficiencies.1

A more promising and sustainable path forward lies in a paradigm shift informed by the profound and time-tested principles of dynamic object-oriented systems. This research plan details a concrete and achievable path toward this new paradigm. By reconceptualizing LLM memory as a structured graph of encapsulated objects, we can overcome the fragility of the context window and enable persistent, precise, multi-hop reasoning. By embracing the fluidity of prototypes as a model for knowledge, we can create systems that dynamically synthesize new semantic understanding and adapt to novel domains with minimal data. And by structuring computation as message passing between a society of specialized agents, we can build scalable and robust reasoning engines capable of tackling complexity far beyond the reach of any single model.

The object is the memory; the prototype is the concept; the message is the computation. Together, these principles provide the architectural blueprint for the next generation of precise, reliable, and truly intelligent systems. The ultimate vision of this work is the creation of a "live system," an AI that functions less like a static, pre-trained artifact and more like a persistent, dynamic, and continuously learning cognitive environment.1 The successful execution of this plan will not only produce a superior AI architecture but will also represent a significant step toward systems that possess the capacity for genuine understanding and autonomous growth.

Works cited

Dynamic OO Enhancing LLM Understanding

Actor model - Wikipedia, accessed September 11, 2025, https://en.wikipedia.org/wiki/Actor_model

Build applications with Neo4j and Python - Neo4j Python Driver Manual, accessed September 11, 2025, https://neo4j.com/docs/python-manual/current/

Neo4j Tutorial: Using And Querying Graph Databases in Python - DataCamp, accessed September 11, 2025, https://www.datacamp.com/tutorial/neo4j-tutorial

Neo4j Python Driver 5.28, accessed September 11, 2025, https://neo4j.com/docs/api/python-driver/current/

Model objects from the model catalog - Neo4j Graph Data Science Client, accessed September 11, 2025, https://neo4j.com/docs/graph-data-science-client/current/model-object/

The graph object - Neo4j Graph Data Science Client, accessed September 11, 2025, https://neo4j.com/docs/graph-data-science-client/current/graph-object/

Object-Centric Process Mining (and More) Using a Graph- Based Approach With PromG - TUE Research portal - Eindhoven University of Technology, accessed September 11, 2025, https://research.tue.nl/files/352938556/paper_9922.pdf

Object-Centric Process Mining (and More) Using a Graph-Based Approach With PromG - CEUR-WS, accessed September 11, 2025, https://ceur-ws.org/Vol-3648/paper_9922.pdf

How do methods __getattr__ and __getattribute__ work? : r/learnpython - Reddit, accessed September 11, 2025, https://www.reddit.com/r/learnpython/comments/abijbg/how_do_methods_getattr_and_getattribute_work/

Python Magic Methods and __getattr__ | by Santiago Basulto | rmotr.com, accessed September 11, 2025, https://blog.rmotr.com/python-magic-methods-and-getattr-75cf896b3f88

Table Of Contents - Thespian Python Actors, accessed September 11, 2025, https://thespianpy.com/doc/

The power of Actor Model and the effect of Pykka in Asynchronous Programming at Python, accessed September 11, 2025, https://www.zen8labs.com/insights/programming/the-power-of-actor-model-and-the-effect-of-pykka-in-asynchronous-programming-at-python/

Concurrency with Python: Actor Models - Bytes by Ying, accessed September 11, 2025, https://bytes.yingw787.com/posts/2019/02/02/concurrency_with_python_actor_models

Pykka 1.2.0 documentation, accessed September 11, 2025, https://pykka.org/en/latest/

Pykka — Pykka 4.2.0 documentation, accessed September 11, 2025, https://pykka.readthedocs.io/

supervisor strategy with actors using Thespian - Google Groups, accessed September 11, 2025, https://groups.google.com/g/thespianpy/c/ThfsaoDt-3M

Classic Supervision - Akka Documentation, accessed September 11, 2025, https://doc.akka.io/libraries/akka-core/current/supervision-classic.html

Supervision | Akka.NET Documentation, accessed September 11, 2025, https://getakka.net/articles/concepts/supervision.html

Classic Supervision · Apache Pekko Documentation, accessed September 11, 2025, https://pekko.apache.org/docs/pekko/current/supervision-classic.html

pybind11 documentation, accessed September 11, 2025, https://pybind11.readthedocs.io/

Cython, pybind11, cffi – which tool should you choose? | Stefans Welt, accessed September 11, 2025, http://blog.behnel.de/posts/cython-pybind11-cffi-which-tool-to-choose.html

Cython, pybind11, cffi – when to use what? - Stefans Welt, accessed September 11, 2025, http://blog.behnel.de/posts/cython-pybind11-cffi-when-to-use-what.html

c++ - Extending python - to swig, not to swig or Cython - Stack Overflow, accessed September 11, 2025, https://stackoverflow.com/questions/456884/extending-python-to-swig-not-to-swig-or-cython

Pybind11 Tutorial: Binding C++ Code to Python | by Ahmed Gad - Medium, accessed September 11, 2025, https://medium.com/@ahmedfgad/pybind11-tutorial-binding-c-code-to-python-337da23685dc

First steps - pybind11 documentation, accessed September 11, 2025, https://pybind11.readthedocs.io/en/stable/basics.html

SWIG Tutorial, accessed September 11, 2025, https://www.swig.org/tutorial.html

2. Simplified Wrapper Interface Generator (SWIG) — OpenMOC Documentation - GitHub Pages, accessed September 11, 2025, https://mit-crpg.github.io/OpenMOC/devguide/swig.html

Wrapping C/C++ for Python using SWIG - Set 1 - GeeksforGeeks, accessed September 11, 2025, https://www.geeksforgeeks.org/python/wrapping-cc-python-using-swig-set-1/

Genode Porting Guide, accessed September 11, 2025, https://genode.org/documentation/developer-resources/porting_applications

Release notes for the Genode OS Framework 19.11 - Genode, accessed September 11, 2025, https://genode.org/documentation/release-notes/19.11

Genode's Browser Odyssey - Genodians.org, accessed September 11, 2025, https://genodians.org/nfeske/2022-01-27-browser-odyssey

Porting to a new platform - Python Developer's Guide, accessed September 11, 2025, https://devguide.python.org/developer-workflow/porting/

Genode on seL4 - IPC and virtual memory, accessed September 11, 2025, https://genode.org/documentation/articles/sel4_part_2

Inter-component communication - Genode OS Framework Foundations, accessed September 11, 2025, https://genode.org/documentation/genode-foundations/20.05/architecture/Inter-component_communication.html

Porting MicroPython — MicroPython 1.13 documentation - Read the Docs, accessed September 11, 2025, https://micropythpn.readthedocs.io/en/docs-chap1/develop/porting.html

Porting MicroPython — MicroPython latest documentation, accessed September 11, 2025, https://docs.micropython.org/en/latest/develop/porting.html

Remote procedure calls - Genode OS Framework Foundations, accessed September 11, 2025, https://genode.org/documentation/genode-foundations/19.05/functional_specification/Remote_procedure_calls.html

A simple client-server scenario - Genode, accessed September 11, 2025, https://genode.org/documentation/developer-resources/client_server_tutorial

System configuration - Genode OS Framework Foundations, accessed September 11, 2025, https://genode.org/documentation/genode-foundations/25.05/system_configuration/index.html

Feature | Thespian | Pykka

Concurrency Model | Threads or Processes (configurable) 12 | Threads 15

Distribution Model | Supported via TCP-based transport; actors are location-independent 12 | Not supported; designed for single-process applications 16

Supervision Strategy | Parental supervision supported via ChildActorExited system messages 17 | Not supported 16

Key Differentiator for MVCC | Full-featured implementation supporting fault tolerance and distribution, which are critical for the long-term vision. The separation of actor logic from the transport layer enables deterministic development and seamless scaling from a single process to a distributed system.12 | Simple and lightweight, but lacks the necessary features for building a robust, scalable, and fault-tolerant cognitive system. Its scope is limited to in-process concurrency.13

Technology | Development Paradigm | C++ Support | Performance | Build Complexity

pybind11 | C++-first: Bindings are written in C++ using a header-only library.22 | Excellent: Designed for modern C++ (11/14/17+), supports classes, templates, and STL containers.21 | High: Generates statically compiled extension modules with minimal overhead.23 | Low: Header-only library integrates cleanly with standard build systems like CMake.21

SWIG | Interface-first: Bindings are generated from a separate interface (.i) file.27 | Good: Supports C++, but the parser can be limited and may require manual hints for complex code.24 | High: Generates statically compiled extension modules.28 | Medium: Requires maintaining separate interface files and adds an extra generation step to the build process.29

CFFI | Python-first: Bindings are defined in Python, interacting with a C-level ABI.22 | Limited: Primarily designed for C ABIs. Interfacing with C++ classes requires manual C wrapper functions.23 | Lower: Incurs runtime overhead for dynamic calls, though faster than ctypes.22 | Low for simple C libraries, but higher for C++ due to the need for manual C wrappers.