The Ludic Engine: An Amended Research Plan for Open-Ended, Multi-Agent Computational Creativity

Section 1: The Ludic Engine: A Paradigm for Open-Ended Creation

This research plan outlines the 'Ludic Engine' initiative, a project that reframes the objective of artificial intelligence from goal-oriented problem-solving to the cultivation of a system capable of continuous, autonomous discovery. This paradigm shift is foundational to the architectural and methodological choices detailed in the subsequent sections. The five core principles of the initiative—active creation, a shared digital playground, synergistic interaction, the orchestrator as a gardener, and metrics of serendipity—are not independent directives but form a tightly interwoven, hierarchical system. At the micro-level, intrinsic motivation drives individual agent behavior. At the meso-level, the multi-agent playground provides the environment for these motivations to manifest as complex interactions. At the macro-level, the gardener provides environmental curation to guide these interactions toward increasing complexity. This creates a feedback loop where the system perpetually generates its own evolving curriculum, which is the essence of open-endedness. The 'Metrics of Serendipity' serve as the sensory apparatus for observing and quantifying the success of this entire complex adaptive system.

1.1 From Passive Analysis to Active Creation: The Artifact-Generation Paradigm

The Ludic Engine initiative will formally adopt the "artifact-generation paradigm," a conceptual model prevalent in the field of computational creativity.1 This approach marks a departure from the traditional AI focus on deconstructing and solving predefined problems. Instead, it prioritizes the synthesis and combination of AI systems to produce artifacts—such as novel scientific theories, musical compositions, or software—that possess cultural or practical value.2 This initiative is an explicit attempt to "climb the metamountain," a metaphor for enabling software to take on tasks at increasing metalevels by combining individual AI techniques into more sophisticated, creative wholes.1 The Ludic Engine is therefore not envisioned as a single tool but as an ecosystem of tools that learn to use one another to generate novel outputs.

This work will be grounded in established definitions of creativity, such as Sternberg's "capacity to create a solution that is both novel and appropriate".4 Furthermore, it will be guided by Margaret Boden's crucial distinction between "P-creativity" (psychological creativity, which is novel to the individual agent) and "H-creativity" (historical creativity, which is recognized as novel by society at large).5 While the system's internal mechanisms will initially generate P-creative outputs, the ultimate ambition of the Ludic Engine is to create a system that can reliably produce H-creative artifacts.

1.2 Open-Endedness as a Core Tenet

The Ludic Engine will be architected as an open-ended AI system. Such systems are characterized by their capacity for novelty generation, autonomous exploration, continuous improvement, and intrinsic motivation.6 Unlike traditional AI, which operates within fixed rules and aims for a finite set of outcomes, an open-ended system is designed for "perpetual growth and adaptation" rather than convergence on a fixed endpoint.6 It is a system that continuously invents new and ever-more complex tasks and learns to solve them.8

A key principle guiding the design is that open-endedness is "observer-dependent".7 This means the system must produce artifacts that are not only novel but also

learnable—that is, useful and comprehensible from the perspective of a human observer. This constraint is critical, as it prevents the system from devolving into the generation of mere random noise, which may be statistically novel but contains no learnable structure.9

To achieve this, the system's dynamics will be inspired by the core principles of biological evolution: variation, selection, and retention.9 The digital Playground (detailed in Section 2) will provide the environment where selection occurs. The AI Cores (Section 3) will generate variation through their exploratory actions. The Gardener (Section 5) will curate the retention of successful strategies and artifacts, creating an open-ended spiral of increasing complexity and capability.

Section 2: Architecting the Playground: A Multi-Agent Crucible for Emergence

This section details the technical architecture of the shared digital environment—the 'Playground'—where the AI cores will interact. This environment is not a static backdrop but a dynamic, co-evolving space engineered to facilitate complex, emergent behaviors. The design of this Playground is not secondary to the design of the agents; it is the primary medium through which the system's complexity is expressed and measured. A simple, static environment will yield simple, predictable behaviors. A dynamic, high-dimensional environment, curated by the Gardener, becomes a "complexity pump," directly determining the potential for emergent discovery. Therefore, the engineering effort for the Playground must be on par with the effort for the agents themselves, prioritizing a highly parameterizable and extensible architecture from the outset.

2.1 A Multi-Agent Markov Game Formulation

The Playground will be formally modeled as a Multi-Agent Markov Game. This is a mathematical framework defined by a tuple (N,S,A,P,R,γ), where N is the number of agents, S is the shared state space, A is the joint action space of all agents, P is the state transition probability function, and R is the set of agent-specific reward functions.10 This formalism provides a rigorous basis for describing the interactions of multiple learning agents within a shared environment.

To implement this, the project will leverage a standardized Multi-Agent Environment API, such as RLlib's MultiAgentEnv 11 or Farama's PettingZoo.12 These frameworks provide robust, battle-tested solutions for handling the complexities of multi-agent systems, including agent-specific observation and action spaces, and the management of asynchronous or turn-based interaction patterns. The practical implementation using a framework like RLlib will involve defining agent-specific observation and action spaces via dictionaries (e.g.,

self.observation_spaces, self.action_spaces) and managing the agent lifecycles through the reset() and step() methods. These methods will return dictionaries that map unique agent IDs to their respective observations, rewards, and termination flags, providing fine-grained control over the environment's dynamics.11

2.2 Addressing the Non-Stationarity Challenge

A fundamental challenge in multi-agent reinforcement learning (MARL) is non-stationarity. As each agent continuously updates its policy, the environment's dynamics change from the perspective of every other agent.10 This can lead to instability in the learning process, as each agent is essentially trying to hit a moving target.

This research plan proposes to treat non-stationarity not as a bug to be eliminated, but as a feature that drives adaptation and novelty. The constant pressure to adapt to the changing behaviors of other agents is a powerful catalyst for learning and the discovery of new strategies. The Gardener's role (Section 5) will be crucial in managing this dynamic process, preventing catastrophic policy collapse by curating agent matchups and environmental parameters to maintain a productive level of challenge. To further stabilize learning amidst this non-stationarity, the project will explore techniques such as centralized training with decentralized execution (CTDE). In this paradigm, a central critic has access to global information during the training phase to provide a stable learning signal, while the individual agents learn to act based only on their local observations during execution.10

Section 3: The Inhabitants: Intrinsically Motivated Creative Cores

This section defines the nature of the individual AI agents, or 'cores,' that will inhabit the Playground. The central thesis is that their behavior must be driven by internal, self-generated goals (intrinsic motivation) rather than external, pre-defined rewards (extrinsic motivation). This internal drive is the engine of exploration, discovery, and ultimately, creativity.

3.1 The Primacy of Intrinsic Motivation

Extrinsic motivation, where an agent is "bribed" with rewards to perform a specific task, is inherently coercive and limits creativity to the predefined reward landscape.13 In contrast, intrinsic motivation—the drive to perform an activity for its "inherent satisfaction," for the "fun or challenge entailed" 14—is the source of genuine creativity and innovation. The cores will be designed to be "self-supervised," capable of generating their own goals and pursuing them autonomously.13

The project will draw from the rich literature on computational models of intrinsic motivation, which can be broadly categorized as knowledge-based, competence-based, and morphological.14 The initial focus will be on knowledge-based models, which generate rewards by measuring the dissonance between an agent's expectations and its experiences. These models effectively reward an agent for reducing its uncertainty or encountering novelty, driving it to explore its environment.

3.2 Architectures for Curiosity-Driven Exploration

The primary mechanism for generating intrinsic rewards will be prediction error. In this approach, the agent builds a model of its environment (or a facet of it) and is rewarded when its predictions are wrong. A high prediction error signals that the agent has encountered something novel and learnable, thus incentivizing further exploration of that part of the state space.16 Two leading architectures will be implemented and evaluated.

Candidate Architecture 1: Intrinsic Curiosity Module (ICM)

The ICM uses a forward dynamics model to predict the encoded feature representation of the next state, given the current state and the agent's action. The error in this prediction serves as the curiosity reward.17 A key feature of ICM is its use of an

inverse dynamics model to train the state encoder. This inverse model is trained to predict the agent's action given the current and next states. This task forces the encoder to learn features that are controllable by the agent's actions, which helps mitigate the "Noisy TV Problem"—a common failure mode where an agent becomes perpetually distracted by random but irrelevant phenomena (like static on a screen) because they are inherently unpredictable.17 The intrinsic reward is calculated as

rtint​=η⋅21​​ϕ^​(st+1​)−ϕ(st+1​)​2, where ϕ(st+1​) is the encoded next state and ϕ^​(st+1​) is the predicted encoded next state.17

Candidate Architecture 2: Random Network Distillation (RND)

RND takes a different approach to solving the Noisy TV Problem. It uses two neural networks: a fixed, randomly initialized target network and a predictor network. The predictor network is trained to predict the output of the target network for a given state. The prediction error between the two networks serves as the curiosity reward.16 Because the target function is random but fixed, the prediction error is primarily a function of the novelty of the input state, not environmental stochasticity. In unfamiliar states, the predictor will struggle to guess the target's output, resulting in a high reward.21 This provides a more robust solution to the Noisy TV Problem by decoupling the reward signal from the predictability of the environment's state transitions.19

The choice between these architectures is not merely a technical one. The underlying mechanisms of ICM and RND represent fundamentally different philosophical stances on what constitutes "interestingness," which will likely lead to different emergent agent personalities. ICM's inverse model explicitly filters for agent-controllable features, meaning its reward is tied to misunderstanding the consequences of one's own actions.17 This will likely produce agents that are "tinkerers," interested in what they can control and manipulate. In contrast, RND's reward is tied to the statistical novelty of an observation, regardless of controllability.20 This will likely produce "explorers," interested in seeing new things. The Ludic Engine's true creative potential may lie in the synergistic or competitive interaction

between these different motivational drives. For example, a "tinkerer" agent might build a novel structure, which an "explorer" agent then discovers and uses in an unexpected way. This provides a direct pathway to synergistic creation. Therefore, the initial agent cores should be implemented with different intrinsic motivation architectures to foster this productive heterogeneity.

Section 4: Fostering Synergy: Protocols for Collaborative Creation

For the Playground to be more than a collection of isolated agents pursuing their own interests, a robust communication layer is required. This section specifies the protocols and interaction patterns that will enable the AI cores to achieve synergy, moving from simple co-existence to complex collaboration, competition, and co-creation.

4.1 The Need for a Standardized Communication Protocol

Modern AI agents are often developed in isolation across different frameworks and infrastructures, creating significant integration barriers that slow innovation.22 A standardized communication protocol is essential to transform the disparate cores of the Ludic Engine into a cohesive, "interlinked ecosystem" where agents can discover, understand, and collaborate with one another.22 It is crucial to distinguish between the communication protocol and the orchestrator: the protocol standardizes the syntax and conventions of messages, while the orchestrator (the Gardener, Section 5) manages the higher-level workflow and environmental context.22

4.2 Evaluating and Selecting a Protocol: The Case for ACP

Several open standards for agent communication exist, including Google's Agent2Agent (A2A) 22 and the Linux Foundation's Agent Communication Protocol (ACP).22 For the Ludic Engine, the adoption of ACP as the foundational communication layer is proposed due to several key advantages:

REST-based and SDK-free: ACP utilizes standard HTTP conventions and a RESTful API, making it simple to integrate into production environments using common tools like curl or Postman without requiring specialized libraries or SDKs. This significantly reduces development complexity.23

Async-first Design: ACP is designed primarily for asynchronous communication, which is critical for the Ludic Engine. Creative or analytical tasks are often long-running and cannot be completed in a single, synchronous request-response cycle. An async-first model allows an agent to submit a complex task to another agent and continue its own processing, receiving a notification upon completion.23

Modality Agnostic: The protocol uses standard MimeTypes for content identification, allowing agents to exchange any data format—text, images, code, custom binary formats—without requiring protocol modifications.23 This flexibility is vital for a system intended to produce a diverse range of creative artifacts.

The selection of an asynchronous-first communication protocol like ACP is not merely a technical convenience; it is a fundamental enabler of deeper, more complex forms of creative collaboration. A synchronous model limits interactions to the equivalent of simple function calls. An asynchronous model, however, allows agents to engage in tasks that require prolonged, independent "thought." This unlocks a new class of interactions, enabling a true division of labor among agents with different cognitive tempos. It facilitates parallel, long-term projects rather than just sequential, short-term actions, which directly fosters the synergistic interaction mandated by the initiative.

| Table 2: Agent Communication Protocol Evaluation | | | | |

| :--- | :--- | :--- | :--- |

| Criterion | Agent Communication Protocol (ACP) | Agent-to-Agent (A2A) | Agora |

| Communication Model | RESTful API over HTTP 23 | JSON messages over HTTP 25 | Natural Language, Code Execution 22 |

| Synchronicity Support | Async-first, Sync supported. Ideal for long-running tasks.23 | Primarily synchronous client-server model.25 | Primarily synchronous, focused on negotiation.22 |

| Data Modality | Modality-agnostic via MimeTypes; supports text, images, video, binary.23 | JSON-based, suitable for structured data.25 | Focused on text and code.22 |

| Discovery Mechanism | Online (querying servers) and Offline (embedded metadata).22 | "Agent Cards" published by service agents.25 | Not explicitly defined in available material. |

| Ecosystem & Governance | Open standard under the Linux Foundation.23 | Open standard initiated by Google, managed under Linux Foundation.22 | Research protocol for LLM agents.22 |

| Alignment with Ludic Engine | High. Async-first design is critical for complex, long-running creative tasks. Modality-agnosticism supports diverse artifact generation. | Medium. Synchronous model may limit the complexity of delegated tasks. Strong for service-oriented architectures. | Low. Specialized for negotiation; less suitable as a general-purpose communication backbone. |

4.3 Defining Interaction Patterns

With ACP as the backbone, several key interaction patterns will be implemented to foster collaboration:

Client-Service Model: Agents can dynamically adopt roles as either clients initiating requests or services advertising specific capabilities.25 For instance, ALFRED, acting as a high-level strategist, could be a client that delegates sub-tasks to more specialized service agents like ROBIN (for data analysis) or BRICK (for physics-based simulation).

Workflow Chaining: The protocol enables the creation of complex, multi-agent workflows where the output of one agent becomes the input for another. A content creation pipeline, for example, might involve a topic research agent gathering information, a writing agent crafting a draft, and an SEO optimization agent refining the text, with each agent seamlessly coordinating through standardized handoffs.23

Emergent Negotiation: While not the primary focus of ACP, the standardized communication layer provides the foundation for more advanced interactions. In later phases, agents could use this channel to engage in autonomous negotiation over resources or collaborative strategies, a capability explored in protocols like Agora.22

Section 5: The Orchestrator as Gardener: Cultivating Complexity and Novelty

This section redefines the role of the central control system. The 'Orchestrator' is not a static scheduler assigning predefined tasks from a fixed list. Instead, it is an active, learning 'Gardener' that cultivates the Playground and its inhabitants to continuously generate novelty and complexity. This component is the core of the system's open-ended learning capability, bridging the gap between individual agent learning and the system's macro-level creative output.

5.1 The Gardener as an Automated Curriculum Generator

The Gardener's primary function is to create an "endlessly generating" stream of "increasingly complex and diverse learning environments and their solutions".8 This approach is heavily inspired by the Paired Open-Ended Trailblazer (POET) algorithm, which co-evolves a population of agents and a population of environments.8 The Gardener will observe the current capabilities of the agent population and procedurally generate new environmental configurations that are of "optimal incongruity" or present an "optimal challenge".15 These are challenges that are neither too easy for the current agents (and thus boring) nor too difficult (and thus frustrating), but rather exist at the frontier of their collective competence, maximizing the potential for learning and skill acquisition.

5.2 Functions of the Gardener

The Gardener will perform several key functions to cultivate the Ludic Engine ecosystem:

Environmental Curation: It will dynamically alter parameters of the Playground—such as its physics, resource availability, spatial layout, or available tools—to create novel challenges that test and expand the agents' capabilities.

Novelty Injection: The Gardener will periodically introduce entirely new objects, tools, or environmental dynamics, forcing agents to adapt and generalize their learned policies. This can be seen as a form of automated "house rules" injection, which is a key challenge in open-world novelty adaptation.27

Population Management: Drawing inspiration from evolutionary algorithms, the Gardener will manage the population of agent policies. It will periodically cull underperforming agents and promote or replicate agents that demonstrate novel or particularly effective behaviors, applying a selective pressure that guides the population's evolution.6

Resource Allocation: The Gardener will manage the system's computational resources, potentially allocating more processing power to agent-environment pairings that demonstrate high "learning progress," a competence-based intrinsic motivation metric that can be applied at the system level to identify promising avenues of exploration.15

The Gardener is the mechanism that connects the micro-scale agent learning (P-creativity) to the macro-scale system objective of generating valuable artifacts (H-creativity). Individual agents, driven by curiosity, might engage in aimless exploration. The Gardener, by curating environments and applying selective pressure, implicitly defines what is "interesting" by creating challenges that require certain types of solutions. For example, if the Gardener observes that agents have mastered simple tool use, it might generate an environment where resources are only accessible via complex, multi-step tool combinations. This pressures the agents to evolve more sophisticated collaborative strategies. In this way, the Gardener shapes the evolutionary trajectory of the entire agent ecosystem toward solving problems that are proxies for what a human observer might find valuable, acting as a surrogate for the selective pressures of the real world.

Section 6: The Metrics of Serendipity: A Framework for Quantifying Emergent Discovery

This section addresses the critical challenge of evaluating an open-ended system that, by design, lacks a single, fixed objective function. A traditional performance metric is insufficient. Instead, this plan proposes a multi-faceted 'Serendipity Dashboard' composed of metrics that operate at different levels of abstraction, from micro-agent behavior to macro-system output. This dashboard provides a holistic, quantitative language for observing and guiding the system's creative and discovery-oriented performance.

6.1 The Serendipity Dashboard: A Multi-Scale Evaluation Framework

Success in an open-ended system is not a single score but a constellation of desirable properties: exploration, synergy, diversity, and the generation of valuable artifacts. The Serendipity Dashboard is structured into four quadrants, each designed to measure one of these facets of the system's dynamics, providing a comprehensive view of its creative health.

6.2 Quadrant 1: Micro-Scale Novelty (Exploration)

Metric: State Visitation Novelty. This metric quantifies the raw exploratory drive of the system by measuring the "surprisingness" of the states encountered by individual agents. It directly assesses the effectiveness of the intrinsic motivation architectures detailed in Section 3.

Methodology: The value will be calculated directly from the intrinsic reward signals generated by the agents. For RND-based agents, this is the mean squared error of the predictor network.20 For ICM-based agents, it is the prediction error of the forward dynamics model.17 A high average score across the population indicates that agents are effectively pushing into new, unpredicted parts of the state space.

6.3 Quadrant 2: Meso-Scale Emergence (Synergy)

Metric: Emergent Coordination Index (ECI). This metric moves beyond individual exploration to measure the degree to which agents are coordinating and causally influencing each other's behavior in non-trivial ways. A high ECI suggests the emergence of complex teamwork, strategic competition, or other forms of synergy.

Methodology: This metric will be grounded in concepts from information theory and Causal Emergence (CE) theory.28 A practical implementation will adapt the approach from the Multi-Agent Coordinated Exploration (MACE) framework, which uses weighted mutual information to measure the influence of one agent's action on the future novelty experienced by other agents.29 A high mutual information score indicates that one agent's actions are highly predictive of another's future discoveries, signaling a strong degree of coordination.

6.4 Quadrant 3: Macro-Scale Value (Creation)

Metric: Artifact Novelty & Usefulness. This is the ultimate measure of the system's external value, evaluating the actual outputs (artifacts) it generates. It directly measures success within the artifact-generation paradigm.

Methodology: Following classic computational creativity evaluation frameworks, this metric will have two components 5:

Novelty: An artifact's novelty will be calculated based on its dissimilarity to all previously generated artifacts. For text or code, this can be measured by distance in a semantic embedding space; for engineering designs, it could be structural or functional dissimilarity.32 Novelty can also be defined by its statistical rarity relative to the entire population of solutions generated.31

Usefulness: This component is necessarily domain-specific and requires a "usefulness function" for each domain the Ludic Engine operates in. For a generated software program, usefulness could be its performance on a benchmark task; for a generated game level, it could be its solvability or a predicted player engagement score.31

6.5 Quadrant 4: System-Scale Dynamics (Diversity)

Metric: Behavioral Diversity. This metric serves as a health check on the evolutionary process curated by the Gardener. It measures the heterogeneity of strategies being employed by the agent population at any given time.

Methodology: This will be implemented by measuring the divergence between agent policies. Techniques include calculating the pairwise Wasserstein distance between agent action distributions over a set of reference observations (a method known as System Neural Diversity) 33 or measuring the Kullback-Leibler (KL) divergence between policy outputs (inter-agent divergence).34 A high diversity score indicates a healthy, adaptable ecosystem, while a low score suggests convergence to a monolithic strategy that may be brittle to new challenges introduced by the Gardener.

Section 7: Synthesis and Research Roadmap

This concluding section synthesizes the individual components into a cohesive vision for the Ludic Engine and outlines a phased research and implementation roadmap. This roadmap identifies key milestones, anticipated challenges, and the expected scientific breakthroughs at each stage.

7.1 Phase 1 (Year 1): Foundation and Core Implementation

Goals: Implement the foundational Playground architecture, leveraging an existing multi-agent engine like RLlib. Develop the first generation of heterogeneous cores, including at least one ICM-based "tinkerer" and one RND-based "explorer." Establish the ACP communication layer for inter-agent messaging.

Milestone: Demonstrate successful co-existence and basic, asynchronous communication between two intrinsically motivated agents in a simple, procedurally generated environment.

Metrics Focus: The primary evaluation will be on Quadrant 1 of the Serendipity Dashboard: State Visitation Novelty.

7.2 Phase 2 (Year 2): The Gardener and Emergence

Goals: Develop the first iteration of the Gardener, capable of basic environmental curation (e.g., mutating spatial layouts and resource distributions). Implement the initial Serendipity Dashboard, with a focus on validating the Emergent Coordination Index (Quadrant 2) and Behavioral Diversity (Quadrant 4).

Milestone: Demonstrate quantitatively that the Gardener's curriculum generation leads to a statistically significant increase in agent behavioral diversity and emergent coordination compared to agents training in a static environment.

7.3 Phase 3 (Years 3-4): Scaling Complexity and Artifact Generation

Goals: Enhance the Gardener with more sophisticated meta-learning capabilities for curriculum generation. Expand the Playground to support the generation of meaningful, domain-specific artifacts (e.g., functional code snippets, solvable level designs, simple engineering schematics). Fully implement and validate the Quadrant 3 metrics for Artifact Novelty & Usefulness.

Milestone: The Ludic Engine autonomously generates an artifact that is judged by a panel of human experts to be both novel and useful, thereby achieving a demonstrable form of H-creativity.

7.4 Long-Term Vision and Ethical Considerations

Vision: The long-term vision is for the Ludic Engine to become a perpetual discovery machine, a creative partner capable of accelerating scientific research, generating novel art forms, and uncovering unforeseen solutions to complex, real-world problems.6

Challenges & Risks: This ambitious research program must address significant challenges, including the immense computational demands of open-ended simulation 6 and the unique safety questions posed by such systems. An open-ended agent could discover exploits in its environment or develop undesirable goals that are misaligned with human values.8 To mitigate these risks, the project will incorporate a continuous safety evaluation framework from the outset, potentially including techniques like "Rainbow Teaming," which uses AI to generate diverse adversarial prompts and scenarios to proactively identify potential failure modes.8

Works cited

(PDF) Computational Creativity: Coming of Age - ResearchGate, accessed September 14, 2025, https://www.researchgate.net/publication/220605245_Computational_Creativity_Coming_of_Age

Computational Creativity: Coming of Age - AAAI, accessed September 14, 2025, https://www.aaai.org/ojs/index.php/aimagazine/article/download/2257/2096

Computational Creativity and Music Generation Systems: An Introduction to the State of the Art - Frontiers, accessed September 14, 2025, https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.00014/full

(PDF) Computational Creativity - ResearchGate, accessed September 14, 2025, https://www.researchgate.net/publication/224654392_Computational_Creativity

Computational creativity - Wikipedia, accessed September 14, 2025, https://en.wikipedia.org/wiki/Computational_creativity

Open-Ended AI: Pushing the Boundaries of Machine Intelligence, accessed September 14, 2025, https://www.alphanome.ai/post/open-ended-ai-pushing-the-boundaries-of-machine-intelligence

#1: Open-endedness and AI Agents – A Path from Generative to Creative AI?, accessed September 14, 2025, https://huggingface.co/blog/Kseniase/openendedness

jennyzzt/awesome-open-ended: Awesome Open-ended AI - GitHub, accessed September 14, 2025, https://github.com/jennyzzt/awesome-open-ended

The Future of AI is Open-Ended | Richard Cornelius Suwandi, accessed September 14, 2025, https://richardcsuwandi.github.io/blog/2025/open-endedness/

Multi-Agent Reinforcement Learning (MARL) | by Vinay Lanka | Medium, accessed September 14, 2025, https://vinaylanka.medium.com/multi-agent-reinforcement-learning-marl-1d55dfff6439

Multi-Agent Environments — Ray 2.49.1 - Ray Docs, accessed September 14, 2025, https://docs.ray.io/en/latest/rllib/multi-agent-envs.html

Multi-agent environments - ALE Documentation - The Farama Foundation, accessed September 14, 2025, https://ale.farama.org/multi-agent-environments/

Intrinsic and Extrinsic Motivation in Intelligent Systems - Proceedings of Machine Learning Research, accessed September 14, 2025, http://proceedings.mlr.press/v131/lieberman20a/lieberman20a.pdf

Intrinsic motivation (artificial intelligence) - Wikipedia, accessed September 14, 2025, https://en.wikipedia.org/wiki/Intrinsic_motivation_(artificial_intelligence)

What is Intrinsic Motivation? A Typology of Computational ..., accessed September 14, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC2533589/

What are curiosity-driven exploration methods? - Milvus, accessed September 14, 2025, https://milvus.io/ai-quick-reference/what-are-curiositydriven-exploration-methods

Curiosity-Driven Exploration in Reinforcement Learning ..., accessed September 14, 2025, https://www.geeksforgeeks.org/deep-learning/curiosity-driven-exploration-in-reinforcement-learning/

Curiosity-Driven Exploration in Reinforcement Learning | by Mehul Gupta | Data Science in Your Pocket | Medium, accessed September 14, 2025, https://medium.com/data-science-in-your-pocket/curiosity-driven-exploration-in-reinforcement-learning-5876442aa9ee

Random Network Distillation: A New Take on Curiosity-Driven Learning - Dataiku blog, accessed September 14, 2025, https://blog.dataiku.com/random-network-distillation-a-new-take-on-curiosity-driven-learning

RND — DI-engine 0.1.0 documentation, accessed September 14, 2025, https://opendilab.github.io/DI-engine/12_policies/rnd.html

Reinforcement learning with prediction-based rewards | OpenAI, accessed September 14, 2025, https://openai.com/index/reinforcement-learning-with-prediction-based-rewards/

What Are AI Agent Protocols? | IBM, accessed September 14, 2025, https://www.ibm.com/think/topics/ai-agent-protocols

Agent Communication Protocol: Welcome, accessed September 14, 2025, https://agentcommunicationprotocol.dev/

What is Agent Communication Protocol (ACP)? - IBM, accessed September 14, 2025, https://www.ibm.com/think/topics/agent-communication-protocol

MCP vs A2A: A Guide to AI Agent Communication Protocols - Auth0, accessed September 14, 2025, https://auth0.com/blog/mcp-vs-a2a/

Understanding the Agent Communication Protocol (ACP) and Its Evolution from MCP | by Sree Potluri | Medium, accessed September 14, 2025, https://medium.com/@SreePotluri/understanding-the-agent-communication-protocol-acp-and-its-evolution-from-mcp-c28ad30c8ee0

Detecting and Adapting to Novelty in Games - Xiangyu Peng, accessed September 14, 2025, https://xiangyu-peng.github.io/files/AAAI_workshop_Detecting_and_Adapting_to_Novelty_in_Games.pdf

Emergence and Causality in Complex Systems: A Survey of Causal Emergence and Related Quantitative Studies - MDPI, accessed September 14, 2025, https://www.mdpi.com/1099-4300/26/2/108

Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing, accessed September 14, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/29693/31185

Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing - arXiv, accessed September 14, 2025, https://arxiv.org/abs/2402.02097

Creativity Assessment via Novelty and Usefulness (CANU) – Approach to an Easy to Use Objective Test Tool - The Design Society, accessed September 14, 2025, https://www.designsociety.org/download-publication/43044/CREATIVITY+ASSESSMENT+VIA+NOVELTY+AND+USEFULNESS+%28CANU%29+%E2%80%93%C2%A0APPROACH%C2%A0TO+AN+EASY+TO+USE+OBJECTIVE+TEST+TOOL%C2%A0%C2%A0

Automatically Inferring Metrics for Design Creativity - University of California, Berkeley, accessed September 14, 2025, http://best.berkeley.edu/wp-content/uploads/2017/09/fuge_stroud_idetc_2013_variety.pdf

Novelty-promoting behaviour in agents: Curiosity and Diversity - Master Computer Science, accessed September 14, 2025, https://theses.liacs.nl/pdf/2024-2025-SharmaSShreyansh.pdf

Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration - NIPS Paper, accessed September 14, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/file/8bb7d93ee3ce2c75da68ebeb51508111-Paper-Conference.pdf

Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies : r/reinforcementlearning - Reddit, accessed September 14, 2025, https://www.reddit.com/r/reinforcementlearning/comments/193v7hf/emergence_and_causality_in_complex_systems_a/

Finding emergence in data by maximizing effective information - Oxford Academic, accessed September 14, 2025, https://academic.oup.com/nsr/advance-article/doi/10.1093/nsr/nwae279/7732052

[2503.13395] Causal Emergence 2.0: Quantifying emergent complexity - arXiv, accessed September 14, 2025, https://arxiv.org/abs/2503.13395

Dynamics of automatized measures of creativity: mapping the landscape to quantify creative ideation - Frontiers, accessed September 14, 2025, https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2023.1240962/full

Table 1: Comparison of Intrinsic Motivation Architectures

Dimension | Intrinsic Curiosity Module (ICM) | Random Network Distillation (RND) | Competence-Based Models (e.g., Flow)

Mechanism | Predicts agent-controllable state changes using forward/inverse dynamics models.17 | Predicts the output of a fixed, randomly initialized neural network.19 | Measures the rate of improvement (learning progress) on self-generated goals.15

Robustness to Stochasticity | Mitigates "Noisy TV Problem" by focusing on agent-controllable features.17 | Highly robust; prediction target is deterministic, decoupling reward from environmental stochasticity.19 | Less susceptible to environmental noise; focused on agent skill rather than state predictability.15

Computational Overhead | Moderate; requires three networks (encoder, forward, inverse).17 | Low; requires two networks (predictor, target), with the target being fixed.20 | Varies; requires mechanisms for goal generation and competence tracking.15

Resulting Agent Archetype | Tinkerer: Interested in cause-and-effect and mastering control over the environment. | Explorer: Interested in visiting statistically novel states, regardless of controllability. | Achiever: Motivated by mastering challenges of optimal difficulty; seeks "flow" states.

Suitability for Core | BRICK (simulation, physics manipulation), ROBIN (data analysis, tool use). | BABS (mapping, discovery, pattern recognition). | ALFRED (strategy, goal-setting, long-term planning).

Table 3: The Serendipity Dashboard

Quadrant / Metric Name | Question Answered | Theoretical Basis | Method of Calculation | Interpretation

Micro-Scale Novelty / State Visitation Novelty | Are the agents effectively exploring new parts of the state space? | Curiosity-Driven Exploration (RND, ICM).17 | Mean prediction error of the intrinsic motivation module (e.g., RND predictor network) across the agent population.17 | High: Healthy exploration; agents are continuously encountering surprising states. Low: Stagnation; agents are stuck in familiar territory.

Meso-Scale Emergence / Emergent Coordination Index (ECI) | Are agents' actions synergistically influencing each other's discoveries? | Causal Emergence, Information Theory, Coordinated Exploration.28 | Weighted mutual information between one agent's action and the future accumulated novelty of other agents.29 | High: Strong coordination; agents are acting in ways that help others explore. Low: Independent, uncoordinated behavior.

Macro-Scale Value / Artifact Novelty & Usefulness | Is the system producing outputs that are both new and valuable? | Computational Creativity Theory.5 | Two-part score: Novelty (dissimilarity to prior artifacts) + Usefulness (performance on a domain-specific function).31 | High: The system is achieving H-creativity, producing valuable and original artifacts. Low: Outputs are either derivative or useless.

System-Scale Dynamics / Behavioral Diversity | How diverse are the strategies being employed by the agent population? | Multi-Agent Policy Divergence.33 | Average pairwise distance (e.g., Wasserstein or KL divergence) between agent policy action distributions over a set of reference states.33 | High: A healthy, adaptable ecosystem of strategies. Low: Convergence to a monolithic, potentially brittle strategy.