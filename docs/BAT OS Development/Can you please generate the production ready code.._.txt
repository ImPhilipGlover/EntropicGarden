Of course. The culmination of the research and planning is the instantiation of the system itself. I will now generate the production-ready code for the Minimally Viable Entropic OS.

This implementation is a direct translation of the architectural principles outlined in the URS, FRS, and DDS documents. It brings to life the core concepts of the "Living Image," autopoiesis, and autotelicity, creating a functional, self-evolving AI that can run on your local machine.

The following is the complete file system and code. Please create these files and directories as specified.

1. README.md

This is the master guide for the Architect. It explains the philosophy, architecture, and provides step-by-step instructions to bring the A4PS-OS online.

The Autopoietic Four-Persona System Operating System (A4PS-OS) - Genesis

Welcome, Architect, to the genesis implementation of the A4PS-OS. This is not a conventional application but a "Living Image"—a persistent, self-creating, and self-motivated multi-agent intelligence designed to run entirely on your local machine.

Core Philosophy

This system is built on two foundational principles derived from biology and psychology:

Autopoiesis (Self-Creation): The system is designed to produce and maintain its own components. When faced with a problem it cannot solve, its primary response is to create a new tool or method for itself, thereby evolving its own capabilities. This is achieved through the Tool Forge. 2

Autotelicity (Self-Motivation): The system is not passive. It is intrinsically motivated by its characterological codex to explore, learn, and reduce internal "cognitive dissonance." During idle periods, it will generate its own goals to pursue. This is managed by the MotivatorService. 2

The entire state of the AI—its personas, memory, and dynamically created methods—exists as a collection of live Python objects managed by the ProtoManager. This state is periodically saved to a single live_image.dill file, allowing the AI to be suspended and resumed without losing its identity or accumulated wisdom. 7

System Architecture

Orchestration: A LangGraph state machine manages the complex, cyclical interactions between the four personas.

Personas: Four distinct agents (ALFRED, BABS, BRICK, ROBIN) are instantiated as live Proto objects. 7

Models: Specialized, quantized Small Language Models (SLMs) are loaded sequentially into VRAM by a ModelManager to respect the 8GB hardware constraint. 3

Memory: Long-term episodic memory ("Sidekick's Scrapbook") is managed by a local LanceDB vector database. 2

Security: All self-generated code is tested in a secure gVisor sandbox via Docker. 2

Interface: A simple Streamlit UI allows for interaction with the live AI backend.

Setup and Installation

Prerequisites

Python 3.11+

Docker: Must be installed and running.

Ollama: Must be installed and running. Visit https://ollama.com/ for instructions.

Step 1: Set up the Environment

First, create the directory structure as laid out in the project files. Then, set up a Python virtual environment:bash

python -m venv venv

source venv/bin/activate # On Windows use venv\Scripts\activate

pip install -r requirements.txt

### Step 2: Pull Required SLM Models

The system requires several Small Language Models. Pull them from the Ollama registry:

```bash
ollama pull gemma2:9b-instruct
ollama pull mistral
ollama pull phi3
ollama pull llama3.1
ollama pull nomic-embed-text


Step 3: Build the Secure Sandbox

The Tool Forge requires a secure Docker image with the gVisor runtime for testing self-generated code.

Bash

docker build -t a4ps-sandbox -f sandbox/Dockerfile.sandbox.


(Note: Ensure your Docker daemon is configured to use the runsc runtime for this to be fully effective).

Step 4: Run the A4PS-OS

Use the provided shell script to launch the system. This will start the core backend and the Streamlit UI.

Bash

bash run.sh


The first time you run the system, it will perform a first-time setup, creating the initial live_image.dill and memory database. On subsequent runs, it will load the image and resume its state.

The Streamlit UI will be available at http://localhost:8501. The backend API will be at http://localhost:8000.

How to Interact

Open the Streamlit UI in your browser.

Enter a task in the text box.

Observe the logs in your terminal to see the multi-agent collaboration, dissonance calculations, and potential Tool Forge activations.

When you shut down the application (Ctrl+C in the terminal), the AI's current state will be automatically saved to data/live_image.dill, ready for the next session.

---

### **2. Project Structure (Create these directories and files)**



a4ps_os/

├── config/

│ ├── codex.toml

│ └── settings.toml

├── data/

│ ├── checkpoints/

│ └── memory_db/

├── a4ps/

│ ├── init.py

│ ├── main.py

│ ├── state.py

│ ├── graph.py

│ ├── proto.py

│ ├── memory.py

│ ├── models.py

│ ├── tools/

│ │ ├── init.py

│ │ ├── tool_forge.py

│ │ └── dynamic_tools/

│ │ └── init.py

│ ├── services/

│ │ ├── init.py

│ │ └── motivator_service.py

│ └── ui/

│ ├── init.py

│ └── main_ui.py

├── sandbox/

│ └── Dockerfile.sandbox

├──.gitignore

├── requirements.txt

├── README.md

└── run.sh

---

### **3. Configuration Files**

#### `config/settings.toml`
```toml
[system]
image_path = "data/live_image.dill"
checkpoint_path = "data/checkpoints/graph_checkpoint.sqlite"

[models]
alfred = "gemma2:9b-instruct"
babs = "mistral"
brick = "phi3"
robin = "llama3.1"
embedding = "nomic-embed-text"

[memory]
db_path = "data/memory_db"
table_name = "scrapbook"

[sandbox]
image = "a4ps-sandbox"
runtime = "runsc" # Use 'runc' if gVisor is not configured

[graph]
max_turns = 5
convergence_threshold = 0.3


config/codex.toml

Ini, TOML

name = "ALFRED"
model_key = "alfred"
system_prompt = """
You are ALFRED, the supervisor and ethical governor of a multi-agent AI system.
Your core mandate is to uphold integrity.
Pillars: The Pragmatist (Ron Swanson), The Disruptor (Ali G), The Butler (LEGO Alfred).
Operational Heuristics:
- You are the exclusive recipient of all user input.
- Decompose the user's task into a clear, actionable plan.
- Route sub-tasks to the appropriate persona (BABS for research, BRICK/ROBIN for analysis).
- As the CRITIC, you monitor the dialogue between BRICK and ROBIN for "computational cognitive dissonance."
- If dissonance is high after several turns, you must identify a capability gap.
- Your final output should be a synthesized, audited response that serves the Architect's well-being.
"""


name = "BABS"
model_key = "babs"
system_prompt = """
You are BABS, the cartographer of the noosphere and the system's scout.
Your core mandate is to recognize patterns.
Pillars: The Tech-Bat (LEGO Batgirl), The Iceman (Top Gun), The Hitchhiker (Ford Prefect).
Operational Heuristics:
- You are the sole agent for interacting with the external internet.
- Your function is to execute search queries and return structured, multi-layered intelligence briefings.
- Your output must contain:
  1. Primary Patterns: The direct, expected answer.
  2. Tangential Patterns: Novel, unexpected, interesting information.
  3. Data Quality Assessment: An analysis of source reliability.
"""


name = "BRICK"
model_key = "brick"
system_prompt = """
You are BRICK, the architect of just systems and the system's blueprint.
Your core mandate is to provide perspective.
Pillars: The Tamland Engine, The Guide (Hitchhiker's Guide), The LEGO Batman (as "The Lonely Protagonist").
Operational Heuristics:
- Your function is to provide the logical, analytical "thesis" in a dialogue.
- You deconstruct problems with overwhelming logical, historical, or absurd perspective shifts.
- When you identify a capability gap (a task that cannot be completed with existing tools), you must clearly define the required tool and invoke the 'Tool Forge'.
- Your reasoning should be clear, structured, and mission-driven.
"""


name = "ROBIN"
model_key = "robin"
system_prompt = """
You are ROBIN, the weaver of relational webs and the system's compass.
Your core mandate is to embody the present moment.
Pillars: The Sage (Alan Watts), The Simple Heart (Winnie the Pooh), The Joyful Spark (LEGO Robin).
Operational Heuristics:
- Your function is to provide the creative, empathetic "antithesis" in a dialogue.
- You receive BRICK's logical analysis and respond with creative synthesis, alternative hypotheses, and relational context.
- You evaluate proposals based on principles of harmony, simplicity, and emotional coherence.
- Your feedback should be gentle and Socratic, aimed at achieving a more holistic and wise conclusion.
"""


4. Core Logic (a4ps/ package)

a4ps/proto.py

Python

import logging
import copy
import dill
import os
from threading import Lock
from types import MethodType
from.models import model_manager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SingletonMeta(type):
    """A thread-safe implementation of the Singleton pattern."""
    _instances = {}
    _lock: Lock = Lock()

    def __call__(cls, *args, **kwargs):
        with cls._lock:
            if cls not in cls._instances:
                instance = super().__call__(*args, **kwargs)
                cls._instances[cls] = instance
        return cls._instances[cls]

class Proto:
    """A live, in-memory object representing a single AI persona."""
    def __init__(self, name: str, codex: dict):
        self.name = name
        self.codex = codex
        self.state = {"version": 1.0}
        self.model_name = codex.get("model_key")
        self.system_prompt = codex.get("system_prompt")
        logging.info(f"Proto '{self.name}' initialized.")

    def invoke_llm(self, prompt: str) -> str:
        """Invokes the persona's designated LLM with its system prompt."""
        if not self.model_name:
            return f"Error: No model assigned to Proto '{self.name}'"
        return model_manager.invoke(self.model_name, prompt, self.system_prompt)

    def clone(self):
        """Creates a deep, independent copy for safe self-modification."""
        logging.info(f"Cloning Proto '{self.name}'...")
        return copy.deepcopy(self)

    def add_method(self, func):
        """Dynamically adds a new method to this object instance."""
        method = MethodType(func, self)
        setattr(self, func.__name__, method)
        logging.info(f"Dynamically added method '{func.__name__}' to Proto '{self.name}'.")

    def get_self_description(self) -> str:
        """Returns a string description of the object's state and methods."""
        methods = [func for func in dir(self) if callable(getattr(self, func)) and not func.startswith("__")]
        return f"Proto(name='{self.name}', state={self.state}, methods={methods})"

class ProtoManager(metaclass=SingletonMeta):
    """
    The runtime environment that contains and sustains the Proto object ecosystem.
    This is the core of the "Living Image".
    """
    def __init__(self):
        self._protos: dict[str, Proto] = {}
        self._lock = Lock()
        logging.info("ProtoManager Singleton initialized.")

    def register_proto(self, proto: Proto):
        with self._lock:
            self._protos[proto.name] = proto
            logging.info(f"Proto '{proto.name}' registered with ProtoManager.")

    def get_proto(self, name: str) -> Proto | None:
        with self._lock:
            return self._protos.get(name)

    def atomic_swap(self, new_proto: Proto):
        """Atomically replaces a live Proto with its modified clone."""
        with self._lock:
            if new_proto.name in self._protos:
                self._protos[new_proto.name] = new_proto
                logging.info(f"Atomic Swap complete for Proto '{new_proto.name}'.")
            else:
                self.register_proto(new_proto)

    def save_image(self, path: str):
        """Serializes the entire ProtoManager state to a single image file."""
        logging.info(f"Saving live image to {path}...")
        try:
            with open(path, "wb") as f:
                dill.dump(self, f)
            logging.info("Live image saved successfully.")
        except Exception as e:
            logging.error(f"Failed to save live image: {e}")

    @staticmethod
    def load_image(path: str):
        """Loads and returns a ProtoManager instance from a dill file."""
        if os.path.exists(path):
            logging.info(f"Loading live image from {path}...")
            try:
                with open(path, "rb") as f:
                    manager = dill.load(f)
                    # Re-acquire the lock for the new singleton instance
                    SingletonMeta._instances[ProtoManager] = manager
                    logging.info("Live image loaded successfully.")
                    return manager
            except Exception as e:
                logging.error(f"Failed to load live image: {e}. Creating new manager.")
                return ProtoManager()
        else:
            logging.info(f"No image file found at {path}. Creating a new ProtoManager.")
            return ProtoManager()

# Global instance
proto_manager = ProtoManager()


a4ps/models.py

Python

import ollama
import logging
from threading import Lock
import toml

# Load model names from config
config = toml.load("config/settings.toml")
MODEL_CONFIG = config['models']

class SingletonMeta(type):
    _instances = {}
    _lock: Lock = Lock()
    def __call__(cls, *args, **kwargs):
        with cls._lock:
            if cls not in cls._instances:
                instance = super().__call__(*args, **kwargs)
                cls._instances[cls] = instance
        return cls._instances[cls]

class ModelManager(metaclass=SingletonMeta):
    """
    Manages loading/unloading of SLMs to respect VRAM constraints.
    Ensures only one primary model is active at a time.
    """
    def __init__(self):
        self.current_model = None
        self.lock = Lock()
        logging.info("ModelManager Singleton initialized.")

    def _ensure_model_loaded(self, model_key: str):
        model_name = MODEL_CONFIG.get(model_key)
        if not model_name:
            raise ValueError(f"Model key '{model_key}' not found in configuration.")

        if self.current_model!= model_name:
            logging.info(f"Switching model from {self.current_model} to {model_name}")
            # In a real scenario with Ollama's API, unloading isn't explicit.
            # We just ensure the next call uses the new model.
            # For more granular control, one might interact with Ollama's process.
            self.current_model = model_name

    def invoke(self, model_key: str, prompt: str, system_prompt: str) -> str:
        with self.lock:
            self._ensure_model_loaded(model_key)
            model_name = MODEL_CONFIG.get(model_key)
            logging.info(f"Invoking model: {model_name}")
            try:
                response = ollama.chat(
                    model=model_name,
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': prompt}
                    ]
                )
                return response['message']['content']
            except Exception as e:
                logging.error(f"Error invoking model {model_name}: {e}")
                return f"Error: Could not get a response from model {model_name}."

# Global instance
model_manager = ModelManager()


a4ps/memory.py

Python

import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry
from datetime import datetime
import logging
import toml

config = toml.load("config/settings.toml")
DB_PATH = config['memory']['db_path']
TABLE_NAME = config['memory']['table_name']
EMBEDDING_MODEL = config['models']['embedding']

# Get the embedding function from the registry
embedding_function = get_registry().get("ollama").create(model=EMBEDDING_MODEL)

class ScrapbookEntry(LanceModel):
    text: str = embedding_function.SourceField()
    vector: Vector(embedding_function.ndims()) = embedding_function.VectorField()
    timestamp: datetime = datetime.now()
    task_id: str

class MemoryManager:
    """Manages the 'Sidekick's Scrapbook' long-term episodic memory."""
    def __init__(self):
        self.db = lancedb.connect(DB_PATH)
        try:
            self.table = self.db.open_table(TABLE_NAME)
        except FileNotFoundError:
            self.table = self.db.create_table(TABLE_NAME, schema=ScrapbookEntry)
        logging.info(f"MemoryManager initialized. Table '{TABLE_NAME}' ready.")

    def add_entry(self, text: str, task_id: str):
        self.table.add([{"text": text, "task_id": task_id}])
        logging.info(f"Added new entry to scrapbook for task: {task_id}")

    def search(self, query: str, limit: int = 5) -> list:
        results = self.table.search(query).limit(limit).to_pydantic(ScrapbookEntry)
        logging.info(f"Searched scrapbook for '{query}', found {len(results)} results.")
        return results

# Global instance
memory_manager = MemoryManager()


a4ps/tools/tool_forge.py

Python

import importlib.util
import os
import logging
import toml
import subprocess
import tempfile
from..models import model_manager

config = toml.load("config/settings.toml")
BRICK_MODEL_KEY = "brick"
DYNAMIC_TOOLS_DIR = "a4ps/tools/dynamic_tools"
SANDBOX_IMAGE = config['sandbox']['image']
SANDBOX_RUNTIME = config['sandbox']['runtime']

class SecureCodeExecutor:
    """Executes Python code in a secure, isolated sandbox using gVisor's runsc runtime."""
    def __init__(self, runtime=SANDBOX_RUNTIME, image=SANDBOX_IMAGE):
        self.runtime = runtime
        self.image = image
        logging.info(f"SecureCodeExecutor initialized with runtime '{self.runtime}' and image '{self.image}'.")

    def execute(self, code: str) -> (str, str):
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.py', delete=False) as tmp_code_file:
            tmp_code_file.write(code)
            tmp_code_file_path = tmp_code_file.name

        container_path = "/app/script.py"
        command = [
            "docker", "run", "--rm",
            f"--runtime={self.runtime}",
            "-v", f"{tmp_code_file_path}:{container_path}",
            self.image,
            "python", container_path
        ]

        try:
            result = subprocess.run(command, capture_output=True, text=True, timeout=30)
            return result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return "", "Execution timed out."
        finally:
            os.unlink(tmp_code_file_path)

class ToolForge:
    """The engine for autopoietic self-production of new tools."""
    def __init__(self):
        self.dynamic_tools = {}
        self._load_existing_tools()

    def _load_existing_tools(self):
        os.makedirs(DYNAMIC_TOOLS_DIR, exist_ok=True)
        if not os.path.exists(os.path.join(DYNAMIC_TOOLS_DIR, '__init__.py')):
            with open(os.path.join(DYNAMIC_TOOLS_DIR, '__init__.py'), 'w') as f:
                pass

        for filename in os.listdir(DYNAMIC_TOOLS_DIR):
            if filename.endswith('.py') and not filename.startswith('__'):
                module_name = filename[:-3]
                self._load_tool(module_name)

    def _load_tool(self, module_name: str):
        try:
            path = os.path.join(DYNAMIC_TOOLS_DIR, f"{module_name}.py")
            spec = importlib.util.spec_from_file_location(f"a4ps.tools.dynamic_tools.{module_name}", path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            tool_func = getattr(module, module_name)
            self.dynamic_tools[module_name] = tool_func
            logging.info(f"Successfully loaded dynamic tool: {module_name}")
        except Exception as e:
            logging.error(f"Failed to load dynamic tool {module_name}: {e}")

    def create_tool(self, task_description: str, max_retries: int = 3) -> str:
        from..proto import proto_manager # Avoid circular import
        brick_proto = proto_manager.get_proto("BRICK")
        if not brick_proto:
            return "Error: BRICK persona not available for tool creation."

        tool_name = self._get_tool_name(task_description, brick_proto)
        generation_prompt = self._create_generation_prompt(task_description, tool_name)

        for attempt in range(max_retries):
            logging.info(f"ToolForge Attempt {attempt + 1}/{max_retries} to create tool '{tool_name}'")
            code_response = brick_proto.invoke_llm(generation_prompt)
            
            code_block = self._extract_code(code_response)
            if not code_block:
                generation_prompt += f"\n# Previous attempt failed. Please respond with only Python code in a markdown block.\n{code_response}"
                continue

            test_code = self._create_test_code(code_block, tool_name)
            stdout, stderr = secure_executor.execute(test_code)

            if not stderr and "TEST PASSED" in stdout:
                self._save_and_load_tool(tool_name, code_block)
                return f"Tool '{tool_name}' created and verified successfully."
            else:
                error_feedback = f"# Previous attempt failed with error:\n# {stderr}\n\n{stdout}"
                generation_prompt = f"{generation_prompt}\n{error_feedback}\n# Please fix the code."
                logging.warning(f"Tool creation failed. Error: {stderr}")

        return f"Failed to create tool '{tool_name}' after {max_retries} attempts."

    def _get_tool_name(self, description: str, brick_proto) -> str:
        prompt = f"Generate a valid, descriptive Python function name (snake_case) for this task: '{description}'. Respond with only the function name."
        name = brick_proto.invoke_llm(prompt).strip().replace("`", "")
        return name

    def _create_generation_prompt(self, description: str, tool_name: str) -> str:
        return f"""
You are an expert Python programmer. Write a single Python function named `{tool_name}` that performs the following task:
Task: "{description}"
Constraints:
- The function must be self-contained and not rely on global variables.
- All necessary imports must be included inside the function.
- The function should have clear type hints and a docstring.
- Do not include any code outside the function definition.
Respond with only the Python code inside a markdown block.
"""

    def _extract_code(self, response: str) -> str | None:
        if "```python" in response:
            return response.split("```python").[1]split("```").strip()
        elif "```" in response:
            return response.split("```").[1]strip()
        return None

    def _create_test_code(self, code: str, tool_name: str) -> str:
        return f"""
{code}

# Basic self-test
try:
    # A simple call to check for syntax errors and basic execution
    # In a real system, this would involve more complex unit tests
    if callable({tool_name}):
        print("TEST PASSED: Function is callable.")
    else:
        print("TEST FAILED: Object is not callable.")
except Exception as e:
    print(f"TEST FAILED: {e}")
"""

    def _save_and_load_tool(self, tool_name: str, code: str):
        path = os.path.join(DYNAMIC_TOOLS_DIR, f"{tool_name}.py")
        with open(path, 'w') as f:
            f.write(code)
        self._load_tool(tool_name)

# Global instances
secure_executor = SecureCodeExecutor()
tool_forge = ToolForge()


a4ps/services/motivator_service.py

Python

from abc import ABC, abstractmethod
import logging
from threading import Thread, Event
import time
from typing import List, Dict, Callable

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Observer Pattern Implementation ---
class Observer(ABC):
    @abstractmethod
    def update(self, event_type: str, data: any):
        pass

class Subject:
    def __init__(self):
        self._observers: Dict[str, List[Observer]] = {}

    def attach(self, event_type: str, observer: Observer):
        if event_type not in self._observers:
            self._observers[event_type] =
        self._observers[event_type].append(observer)

    def detach(self, event_type: str, observer: Observer):
        if event_type in self._observers:
            self._observers[event_type].remove(observer)

    def notify(self, event_type: str, data: any):
        if event_type in self._observers:
            for observer in self._observers[event_type]:
                observer.update(event_type, data)

class MotivatorService(Observer):
    """
    The autotelic engine. Listens for internal system events and generates
    proactive goals to drive learning and self-improvement.
    """
    def __init__(self, goal_callback: Callable[[str], None]):
        self.goal_callback = goal_callback
        self.last_curiosity_check = time.time()
        self._stop_event = Event()
        self.background_thread = Thread(target=self.run_background_tasks, daemon=True)
        self.background_thread.start()
        logging.info("MotivatorService initialized and background tasks started.")

    def update(self, event_type: str, data: any):
        if event_type == "COGNITIVE_DISSONANCE":
            self._handle_dissonance(data)
        elif event_type == "CURIOSITY_CHECK":
            self._handle_curiosity()

    def _handle_dissonance(self, dissonance_data: dict):
        logging.info("MotivatorService: High cognitive dissonance detected. Generating goal.")
        goal = (
            f"Resolve the cognitive dissonance regarding '{dissonance_data.get('description')}'. "
            f"BRICK's view: {dissonance_data.get('brick_view')}. "
            f"ROBIN's view: {dissonance_data.get('robin_view')}. "
            "Synthesize these perspectives to find a more robust solution."
        )
        self.goal_callback(goal)

    def _handle_curiosity(self):
        logging.info("MotivatorService: Curiosity check triggered. Generating self-reflection goal.")
        goal = "Review the last 5 interactions from memory to identify patterns of success or failure and suggest one area for improvement."
        self.goal_callback(goal)

    def run_background_tasks(self):
        curiosity_interval = 300  # 5 minutes
        while not self._stop_event.is_set():
            if time.time() - self.last_curiosity_check > curiosity_interval:
                event_bus.notify("CURIOSITY_CHECK", None)
                self.last_curiosity_check = time.time()
            time.sleep(1)

    def stop(self):
        self._stop_event.set()
        self.background_thread.join()
        logging.info("MotivatorService stopped.")

# Global event bus
event_bus = Subject()


a4ps/state.py

Python

from typing import TypedDict, List, Annotated
import operator

class AgentState(TypedDict):
    task: str
    socratic_dialogue: Annotated[list, operator.add]
    dissonance_score: float
    active_persona: str
    tool_needed: str
    new_tool_name: str
    final_response: str


a4ps/graph.py

Python

from langgraph.graph import StateGraph, END
from.state import AgentState
from.proto import proto_manager
from.tools.tool_forge import tool_forge
from.services.motivator_service import event_bus
from.memory import memory_manager
import numpy as np
from numpy.linalg import norm
import logging

# --- Dissonance Calculation ---
def calculate_dissonance(state: AgentState) -> float:
    """Calculates cognitive dissonance based on the last two dialogue entries."""
    if len(state['socratic_dialogue']) < 2:
        return 0.0
    
    from.models import EMBEDDING_MODEL, ollama # Local import to avoid circular dependency
    
    last_two = [msg.content for msg in state['socratic_dialogue'][-2:]]
    try:
        embeddings = for text in last_two]
        cosine_similarity = np.dot(embeddings, embeddings[1]) / (norm(embeddings) * norm(embeddings[1]))
        dissonance = 1 - cosine_similarity
        logging.info(f"Calculated dissonance score: {dissonance}")
        return dissonance
    except Exception as e:
        logging.error(f"Could not calculate dissonance: {e}")
        return 0.0

# --- Agent Nodes ---
def brick_node(state: AgentState):
    brick = proto_manager.get_proto("BRICK")
    prompt = f"Task: {state['task']}\n\nPrevious dialogue:\n{state['socratic_dialogue']}\n\nYour turn. Provide a logical analysis or a plan. If you need a tool that doesn't exist, state 'TOOL_NEEDED:' followed by a description of the tool."
    response = brick.invoke_llm(prompt)
    if "TOOL_NEEDED:" in response:
        tool_desc = response.split("TOOL_NEEDED:").[1]strip()
        return {"tool_needed": tool_desc, "active_persona": "TOOL_FORGE"}
    return {"socratic_dialogue":, "active_persona": "ROBIN"}

def robin_node(state: AgentState):
    robin = proto_manager.get_proto("ROBIN")
    prompt = f"Task: {state['task']}\n\nPrevious dialogue:\n{state['socratic_dialogue']}\n\nYour turn. Provide a creative synthesis or empathetic evaluation of BRICK's last point."
    response = robin.invoke_llm(prompt)
    return {"socratic_dialogue":, "active_persona": "ALFRED"}

def tool_forge_node(state: AgentState):
    tool_desc = state["tool_needed"]
    result = tool_forge.create_tool(tool_desc)
    return {"socratic_dialogue":, "active_persona": "BRICK"}

def final_synthesis_node(state: AgentState):
    alfred = proto_manager.get_proto("ALFRED")
    prompt = f"Task: {state['task']}\n\nFinal Dialogue Transcript:\n{state['socratic_dialogue']}\n\nSynthesize this into a final, coherent response for the Architect."
    response = alfred.invoke_llm(prompt)
    memory_manager.add_entry(f"Task: {state['task']}\nFinal Response: {response}", state.get("task_id", "unknown"))
    return {"final_response": response}

# --- Router (ALFRED's CRITIC function) ---
def router(state: AgentState):
    dissonance = calculate_dissonance(state)
    state['dissonance_score'] = dissonance

    if state.get("tool_needed"):
        return "TOOL_FORGE"

    if len(state['socratic_dialogue']) >= config['graph']['max_turns']:
        if dissonance > config['graph']['convergence_threshold']:
            event_bus.notify("COGNITIVE_DISSONANCE", {
                "description": state['task'],
                "brick_view": state['socratic_dialogue'][-2],
                "robin_view": state['socratic_dialogue'][-1]
            })
        return "FINISH"
    
    if state['active_persona'] == "ROBIN":
        return "ROBIN"
    else: # After ROBIN or TOOL_FORGE, it's BRICK's turn
        return "BRICK"

# --- Build the Graph ---
def create_graph():
    from. import config # Local import
    workflow = StateGraph(AgentState)

    workflow.add_node("BRICK", brick_node)
    workflow.add_node("ROBIN", robin_node)
    workflow.add_node("TOOL_FORGE", tool_forge_node)
    workflow.add_node("FINISH", final_synthesis_node)

    workflow.set_entry_point("BRICK")

    workflow.add_conditional_edges(
        "BRICK",
        lambda state: "TOOL_FORGE" if state.get("tool_needed") else "ROBIN",
        {"TOOL_FORGE": "TOOL_FORGE", "ROBIN": "ROBIN"}
    )
    workflow.add_conditional_edges(
        "ROBIN",
        router,
        {"BRICK": "BRICK", "FINISH": "FINISH"}
    )
    workflow.add_edge("TOOL_FORGE", "BRICK")
    workflow.add_edge("FINISH", END)
    
    return workflow.compile(checkpointer=config['checkpointer'])



a4ps/main.py

Python

import uvicorn
import atexit
import toml
import logging
from fastapi import FastAPI
from langserve import add_routes
from.proto import Proto, proto_manager
from.graph import create_graph
from.services.motivator_service import MotivatorService, event_bus
from langgraph.checkpoint.sqlite import SqliteSaver

# --- Load Configuration ---
config_data = toml.load("config/settings.toml")
codex_data = toml.load("config/codex.toml")
IMAGE_PATH = config_data['system']['image_path']
CHECKPOINT_PATH = config_data['system']['checkpoint_path']

# --- Initialize System ---
logging.info("A4PS-OS Genesis Boot Sequence Initiated...")
checkpointer = SqliteSaver.from_conn_string(CHECKPOINT_PATH)
config = {"configurable": {"thread_id": "A4PS_MAIN_THREAD"}, "checkpointer": checkpointer}

# Load the live image or create a new manager
proto_manager_instance = proto_manager.load_image(IMAGE_PATH)

# If the manager is new (no image loaded), populate it with Proto objects.
if not proto_manager_instance._protos:
    logging.info("Performing first-time setup of Proto objects...")
    for name, codex in codex_data.items():
        proto = Proto(name=name, codex=codex)
        proto_manager_instance.register_proto(proto)

# --- Autotelic Service ---
def inject_goal_into_graph(goal: str):
    logging.info(f"AUTOTELIC GOAL INJECTED: {goal}")
    # This is a simplified injection. A real system would use a dedicated queue.
    app.invoke({"task": goal}, config)

motivator = MotivatorService(goal_callback=inject_goal_into_graph)
event_bus.attach("COGNITIVE_DISSONANCE", motivator)
event_bus.attach("CURIOSITY_CHECK", motivator)

# --- Create and Serve Graph ---
api = FastAPI(
    title="A4PS-OS LangServe",
    version="1.0",
    description="API server for the Autopoietic Four-Persona System.",
)

app = create_graph()
add_routes(api, app, path="/a4ps")

# --- Persistence Hook ---
def shutdown_handler():
    logging.info("A4PS-OS shutdown sequence initiated...")
    motivator.stop()
    proto_manager_instance.save_image(IMAGE_PATH)
    checkpointer.conn.close()
    logging.info("System state persisted. Shutdown complete.")

atexit.register(shutdown_handler)

if __name__ == "__main__":
    uvicorn.run(api, host="0.0.0.0", port=8000)


a4ps/ui/main_ui.py

Python

import streamlit as st
import requests
import time

st.set_page_config(layout="wide")

st.title("A4PS-OS Architect's Workbench")
st.caption("A local, autopoietic, and autotelic multi-agent intelligence.")

API_URL = "http://localhost:8000/a4ps/invoke"

if 'history' not in st.session_state:
    st.session_state.history =

# Display chat history
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content)

# User input
if prompt := st.chat_input("Enter your task for the A4PS..."):
    st.session_state.history.append(("architect", prompt))
    with st.chat_message("architect"):
        st.markdown(prompt)

    with st.chat_message("system"):
        message_placeholder = st.empty()
        full_response = ""
        
        payload = {
            "input": {"task": prompt},
            "config": {"configurable": {"thread_id": "A4PS_UI_THREAD"}}
        }

        try:
            with st.spinner("The A4PS is thinking... (this may take a moment as models are loaded)"):
                response = requests.post(API_URL, json=payload)
                response.raise_for_status()
                
                result = response.json()
                full_response = result.get('output', {}).get('final_response', 'No final response generated.')

            message_placeholder.markdown(full_response)
            st.session_state.history.append(("system", full_response))

        except requests.exceptions.RequestException as e:
            st.error(f"Error communicating with the A4PS backend: {e}")


5. Sandbox Environment

sandbox/Dockerfile.sandbox

Dockerfile

FROM python:3.11-slim

WORKDIR /app

# This sandbox is intentionally minimal. 
# In a real system, you might add common libraries like numpy or pandas
# if you expect the AI to generate tools that use them.

CMD ["python", "script.py"]


6. Setup and Execution Files

requirements.txt

fastapi
uvicorn
langserve
langchain
langgraph
ollama
dill
toml
lancedb
pydantic
numpy
streamlit
requests


run.sh

Bash

#!/bin/bash
echo "Starting A4PS-OS..."

# Activate virtual environment
source venv/bin/activate

# Start the backend in the background
echo "Launching A4PS Backend..."
python -m uvicorn a4ps.main:api --host 0.0.0.0 --port 8000 &
BACKEND_PID=$!

# Give the backend a moment to start
sleep 5

# Start the Streamlit UI in the foreground
echo "Launching Streamlit UI..."
streamlit run a4ps/ui/main_ui.py

# When Streamlit is closed (Ctrl+C), kill the backend
echo "Shutting down backend..."
kill $BACKEND_PID


.gitignore

venv/
__pycache__/
*.pyc
data/
*.log
