{"cells":[{"cell_type":"code","source":"# orchestrator.py\n# This script is the central nervous system of the Resilient Garden.\n# It runs a FastAPI server to manage all core logic, including:\n# - A state machine for switching between WORKBENCH and AUTONOMOUS modes.\n# - A sophisticated routing layer to conduct the four-persona conversation.\n# - Background threads to monitor for idle time and run the Emergence Engine.\n# - All interactions with the LLM pods and the ChromaDB memory core.\n\nimport os\nimport threading\nimport time\nimport uuid\nfrom contextlib import asynccontextmanager\n\nimport chromadb\nimport ollama\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n# --- Local Module Imports ---\n# The logic for the autonomous learning cycle is cleanly encapsulated here.\nfrom emergence_engine import run_inquiry_cycle\n\n# --- Configuration ---\n# Load persona configurations from environment files\n# This makes it easy to change models or system prompts without editing code.\nPERSONA_CONFIG = {\n    \"BRICK\": {\"model\": os.getenv(\"BRICK_MODEL\", \"codellama:7b-instruct-q5_K_M\"), \"system_prompt_file\": \"personas/brick.txt\"},\n    \"ROBIN\": {\"model\": os.getenv(\"ROBIN_MODEL\", \"llama3.1:8b-instruct-q4_K_M\"), \"system_prompt_file\": \"personas/robin.txt\"},\n    \"BABS\": {\"model\": os.getenv(\"BABS_MODEL\", \"qwen2:7b-instruct-q5_K_M\"), \"system_prompt_file\": \"personas/babs.txt\"},\n    \"ALFRED\": {\"model\": os.getenv(\"ALFRED_MODEL\", \"phi3:3.8b-mini-instruct-q5_K_M\"), \"system_prompt_file\": \"personas/alfred.txt\"},\n}\n\nIDLE_TIMEOUT = 900  # 15 minutes\nAUTONOMOUS_CYCLE_INTERVAL = 900 # 15 minutes\n\n# --- System State & Memory ---\n# This dictionary manages the global state of the orchestrator.\nSYSTEM_STATE = {'mode': 'AUTONOMOUS', 'last_interaction_time': time.time()}\n# Thread lock to prevent race conditions when changing state.\nstate_lock = threading.Lock()\n\n# Initialize ChromaDB client for permanent memory.\n# This connection is persistent for the life of the server.\nchroma_client = chromadb.PersistentClient(path=\"./db\")\nliving_codex = chroma_client.get_or_create_collection(\"living_codex\")\npending_inquiry_collection = chroma_client.get_or_create_collection(\"pending_inquiry\")\n\n# Session history is held in RAM for the duration of a single conversation.\n# A more robust implementation might use Redis or a file-based session manager.\nsession_history = []\n\n# --- Background Threads for Autonomous Operation ---\n\ndef autonomous_loop():\n    \"\"\"\n    The background thread that runs the Emergence Engine during idle time.\n    \"\"\"\n    while True:\n        with state_lock:\n            is_autonomous = SYSTEM_STATE['mode'] == 'AUTONOMOUS'\n        \n        if is_autonomous:\n            print(\"ORCHESTRATOR: [AUTONOMOUS] Starting new Emergence Engine cycle.\")\n            try:\n                # Run one full, self-contained learning cycle.\n                run_inquiry_cycle(PERSONA_CONFIG, living_codex, pending_inquiry_collection)\n            except Exception as e:\n                print(f\"ORCHESTRATOR: [ERROR] Emergence Engine cycle failed: {e}\")\n        \n        time.sleep(AUTONOMOUS_CYCLE_INTERVAL)\n\ndef monitor_idle_state():\n    \"\"\"\n    Checks for user inactivity to switch the system back to autonomous mode.\n    \"\"\"\n    while True:\n        with state_lock:\n            if SYSTEM_STATE['mode'] == 'WORKBENCH':\n                if time.time() - SYSTEM_STATE['last_interaction_time'] > IDLE_TIMEOUT:\n                    print(\"ORCHESTRATOR: [STATE] Idle timeout reached. Switching to AUTONOMOUS mode.\")\n                    SYSTEM_STATE['mode'] = 'AUTONOMOUS'\n        time.sleep(60)\n\n# --- FastAPI Application Setup ---\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # This context manager runs code on application startup and shutdown.\n    print(\"ORCHESTRATOR: Launching background threads...\")\n    threading.Thread(target=autonomous_loop, daemon=True).start()\n    threading.Thread(target=monitor_idle_state, daemon=True).start()\n    print(\"ORCHESTRATOR: System is fully operational.\")\n    yield\n    print(\"ORCHESTRATOR: Shutting down.\")\n\napp = FastAPI(lifespan=lifespan)\n\nclass Prompt(BaseModel):\n    text: str\n\n# --- Core Helper Functions ---\n\ndef get_system_prompt(persona_name):\n    \"\"\"Loads the persona's system prompt from its dedicated file.\"\"\"\n    filepath = PERSONA_CONFIG[persona_name][\"system_prompt_file\"]\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            return f.read()\n    except FileNotFoundError:\n        print(f\"WARNING: System prompt file not found for {persona_name} at {filepath}. Using default.\")\n        return \"You are a helpful AI assistant.\"\n\ndef call_ollama_pod(persona_name: str, history: list):\n    \"\"\"\n    Makes a serial, blocking call to a specific persona's LLM.\n    This is the core function for serial VRAM management.\n    \"\"\"\n    model_name = PERSONA_CONFIG[persona_name][\"model\"]\n    system_prompt = get_system_prompt(persona_name)\n    \n    # Prepend the system prompt to the history for this specific call\n    messages = [{'role': 'system', 'content': system_prompt}] + history\n    \n    print(f\"ORCHESTRATOR: [WORKBENCH] Calling {persona_name} ({model_name})...\")\n    try:\n        response = ollama.chat(model=model_name, messages=messages)\n        content = response['message']['content']\n        print(f\"ORCHESTRATOR: [WORKBENCH] {persona_name} responded.\")\n        return content\n    except Exception as e:\n        print(f\"ORCHESTRATOR: [ERROR] Failed to get response from {persona_name}: {e}\")\n        return f\"({persona_name} is currently unavailable due to a system error.)\"\n\ndef determine_conversational_flow(prompt_text: str):\n    \"\"\"\n    Implements the \"Roundtable\" routing logic based on prompt content.\n    \"\"\"\n    prompt_lower = prompt_text.lower()\n    \n    # BABS Triggers (External Data Needs)\n    babs_keywords = [\"find\", \"look up\", \"what is the latest\", \"how much does\", \"who was\", \"research\"]\n    if any(keyword in prompt_lower for keyword in babs_keywords):\n        return [\"BABS\", \"BRICK\", \"ROBIN\"]\n\n    # ALFRED Triggers (Meta-Analysis & System State)\n    alfred_keywords = [\"this is confusing\", \"let's recap\", \"our goal\", \"feeling overwhelmed\", \"this isn't working\"]\n    if any(keyword in prompt_lower for keyword in alfred_keywords):\n        return [\"ALFRED\", \"BRICK\", \"ROBIN\"]\n\n    # Default Flow (BRICK & ROBIN)\n    return [\"BRICK\", \"ROBIN\"]\n\n# --- API Endpoints ---\n\n@app.post(\"/chat\")\ndef handle_chat(prompt: Prompt):\n    \"\"\"\n    The main endpoint for handling user interaction in Workbench Mode.\n    \"\"\"\n    global session_history\n    with state_lock:\n        if SYSTEM_STATE['mode'] == 'AUTONOMOUS':\n            print(\"ORCHESTRATOR: [STATE] Architect interaction detected. Switching to WORKBENCH mode.\")\n            session_history = [] # Start a fresh session history\n        SYSTEM_STATE['mode'] = 'WORKBENCH'\n        SYSTEM_STATE['last_interaction_time'] = time.time()\n\n    session_history.append({'role': 'user', 'content': prompt.text})\n    \n    flow = determine_conversational_flow(prompt.text)\n    \n    full_response = []\n    for persona in flow:\n        response_text = call_ollama_pod(persona, session_history)\n        session_history.append({'role': 'assistant', 'name': persona, 'content': response_text})\n        full_response.append(f\"**{persona}:** {response_text}\")\n\n    # Write the complete turn to permanent memory\n    try:\n        doc_id = str(uuid.uuid4())\n        full_turn_text = \"\\n\".join([f\"{msg['role']}: {msg.get('content', '')}\" for msg in session_history[-len(flow)-1:]])\n        living_codex.add(documents=[full_turn_text], metadatas=[{\"timestamp\": time.time()}], ids=[doc_id])\n    except Exception as e:\n        print(f\"ORCHESTRATOR: [ERROR] Failed to write to ChromaDB: {e}\")\n\n    return {\"response\": \"\\n\\n\".join(full_response)}\n\n@app.get(\"/status\")\ndef get_status():\n    \"\"\"A simple endpoint to check the current system mode.\"\"\"\n    with state_lock:\n        return {\"mode\": SYSTEM_STATE['mode']}\n\n@app.get(\"/pending_inquiry\")\ndef get_pending_inquiry():\n    \"\"\"Endpoint for the UI to fetch the question generated by the Emergence Engine.\"\"\"\n    try:\n        inquiries = pending_inquiry_collection.get(limit=1)\n        if inquiries and inquiries['documents']:\n            doc_id = inquiries['ids'][0]\n            question = inquiries['documents'][0]\n            # Clear the inquiry once it's been fetched\n            pending_inquiry_collection.delete(ids=[doc_id])\n            return {\"question\": question}\n    except Exception as e:\n        print(f\"ORCHESTRATOR: [ERROR] Could not fetch pending inquiry: {e}\")\n    return {\"question\": None}","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}