A Principled Geometric-to-Algebraic Mapping for a Unified Memory Framework: Proposing the Laplace Kernel Encoder for Knowledge Discovery




I. The Homomorphic Imperative: Why a Continuous Mathematical Transformation is Essential for Knowledge Discovery


The Unified Memory Framework, as conceptualized, presents a sophisticated cognitive architecture designed to bridge the chasm between sub-symbolic perception and symbolic reasoning.1 Its central innovation is the cyclical interplay between a Geometric Context Engine (GCE), a Hyperdimensional Reasoning Core (HRC), and an Associative Grounding Loop (AGL). While the GCE provides rich, context-aware semantic grounding and the HRC supplies transparent algebraic reasoning, the ultimate cognitive power of the system—its capacity for genuine knowledge discovery—is contingent upon the nature of the interface connecting them. This analysis posits that for the framework to transcend simple data retrieval and achieve generative reasoning, the mapping from the GCE's geometric embeddings to the HRC's hypervectors cannot be a discrete, lookup-based system. It must be a continuous, structure-preserving mathematical function—a homomorphism—that faithfully translates the nuanced topology of the semantic space into the algebraic structure of the hyperspace.
A discrete mapping, such as a dictionary that assigns a pre-generated random hypervector to each known concept embedding, is fundamentally static. Such a system is inherently limited to a pre-defined vocabulary of concepts. It cannot process or reason about novel entities encountered in new data, as there would be no corresponding entry in its lookup table. More critically, a discrete system is incapable of interpreting the novel composite hypervectors that are the primary output of the HRC's algebraic operations. The HRC's power lies in its ability to construct new representations, such as through the bundling (addition) of multiple hypervectors (hresult​=hA​+hB​+hC​) or the binding of roles to fillers (hrecord​=(ROLE⊙FILLER)).1 The resulting hypervector,
hresult​, is a new point in the high-dimensional space, distinct from any of its constituents. A discrete lookup system, when presented with this novel vector in the reverse AGL step, would find no match and would be forced to treat the result as uninterpretable noise. This limitation would fundamentally cripple the AGL, reducing it from a mechanism for "creative inference and hypothesis generation" to a simple error-correction routine for known concepts, thereby nullifying the framework's most innovative aspect.1
In contrast, a continuous mathematical function, formally a mapping ϕ:Rd→{−1,1}D, can, by its very definition, transform any point in the GCE's d-dimensional continuous geometric space into a corresponding hypervector in the HRC's D-dimensional discrete space. This property immediately endows the system with the ability to generalize. When the GCE encounters a new concept and generates a new embedding vector for it, a continuous encoder can instantly produce a corresponding hypervector, allowing the HRC to incorporate this new knowledge into its reasoning processes without requiring any retraining or architectural changes. This capacity for open-vocabulary reasoning is a prerequisite for any system intended to learn and adapt in dynamic environments.
The true transformative potential of a continuous mapping is realized in the AGL, through the inverse mapping or decoder, ϕ−1. The "lossy" but structured output of the HRC, hresult​, is not an error but a synthesized abstraction. A continuous decoder maps this novel hypervector back to a new point, cresult​, in the GCE's geometric space. The location of this point is not arbitrary; it is a direct consequence of the mathematical properties of the transformation function. If the mapping is structure-preserving (homomorphic), the geometric location of cresult​ will be semantically meaningful. For example, an HRC operation corresponding to the analogy "king is to man as queen is to woman" would produce a hypervector that, when decoded, lands in the geometric vicinity of the embedding for "queen".1 The subsequent nearest-neighbor search performed by the GCE using
cresult​ as a query is therefore not merely a "clean-up" of a known concept. Instead, it is a query for the most plausible known concepts corresponding to the abstract result of the HRC's reasoning. The retrieved neighbors represent the system's generated hypothesis about the meaning of the HRC's composition. This process directly enables the "creative inference" envisioned in the framework's design, transforming the AGL from a simple feedback loop into a generative engine for discovery. The choice of the mapping function is therefore not a minor implementation detail; it is the core determinant of the framework's cognitive modality, elevating it from a retrieval-augmented system to a truly generative reasoning architecture.


II. A Formal Proposal: The Laplace-HDC Encoder


To fulfill the homomorphic imperative, a specific mathematical function is required that is not only continuous but is also fundamentally aligned with the algebraic properties of Hyperdimensional Computing. This report formally proposes the Laplace-HDC encoder as the principled mathematical transformation for mapping the GCE's geometric embeddings to the HRC's hypervectors. This method is not an arbitrary choice or a black-box machine learning model; it is a constructive algorithm derived from a deep theoretical result that connects the geometry of the HDC binding operator to the Laplace kernel, demonstrating that this kernel "naturally arises in this setting".3 This inherent isomorphism provides a strong theoretical guarantee that the rich semantic structure of the GCE will be faithfully preserved in the HRC's algebraic domain.
The Laplace-HDC encoding process is an analytical procedure that transforms a local neighborhood of geometric concept vectors into a corresponding set of bipolar hypervectors. The mathematical formulation proceeds through five distinct steps:
1. Input: The process begins with a set of n geometric embedding vectors, {v1​,v2​,...,vn​}, retrieved by the GCE in response to a query. Each vector vi​ is an element of a d-dimensional real-valued space, vi​∈Rd. These vectors represent the local semantic context for the reasoning task.
2. Step 1: Construct the Similarity Matrix (K): The first step is to capture the complete geometric structure of the local context. This is achieved by computing the pairwise cosine similarity between all vectors in the input set. The result is an n×n symmetric matrix K, where each element Kij​ is defined as:

Kij​=∥vi​∥∥vj​∥vi​⋅vj​​

This matrix K serves as a complete representation of the relative semantic distances and angles between all concepts in the current working memory.
3. Step 2: Kernel Transformation (W): This is the most critical step, where the geometric relationships are transformed into a structure that is isomorphic to the similarity patterns produced by HDC's algebraic operations. Research has shown that the similarity structure induced by the HDC binding operator is intrinsically related to the Laplace kernel.3 To leverage this, the similarity matrix
K is transformed element-wise into a new matrix W. A specific formulation for this transformation is given by:

Wij​=sin(2π​Kij​)

This sinusoidal transformation maps the cosine similarities, which represent distances on the surface of a hypersphere, into a space that aligns with the algebraic properties of binary hypervectors, effectively bridging the representational gap between the two domains.3
4. Step 3: Eigendecomposition: To extract the principal components of the transformed semantic structure, an eigendecomposition is performed on the matrix W:

W=USUT

Here, U is an orthogonal matrix whose columns are the eigenvectors of W, and S is a diagonal matrix containing the corresponding eigenvalues, λi​. The eigenvectors represent the principal axes of semantic variation within the local context, and the eigenvalues represent the magnitude of that variation along each axis. This step distills the complex relational structure into its most significant, orthogonal components.
5. Step 4: Stochastic Projection: The low-dimensional semantic structure captured by the eigenvectors and eigenvalues is now projected into the target high-dimensional space. This is accomplished using a stochastic projection matrix. First, a random matrix G∈RD×m is generated, where its entries are drawn from a standard normal distribution. Here, D is the target hypervector dimension (e.g., D≥10,000) and m is the number of significant eigenvalues to retain from the decomposition (typically m≪n). The projection matrix P∈RD×n is then computed as:

P=GS+1/2​UT

The term S+1/2​ is the diagonal matrix of the square roots of the positive eigenvalues, which scales the projection along the principal axes of variation. This operation effectively "splatters" the low-dimensional structure across the full dimensionality of the hyperspace, creating a distributed, holographic representation in a manner related to Random Fourier Features.4
6. Step 5: Binarization: The final step is to convert the real-valued projection into the discrete, bipolar hypervectors required by the HRC. This is achieved by applying the sign function element-wise to the projection matrix P:

H=sign(P)

The output is a matrix H∈{−1,1}D×n, where each column is a D-dimensional bipolar hypervector hi​ corresponding to the input embedding vi​. These hypervectors are now ready for manipulation by the HRC's algebra of binding (element-wise multiplication), bundling (element-wise addition), and permutation.2
The elegance of the Laplace-HDC method lies in its principled foundation. Instead of attempting to learn a complex, non-linear mapping and hoping it implicitly discovers the correct structural correspondence, this method begins with the known mathematical isomorphism between the source geometry and the target algebra. It is a constructive, transparent algorithm that provides a strong theoretical guarantee of structure preservation, a property that purely data-driven, learned methods can only approximate.


III. Theoretical Analysis of Superiority for Generative Reasoning


The selection of an encoding function for the GCE-to-HRC interface is a critical architectural decision that dictates the system's capacity for robust and generative reasoning. The proposed Laplace-HDC encoder's superiority can be rigorously established through a comparative theoretical analysis against two primary alternatives identified within the framework's design considerations: Record-Based Encoding and Learned Projections, such as autoencoders.1 The analysis reveals that Laplace-HDC uniquely satisfies the requirements for structure preservation, interpretability, and algebraic alignment necessary for true knowledge discovery.


vs. Record-Based Encoding


Record-based encoding is a constructive method that discretizes each dimension of an input embedding into a set of quantized levels. A unique, random basis hypervector is assigned to each dimension (the "role") and to each quantized level (the "filler"). The final hypervector is formed by binding each role to its corresponding filler and bundling the results.
While this method is simple and interpretable, it suffers from a fatal flaw for a system aimed at nuanced reasoning: quantization error. By forcing the continuous values of the geometric embedding into discrete bins, it irrevocably loses the fine-grained semantic relationships that the GCE is designed to capture. Two concepts that are semantically very close but happen to fall on opposite sides of a quantization boundary will be mapped to significantly different hypervectors. This creates a brittle and discontinuous mapping that violates the core principle of structure preservation. This approach is analogous to rote memorization in human learning, where facts are stored as discrete items without a deep understanding of their connections.5 It cannot support the kind of flexible, relational thinking that underpins meaningful learning and problem-solving.6 The Laplace-HDC encoder, being a continuous function of the input vectors' similarities, avoids this pitfall entirely, ensuring that subtle changes in semantic meaning result in correspondingly subtle changes in the hypervector representation.


vs. Learned Projections (Autoencoders)


A more sophisticated alternative is to use a neural network, such as an autoencoder, to learn a mapping from the low-dimensional embedding space to the high-dimensional hyperspace. Advanced variants, like the Graph Geometry-Preserving Autoencoder (GGAE), can even be trained with an explicit regularization term to minimize geometric distortion, thereby learning to preserve the structure of the data manifold.8
However, these learned methods have two significant disadvantages in the context of the Unified Memory Framework. First, their guarantee of structure preservation is purely empirical and data-dependent. The quality of the learned mapping is contingent on the diversity of the training data, the choice of hyperparameters, and the success of the optimization process. There is no underlying theoretical guarantee of a true homomorphic mapping. Second, and more importantly, they are fundamentally opaque. The transformation is encoded within the learned weights of a neural network, creating a "black box" at the most critical interface of the cognitive architecture. This opacity directly contradicts a primary motivation for neuro-symbolic AI: the creation of transparent, explainable, and auditable reasoning systems.1 The HRC is designed to be a transparent algebraic engine, but its reasoning process would be contaminated from the outset if its inputs are generated by an inscrutable neural network.
The Laplace-HDC encoder, in stark contrast, offers a principled and transparent guarantee of structure preservation. The mapping is not learned but is constructed via a deterministic and auditable algorithm derived from the first principles of HDC algebra. This aligns perfectly with the goal of creating a "System 2"-like reasoning engine, where each step of the process is explicit and traceable.1 This approach is analogous to meaningful conceptual mapping, where the relationships between ideas are explicitly and logically structured, leading to deeper understanding and enhanced problem-solving capabilities.10
The following table provides a concise, structured summary of this comparative analysis, highlighting the clear advantages of the proposed Laplace-HDC encoder across all criteria relevant to the framework's goals.
Table 1: Comparative Analysis of GCE-to-HRC Encoding Strategies
Feature
	Record-Based Encoding
	Learned Projection (Autoencoder)
	Laplace-HDC Encoder (Proposed)
	Structure Preservation
	Low (Quantization Error)
	Empirical (Data-Dependent)
	High (Principled, Kernel-Based)
	Generalization
	Poor (Brittle at boundaries)
	Good (If trained on diverse data)
	Excellent (Continuous function)
	Interpretability
	High (Explicit construction)
	Low (Opaque neural network)
	High (Transparent algorithm)
	Training Requirement
	None (Constructive)
	High (Gradient-based)
	Low (Analytic, requires Eigendecomp.)
	Alignment with HRC Algebra
	None (Arbitrary mapping)
	Indirect (Learned correlation)
	Inherent (Isomorphic via Laplace Kernel)
	Cognitive Analogy
	Rote Memorization
	Implicit Pattern Matching
	Meaningful Conceptual Mapping
	

IV. A Rigorous Framework for Empirical Validation


A theoretical proposal, however well-founded, must be subjected to rigorous empirical validation. To definitively test the hypothesis that the Laplace-HDC encoder is superior for enabling knowledge discovery within the Unified Memory Framework, this report proposes a concrete, multi-stage experimental plan. This plan is designed to be actionable and reproducible, providing quantitative metrics to compare the proposed method against well-defined baselines at each stage of the framework's operational cycle, mirroring the phased research roadmap of component-level validation, open-loop simulation, and closed-loop analysis.1


Baselines for Comparison


To ensure a robust evaluation, the performance of the Laplace-HDC encoder will be compared against two strong baselines, each representing an alternative encoding strategy:
   1. Baseline 1 (Record-Based): A carefully implemented version of the record-based encoding scheme, which serves as a representative of discrete, constructive mapping methods.1
   2. Baseline 2 (Learned Projection): A variational autoencoder trained with a geometric distortion regularizer, conceptually similar to the GGAE.8 This baseline represents the state-of-the-art in learned, structure-preserving mappings.


Stage 1: Component-Level Validation (Mapping Fidelity)


The first stage directly assesses the fundamental claim of superior structure preservation.
   * Task: To quantify the degree to which each encoding method preserves the geometric relationships of the source embedding space.
   * Methodology: A set of concept vectors will be sampled from a pre-trained GNN embedding space with a known hierarchical structure (e.g., embeddings of the WordNet noun hierarchy). These vectors will be encoded into hypervectors using the Laplace-HDC method and both baseline encoders. For each method, two matrices will be computed: the pairwise cosine similarity matrix Kgce​ in the original geometric space, and the pairwise normalized dot product matrix Khrc​ in the target hyperspace.
   * Metric: The primary metric will be the Geometric Distortion Score (GDS), defined as the Frobenius norm of the difference between the two similarity matrices: GDS=∥Kgce​−Khrc​∥F​. A lower GDS indicates better preservation of the original geometric structure.
   * Success Criterion: The Laplace-HDC encoder must yield a GDS that is statistically significantly lower (e.g., via a paired t-test with p<0.01) than the GDS produced by both baselines.


Stage 2: Open-Loop Simulation (Reasoning Integrity)


The second stage evaluates how the fidelity of the mapping translates into the coherence of a single, forward-pass reasoning operation.
   * Task: To test the system's ability to perform analogical reasoning, a canonical task for VSA/HDC systems that relies on the compositional integrity of vector operations.1
   * Methodology: The experiment will use a standard benchmark, such as the Google Analogy Dataset, which contains analogies of the form "A is to B as C is to D". For each analogy (e.g., king - man + woman = queen), the following open-loop procedure will be executed for each encoding method:
   1. Retrieve the GCE embeddings for the source concepts (king, man, woman).
   2. Encode them into bipolar hypervectors (hking​,hman​,hwoman​).
   3. Perform the corresponding HRC algebraic operation: hquery​=hking​⊙hman​⊙hwoman​ (using element-wise multiplication, where for bipolar vectors, the inverse is the vector itself).
   4. Decode the resulting query hypervector hquery​ back into a geometric vector cquery​ using the appropriate inverse mapping.
   5. Perform a k-Nearest Neighbors search in the full GCE vocabulary with cquery​ and verify if the embedding for the target concept (queen) is present in the top-k results.
   * Metric: Top-k accuracy (e.g., Top-1 and Top-5).
   * Success Criterion: The Laplace-HDC encoder must achieve a statistically significantly higher Top-k accuracy than both baselines, demonstrating that its superior structure preservation directly enables more coherent and accurate algebraic reasoning.


Stage 3: Closed-Loop Simulation (Generative Capacity)


The final and most comprehensive stage assesses the entire system's ability to perform multi-step, generative reasoning by closing the AGL feedback loop.
   * Task: To evaluate the system's performance on a compositional, multi-hop question-answering task over a large-scale knowledge graph. An example query might be: "Which drug, targeting a protein associated with Alzheimer's, was developed by a company founded in Cambridge?" This requires a chain of retrieval, reasoning, and grounding steps.
   * Methodology: Using a dataset like MetaQA or Freebase QA, the system will be required to answer complex questions by iteratively using the GCE-HRC-AGL loop. The grounded output of one reasoning cycle serves as a contextual input for the next.
   * Metrics:
   1. Final Answer Accuracy: The F1-score of the final grounded answer against the ground truth.
   2. Intermediate Plausibility Score (IPS): At the end of each intermediate reasoning cycle, the AGL produces a "noisy" vector cresult​. The semantic coherence of the top-k nearest neighbors to this vector in the GCE will be measured (e.g., using a pre-trained semantic textual similarity model against a human-annotated "gold" intermediate concept). A high IPS indicates that the system is following a plausible chain of thought.
   * Success Criterion: The Laplace-HDC-powered system must not only achieve a higher final answer F1-score but also demonstrate significantly higher Intermediate Plausibility Scores throughout the reasoning process. This would provide strong evidence that its principled mapping fosters a more stable, robust, and creative reasoning process, fulfilling the ultimate goal of knowledge discovery.
The following table formalizes this experimental protocol, providing a clear and actionable plan for the empirical validation of the proposed encoder.
Table 2: Protocol for Empirical Validation
Stage
	Task
	Dataset
	Baselines
	Primary Metric(s)
	Success Criterion
	1. Mapping Fidelity
	Structure Preservation
	WordNet noun hierarchy embeddings
	Record-Based, Autoencoder
	Geometric Distortion Score (GDS)
	Lowest GDS (p<0.01)
	2. Reasoning Integrity
	Analogical Reasoning
	Google Analogy Dataset
	Record-Based, Autoencoder
	Top-5 Accuracy
	Highest accuracy (p<0.01)
	3. Generative Capacity
	Multi-hop QA
	Freebase QA, MetaQA
	Record-Based, Autoencoder
	Final Answer F1-Score; Intermediate Plausibility Score
	Highest F1 and Plausibility
	

V. System-Level Implications and Future Directions


The adoption of the Laplace-HDC encoder as the core interface between the GCE and HRC has profound implications for the overall behavior, performance, and utility of the Unified Memory Framework. Its principled design not only enhances the system's reasoning capabilities but also contributes to its stability, explainability, and potential for future advancement.


Impact on Associative Grounding Loop (AGL) Stability


The stability of the feedback loop is paramount for reliable multi-step reasoning. A learned mapping, being a complex non-linear function, could potentially exhibit chaotic or divergent behavior, leading to error propagation and unstable reasoning trajectories. Small errors in an early reasoning step could be amplified by the encoder-decoder cycle, causing the system to converge on an erroneous result or fail to converge at all. The Laplace-HDC mapping, being a well-behaved and mathematically transparent function, is inherently more stable. Its properties are not subject to the vagaries of a training process. This stability is critical for ensuring that the AGL acts as a convergent process, reliably guiding the system's chain of thought toward plausible and coherent conclusions.


Computational Complexity and Tractability


A potential concern with the proposed method is the computational cost of the eigendecomposition of the n×n similarity matrix K. For a very large number of concepts n, this operation can be computationally expensive. However, this concern is effectively mitigated by the architecture of the framework itself. The mapping is not performed on the entire GCE memory at once. Instead, it is applied only to the small, context-relevant subset of k concepts retrieved by the GCE's k-Nearest Neighbors search in response to a query. Since k is typically a small number (e.g., 10-100), the eigendecomposition of the k×k matrix is computationally trivial and can be performed efficiently at inference time, making the approach highly tractable for real-world applications.


Enhancing Explainable AI (XAI)


A key motivation behind neuro-symbolic AI is to move beyond the opacity of monolithic deep learning models toward systems that can explain their conclusions.1 The algorithmic transparency of the Laplace-HDC encoder is a significant contribution to this goal. It allows for the creation of a complete, end-to-end audit trail for any reasoning task. One can trace the system's process from the initial geometric context retrieved by the GCE, through the transparent mathematical steps of the Laplace-HDC transformation, through the explicit sequence of algebraic operations in the HRC, and finally back through the decoder to the grounded concept identified by the AGL. This provides a level of interpretability and traceability that is impossible to achieve when a critical interface is a "black box" neural network, making the framework a more trustworthy and verifiable tool for high-stakes decision-making domains.


Future Research Directions


The proposal of the Laplace-HDC encoder serves as a robust foundation but also opens several promising avenues for future research to further enhance the framework's capabilities:
   1. Kernel Exploration: The established connection between the Laplace kernel and HDC algebra invites investigation into other kernel functions. Could other kernels, such as the Gaussian RBF kernel, correspond to different or more powerful algebraic structures within alternative VSA models? A deeper theoretical exploration of this "kernel-algebra duality" could unlock new forms of reasoning.
   2. Online and Incremental Updates: The current formulation is batch-oriented, operating on the context set retrieved for a given query. Future work could focus on developing online or incremental versions of the algorithm that can efficiently update the mapping as new concepts are added to the GCE's long-term memory, avoiding the need to recompute from scratch and enabling more fluid, lifelong learning.
   3. Hardware Co-Design: The specific mathematical operations of the Laplace-HDC encoder—dot products, matrix multiplications, and the sign function—are highly parallelizable and amenable to hardware acceleration. Research into co-designing hardware architectures, such as in-memory computing platforms or specialized ASICs, could dramatically improve the speed and energy efficiency of the GCE-HRC interface, making the deployment of such advanced cognitive architectures feasible on edge devices and in real-time systems.12


VI. Conclusion: Towards a More Integrated Artificial Intelligence


This report has presented a comprehensive proposal for a specific mathematical function—the Laplace-HDC encoder—to serve as the critical interface within a Unified Memory Framework. The analysis concludes that this principled, transparent, and structure-preserving function is demonstrably superior to alternatives for the task of knowledge discovery. By establishing a theoretically grounded and computationally tractable bridge between the GCE's geometric perception and the HRC's algebraic reasoning, the proposed encoder elevates the framework from a simple hybrid model to a truly integrated cognitive architecture.
The core argument rests on the homomorphic imperative: for the system to achieve generative reasoning and creative inference, the mapping between its perceptual and symbolic components must be a continuous mathematical function that preserves semantic structure. The Laplace-HDC encoder fulfills this imperative not through black-box learning but through an elegant, constructive algorithm derived from the inherent mathematical isomorphism between the Laplace kernel and the algebra of hyperdimensional computing. This provides a robust theoretical foundation that translates into tangible benefits, including superior generalization, enhanced reasoning integrity, and greater stability within the system's crucial feedback loop.
Furthermore, the report has laid out a rigorous, multi-stage framework for the empirical validation of these claims, ensuring that the theoretical proposal is grounded in testable hypotheses and measurable outcomes. The adoption of this encoder has significant system-level implications, most notably in advancing the goal of explainable AI by ensuring end-to-end transparency in the reasoning process.
While challenges remain, the path forward is clear. The successful implementation and validation of the Laplace-HDC encoder within the Unified Memory Framework would represent a significant step toward resolving the long-standing divide between sub-symbolic and symbolic AI. It offers a compelling blueprint for a new generation of AI systems that can seamlessly integrate perception, memory, and reasoning, moving us closer to an artificial intelligence that exhibits a deeper, more robust, and more human-like form of understanding.
Works cited
   1. Unified AI Memory Framework Exploration
   2. HD/VSA, accessed September 24, 2025, https://www.hd-computing.com/
   3. Laplace-HDC: Understanding the Geometry of Binary ..., accessed September 24, 2025, https://www.jair.org/index.php/jair/article/download/17688/27147
   4. Laplace-HDC: Understanding the geometry of binary hyperdimensional computing - Semantic Scholar, accessed September 24, 2025, https://www.semanticscholar.org/paper/Laplace-HDC%3A-Understanding-the-geometry-of-binary-Pourmand-Whiting/f7436636442fa5f046ab363a2fec424d44091175
   5. Mapping math: 5 ways to use concept maps in the math classroom - Teach. Learn. Grow., accessed September 24, 2025, https://www.nwea.org/blog/2024/mapping-math-5-ways-to-use-concept-maps-in-the-math-classroom/
   6. Full article: Digital conceptual mapping for enhancing mathematical concept formation and creative mathematical problem-solving through cognitive flexibility skills: a mixed methods study, accessed September 24, 2025, https://www.tandfonline.com/doi/full/10.1080/2331186X.2025.2494945
   7. How to Use Concept Maps in Mathematics - Creately, accessed September 24, 2025, https://creately.com/guides/concept-maps-in-mathematics/
   8. Graph Geometry-Preserving Autoencoders - GitHub, accessed September 24, 2025, https://raw.githubusercontent.com/mlresearch/v235/main/assets/lim24a/lim24a.pdf
   9. Neuro-symbolic AI - Wikipedia, accessed September 24, 2025, https://en.wikipedia.org/wiki/Neuro-symbolic_AI
   10. (PDF) Concept mapping learning strategy to enhance students' mathematical connection ability - ResearchGate, accessed September 24, 2025, https://www.researchgate.net/publication/317260382_Concept_mapping_learning_strategy_to_enhance_students'_mathematical_connection_ability
   11. Using Concepts Maps in a Foundation Mathematics Course: What Have we Learnt?, accessed September 24, 2025, https://www.ejmste.com/download/using-concepts-maps-in-a-foundation-mathematics-course-what-have-we-learnt-9700.pdf
   12. Neuro-symbolic AI - IBM Research, accessed September 24, 2025, https://research.ibm.com/topics/neuro-symbolic-ai
   13. [2502.11269] Unlocking the Potential of Generative AI through Neuro-Symbolic Architectures: Benefits and Limitations - arXiv, accessed September 24, 2025, https://arxiv.org/abs/2502.11269