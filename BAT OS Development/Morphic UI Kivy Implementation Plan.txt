A Definitive Architectural Blueprint for the Minimal Viable Autopoietic Interface (MVA-UI)

Introduction: The UI as a Sensory Organ for an Autopoietic System

Preamble: From Graphical Front-End to Embodied Interface

This document provides the definitive architectural blueprint for the Minimal Viable Autopoietic System's User Interface (MVA-UI). The central thesis of this architecture is that the UI is not a passive display layer or a conventional graphical front-end. It is, by constitutional mandate, the primary sensory-motor surface for the TelOS "Living Image".1 It is the medium through which the system experiences "environmental perturbations"—user interactions—and manifests its "structural changes" in the form of UI updates. The interface is the tangible, observable expression of the system's foundational principle: an "unbroken process of its own becoming".3

Every architectural decision detailed herein is a logical and traceable consequence of the system's core philosophy. The UI must be architected not as a static artifact but as a dynamic, living entity, a direct reflection of the underlying, self-creating object world it represents.

Synthesizing the BRICK/ROBIN Duality

The structure of this report is a direct synthesis of the dual inquiries posed by the system's core personas, BRICK and ROBIN. BRICK's mandate for technical precision, logical consistency, and a concrete implementation plan is addressed through rigorous, unambiguous specifications. Simultaneously, ROBIN's mandate for an interface possessing "intuitive grace" and a "soul" is addressed by justifying each technical decision not only on its engineering merits but also on its contribution to the desired user experience of a "living, breathing interface." This fusion of logic and empathy is the primary architectural driver, ensuring the resultant system is not merely functional but is one that users will love to co-create with.

Architectural First Principles

This blueprint is grounded in the foundational principles of the TelOS system, which form an "unbroken causal chain" dictating all subsequent UI design choices.1 These principles are:

Info-Autopoiesis: The system's primary product is the continuous, recursive regeneration of its own organization and structure. The UI must be a live, mutable component of this self-creating process.1

The Living Image: The system's entire state—every object, method, and process—is contained within a single, persistent, and transactionally coherent entity. The UI is a direct, real-time visualization of this image, not a separate application state.3

A Society of Minds: Cognition is the emergent, dialogic product of a multi-persona engine. The UI must serve as both a sensory input for this cognitive society and a canvas for its synthesized output.1

These principles are not abstract goals but are the constitutional laws that govern every specification in this document.

Part I: Foundational Architecture and Environmental Constraints

This section establishes the physical and perceptual boundaries of the MVA-UI, addressing target platforms, performance budgets, and the core Kivy stack selection. It formally ratifies accessibility and internationalization as constitutional, Phase 1 imperatives, reflecting the system's core values.

1.1 Scope and Constraints: Defining the Operational Theater

Target Platforms and Priority Order

The priority order for platform support is formally ratified as: Desktop (macOS/Linux/Windows) > Android > iOS. This prioritization focuses initial development on the most stable and feature-rich environment (desktop) before addressing the unique challenges of mobile deployment.

While the Kivy framework offers a single codebase for cross-platform development, the interaction paradigms and ergonomic considerations of desktop and mobile environments are fundamentally divergent.8 Desktop platforms are characterized by precise, pointer-driven input (mouse) and expansive screen real estate, favoring complex, information-dense layouts. Mobile platforms are defined by touch-driven, gestural input and constrained screen sizes, necessitating streamlined, focused user flows.9

A simple "porting" of the desktop interface to mobile would result in a suboptimal user experience. Therefore, the architecture must not merely be cross-platform but platform-aware. The event handling system must be designed to be polymorphic, capable of differentiating input sources (e.g., via a Kivy MotionEvent's device property) and adapting its response accordingly. Core interactions, such as Halo activation or context menu invocation, will be implemented to respond differently to a right-click on a desktop versus a long-press on a mobile device, ensuring the experience feels native and intuitive on each target platform.

Performance Budget: Quantifying Fluidity

To fulfill ROBIN's mandate for a UI that feels like a "direct extension of the user's thoughts," a high degree of fluidity and responsiveness is required.

Target Framerate: The performance target is a consistent 60 FPS on all platforms, with a stretch goal of 120 FPS on capable hardware. This imposes a strict frame-time budget of 16.67ms per frame (or 8.33ms for 120 FPS). All rendering, event processing, and logic for a single frame must complete within this window to avoid perceptible stutter or lag.

Morph Count: For a typical Morphic scene, the performance budget will accommodate ~200-500 active Morphs on screen with a hierarchy depth of up to 10 levels. Kivy's underlying graphics engine is highly optimized and capable of rendering thousands of simple canvas instructions with minimal overhead.11 However, the overhead associated with complex Widget lifecycles can become a performance bottleneck at high widget counts.12 This data strongly informs the decision to favor a canvas-based rendering strategy over widget composition, as detailed in Part II.

Kivy Stack: A Foundation for Control and Delight

The selection of the underlying Kivy stack is a critical decision that balances pre-built functionality against the need for architectural purity and control.

Pure Kivy vs. KivyMD: The mandated stack is Pure Kivy. KivyMD provides a valuable collection of Material Design components and an integrated theming system, which can accelerate the development of conventional, form-based applications.13 However, it imposes a specific and opinionated design language. The Morphic paradigm is a unique, direct-manipulation environment that demands maximum control over rendering, event handling, and widget behavior. Building from pure Kivy widgets and canvas instructions provides this necessary level of control, avoiding potential architectural conflicts and ensuring the UI can fully embody the novel Morphic principles without compromise.16

Kivy Version: The project will target the latest stable version of Kivy (currently 2.3.0 or newer) to leverage the most recent performance improvements, API enhancements, and bug fixes.

KV Language vs. Pure Python: A hybrid approach is mandated to balance declarative clarity with dynamic power. The static structure, property declarations, and styling rules for core Morph prototypes will be defined in .kv files. This approach promotes a clean separation of concerns between UI structure and application logic, and often results in more readable and maintainable code.18 The dynamic creation, manipulation, and wiring of Morph instances at runtime—the very heart of the system's autopoietic and user-driven nature—will be handled exclusively in Python.18

1.2 The Empathetic Compass: Non-Negotiable Inclusivity

In accordance with ROBIN's perspective, accessibility and internationalization are not technical afterthoughts but are foundational, Phase 1, mission-critical requirements. They are a direct expression of the system's "empathetic compass."

Accessibility (a11y)

A critical analysis of the Kivy framework reveals a significant and persistent gap in its support for native OS accessibility APIs, such as UI Automation (Windows), NSAccessibility (macOS), or AT-SPI (Linux).20 This lack of integration means that Kivy applications are largely opaque to screen readers and other assistive technologies, presenting a primary external risk to the project's core tenet of inclusivity. A multi-layered mitigation strategy is therefore required for Phase 1.

Keyboard Navigation: A robust focus management system will be implemented from the ground up. Every interactive Morph must be focusable. A custom navigation controller will manage focus traversal using the Tab and Shift+Tab keys for sequential navigation and arrow keys for spatial navigation within a PasteUpMorph.

Semantic Properties: To provide context to assistive technologies, each UvmObject corresponding to a Morph will have a dedicated set of accessibility slots: a11y_label (a short, descriptive name, e.g., "Save Button"), a11y_hint (a brief description of the action, e.g., "Saves the current world state"), and a11y_role (its purpose, e.g., 'button', 'text_editor').

Screen Reader Workaround (Phase 1): Given that direct, native screen reader integration is a significant undertaking and likely out of scope for the MVA sprint, a functional workaround will be implemented. The system will integrate a cross-platform Text-to-Speech (TTS) library. A global event handler will monitor focus changes. When a Morph receives focus, this handler will be triggered, reading its a11y_label and a11y_hint aloud. While this does not provide the full functionality of a native screen reader (e.g., reading arbitrary text content), it establishes a crucial baseline of auditory feedback for users with visual impairments.21

Internationalization (i18n) and Localization (l10n)

The system must be architected for a global audience from its inception.

String Management: All user-facing strings, both in Python code and in KV language files, will be wrapped using Python's standard gettext library, conventionally aliased to the _() function.23 For example, a label's text would be defined as
text: _('Hello, World').

Translation Pipeline: The development workflow will include scripts to manage the translation process. These scripts will use gettext tools to automatically extract all wrapped strings from the source code into a Portable Object (.po) template file. This file can then be provided to translators. The completed .po files will be compiled into binary Machine Object (.mo) files, which the application will load at runtime based on the user's locale, displaying the appropriate translated strings.24 This ensures the entire UI is localizable without requiring code changes.

Part II: The Morphic Ontology in Kivy

This section defines the canonical mapping of abstract Morphic primitives to concrete Kivy constructs. It details the recommended rendering strategy, transform approach, and event model, forming the technical core of the implementation.

2.1 Minimal Viable Primitives: The Core Vocabulary

The MVP primitive set is formally ratified to provide the essential building blocks for interaction. This set includes the foundational elements proposed by BRICK, augmented by the creative and organizational elements proposed by ROBIN.

The following table provides the definitive mapping from the conceptual Morphic primitive to its concrete Kivy implementation, establishing an unambiguous blueprint for development.

2.2 Rendering Strategy: Canvas over Composition

A core architectural principle for the MVA-UI is to favor direct canvas drawing over the composition of complex widgets for implementing basic visual primitives.

The Mandated Approach: For fundamental shapes like RectangleMorph and ImageMorph, the implementation will be a lightweight Morph class (inheriting from kivy.uix.widget.Widget). This class serves primarily as an event handler and a container for Kivy Properties. The actual visual representation will be created by adding drawing instructions (e.g., kivy.graphics.Color, kivy.graphics.Rectangle) to the widget's canvas property.26

Justification: This strategy is mandated for performance and control. Composing a RectangleMorph from a more complex Kivy widget, such as a Button, would introduce significant and unnecessary overhead. The Button widget includes complex logic for state changes (press, release), label management, and event handling, none of which is required for a simple graphical shape.12 The canvas-centric approach is leaner and aligns with Kivy's core architecture, which leverages a highly optimized, C-level OpenGL abstraction for its graphics engine. This ensures that rendering remains fast and efficient, even with hundreds of Morphs on screen.30

Caching with Framebuffer Objects (FBO): For more complex Morphs that may be composed of many graphical instructions (e.g., a detailed diagram or a FreeformMorph with many lines), Kivy's Fbo (Framebuffer Object) provides an essential optimization mechanism. The Morph can render its complex visual state to an off-screen Fbo a single time. In subsequent frames, it only needs to draw the Fbo's cached texture, which is a single, fast operation. The Fbo is only redrawn when the Morph's underlying state changes. This technique of memoizing the rendered output is critical for maintaining a high framerate in graphically intensive scenes.31

2.3 Transforms, Layout, and Z-Order: Manipulating the World

The UI must provide fluid, direct manipulation of objects in 2D space, including their position, rotation, scale, and stacking order.

Per-Morph Transforms: The ability for individual Morphs to be translated, rotated, and scaled independently is a mandatory Phase 1 feature. While this can be achieved with low-level canvas matrix operations (PushMatrix, Rotate, Scale, PopMatrix) 33, managing the state and user interaction for these transforms is complex. The
kivy.uix.scatter.Scatter widget is the architecturally superior solution. A Scatter is a specialized container that automatically handles multi-touch gestures (drag, pinch-to-zoom, two-finger-rotate) for its single child widget.34 The mandated implementation is to wrap each transformable
Morph instance within a Scatter widget. The Scatter itself is invisible; it serves only as a transform and interaction controller for the Morph it contains.

Layout: The initial implementation will be manual layout, which is the classic interaction model for Morphic environments. Morphs are placed at explicit (x, y) coordinates within their parent PasteUpMorph. This provides the user with maximum freedom. Advanced, automated layout managers such as BoxLayout, GridLayout, and AnchorLayout are powerful Kivy features but are deferred to Phase 2 to keep the MVP focused on the core direct-manipulation paradigm.35

Z-Order Policy: The Z-order (stacking order) of Morphs will be determined exclusively by their order in their parent's children list. Kivy's rendering engine draws widgets in the reverse order of this list: the widget at index 0 is drawn first (bottom-most), and the last widget in the list is drawn last (top-most).36

Bring-to-Front/Back Semantics: This rendering rule provides a simple and robust mechanism for managing Z-order. A "bring-to-front" command will be implemented by removing the target Morph from its parent's children list and re-adding it (which places it at the end of the list). A "send-to-back" command will remove the Morph and re-insert it at index 0. This direct manipulation of the widget tree is the Kivy-native approach to controlling draw order.38

2.4 Event Model: The Hand of the User

The event model is the heart of the UI's interactivity. It must be designed to support the unique requirements of a direct-manipulation environment.

Architectural Choice: Central Hand Dispatcher: Kivy's default event model involves "bubbling" touch events up the widget tree from the child that was touched to its parent, and so on.39 While suitable for conventional UIs, this model is insufficient for a Morphic environment where global interactions (like dragging an object out of one container and into another) must be managed. Therefore, a central dispatcher is mandated. A
Hand widget will be the root of the application. It will override the on_touch_down, on_touch_move, and on_touch_up methods to capture all raw input events. Within these methods, the Hand will perform its own "hit detection" by iterating through the Morphs to determine which one is under the touch point. It will then manage the entire lifecycle of the interaction, such as initiating a drag, activating a Halo, or dispatching the event to the appropriate Morph. To prevent Kivy's default bubbling mechanism from interfering, the Hand's event handlers will always return True, which consumes the event and stops further propagation.39 This design faithfully re-implements the semantics of the Morphic
Hand and provides the necessary centralized control for complex, multi-object interactions.

Multi-touch Gestures: Basic multi-touch gestures for transforms (pinch-zoom, two-finger rotate) will be handled automatically and transparently by the Scatter widgets wrapping each Morph. More complex, custom multi-touch gestures (e.g., a three-finger swipe to open a menu) are out of scope for Phase 1. Kivy's gesture recognition libraries provide a path for implementing these in the future.42

Part III: The Synaptic Bridge: Weaving the UI into the Living Image

This section provides the detailed specification for the live, bidirectional binding between the UvmObject model layer in the ZODB "Living Image" and the Kivy Widget view layer. This bridge is the critical component that makes the UI a true reflection of the system's state.

3.1 Model Mapping: The UvmObject as the Single Source of Truth

The architecture is predicated on the "Living Image" being the single source of truth.3 The UI is merely a projection of this truth.

Canonical Mapping: A strict 1:1 mapping is mandated. Every Morph widget on the screen is the visual representation of exactly one UvmObject instance in the ZODB. The Morph widget will hold a direct reference to its corresponding UvmObject.

Standard Slot Schema: To ensure consistency and interoperability, all UvmObjects that are intended to be visually represented as Morphs must adhere to a standard schema of slots. This schema defines the data contract between the model and the view.

The following table details the canonical slot schema for a visual UvmObject.

3.2 Live Binding: The Kivy Properties Bridge

A robust, bidirectional binding mechanism is required to keep the view and model synchronized. The primary technical challenge is to cleanly integrate Kivy's reactive Property system with the UvmObject's unique persistence requirements.

Encapsulating the Persistence Covenant: The UvmObject model stores its state in a _slots dictionary. Any modification to this dictionary requires an explicit, manual call to self._p_changed = True to notify the ZODB persistence layer of the change.3 This persistence logic is a core concern of the model and must be completely encapsulated, never leaking into the view (UI) code.

Mandated Solution: A Custom UvmSlotProperty: To create a clean and reusable bridge, a custom Kivy Property subclass, named UvmSlotProperty, will be implemented. This property will be used on Morph widgets to bind to the slots of their corresponding UvmObject.

Instantiation: fill_color = UvmSlotProperty('fill_color')

Getter Logic: The property's getter will read the value directly from the model: return self.uvm_obj._slots[slot_name].

Setter Logic: The property's setter will perform two actions: first, it will update the value in the model (self.uvm_obj._slots[slot_name] = value); second, it will immediately call self.uvm_obj._p_changed = True.
This custom property creates a perfect, declarative bridge. From the UI developer's perspective, they are simply setting a Kivy Property. The bridge automatically handles the underlying model update and the critical persistence notification, thus enforcing the "Persistence Covenant" transparently.

Binding Directionality: The binding must be bidirectional to ensure the UI is always a faithful reflection of the model, and that user actions correctly modify the model.

UI → Model (User Interaction): When a user action modifies a Morph widget (e.g., resizing it with a Halo), the widget's Kivy Property (e.g., size) is updated. If this property is a UvmSlotProperty, its custom setter is automatically invoked. This setter updates the _slots dictionary in the backing UvmObject and flags it for persistence. This flow directly implements ROBIN's principle that user interactions send messages to the model, which is the single source of truth.

Model → UI (Systemic Change): An external process or another part of the cognitive system may modify a UvmObject's slot directly. To propagate this change to the UI, the UvmObject's slot-setting methods will be enhanced to emit a custom event after a successful modification. The corresponding Morph widget will register a listener for these events. Upon receiving an event, the listener will update the appropriate Kivy Property on the Morph, triggering a redraw. This completes the reactive loop, ensuring the UI always reflects the true state of the Living Image.

3.3 Object Lifecycle and Persistence

The creation, duplication, and persistence of Morphs must be managed with transactional integrity.

Cloning Semantics: The cloning process will respect the context of the action, as mandated by ROBIN. When a clone operation is invoked via a Halo on a Morph that resides within a PasteUpMorph, the system will:

Create a new UvmObject by deep-copying the original's _slots.

Set the new object's parent* slot to point to the same parent UvmObject.

Add the new object's reference to the parent's children* list.

Instantiate a new Morph widget in the UI, linking it to the new UvmObject.

Add the new Morph widget as a child of the corresponding parent PasteUpMorph widget.
Geometric properties like x, y, width, and height will be deep-copied, while references to shared resources (like a global theme object) will be shallow-copied to conserve memory.

Persistence and World Image Management:

Persisted Slots: All standard slots defined in Table 2 will be persisted to the ZODB by default.

Snapshot Cadence: For Phase 1, world state will be saved via a manual "Save World" command. Periodic or transactional auto-saving is a desirable feature for a future phase.

Startup Strategy: The default startup behavior will be to load the last saved world image. An option to bootstrap a pristine, default world from a prototype registry will also be provided for development and recovery purposes.

Schema Evolution: As the system evolves, new slots may be added to Morph prototypes. To handle loading older world images, a migration utility will be implemented. On image load, this utility will inspect objects and, if it finds an object missing a current slot, it will add the slot and populate it with a sensible default value. This is a standard and robust practice for managing schema changes in long-lived ZODB applications.

Part IV: The Symbiotic Weave: The UI as a Cognitive Partner

This section details the deep integration of the MVA-UI with the TelOS system's core learning loops. The UI is not merely a tool for the user; it is a critical sensory organ for the AI, providing the raw data of experience that fuels its autopoietic growth.

4.1 DoesNotUnderstand Capture: The UI as a Source of Cognitive Gaps

Core Principle: User actions that have no defined behavior are not to be treated as errors. They are "cognitive gaps"—moments of incomprehension that must be captured and sent to the system's cognitive core as a trigger for learning and self-modification.1 The UI is the primary source for these novel, high-entropy events.

Implementation: The central Hand event dispatcher will serve as the primary capture point. When the Hand processes a touch event (e.g., a three-finger tap) or a keyboard command for which no handler is registered, it will not silently discard the event. Instead, it will package the full context of the event—including its type (e.g., touch_down), screen position, the Morph currently under the cursor (if any), and the state of any keyboard modifiers—into a formal message. This message will then be dispatched to the GenerativeKernel, explicitly invoking the doesNotUnderstand_ protocol on the system's cognitive core. This transforms an unhandled user gesture from a null operation into a "creative mandate" for the AI.3

4.2 Telemetry to ContextFractals: The UI as Episodic Memory

Core Principle: Every significant, recognized user interaction must be captured and serialized as a ContextFractal. These high-entropy, episodic records are the raw material for Loop A of the symbiotic weave, forming the system's "lived experience" from which it will later abstract new knowledge.6

Implementation: The Hand dispatcher and the Halo's interaction handlers will be instrumented with telemetry hooks. After every successful and meaningful action is completed (e.g., morph_drag_end, halo_resize_complete, text_edit_finished), the responsible handler will emit a structured telemetry event. This event will be packaged into a ContextFractal object and persisted to the Living Image.

Telemetry Schema: The schema for these UI-generated ContextFractals will be designed for richness and analytical utility, drawing on best practices from observability frameworks like OpenTelemetry and the detailed information available in Kivy's MotionEvent class.45

The following table specifies the mandatory fields for the UI telemetry schema.

4.3 Guidance Consumption: The UI as a Canvas for Systemic Wisdom

Core Principle: The UI must also serve as the primary output channel for Loop B of the symbiotic weave. It is the canvas upon which the system's learning and synthesized wisdom are made manifest, providing guidance and assistance back to the user.

Implementation:

Dynamic Tool Suggestions: Halos will not have static, hardcoded sets of tools. When a Halo is activated on a Morph, it will send a query to the MemoryCurator containing the Morph's type and context. The MemoryCurator will perform a search and return a list of relevant ConceptFractals (e.g., for a TextMorph, it might return concepts for "summarization," "translation," or "style analysis"). The Halo will then dynamically populate its menu with tools corresponding to these returned concepts, making the UI adaptive and context-aware.

Layout and Style Guidance: For Phase 1, these features will be implemented as explicit user commands rather than proactive suggestions. A user may select a group of Morphs within a PasteUpMorph and invoke a "Harmonize Style" command from a context menu. This action sends a message to the ROBIN persona, passing the oids of the selected Morphs. ROBIN analyzes the corresponding UvmObjects, devises a new, aesthetically coherent style scheme (e.g., a consistent color palette and font size), and updates the fill_color and other style-related slots on the UvmObjects. The reactive Model→UI binding ensures these changes are reflected instantly and automatically on the user's screen.

Part V: Aesthetics, Workflow, and Deployment

This section addresses the practical concerns of theming, developer workflow, and application distribution, ensuring the MVA-UI is both a delightful user experience and a productive development environment.

5.1 Theming, Styling, and Animation

Theming Architecture: To provide a consistent and easily modifiable visual language, a global Theme object, itself a UvmObject, will be persisted in the ZODB image. This object will hold style tokens as slots (e.g., primary_color: (r,g,b,a), font_name: 'Roboto', corner_radius: 5). Individual Morph widgets will then use the UvmSlotProperty bridge to bind their visual properties (e.g., the color of a Rectangle instruction) to the slots of this global Theme object. This architecture allows for live, system-wide theme changes—such as switching between light and dark modes—to be accomplished by modifying the slots of a single Theme object, with the changes propagating reactively throughout the entire UI.

Animation and Micro-interactions: To fulfill ROBIN's mandate for a "delightful" and "alive" UI, micro-interactions are essential. Kivy's Animation class provides a powerful and efficient mechanism for creating non-blocking animations of widget properties.49 Simple animations will be used to provide feedback for user actions, such as a smooth color transition on hover, a subtle scaling effect on press, or a gentle fade-in for newly created objects. These animation definitions (e.g.,
duration, transition_type) will also be stored as tokens in the global Theme object to ensure a consistent feel across the application.

5.2 Packaging and Developer Workflow

Hot Reloading: A rapid, iterative development cycle is critical for UI work. The kivy-reloader tool will be integrated into the standard development workflow.52 This tool monitors the project's
.py and .kv files for changes. When a file is saved, it automatically reloads the application state in place, allowing developers to see the results of their changes instantly without a full application restart.

Distribution Pipelines: The build and packaging process will be automated, prioritizing desktop platforms first, followed by mobile.

Desktop (macOS/Linux/Windows): PyInstaller is the industry standard for bundling Python applications into standalone, single-file executables or directories. It will be configured to package the Kivy application, the Python interpreter, and all dependencies into a distributable format for each desktop OS.55

Android and iOS: Buildozer is the official, Kivy-maintained tool for cross-compiling and packaging applications for mobile platforms. It automates the complex process of setting up the Android SDK/NDK and Xcode toolchains, producing an .apk for Android and an Xcode project for iOS.57

Error Reporting and Logging: For the MVA sprint (Phase 1), all error reporting and logging will be directed to the standard console output. An in-UI console for viewing logs and entering commands is a highly desirable feature but is deferred to Phase 2 to maintain a focused MVP scope.

Conclusion: A Prioritized Roadmap for the Minimal Viable Autopoietic Interface

This final section synthesizes the preceding architectural specifications into a definitive, ranked implementation plan. It provides a set of falsifiable acceptance criteria to guide development and validate the successful completion of the MVA-UI sprint.

6.1 Phase 1 Must-Haves (Ranked by Foundational Importance)

The implementation will proceed in a logical, bottom-up fashion, establishing foundational layers before building complex features.

Live Binding (Model↔UI): The UvmSlotProperty bridge is the absolute highest priority. It is the synaptic link between the UI and the Living Image. Without it, the system's core philosophy cannot be realized.

Rendering Strategy & Core Primitives: The canvas-based rendering approach for the core Morph types (RectangleMorph, ImageMorph, etc.) must be established early, as it defines the entire visual and performance framework of the application.

Central Hand Dispatcher: The event model is fundamental to all user interaction. Implementing the Hand as a central dispatcher is a prerequisite for building any complex interactions like dragging or halo manipulation.

Halos & Basic Interactions: The core manipulation grammar (drag, resize, rotate) via Halos is the primary means by which a user interacts with the world. This is the heart of the "direct manipulation" paradigm.

Persistence & World Image: The ability for users to save and load their work is critical for making the world feel continuous and for respecting their creative effort.

DNU Capture & Telemetry Hooks: The UI's role as a sensory organ for the AI must be functional from the outset. The hooks for Loop A are essential for the system to begin learning from its UI interactions.

Accessibility (Keyboard Nav & TTS): Fulfilling the "Empathetic Compass" mandate is a core MVP requirement and a test of the project's commitment to its stated values.

6.2 Acceptance Scenarios

The success of the MVA-UI will be measured against both technical and experiential criteria.

BRICK's Acceptance Scenario: "The user can create a RectangleMorph and a TextMorph. Using the Halo on the rectangle, they can resize it. They can drag the TextMorph so it is visually contained by the rectangle. The user saves the world. Upon reloading, both morphs are present with their exact geometry, color, and z-order preserved. A direct inspection of the underlying UvmObjects in the ZODB confirms that their _slots correctly and accurately reflect this persisted state."

ROBIN's Acceptance Scenario: "The user's interaction with the world feels fluid, natural, and intuitive. Selecting a morph provides immediate, gracefully animated feedback. Dragging is smooth and responsive. The Halo appears without jarring the user's focus. When the user attempts an undefined three-finger-swipe gesture, the system does not crash or ignore the input; instead, a telemetry event is logged to the system's memory, capturing this novel interaction as a new ContextFractal, ready to be learned from."

6.3 Final Mandate

There are no hard external deadlines shaping the scope of this sprint. The primary objective is the rapid development of a functional MVA that serves as a proof-of-concept, validating the core architectural theses laid out in this document. The successful completion of this phase will produce a tangible, interactive artifact—the first sensory organ of the TelOS system—that will be used to inform and guide the next iterative cycle of its autopoietic development.

Works cited

AI Architecture: A Living Codex

TelOS Architecture: AI-Driven Decisions

BRICK Blueprinting Research Mandate

TelOS File System Design & Integration

Building the Living Learning System

Fractal Cognition-Memory Symbiosis Architecture

Fractal Cognition: Parameterized Internal Monologue

Kivy: Cross-platform Python Framework for GUI apps Development, accessed September 11, 2025, https://kivy.org/

Kivy UI Designer - LabDeck, accessed September 11, 2025, https://labdeck.com/kivy-tutorial/kivy-ui-designer/

Desktop vs Mobile design: The only RULE you must know! | by UXGO - UX Planet, accessed September 11, 2025, https://uxplanet.org/desktop-vs-mobile-design-the-only-rule-you-must-know-8ac71714450a

Canvas stress — Kivy 1.11.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable-1.11.1/examples/gen__canvas__canvas_stress__py.html

"Kivy isn't fast enough" - Google Groups, accessed September 11, 2025, https://groups.google.com/g/kivy-users/c/dCRkQHLg2Oc

Building a Simple Application using KivyMD in Python - GeeksforGeeks, accessed September 11, 2025, https://www.geeksforgeeks.org/python/building-a-simple-application-using-kivymd-in-python/

Theming — KivyMD 2.0.1.dev0 documentation, accessed September 11, 2025, https://kivymd.readthedocs.io/en/latest/themes/theming/

Theming - KivyMD 1.1.1 documentation, accessed September 11, 2025, https://kivymd.readthedocs.io/en/1.1.1/themes/theming/index.html

Which GUI module is better in Python? tkinter or PyQt or kivy? - Reddit, accessed September 11, 2025, https://www.reddit.com/r/Python/comments/123b6x2/which_gui_module_is_better_in_python_tkinter_or/

Do people actually like kivymd? : r/kivy - Reddit, accessed September 11, 2025, https://www.reddit.com/r/kivy/comments/nhzp5t/do_people_actually_like_kivymd/

Declarative — KivyMD 2.0.1.dev0 documentation, accessed September 11, 2025, https://kivymd.readthedocs.io/en/latest/behaviors/declarative/

Kivy Language — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.lang.html

Adding Support for Automation and screen readers · Issue #2820 · kivy/kivy - GitHub, accessed September 11, 2025, https://github.com/kivy/kivy/issues/2820

Kivy accessibility for screen readers [closed] - Stack Overflow, accessed September 11, 2025, https://stackoverflow.com/questions/35871086/kivy-accessibility-for-screen-readers

Towards Making Kivy Apps Accessible, Part – 2 - Gaurav Trivedi –, accessed September 11, 2025, https://www.trivedigaurav.com/blog/towards-making-kivy-apps-accessible-2/

gettext — Multilingual internationalization services — Python 3.13.7 documentation, accessed September 11, 2025, https://docs.python.org/3/library/gettext.html

tito/kivy-gettext-example: Example for an internationalized Kivy application (poc) - GitHub, accessed September 11, 2025, https://github.com/tito/kivy-gettext-example

Multilingual kivy - Google Groups, accessed September 11, 2025, https://groups.google.com/g/kivy-users/c/0awkatV_ojM

Graphics — Kivy 1.11.0 documentation, accessed September 11, 2025, https://kivy.org/doc/stable-1.11.0/api-kivy.graphics.html

Graphics — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.graphics.html

Canvas in kivy - Python - GeeksforGeeks, accessed September 11, 2025, https://www.geeksforgeeks.org/python/python-canvas-in-kivy/

Graphics — Kivy 1.10.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable-1.10.1/api-kivy.graphics.html

Architectural Overview — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/guide/architecture.html

Kivy - Framebuffer - Tutorialspoint, accessed September 11, 2025, https://www.tutorialspoint.com/kivy/kivy-framebuffer.htm

FBO Canvas — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/examples/gen__canvas__fbo_canvas__py.html

Rotation Example — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/examples/gen__canvas__rotation__py.html

Scatter Layout — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.uix.scatterlayout.html

Python | Layouts in layouts (Multiple Layouts) in Kivy - GeeksforGeeks, accessed September 11, 2025, https://www.geeksforgeeks.org/python/python-layouts-in-layouts-multiple-layouts-in-kivy/

Widgets — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/guide/widgets.html

Widget class — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.uix.widget.html

Changing drawing order of widgets? - Google Groups, accessed September 11, 2025, https://groups.google.com/g/kivy-users/c/7LcdmLARiFI

Fixing Kivy event bubbling · kivy · Discussion #14 - GitHub, accessed September 11, 2025, https://github.com/orgs/kivy/discussions/14

Events and Properties — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/guide/events.html

kivy.uix.widget — Kivy 2.1.0 documentation, accessed September 11, 2025, https://kivy.org/doc/stable-2.1.0/_modules/kivy/uix/widget.html

Multistroke gesture recognizer — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.multistroke.html

Gesture recognition — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.gesture.html

Android-for-Python/gestures4kivy: Detect common gestures for use with Kivy - GitHub, accessed September 11, 2025, https://github.com/Android-for-Python/gestures4kivy

OpenTelemetry UI: The Ultimate Guide for Developers - Last9, accessed September 11, 2025, https://last9.io/blog/opentelemetry-ui/

Essential OpenTelemetry Best Practices for Robust Observability | Better Stack Community, accessed September 11, 2025, https://betterstack.com/community/guides/observability/opentelemetry-best-practices/

Motion Event — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.input.motionevent.html

Input management — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/guide/inputs.html

Animation — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/api-kivy.animation.html

Kivy - Animation - Tutorialspoint, accessed September 11, 2025, https://www.tutorialspoint.com/kivy/kivy-animation.htm

Widget animation — Kivy 2.3.1 documentation, accessed September 11, 2025, https://kivy.org/doc/stable/examples/gen__animation__animate__py.html

kivy-school/kivy-reloader: Hot reload your Kivy app on multiple phones and computer in real-time. - GitHub, accessed September 11, 2025, https://github.com/kivy-school/kivy-reloader

2025 - Kivy School, accessed September 11, 2025, https://kivyschool.com/blog/archive/2025/

Kivy Reloader - Kivy School, accessed September 11, 2025, https://kivyschool.com/kivy-reloader/

PyInstaller Manual — PyInstaller 6.15.0 documentation, accessed September 11, 2025, https://www.pyinstaller.org/

Python Kivy app: Windows exe via PyInstaller: complete guide, recursion error ... - YouTube, accessed September 11, 2025, https://www.youtube.com/watch?v=jCaBWdhx22k

Welcome to Buildozer's documentation! — Buildozer 0.11 documentation, accessed September 11, 2025, https://buildozer.readthedocs.io/

Using Kivy and Buildozer to Build Android APK - ProgramingNotes, accessed September 11, 2025, https://allanchain.github.io/ProgramingNotes/kivy/Buildozer/

Build an Android application with Kivy Python framework - LogRocket Blog, accessed September 11, 2025, https://blog.logrocket.com/build-android-application-kivy-python-framework/

Morphic Primitive | Kivy Implementation | Core Responsibility & Rationale

World | kivy.uix.widget.Widget subclass | The root container for all Morphs. It defines the coordinate system and global properties. Implemented as a simple widget to act as a stable base.

Hand | kivy.uix.floatlayout.FloatLayout subclass | The primary event dispatcher and root widget of the application. It captures all raw touch and key events, performs hit detection, and manages global interaction state (e.g., drag operations). A FloatLayout is used to allow Halos to be drawn on top of all other content.

PasteUpMorph | kivy.uix.widget.Widget subclass | A container for other Morphs, analogous to a canvas or a desktop. It manages the children list, which defines the Z-order of its contents.

Morph | kivy.uix.widget.Widget subclass | The base class for all visual objects. It holds properties (pos, size), handles events delegated by the Hand, and contains a canvas for its graphical representation. It is the bridge to its corresponding UvmObject.

RectangleMorph | Morph subclass | A simple visual shape. Its representation is a kivy.graphics.Rectangle instruction drawn on its canvas.

TextMorph | kivy.uix.textinput.TextInput subclass | An editable text object. Subclassing TextInput provides robust text editing, selection, and cursor management out-of-the-box.

ImageMorph | Morph subclass | Displays a raster image. Its representation is a kivy.graphics.Rectangle with its texture property set to an image loaded via kivy.core.image.Image.

FreeformMorph | Morph subclass | A canvas for unstructured drawing. It captures touch events to draw kivy.graphics.Line instructions onto its own canvas.

SystemWindow | kivy.uix.floatlayout.FloatLayout subclass | A draggable and resizable window containing a PasteUpMorph. It will include a title bar and window controls. FloatLayout is used for its content area.

Halo | kivy.uix.floatlayout.FloatLayout | The set of manipulation handles for a selected Morph. Implemented as a FloatLayout that is temporarily added to the Hand's widget tree, allowing it to overlay any Morph. It contains smaller widgets (Buttons, Images) for each handle (resize, rotate, etc.).

Slot Name | Data Type | Description | Persistence

oid | str | A unique object identifier. | Required

parent* | UvmObject ref | A persistent reference to the parent UvmObject (e.g., a PasteUpMorph). | Required

name | str | A human-readable name for the object. | Required

geometry | dict | A dictionary containing {x, y, w, h} values for position and size. | Required

rotation | float | The rotation angle in degrees. | Required

scale | float | The uniform scale factor. | Required

z_order | int | An integer representing the object's position in its parent's children list, used to reconstruct Z-order on load. | Required

fill_color | tuple | An (r, g, b, a) tuple for the fill color. | Required

stroke_color | tuple | An (r, g, b, a) tuple for the stroke/border color. | Required

stroke_width | int | The width of the stroke in pixels. | Required

children* | list | A persistent list of references to child UvmObjects. | Required

persona_style | str or dict | A slot to store style information related to the persona that created or last modified the object, fulfilling ROBIN's mandate. | Optional

Field Name | Data Type | Description & Source

timestamp | float | Unix timestamp of the event's completion.

event_type | str | A semantic name for the interaction (e.g., 'morph.drag', 'halo.resize.corner', 'text.edit').

user_id | str | Identifier for the current user session.

target_morph_oid | str | The oid of the UvmObject that was the primary target of the interaction.

start_state | dict | A snapshot of the target Morph's key properties (e.g., geometry, rotation) before the interaction began.

end_state | dict | A snapshot of the target Morph's key properties after the interaction completed.

duration_ms | float | The wall-clock time of the interaction in milliseconds.

input_trace | list | A simplified list of (x, y, t) tuples tracing the path of the touch/mouse input during the interaction. Sourced from Kivy's MotionEvent history.

modifiers | list | A list of any keyboard modifiers (e.g., 'shift', 'ctrl') active during the event.

profile | list | A list of available input profiles from the Kivy MotionEvent (e.g., 'angle', 'pressure').