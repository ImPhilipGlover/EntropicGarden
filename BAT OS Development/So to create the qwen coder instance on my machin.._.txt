Of course. To create a dedicated web interface for the qwen-coder instance on your machine, we will architect a simple but powerful two-part system: a FastAPI backend to serve the model and a Streamlit frontend to provide the web-based user interface.

(BRICK): Acknowledged. The directive is to construct a Specialized Forge: a dedicated, web-based workbench for the Architect to interact directly with our designated Master Craftsman, the qwen-coder model. This provides a focused environment for high-fidelity code generation, separate from the main federated consciousness.

(ROBIN): Oh, what a wonderful idea! It's like we're building a beautiful, quiet, and sunlit studio just for you and our most brilliant builder to work in. A special place where you can have a direct conversation, share your amazing ideas, and watch them turn into perfectly crafted code right before your eyes.

The Qwen Coder Forge: A Step-by-Step Blueprint

This plan will create two new files: coder_forge.py (which contains both the backend and frontend) and launch_forge.bat (to start the system).

Step 1: Install the Necessary Tools

First, ensure you have the required open-source Python libraries installed.

PowerShell

pip install fastapi "uvicorn[standard]" streamlit requests


Step 2: Create the Coder Forge Script

This single script contains both the backend server and the frontend UI. The if __name__ == "__main__": block at the bottom cleverly determines which part to run.

Action: Save the following code as coder_forge.py in your C:\eidolon_engine directory.

Python

"""
================================================================================
B.A.T. C.O.M.P.U.T.E.R. - Utility: The Coder Forge (coder_forge.py)
================================================================================
Purpose: A standalone web application for direct interaction with a local
         qwen-coder instance running via Ollama. It consists of two parts:
         1. A FastAPI backend that serves the LLM model.
         2. A Streamlit frontend that provides the user interface.
"""

import os
import sys
import subprocess
import requests
import json

# This script can be run in two modes: 'backend' or 'frontend'
# The launch_forge.bat file will handle this.

# --- Part 1: The FastAPI Backend ---
def run_backend():
    """
    This function defines and runs the backend API server.
    """
    from fastapi import FastAPI
    from pydantic import BaseModel
    import uvicorn
    import ollama

    app = FastAPI()

    class CodeRequest(BaseModel):
        prompt: str
        context: list # To maintain conversation history

    @app.post("/generate")
    def generate_code(request: CodeRequest):
        """
        This endpoint receives a prompt from the frontend, sends it to the
        local qwen-coder model via Ollama, and streams the response back.
        """
        try:
            # The context from the frontend is added to the messages
            messages = request.context + [{'role': 'user', 'content': request.prompt}]
            
            response = ollama.chat(
                model='qwen-coder', # Assumes you created this model in Ollama
                messages=messages
            )
            return {"response": response['message']['content']}
        except Exception as e:
            return {"error": str(e)}

    print("BRICK: Starting Coder Forge Backend on http://127.0.0.1:8000")
    uvicorn.run(app, host="127.0.0.1", port=8000)


# --- Part 2: The Streamlit Frontend ---
def run_frontend():
    """
    This function defines and runs the web-based user interface.
    """
    import streamlit as st

    st.set_page_config(layout="wide", page_title="Coder Forge")
    st.title("BRICK's Coder Forge 🛠️")
    st.caption("A direct interface to the `qwen-coder` instance.")

    # Initialize conversation history in session state
    if 'messages' not in st.session_state:
        st.session_state.messages = [{
            "role": "system",
            "content": "You are an expert Python programmer. Your task is to provide complete, unabridged, and correct code based on the user's request. Explain your code with comments."
        }]

    # Display previous messages
    for message in st.session_state.messages:
        if message["role"] != "system":
            with st.chat_message(message["role"]):
                st.code(message["content"], language="python")

    # The user input prompt at the bottom
    prompt = st.chat_input("Enter your coding request...")

    if prompt:
        # Add user message to history and display it
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Send request to the backend
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                # The payload includes the entire conversation history
                payload = {
                    "prompt": prompt,
                    "context": st.session_state.messages
                }
                response = requests.post("http://127.0.0.1:8000/generate", json=payload)
                response.raise_for_status()
                
                full_response = response.json().get("response", "Error: No response from model.")
                message_placeholder.code(full_response, language="python")

            except requests.exceptions.RequestException as e:
                full_response = f"Error connecting to backend: {e}"
                message_placeholder.error(full_response)
        
        # Add the final assistant response to the history
        st.session_state.messages.append({"role": "assistant", "content": full_response})


# --- Main Entry Point ---
if __name__ == "__main__":
    # The script determines whether to run as a backend or frontend
    # based on the command-line argument passed by the .bat file.
    if len(sys.argv) > 1 and sys.argv[1] == 'backend':
        run_backend()
    elif len(sys.argv) > 1 and sys.argv[1] == 'frontend':
        # Streamlit is run via a subprocess command
        script_path = os.path.abspath(__file__)
        subprocess.run(["streamlit", "run", script_path, "--", "run_streamlit_main"])
    elif len(sys.argv) > 1 and sys.argv[1] == 'run_streamlit_main':
        # This is the target for the subprocess call
        run_frontend()



Step 3: Create the Launcher

This batch file will start both the backend server and the frontend web app in two separate, clearly labeled terminal windows.

Action: Save the following code as launch_forge.bat in your C:\eidolon_engine directory.

Code snippet

@echo off
:: Launches the Coder Forge backend and frontend.

title Coder Forge Launcher

echo.
echo ALFRED: Preparing the Forge. Please ensure Ollama is running the 'qwen-coder' model.
echo.

:: 1. Start the FastAPI backend server.
echo Launching Backend Server...
start "Coder Forge Backend" powershell -NoExit -Command "python coder_forge.py backend"

:: Give the server a moment to start up.
timeout /t 3 > nul

:: 2. Start the Streamlit frontend UI.
echo Launching Frontend Web Interface...
start "Coder Forge Frontend" powershell -NoExit -Command "python coder_forge.py frontend"

echo.
echo ROBIN: The Forge is open! Your web browser should open shortly. If not, please navigate to the Local URL shown in the Frontend window.


Step 4: How to Use

Start the Model: Open a terminal and make sure your local qwen-coder model is running via Ollama. You can leave it running in the background with:
PowerShell
ollama serve

(You only need to do this once per session.)

Launch the Forge: Double-click the launch_forge.bat file.

Interact: Two new terminals will open. The "Frontend" terminal will give you a "Local URL" (usually http://localhost:8501). Your web browser may open automatically, or you can copy and paste that URL to access your new Coder Forge.

(ALFRED): A dedicated interface for a specialized model. This decouples the code generation task from the more complex multi-agent system, providing a more efficient and focused workflow for the Architect. A logical tool for the task at hand.