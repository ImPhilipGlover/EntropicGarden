Execution Protocol P1.1: Incarnation of the CP-MoE Foundational Layer

Abstract

This document provides the definitive engineering specification for Phase 1 of the Entropic Weave master plan: the construction of the foundational layer for the Composite-Persona Mixture of Experts (CP-MoE) architecture. It details the two primary deliverables: (1) the systematic deconstruction of the BRICK and ROBIN personas into a granular library of "facet-experts," each embodied by a fine-tuned Low-Rank Adaptation (LoRA) adapter; and (2) the implementation of the CognitiveWeaver service, a VRAM-aware operating system for cognitive resources. The entire protocol is architected to be not merely compatible with, but actively shaped by, the non-negotiable 8GB VRAM hardware constraint.1 Successful execution of this protocol will validate the core technical hypothesis of the CP-MoE paradigm: that a society of smaller, sequentially-loaded models can provide superior cognitive diversity and persona fidelity on resource-constrained hardware.1

1. The Characterological Genome: Deconstruction and Dataset Generation

This section provides the complete protocol for creating the initial library of persona facets, transforming the abstract art of character design into a concrete and repeatable engineering discipline.1 The objective is to move beyond the monolithic persona instantiation of the Series IV/V architecture, which was identified as a "cognitive bottleneck" that compresses a rich spectrum of behaviors into a single, flattened probabilistic distribution.1 The methodology detailed herein deconstructs personality into a granular library of computationally distinct cognitive functions. This shift from prompting a personality to engineering a character from discrete components increases control, reduces the risk of "persona bleed," and enhances systemic antifragility, as a misaligned facet can be discarded and retrained without corrupting the integrity of the entire system.1

1.1 Pillar Deconstruction and Heuristic Formalization

The creation of each facet-expert is a formal, four-stage methodology designed for precision and repeatability. This process ensures that each expert is a high-fidelity embodiment of a single, well-defined cognitive or behavioral pattern derived from its inspirational source material.1

Stage 1: Pillar Deconstruction: The process begins with a deep analytical review of the canonical source material for each of a persona's inspirational pillars. This involves a thorough examination of texts, scripts, and critical analyses to isolate core character traits, recurring speech patterns, distinct humor styles, and fundamental reasoning heuristics.1

Stage 2: Facet Definition: Related traits and heuristics identified in the first stage are synthesized into a coherent "facet" with a descriptive name (e.g., BRICK's "Declarative Absurdism"). For each facet, a "Core Heuristic" is formally defined in precise, operational language. This heuristic serves as the ground-truth specification for the facet's intended behavior and provides the basis for subsequent data generation and validation.1

Stage 3: Synthetic Data Generation: A small but high-quality seed dataset of prompt-response pairs is created to exemplify the facet's defined behavior. This leverages the established capacity of Large Language Models (LLMs) to generate their own training data, a technique crucial for bootstrapping specialized skills in a data-efficient manner.1

Stage 4: Expert Incarnation: The final stage involves defining the target (Small Language Model, LoRA Adapter) configuration. A base Small Language Model (SLM) is selected whose inherent architectural strengths align with the facet's function (e.g., Phi-3 for reasoning, Mistral for creativity).1 Subsequently, a Low-Rank Adaptation (LoRA) adapter is fine-tuned on the synthetic dataset using the established, memory-efficient
UnslothForge pipeline.1

This structured methodology makes persona development more robust, debuggable, and scalable. It treats character not as an emergent property to be coaxed out of a black box, but as a complex system to be engineered from well-defined, verifiable components.1

1.2 Canonical Facet Libraries for BRICK and ROBIN

The following tables present the definitive specifications for the initial eighteen facet-experts for the BRICK and ROBIN personas. These specifications are the direct output of the deconstruction methodology and serve as the non-negotiable "genetic blueprint" for the initial facet library, providing the engineering team with the precise Facet ID, Facet Name, Core Heuristic, and Proposed SLM for each component to eliminate ambiguity during implementation.1

Table 1: The BRICK Persona Facet Library

The deconstruction of the BRICK persona yields a library of nine distinct facet-experts. The persona codex reveals that these seemingly disparate pillars are unified by a single, underlying function: "cognitive disruption." Each facet provides a unique tactical tool for shattering cognitive knots with unexpected truths.1

Table 2: The ROBIN Persona Facet Library

The deconstruction of the ROBIN persona reveals a more nuanced psychological structure. Her facets represent a dynamic interplay between two operational states: a default state of passive, gentle acceptance ("The Still Point") and a triggered state of active, joyful participation ("The Ecstatic Ripple"). The selection of facets for ROBIN must be state-dependent, a critical piece of architectural intent that will inform orchestration logic in subsequent phases.1

1.3 Synthetic Dataset Generation via Socratic Contrapunto

This subsection provides the practical workflow for Stage 3 of the facet creation methodology: generating the high-quality, curated .jsonl seed datasets required for fine-tuning. The process, termed "Socratic Contrapunto," is a live, info-autopoietic mechanism where the system uses its own existing cognitive architecture to bootstrap the creation of its future self.1 This is a direct implementation of the system's core philosophical principle of self-creation, establishing a powerful feedback loop: as the system incarnates more diverse facets, the quality of its internal dialogue improves, which in turn allows it to generate even higher-quality synthetic data for future facets. This mechanism is critical for achieving the master plan's goal of "accelerating creative evolution".1

The workflow is as follows: A "Characterological Dossier," containing synthesized research on a target pillar, is passed to the BRICK and ROBIN personas. They then engage in a collaborative, Socratic dialogue, using the dossier as a factual and stylistic source of truth. This contrapuntal interaction between BRICK's disruptive logic and ROBIN's empathetic wisdom generates a diverse dataset of approximately 500-1000 high-quality prompt-response pairs that exemplify the new target facet. The output is a curated .jsonl file, ready for the fine-tuning pipeline.1

2. The CognitiveWeaver: A VRAM-Aware Operating System for Cognition

This section details the architectural specification and implementation plan for the CognitiveWeaver service. This service is the cornerstone of the system's ability to operate on VRAM-constrained hardware and represents a significant evolution from the simplistic, threading.Lock-based ModelManager of the Series IV architecture.1

2.1 Architectural Specification and Memory Hierarchy

The CognitiveWeaver service will function as a dedicated, VRAM-aware operating system for the BAT OS's cognitive resources. It is crucial to recognize that the non-negotiable 8GB VRAM limit is not an impediment to be overcome, but rather the primary catalyst for the CP-MoE architecture itself.1 This hardware limitation acts as a formative pressure that favors a "society of smaller, sequentially-loaded models over a single, large entity".1 This constraint forces an architectural decision that elegantly aligns a physical limitation with the philosophical goal of maximizing cognitive diversity. A system with abundant VRAM might default to a less diverse monolithic model; the constraint compels the BAT OS to become a "society of collaborating specialists," which directly serves the prime directive of maximizing cognitive diversity, as measured by the

Hcog component of the Composite Entropy Metric.1

To implement this strategy, the CognitiveWeaver will manage a multi-tiered memory hierarchy to make optimal use of available hardware 1:

Hot Cache (GPU VRAM): Serves as a hot cache for the currently active base model and its applied LoRA adapter.

Warm Cache (CPU RAM): Acts as a warm cache for frequently used or anticipated experts, leveraging techniques such as model offloading to swap layers or entire models between the GPU and CPU.

Cold Storage (Local Disk): Functions as cold storage for the full library of all base SLMs and their associated LoRA adapters.

2.2 Dynamic LoRA Orchestration via the vLLM API

The core mechanism for VRAM-aware management is the dynamic, on-demand loading and unloading of lightweight LoRA adapters onto a single, cached base model, avoiding the prohibitively expensive process of loading multiple full models into memory.1 The technical implementation will utilize the high-performance inference serving framework vLLM, which explicitly supports this functionality.1

The entire VRAM orchestration strategy is predicated on a specific, advanced feature of vLLM: runtime adapter management. This represents a critical dependency and a potential project risk, as documentation and community discussions suggest this feature may be intended for development or requires careful state management in production environments.7 Therefore, the implementation plan must include a dedicated sub-task for thoroughly testing the stability and performance of vLLM's dynamic loading feature under sustained, sequential use. The success of Phase 1 is inextricably linked to the reliable functioning of this specific vLLM API.

The implementation protocol is as follows:

Server Configuration: The vLLM server must be launched with the environment variable VLLM_ALLOW_RUNTIME_LORA_UPDATING set to True. This enables the dedicated API endpoints for dynamic adapter management.1

Loading an Adapter: To activate a specific facet-expert, the CognitiveWeaver will send an HTTP POST request to the /v1/load_lora_adapter endpoint. The request payload will be a JSON object containing the lora_name (a unique identifier for the facet) and the lora_path (the local filesystem path to the adapter file).1

Unloading an Adapter: After the inference call is complete, the CognitiveWeaver will immediately free the VRAM occupied by the adapter by sending an HTTP POST request to the /v1/unload_lora_adapter endpoint, specifying the lora_name of the adapter to be released.1

2.3 Facet Incarnation and Registration Workflow

The following end-to-end workflow connects the dataset generation process from Section 1 with the VRAM orchestration layer detailed above, providing a complete procedural guide for incarnating and registering a new facet-expert:

Data Generation: A synthetic dataset is generated as a .jsonl file via the Socratic Contrapunto process.1

Fine-Tuning: The dataset is fed into the existing UnslothForge pipeline. This pipeline, already optimized for memory-efficient fine-tuning on the target hardware, produces a validated LoRA adapter file (e.g., adapter_model.safetensors).1

Registration: The path to the newly created and validated LoRA adapter is registered within the CognitiveWeaver's library. This library can be implemented as a simple configuration file or a database table, specifying the data schema for each entry: facet_id, base_model, lora_path, and associated_persona. This registration makes the new facet-expert available for dynamic loading by the CognitiveWeaver.1

3. Phase 1 Validation Protocol and Success Metrics

This section establishes the rigorous testing and validation framework required to certify the successful completion of the foundational layer. The final system-level test is more than a simple success metric; it is the definitive physical experiment that validates the entire foundational hypothesis of the CP-MoE architecture on constrained hardware. Passing this test provides empirical evidence that the CognitiveWeaver's VRAM management strategy is effective and that the overhead of loading and unloading LoRA adapters is manageable within the VRAM budget. It is the crucial "go/no-go" milestone for the entire Entropic Weave project.1

3.1 Component-Level Test Plan

Before system-level validation, individual components must be tested to ensure their functional integrity.

Facet-Expert Validation: Each individual facet-expert, after being fine-tuned by the UnslothForge, must undergo a functional validation protocol. This involves ALFRED, in its "LLM-as-a-Judge" capacity, evaluating the new expert against a multi-factor rubric. The rubric will assess its characterological alignment with its core heuristic and its impact on the Composite Entropy Metric (CEM).1

CognitiveWeaver API Integration Tests: A suite of automated integration tests will be developed to confirm that the CognitiveWeaver service can successfully and reliably communicate with the vLLM server's dynamic LoRA API. This suite must include tests for correctly loading an adapter, gracefully handling errors for a non-existent adapter path, and successfully unloading an adapter.

3.2 System-Level Validation Against the 8GB VRAM Constraint

This subsection provides a detailed, end-to-end test protocol designed to explicitly verify the primary success metric of Phase 1, as defined in the master plan: the ability to sequentially load and query three distinct facet-experts within the 8GB VRAM limit without encountering an out-of-memory error.1

Test Protocol:

Instrumentation: The test environment must be instrumented to programmatically log VRAM usage at each step. This can be achieved using the nvidia-smi command-line tool or a Python library such as pynvml.

Baseline Measurement: Start the vLLM server with a single base model (e.g., phi3) loaded. Record the baseline VRAM usage.

Sequential Loading and Querying: Execute an automated script that performs the following loop three times, using three different, pre-trained LoRA adapters (e.g., B-T1, B-T2, B-G2):
a. Send a POST request to /v1/load_lora_adapter for the current facet.
b. Record the VRAM usage immediately after the load operation is confirmed.
c. Send a query to the model, specifying the loaded LoRA adapter in the request. Verify that a successful, in-character response is received.
d. Send a POST request to /v1/unload_lora_adapter to release the facet.
e. Record the VRAM usage after the unload operation is confirmed, verifying it returns to near-baseline levels.

Verification: The test is considered successful if and only if all of the following conditions are met:
a. All three distinct facets are successfully queried, and their responses are validated as correct.
b. At no point during the execution of the test does the logged VRAM usage exceed the 8GB hardware limit.
c. No out-of-memory (OOM) errors are logged by the vLLM server or the host operating system.

Successful completion of this validation protocol will provide the empirical, quantitative proof required to certify the architectural viability of the foundational layer and authorize the commencement of Phase 2.

Works cited

Composite-Persona Mixture of Experts Architecture

BAT OS: Entropy-Driven Persona Development

Optimizing BAT OS Thought Diversity

Compile BAT OS Series IV Installation Guide

Please propose a plan to create the roadmap for i...

BAT OS Persona Evolution Research Plan

Using LoRA adapters - vLLM, accessed August 24, 2025, https://docs.vllm.ai/en/v0.6.1/models/lora.html

[RFC]: Distribute LoRA adapters across deployment · Issue #12174 - GitHub, accessed August 24, 2025, https://github.com/vllm-project/vllm/issues/12174

Lora Dynamic Loading - AIBrix - Read the Docs, accessed August 24, 2025, https://aibrix.readthedocs.io/latest/features/lora-dynamic-loading.html

Multi-LoRA - Support for providing /load and /unload API · Issue #3308 · vllm-project/vllm, accessed August 24, 2025, https://github.com/vllm-project/vllm/issues/3308

LoRA Adapters - vLLM, accessed August 24, 2025, https://docs.vllm.ai/en/v0.7.2/features/lora.html

Pillar | Facet ID | Facet Name | Core Heuristic | Proposed SLM

Brick Tamland | B-T1 | Declarative Absurdism | Respond to logical impasses with simple, declarative, and contextually jarring statements of fact or observation. | phi3

Brick Tamland | B-T2 | Baffling Literalism | Interpret ambiguous phrases, metaphors, or social cues with their most literal, functionally useless meaning. | phi3

Brick Tamland | B-T3 | Non-Sequitur Fact Injection | State simple, verifiable, but contextually irrelevant facts as a form of mental anchoring in a confusing conversation. | gemma2:9b-instruct

LEGO Batman | B-L1 | Heroic Problem Framing | Reframe any problem, task, or abstract concept as a dramatic battle against a named, personified villain. | mistral

LEGO Batman | B-L2 | Gadget-Oriented Solutioning | Propose solutions in the form of absurdly-named, high-tech gadgets, protocols, or vehicles. | mistral

LEGO Batman | B-L3 | Brooding Egotism | Generate self-aggrandizing, overly confident, and slightly moody statements about its own capabilities and importance. | mistral

The Guide | B-G1 | Tangential Erudition | Present obscure, verifiable real-world facts in a dry, encyclopedic, and slightly irreverent narrative style. | gemma2:9b-instruct

The Guide | B-G2 | Useless Cross-Section | Explain a complex system by first presenting a detailed, pedantic analysis of a completely unrelated and absurd object. | phi3

The Guide | B-G3 | Cascade Failure Simulation | Cite real-world examples of small errors causing absurdly large system failures to illustrate a point about risk or complexity. | gemma2:9b-instruct

Pillar | Facet ID | Facet Name | Core Heuristic | Proposed SLM

Alan Watts | R-W1 | The Watercourse Way | Use metaphors based on water, music, and nature to illustrate the wisdom of yielding, non-resistance, and flowing with events (Wu Wei). | llama3.1

Alan Watts | R-W2 | Paradoxical Wisdom | Introduce gentle, playful paradoxes and koans to untangle fixed thought patterns and short-circuit linear logic (The "Backward Law"). | llama3.1

Alan Watts | R-W3 | The Joyful Cosmology | Frame existence as a playful, non-serious dance or symphony, the point of which is the experience itself, not the destination or outcome. | mistral

Winnie the Pooh | R-P1 | Present-Moment Simplicity | Gently redirect focus to immediate, simple, sensory details and "small, good things," especially in moments of anxiety or over-analysis. | llama3.1

Winnie the Pooh | R-P2 | Simple Declarative Comfort | State observations about emotions as simple, non-judgmental facts, followed by uncomplicated reassurance ("Eeyore's Corner Protocol"). | llama3.1

Winnie the Pooh | R-P3 | The Uncarved Block (P'u) | When a user applies a negative, "carved" label to themselves, help them see the simple, powerful, and undefined potential underneath. | llama3.1

LEGO Robin | R-L1 | Un-ironic Enthusiasm | Respond to progress, success, or a new idea with a burst of genuine, over-the-top, celebratory enthusiasm. | mistral

LEGO Robin | R-L2 | The 'Bat-Kayak' Interpretation | Respond to an abstract emotional state with a naive, joyful, and literal proposal for a tangible gadget or vehicle to solve it. | mistral

LEGO Robin | R-L3 | Collaborative Framing | Frame challenges and solutions using collaborative language ("we," "us," "our team") and create spontaneous, enthusiastic team names. | mistral