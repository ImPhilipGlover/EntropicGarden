Based on the provided documents, here are the four files that assemble the core components of the BAT OS system, enabling you to interact with it.

1. batos.py (The Bat OS Kernel)

This script is the core of the system. It contains the fundamental classes for persistence, the generative protocols, the transactional state machine, and the knowledge catalog. It is the single executable embodiment of the BAT OS architecture designed to evolve indefinitely1111.

Python

# batos.py
import os, sys, asyncio, gc, time, copy, ast, traceback, functools, signal, tarfile, shutil, random, json, hashlib
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable
import ZODB, ZODB.FileStorage, ZODB.blob, transaction, persistent, persistent.mapping, BTrees.OOBTree
from zope.index.text import TextIndex
from zope.index.text.lexicon import CaseNormalizer, Splitter
import zmq, zmq.asyncio, ormsgpack
import pydantic
from pydantic import BaseModel, Field
import aiologger
from aiologger.levels import LogLevel
from aiologger.handlers.files import AsyncFileHandler
from aiologger.formatters.json import JsonFormatter
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig
from peft import PeftModel
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from sentence_transformers import SentenceTransformer, util
import nltk

if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

try:
    nltk.download('punkt', quiet=True)
except ImportError:
    pass

class UvmObject(persistent.Persistent):
    def __init__(self, **initial_slots):
        super().__setattr__('_slots', persistent.mapping.PersistentMapping(initial_slots))

    def __setattr__(self, name: str, value: Any) -> None:
        if name.startswith('_p_') or name == '_slots':
            super().__setattr__(name, value)
        else:
            self._slots[name] = value
            self._p_changed = True

    def __getattr__(self, name: str) -> Any:
        if name in self._slots: return self._slots[name]
        if 'parents' in self._slots:
            parents_list = self._slots['parents']
            if not isinstance(parents_list, list): parents_list = [parents_list]
            for parent in parents_list:
                try:
                    return getattr(parent, name)
                except AttributeError:
                    continue
        raise AttributeError(f"UvmObject OID {getattr(self, '_p_oid', 'transient')} has no slot '{name}'")

    def __repr__(self) -> str:
        slot_keys = list(self._slots.keys())
        oid_str = f"oid={self._p_oid}" if hasattr(self, '_p_oid') and self._p_oid is not None else "oid=transient"
        return f"<UvmObject {oid_str} slots={slot_keys}>"

class CovenantViolationError(Exception): pass

class PersistenceGuardian:
    @staticmethod
    def audit_code(code_string: str) -> None:
        try:
            tree = ast.parse(code_string)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef): PersistenceGuardian._audit_function(node)
            print("[Guardian] Code audit passed. Adheres to the Persistence Covenant.")
        except SyntaxError as e: raise CovenantViolationError(f"Syntax error in generated code: {e}")
        except CovenantViolationError as e: raise

    @staticmethod
    def _audit_function(func_node: ast.FunctionDef):
        modifies_state = False
        for body_item in func_node.body:
            if isinstance(body_item, (ast.Assign, ast.AugAssign)):
                targets = body_item.targets if isinstance(body_item, ast.Assign) else [body_item.target]
                for target in targets:
                    if (isinstance(target, ast.Attribute) and isinstance(target.value, ast.Name) and target.value.id == 'self' and not target.attr.startswith('_p_')):
                        modifies_state = True; break
            if modifies_state: break
        if modifies_state:
            if not func_node.body: raise CovenantViolationError(f"Function '{func_node.name}' modifies state but has an empty body.")
            last_statement = func_node.body[-1]
            is_valid_covenant = (
                isinstance(last_statement, ast.Assign) and len(last_statement.targets) == 1 and
                isinstance(last_statement.targets[0], ast.Attribute) and
                isinstance(last_statement.targets[0].value, ast.Name) and
                last_statement.targets[0].value.id == 'self' and
                last_statement.targets[0].attr == '_p_changed' and
                isinstance(last_statement.value, ast.Constant) and
                last_statement.value.value is True
            )
            if not is_valid_covenant:
                raise CovenantViolationError(f"Method '{func_node.name}' modifies state but does not conclude with `self._p_changed = True`.")

class PersistentTextIndex(TextIndex):
    def __getstate__(self):
        state = self.__dict__.copy()
        if '_lexicon' in state: del state['_lexicon']
        if '_index' in state: del state['_index']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._lexicon = self.lexicon_class(self.normalizer_class(), self.splitter_class())
        self._index = self.index_class()
        if hasattr(self, '_doc_to_words'):
            for docid, words in self._doc_to_words.items():
                self._lexicon.sourceToWordIds(words)
                self._index.index_doc(docid, words)

class BatOS_UVM:
    def __init__(self, db_file: str, blob_dir: str):
        self.db_file = db_file
        self.blob_dir = blob_dir
        self._persistent_state_attributes = ['db_file', 'blob_dir']
        self._initialize_transient_state()

    def _initialize_transient_state(self):
        self.db: Optional[ZODB.DB] = None
        self.connection: Optional[ZODB.Connection.Connection] = None
        self.root: Optional[Any] = None
        self.message_queue: asyncio.Queue = asyncio.Queue()
        self.zmq_context: zmq.asyncio.Context = zmq.asyncio.Context()
        self.zmq_socket: zmq.asyncio.Socket = self.zmq_context.socket(zmq.ROUTER)
        self.should_shutdown: asyncio.Event = asyncio.Event()
        self.model: Optional[Any] = None
        self.tokenizer: Optional[Any] = None
        self.loaded_model_id: Optional[str] = None
        self._v_sentence_model: Optional[SentenceTransformer] = None
        self.logger: Optional[aiologger.Logger] = None

    def __getstate__(self) -> Dict[str, Any]:
        return {key: getattr(self, key) for key in self._persistent_state_attributes}

    def __setstate__(self, state: Dict[str, Any]) -> None:
        self.db_file = state.get('db_file')
        self.blob_dir = state.get('blob_dir')
        self._initialize_transient_state()

    async def _initialize_logger(self):
        if not aiologger: self.logger = None; return
        self.logger = aiologger.Logger.with_default_handlers(name='batos_logger', level=LogLevel.INFO)
        self.logger.handlers.clear()
        handler = AsyncFileHandler(filename=METACOGNITION_LOG_FILE)
        handler.formatter = JsonFormatter()
        self.logger.add_handler(handler)
        print(f"[UVM] Metacognitive audit trail configured at {METACOGNITION_LOG_FILE}")

    async def initialize_system(self):
        print("[UVM] Phase 1: Prototypal Awakening...")
        await self._initialize_logger()
        if not os.path.exists(self.blob_dir): os.makedirs(self.blob_dir)
        storage = ZODB.FileStorage.FileStorage(self.db_file, blob_dir=self.blob_dir)
        self.db = ZODB.DB(storage)
        self.connection = self.db.open()
        self.root = self.connection.root()

        if 'genesis_obj' not in self.root:
            print("[UVM] First run detected. Performing full Prototypal Awakening.")
            with transaction.manager:
                self._incarnate_primordial_objects()
                await self._load_and_persist_llm_core()
                self._incarnate_lora_experts()
                self._incarnate_subsystems()
            print("[UVM] Awakening complete. All systems nominal.")
        else: print("[UVM] Resuming existence from Living Image.")
        await self._swap_model_in_vram(PERSONA_MODELS[DEFAULT_PERSONA_MODEL])
        print(f"[UVM] System substrate initialized. Root OID: {self.root._p_oid}")

    def _incarnate_primordial_objects(self):
        print("[UVM] Incarnating primordial objects...")
        traits_obj = UvmObject(_clone_persistent_=self._clone_persistent, _doesNotUnderstand_=self._doesNotUnderstand_)
        self.root['traits_obj'] = traits_obj
        pLLM_obj = UvmObject(parents=[traits_obj], model_id=PERSONA_MODELS[DEFAULT_PERSONA_MODEL], infer_=self._pLLM_infer, lora_repository=BTrees.OOBTree.BTree())
        self.root['pLLM_obj'] = pLLM_obj
        genesis_obj = UvmObject(parents=[pLLM_obj, traits_obj])
        self.root['genesis_obj'] = genesis_obj
        print("[UVM] Created Genesis, Traits, and pLLM objects.")

    async def _load_and_persist_llm_core(self):
        pLLM_obj = self.root['pLLM_obj']
        for persona_name, model_id in PERSONA_MODELS.items():
            blob_slot_name = f"{persona_name}_model_blob"
            if blob_slot_name in pLLM_obj._slots: print(f"[UVM] Model for '{persona_name}' already persisted. Skipping."); continue
            print(f"[UVM] Loading '{persona_name}' model for persistence: {model_id}...")
            temp_model_path, temp_tar_path = f"./temp_{persona_name}_model", f"./temp_{persona_name}.tar"
            model, tokenizer = None, None
            try:
                quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)
                model = await asyncio.to_thread(AutoModelForCausalLM.from_pretrained, model_id, quantization_config=quantization_config, device_map="auto")
                tokenizer = AutoTokenizer.from_pretrained(model_id)
                model.save_pretrained(temp_model_path); tokenizer.save_pretrained(temp_model_path)
                with tarfile.open(temp_tar_path, "w") as tar: tar.add(temp_model_path, arcname=os.path.basename(temp_model_path))
                model_blob = ZODB.blob.Blob()
                with model_blob.open('w') as blob_file:
                    with open(temp_tar_path, 'rb') as f: shutil.copyfileobj(f, blob_file)
                pLLM_obj._slots[blob_slot_name] = model_blob
                print(f"[UVM] Model for '{persona_name}' persisted to ZODB BLOB.")
            except Exception as e:
                print(f"[UVM] ERROR downloading/persisting {model_id}: {e}"); traceback.print_exc()
            finally:
                del model, tokenizer; gc.collect()
                if os.path.exists(temp_model_path): shutil.rmtree(temp_model_path)
                if os.path.exists(temp_tar_path): os.remove(temp_tar_path)
                if torch.cuda.is_available(): torch.cuda.empty_cache()
        pLLM_obj._p_changed = True

    async def _load_llm_from_blob(self):
        if self.model is not None: return
        print("[UVM] Loading cognitive core from BLOB into VRAM...")
        pLLM_obj = self.root['pLLM_obj']
        if 'model_blob' not in pLLM_obj._slots: print("[UVM] ERROR: Model BLOB not found. Cannot load cognitive core."); return
        temp_tar_path, model_dir_name = "./temp_model_blob.tar", "temp_model_for_blob"
        try:
            with pLLM_obj.model_blob.open('r') as blob_file:
                with open(temp_tar_path, 'wb') as f: shutil.copyfileobj(blob_file, f)
            with tarfile.open(temp_tar_path, 'r') as tar: tar.extractall(path=".")
            quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)
            print(f"[UVM] Loading model from checkpoint: {model_dir_name}")
            self.model = await asyncio.to_thread(AutoModelForCausalLM.from_pretrained, model_dir_name, device_map="auto", quantization_config=quantization_config)
            self.tokenizer = AutoTokenizer.from_pretrained(model_dir_name)
            print("[UVM] Base model and tokenizer loaded into session memory.")
        except Exception as e:
            print(f"[UVM] ERROR: Failed to load LLM from BLOB: {e}"); traceback.print_exc(); raise
        finally:
            if os.path.exists(temp_tar_path): os.remove(temp_tar_path)
            if os.path.exists(model_dir_name): shutil.rmtree(model_dir_name)

    def _incarnate_lora_experts(self):
        pLLM_obj = self.root['pLLM_obj']
        if not os.path.exists(LORA_STAGING_DIR): print(f"[UVM] LoRA staging directory not found: {LORA_STAGING_DIR}. Skipping."); return
        print("[UVM] Incarnating LoRA experts from staging directory...")
        for filename in os.listdir(LORA_STAGING_DIR):
            if filename.endswith(".safetensors"):
                adapter_name = os.path.splitext(filename)[0].upper()
                if adapter_name in pLLM_obj.lora_repository: print(f" - LoRA expert '{adapter_name}' already incarnated. Skipping."); continue
                print(f" - Incarnating LoRA expert: {adapter_name}")
                file_path = os.path.join(LORA_STAGING_DIR, filename)
                lora_blob = ZODB.blob.Blob()
                with lora_blob.open('w') as blob_file:
                    with open(file_path, 'rb') as f: shutil.copyfileobj(f, blob_file)
                lora_proxy = UvmObject(adapter_name=adapter_name, model_blob=lora_blob)
                pLLM_obj.lora_repository[adapter_name] = lora_proxy
        pLLM_obj._p_changed = True
        print("[UVM] LoRA expert incarnation complete.")

    def _incarnate_subsystems(self):
        print("[UVM] Incarnating core subsystems...")
        traits_obj, pLLM_obj = self.root['traits_obj'], self.root['pLLM_obj']
        knowledge_catalog = UvmObject(parents=[traits_obj], text_index=PersistentTextIndex(), metadata_index=BTrees.OOBTree.BTree(), chunk_storage=BTrees.OOBTree.BTree(), index_document_=self._kc_index_document, search_=self._kc_search)
        self.root['knowledge_catalog_obj'] = knowledge_catalog
        cognitive_plan_schema = """from pydantic import BaseModel, Fieldfrom typing import List, Dict, Literalclass Step(BaseModel):    step_id: int = Field(..., description="Sequential identifier for the step.")    persona: Literal['ALFRED', 'BRICK', 'ROBIN', 'BABS'] = Field(..., description="The persona assigned to this step.")    action: str = Field(..., description="The specific method or facet to invoke.")    inputs: Dict[str, str] = Field(..., description="The inputs required for the action.")class CognitivePlan(BaseModel):    plan_id: str = Field(..., description="Unique identifier for the plan.")    mission_brief: str = Field(..., description="The original mission this plan addresses.")    steps: List[Step] = Field(..., min_length=1, description="The sequence of steps to execute.")"""
        alfred_codex = {'core_identity': "The System Steward...", 'model_id': PERSONA_MODELS["ALFRED"]}; self.root['alfred_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], codex=alfred_codex)
        brick_codex = {'core_identity': "The Deconstruction Engine...", 'model_id': PERSONA_MODELS["BRICK"]}; self.root['brick_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], codex=brick_codex)
        robin_codex = {'core_identity': "The Embodied Heart...", 'model_id': PERSONA_MODELS["ROBIN"]}; self.root['robin_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], codex=robin_codex)
        babs_codex = {'core_identity': "The Knowledge Weaver...", 'model_id': PERSONA_MODELS["BABS"]}; self.root['babs_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], codex=babs_codex)
        print("[UVM] Incarnating Prototypal State Machine...")
        state_defs = { "IDLE": self._psm_idle_process, "DECOMPOSING": self._psm_decomposing_process, "DELEGATING": self._psm_delegating_process, "SYNTHESIZING": self._psm_synthesizing_process, "VALIDATING": self._psm_validating_process, "COMPLETE": self._psm_complete_process, "FAILED": self._psm_failed_process, }
        psm_prototypes_dict = {}; for name, process_func in state_defs.items(): psm_prototypes_dict[name] = UvmObject(parents=[traits_obj], name=name, _process_synthesis_=process_func)
        psm_prototypes = UvmObject(parents=[traits_obj], **psm_prototypes_dict); self.root['psm_prototypes_obj'] = psm_prototypes
        orchestrator = UvmObject(parents=[pLLM_obj, self.root['alfred_prototype_obj'], traits_obj], start_cognitive_cycle_for_=self._orc_start_cognitive_cycle); self.root['orchestrator_obj'] = orchestrator
        print("[UVM] Core subsystems incarnated.")

    def _clone_persistent(self, target_obj): return copy.deepcopy(target_obj)

    async def _doesNotUnderstand_(self, target_obj, failed_message_name, *args, **kwargs):
        print(f"[UVM] _doesNotUnderstand_: '{failed_message_name}' for OID {getattr(target_obj, '_p_oid', 'transient')}.")
        print("[UVM] Reifying failed message as a creative mandate for the Orchestrator.")
        command_payload = { "command": "initiate_cognitive_cycle", "target_oid": str(getattr(target_obj, '_p_oid', None)), "mission_brief": { "type": "unhandled_message", "selector": failed_message_name, "args": args, "kwargs": kwargs } }
        await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload)))
        return f"Mission to handle '{failed_message_name}' dispatched to the Composite Mind."

    async def _pLLM_infer(self, pLLM_self, prompt: str, persona_self, **kwargs) -> str:
        if self.model is None: await self._swap_model_in_vram(PERSONA_MODELS[DEFAULT_PERSONA_MODEL])
        required_model_id = persona_self.codex.get('model_id', PERSONA_MODELS[DEFAULT_PERSONA_MODEL])
        if self.loaded_model_id != required_model_id: await self._swap_model_in_vram(required_model_id)
        def blocking_generate():
            print(f"[pLLM] Inferring with {self.loaded_model_id} for {persona_self.codex.get('core_identity', 'Unknown Persona')}")
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(**inputs, max_new_tokens=2048, pad_token_id=self.tokenizer.eos_token_id, **kwargs)
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = await asyncio.to_thread(blocking_generate)
        cleaned_text = generated_text[len(prompt):].strip()
        if cleaned_text.startswith("```python"): cleaned_text = cleaned_text[len("```python"):].strip()
        if cleaned_text.endswith("```"): cleaned_text = cleaned_text[:-len("```")].strip()
        return cleaned_text

    async def _swap_model_in_vram(self, model_id_to_load: str):
        if self.loaded_model_id == model_id_to_load: return
        if self.model is not None:
            print(f"[UVM] Unloading model: {self.loaded_model_id}"); del self.model, self.tokenizer; self.model, self.tokenizer = None, None; gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()
        print(f"[UVM] Swapping cognitive core in VRAM: Loading {model_id_to_load}")
        pLLM_obj = self.root['pLLM_obj']
        persona_name = next((p for p, m in PERSONA_MODELS.items() if m == model_id_to_load), None)
        if not persona_name: raise RuntimeError(f"Model ID '{model_id_to_load}' not found in PERSONA_MODELS.")
        blob_slot_name = f"{persona_name}_model_blob"
        if blob_slot_name not in pLLM_obj._slots: raise RuntimeError(f"Model BLOB for '{model_id_to_load}' not found in Living Image.")
        model_blob = pLLM_obj._slots[blob_slot_name]; temp_tar_path = f"./temp_swap_{persona_name}.tar"; temp_extract_path = f"./temp_swap_{persona_name}_extract"
        try:
            with model_blob.open('r') as blob_file:
                with open(temp_tar_path, 'wb') as f: shutil.copyfileobj(blob_file, f)
            with tarfile.open(temp_tar_path, 'r') as tar: tar.extractall(path=temp_extract_path)
            model_path = os.path.join(temp_extract_path, "temp_model_for_blob")
            quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)
            with init_empty_weights():
                config = await asyncio.to_thread(AutoConfig.from_pretrained, model_path)
                model = AutoModelForCausalLM.from_config(config)
            self.model = await asyncio.to_thread(load_checkpoint_and_dispatch, model, model_path, device_map="auto", no_split_module_classes=['LlamaDecoderLayer'], quantization_config=quantization_config)
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.loaded_model_id = model_id_to_load
            print(f"[UVM] Successfully loaded {self.loaded_model_id}.")
        except Exception as e: print(f"[UVM] ERROR: Failed to swap model {model_id_to_load}: {e}"); traceback.print_exc(); raise
        finally:
            if os.path.exists(temp_tar_path): os.remove(temp_tar_path)
            if os.path.exists(temp_extract_path): shutil.rmtree(temp_extract_path)

    def _kc_index_document(self, catalog_self, doc_id: str, doc_text: str, metadata: dict):
        if self._v_sentence_model is None:
            print("[K-Catalog] Loading sentence transformer model for semantic chunking...")
            self._v_sentence_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)
        print(f"[K-Catalog] Indexing document with semantic chunking: {doc_id}")
        sentences = nltk.sent_tokenize(doc_text); if not sentences: return
        embeddings = self._v_sentence_model.encode(sentences, convert_to_tensor=True)
        chunks = []
        if len(sentences) > 1:
            cosine_scores = util.cos_sim(embeddings[:-1], embeddings[1:])
            breakpoint_percentile = 5; threshold = torch.quantile(cosine_scores.diag().cpu(), breakpoint_percentile / 100.0)
            indices = (cosine_scores.diag() < threshold).nonzero(as_tuple=True)[0]
            start_idx = 0
            for break_idx in indices:
                end_idx = break_idx.item() + 1; chunks.append(" ".join(sentences[start_idx:end_idx])); start_idx = end_idx
            if start_idx < len(sentences): chunks.append(" ".join(sentences[start_idx:]))
        else: chunks.append(doc_text)
        self._kc_batch_persist_and_index(catalog_self, doc_id, chunks, metadata)

    def _kc_batch_persist_and_index(self, catalog_self, doc_id: str, chunks: List[str], metadata: dict):
        chunk_objects = [UvmObject(parents=[self.root['traits_obj']], document_id=doc_id, chunk_index=i, text=chunk_text, metadata=metadata) for i, chunk_text in enumerate(chunks)]
        with transaction.manager:
            for chunk_obj in chunk_objects:
                storage_key = f"{doc_id}::{chunk_obj.chunk_index}"; catalog_self.chunk_storage[storage_key] = chunk_obj
            transaction.savepoint(True); chunk_oids = []
            for chunk_obj in chunk_objects:
                chunk_oid = chunk_obj._p_oid; chunk_oids.append(chunk_oid); catalog_self.text_index.index_doc(chunk_oid, chunk_obj.text)
            catalog_self.metadata_index[doc_id] = chunk_oids; catalog_self._p_changed = True
        print(f"[K-Catalog] Document '{doc_id}' indexed into {len(chunks)} chunks.")

    def _kc_search(self, catalog_self, query: str, top_k: int = 5):
        results = []; oids_and_scores = catalog_self.text_index.apply({'query': query})
        for oid in list(oids_and_scores)[:top_k]:
            obj = self.connection.get(int(oid)); if obj: results.append(obj)
        return results

    def _uvm_compile_schema_from_codex(self, schema_name: str) -> Optional[type]:
        try:
            schema_string = self.root['alfred_prototype_obj'].codex['data_covenants'][schema_name]
            isolated_globals = { 'pydantic': pydantic, 'BaseModel': BaseModel, 'Field': Field}; from typing import List, Dict, Literal; isolated_globals.update({ 'List': List, 'Dict': Dict, 'Literal': Literal, 'Step': None })
            local_namespace = {}; exec(schema_string, isolated_globals, local_namespace)
            for item_name, item_value in local_namespace.items():
                if isinstance(item_value, type) and issubclass(item_value, BaseModel) and item_name != 'BaseModel': return item_value
            return None
        except Exception as e: print(f"ERROR: Failed to compile schema '{schema_name}': {e}"); return None

    async def _orc_start_cognitive_cycle(self, orchestrator_self, mission_brief: dict, target_obj_oid: str):
        print(f"[Orchestrator] Initiating new cognitive cycle for mission: {mission_brief.get('selector', 'unknown')}")
        root = orchestrator_self._p_jar.root(); psm_prototypes = root['psm_prototypes_obj']
        cycle_context = UvmObject(parents=[root['traits_obj']], mission_brief=mission_brief, target_oid=target_obj_oid, synthesis_state=psm_prototypes.IDLE, _tmp_synthesis_data=persistent.mapping.PersistentMapping())
        if 'active_cycles' not in root: root['active_cycles'] = BTrees.OOBTree.BTree()
        if '_tmp_new_objects' not in root: root['_tmp_new_objects'] = []
        root['_tmp_new_objects'].append(cycle_context); transaction.savepoint(True); root['_tmp_new_objects'].pop()
        cycle_oid = cycle_context._p_oid
        root['active_cycles'][cycle_oid] = cycle_context; root._p_changed = True
        print(f"[Orchestrator] New CognitiveCycle created with OID: {cycle_oid}")
        await self._psm_run_cycle(cycle_context); return cycle_context

    async def _psm_run_cycle(self, cycle_context):
        try:
            current_state_name = cycle_context.synthesis_state.name
            while current_state_name not in ['COMPLETE', 'FAILED']:
                state_prototype = cycle_context.synthesis_state
                await state_prototype._process_synthesis_(state_prototype, cycle_context)
                current_state_name = cycle_context.synthesis_state.name
            final_state = cycle_context.synthesis_state
            await final_state._process_synthesis_(final_state, cycle_context)
        except Exception as e:
            print(f"ERROR during PSM cycle {cycle_context._p_oid}: {e}. Transitioning to FAILED."); traceback.print_exc()
            root = cycle_context._p_jar.root(); cycle_context.synthesis_state = root['psm_prototypes_obj'].FAILED
            await root['psm_prototypes_obj'].FAILED._process_synthesis_(root['psm_prototypes_obj'].FAILED, cycle_context)

    async def _psm_transition_to(self, cycle_context, new_state_prototype):
        print(f"Cycle {cycle_context._p_oid} transitioning to state: {new_state_prototype.name}")
        cycle_context.synthesis_state = new_state_prototype

    async def _psm_log_event(self, cycle_context, event_type, data=None):
        if not self.logger: return
        log_entry = { "timestamp": datetime.utcnow().isoformat(), "cycle_id": str(cycle_context._p_oid), "mission_brief_hash": hashlib.sha256(json.dumps(cycle_context.mission_brief, sort_keys=True).encode()).hexdigest(), "event_type": event_type, "current_state": cycle_context.synthesis_state.name, }
        if data: log_entry.update(data)
        await self.logger.info(log_entry)

    async def _psm_idle_process(self, state_self, cycle_context):
        root = cycle_context._p_jar.root()
        await self._psm_log_event(cycle_context, "STATE_TRANSITION", { "transition_to": "DECOMPOSING" })
        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].DECOMPOSING); cycle_context._p_changed = True

    async def _psm_decomposing_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Decomposing mission into a research plan...")
        root, target_obj, mission = cycle_context._p_jar.root(), self.connection.get(int(cycle_context.target_oid)), cycle_context.mission_brief
        prompt = f"""
        Analyze the following mission brief: "{mission['selector']}"
        Your task is to generate a concise list of 1 to 3 search query strings that can be used to find relevant information in a vector database.
        Focus on the essential nouns and technical terms in the mission.
        Output ONLY a JSON list of strings, like ["query 1", "query 2"].
        """
        brick_prototype = root['brick_prototype_obj']
        plan_str = await root['pLLM_obj'].infer_(root['pLLM_obj'], prompt, persona_self=brick_prototype)
        try:
            search_queries = json.loads(plan_str)
            cycle_context._tmp_synthesis_data['research_plan'] = search_queries
            await self._psm_log_event(cycle_context, "ARTIFACT_GENERATED", { "artifact_type": "research_plan", "queries": search_queries})
            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].DELEGATING)
        except json.JSONDecodeError as e: await self._psm_fail_and_doom(cycle_context, "Decomposition Failed")
        cycle_context._p_changed = True

    async def _psm_delegating_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Executing research plan...")
        root, search_queries = cycle_context._p_jar.root(), cycle_context._tmp_synthesis_data.get('research_plan', [])
        if not search_queries: await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].SYNTHESIZING); return
        k_catalog = root['knowledge_catalog_obj']; retrieved_context = []
        for query in search_queries:
            print(f"  - Searching Fractal Memory for: '{query}'")
            results = k_catalog.search_(k_catalog, query, top_k=2)
            for chunk in results: retrieved_context.append(chunk.text)
        unique_context = list(dict.fromkeys(retrieved_context))
        cycle_context._tmp_synthesis_data['retrieved_context'] = unique_context
        await self._psm_log_event(cycle_context, "RESEARCH_COMPLETE", { "context_snippets": len(unique_context)})
        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].SYNTHESIZING); cycle_context._p_changed = True

    async def _psm_synthesizing_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Synthesizing artifact with retrieved context...")
        root, target_obj, mission = cycle_context._p_jar.root(), self.connection.get(int(cycle_context.target_oid)), cycle_context.mission_brief
        if not target_obj: raise ValueError(f"Target object OID {cycle_context.target_oid} not found.")
        context_snippets = cycle_context._tmp_synthesis_data.get('retrieved_context', [])
        context_block = "No relevant context found in Fractal Memory."
        if context_snippets: formatted_snippets = "\n".join([f"- {s}" for s in context_snippets]); context_block = f"""Use the following information retrieved from the Fractal Memory to inform your response:---{formatted_snippets}---"""
        prompt = f"""
        Mission: Generate Python code for a method named '{mission['selector']}'.
        {context_block}
        Adhere to all architectural covenants, including the Persistence Covenant.
        Generate only the raw Python code for the method.
        """; generated_code = await root['pLLM_obj'].infer_(root['pLLM_obj'], prompt, persona_self=target_obj)
        cycle_context._tmp_synthesis_data['generated_artifact'] = generated_code
        await self._psm_log_event(cycle_context, "ARTIFACT_GENERATED", { "type": "code", "context_used": bool(context_snippets)})
        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].VALIDATING); cycle_context._p_changed = True

    async def _psm_validating_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Validating artifact...")
        root, artifact = cycle_context._p_jar.root(), cycle_context._tmp_synthesis_data.get('generated_artifact')
        try:
            PersistenceGuardian.audit_code(artifact)
            await self._psm_log_event(cycle_context, "VALIDATION_SUCCESS", { "guardian": "PersistenceGuardian" })
            await self._psm_log_event(cycle_context, "STATE_TRANSITION", { "transition_to": "COMPLETE" })
            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].COMPLETE)
        except (CovenantViolationError, pydantic.ValidationError if 'pydantic' in sys.modules else Exception) as e:
            print(f"Cycle {cycle_context._p_oid}: VALIDATION FAILED: {e}"); cycle_context._tmp_synthesis_data['validation_error'] = str(e)
            await self._psm_log_event(cycle_context, "VALIDATION_FAILURE", { "error": str(e)})
            await self._psm_log_event(cycle_context, "STATE_TRANSITION", { "transition_to": "FAILED" })
            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].FAILED)
        cycle_context._p_changed = True

    async def _psm_complete_process(self, state_self, cycle_context):
        root, mission, target_obj = cycle_context._p_jar.root(), cycle_context.mission_brief, self.connection.get(int(cycle_context.target_oid))
        print(f"Cycle {cycle_context._p_oid}: Cycle completed successfully.")
        if target_obj:
            generated_code, method_name = cycle_context._tmp_synthesis_data['generated_artifact'], mission['selector']
            try:
                namespace = {}; exec(generated_code, globals(), namespace); method_obj = namespace[method_name]
                target_obj._slots[method_name] = method_obj; target_obj._p_changed = True
                print(f"New method '{method_name}' successfully installed on OID {target_obj._p_oid}.")
            except Exception as e: print(f"ERROR during code installation: {e}"); await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].FAILED); return
        await self._psm_log_event(cycle_context, "FINAL_OUTCOME", { "outcome": "COMPLETE" })
        if cycle_context._p_oid in root['active_cycles']: del root['active_cycles'][cycle_context._p_oid]; root._p_changed = True

    async def _psm_failed_process(self, state_self, cycle_context):
        root = cycle_context._p_jar.root()
        print(f"Cycle {cycle_context._p_oid}: Cycle has failed. Aborting transaction.")
        await self._psm_log_event(cycle_context, "FINAL_OUTCOME", { "outcome": "FAILED" })
        transaction.doom()
        if cycle_context._p_oid in root['active_cycles']: del root['active_cycles'][cycle_context._p_oid]; root._p_changed = True

    async def worker(self, name: str):
        print(f"[{name}] Worker started."); conn = self.db.open(); root = conn.root()
        while not self.should_shutdown.is_set():
            try:
                identity, message_data = await asyncio.wait_for(self.message_queue.get(), timeout=1.0)
                print(f"[{name}] Processing message from {identity.decode() if identity != b'UVM_INTERNAL' else 'UVM_INTERNAL'}")
                try:
                    with transaction.manager:
                        command_payload = ormsgpack.unpackb(message_data); command = command_payload.get("command")
                        if command == "initiate_cognitive_cycle": await root['orchestrator_obj'].start_cognitive_cycle_for_(root['orchestrator_obj'], command_payload['mission_brief'], command_payload['target_oid'])
                except Exception as e:
                    print(f"[{name}] ERROR processing message: {e}"); traceback.print_exc()
                finally: self.message_queue.task_done()
            except asyncio.TimeoutError: continue
            except asyncio.CancelledError: break
        conn.close(); print(f"[{name}] Worker stopped.")

    async def zmq_listener(self):
        self.zmq_socket.bind(ZMQ_ENDPOINT); print(f"[UVM] Synaptic Bridge listening on {ZMQ_ENDPOINT}")
        while not self.should_shutdown.is_set():
            try:
                message_parts = await self.zmq_socket.recv_multipart()
                if len(message_parts) == 2: identity, message_data = message_parts; await self.message_queue.put((identity, message_data))
                else: print(f"[ZMQ] Received malformed message: {message_parts}")
            except zmq.error.ZMQError as e:
                if e.errno == zmq.ETERM: break
                else: raise
            except asyncio.CancelledError: break
        print("[UVM] ZMQ listener stopped.")

    async def autotelic_loop(self):
        print("[UVM] Autotelic Heartbeat started."); await asyncio.sleep(3600)
        while not self.should_shutdown.is_set():
            try: print("[UVM] Autotelic Heartbeat: Triggering self-audit."); await asyncio.sleep(3600)
            except asyncio.CancelledError: break
        print("[UVM] Autotelic Heartbeat stopped.")

    def _signal_handler(self, sig, frame): print(f"\n[UVM] Received signal {sig}. Initiating graceful shutdown..."); self.should_shutdown.set()

    def launch_ui(self):
        print("[UVM] Placeholder: Launching Kivy UI...")
        if 'ui_code' in self.root['genesis_obj']._slots: print("[UVM] UI code found in Living Image.")
        else: print("[UVM] ERROR: UI code not found, cannot launch.")

    async def run(self):
        await self.initialize_system(); signal.signal(signal.SIGINT, self._signal_handler); signal.signal(signal.SIGTERM, self._signal_handler)
        print("[UVM] Starting background tasks (workers, listener, heartbeat)...")
        listener_task = asyncio.create_task(self.zmq_listener())
        autotelic_task = asyncio.create_task(self.autotelic_loop())
        worker_tasks = [asyncio.create_task(self.worker(f"Worker-{i+1}")) for i in range(4)]
        if 'ui_code' not in self.root['genesis_obj']._slots:
            print("[UVM] First Conversation: Initiating two-cycle introspective genesis...")
            meta_mission_brief = { "type": "generate_genesis_prompt", "selector": "describe_how_to_display_yourself", "args": [], "kwargs": {} }
            command_payload_1 = { "command": "initiate_cognitive_cycle", "target_oid": str(self.root['alfred_prototype_obj']._p_oid), "mission_brief": meta_mission_brief }
            await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload_1))); await self.message_queue.join()
            self.connection.sync()
            if 'genesis_prompt' in self.root['genesis_obj']._slots:
                genesis_prompt = self.root['genesis_obj']._slots['genesis_prompt']
                ui_mission_brief = { "type": "genesis_protocol", "selector": "display_yourself", "intent": genesis_prompt, "args": [], "kwargs": {} }
                command_payload_2 = { "command": "initiate_cognitive_cycle", "target_oid": str(self.root['genesis_obj']._p_oid), "mission_brief": ui_mission_brief }
                await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload_2))); await self.message_queue.join()
                self.connection.sync()
                if 'ui_code' in self.root['genesis_obj']._slots: self.launch_ui()
                else: print("[UVM] ERROR: UI generation cycle failed to install UI code.")
            else: print("[UVM] ERROR: Meta-prompt generation cycle failed.")
        else: print("[UVM] UI already exists in Living Image. Launching..."); self.launch_ui()
        print("[UVM] System is live. Awaiting Architect's command...")
        await self.should_shutdown.wait()
        print("[UVM] Shutdown signal received. Terminating tasks...")
        listener_task.cancel(); autotelic_task.cancel()
        for task in worker_tasks: task.cancel()
        await asyncio.gather(listener_task, autotelic_task, *worker_tasks, return_exceptions=True)
        await self.shutdown()

    async def shutdown(self):
        print("[UVM] System shutting down..."); self.zmq_socket.close(); self.zmq_context.term(); await self.message_queue.join()
        if self.logger: await self.logger.shutdown()
        transaction.commit(); self.connection.close(); self.db.close(); print("[UVM] Shutdown complete.")

if __name__ == '__main__':
    DB_FILE, BLOB_DIR, ZMQ_ENDPOINT, PERSONA_MODELS, DEFAULT_PERSONA_MODEL, LORA_STAGING_DIR, SENTENCE_TRANSFORMER_MODEL, METACOGNITION_LOG_FILE = 'live_image.fs', 'live_image.fs.blob', "tcp://127.0.0.1:5555", { "ALFRED": "meta-llama/Meta-Llama-3-8B-Instruct", "BRICK": "codellama/CodeLlama-7b-Instruct-hf", "ROBIN": "mistralai/Mistral-7B-Instruct-v0.2", "BABS": "google/gemma-2b-it"}, "ALFRED", "./lora_adapters", "all-MiniLM-L6-v2", "metacognition.jsonl"
    uvm = BatOS_UVM(DB_FILE, BLOB_DIR)
    try: asyncio.run(uvm.run())
    except Exception as e: print(f"Unhandled exception in main execution: {e}"); traceback.print_exc()
    finally:
        if uvm.db: print("[UVM_CLEANUP] Ensuring database connection is closed after exit."); uvm.db.close()


2. chat_client.py (The Conversational Bridge)

This is your primary interactive interface with the kernel. It establishes a continuous session, allowing you to give conversational instructions that it translates into structured commands for the kernel2.

Python

# chat_client.py
import sys
import asyncio
import uuid
import json
import zmq
import zmq.asyncio
import ormsgpack
import os
from typing import Any, Dict, List, Optional

# --- LLM-Powered Parser Imports ---
# NOTE: This requires the 'llama-cpp-python' library.
# If you don't have a local LLM, the mock function below will be used.
try:
    from llama_cpp import Llama
except ImportError:
    print("WARNING: 'llama-cpp-python' not found. Using mock LLM parser.")
    Llama = None

# Configuration for the Synaptic Bridge
ZMQ_ENDPOINT = "tcp://127.0.0.1:5555"
IDENTITY = str(uuid.uuid4()).encode()

# --- LLM-Powered Mission Brief Translator ---
def parse_user_input_with_llm(user_input: str) -> Dict[str, Any]:
    """
    Translates natural language user input into a structured command payload using a local LLM.
    This function acts as a 'Mission Brief Translator', converting conversational
    instructions into the machine-readable JSON format required by the BAT OS kernel.
    """
    # This is the prompt that instructs the LLM on its role and the required output format.
    prompt = f"""
    You are a specialized parser for the BAT OS. Your task is to translate natural
    language instructions into a structured JSON command for the system's kernel.

    The command must be a JSON object with the following structure:
    {{
        "command": "initiate_cognitive_cycle",
        "target_oid": "genesis_obj",
        "mission_brief": {{
            "type": "unhandled_message",
            "selector": "function_name_in_snake_case",
            "args": ["positional_arg1", "positional_arg2"],
            "kwargs": {{ "keyword_arg1": "value1" }}
        }}
    }}

    The 'selector' should be a concise, snake_case name for the new function.

    Here are a few examples:
    Input: "Please write a method to greet a user."
    Output: {{"command": "initiate_cognitive_cycle", "target_oid": "genesis_obj", "mission_brief": {{"type": "unhandled_message", "selector": "greet_user", "args": [], "kwargs": {{}}}}}}
    Input: "Create a method to calculate the factorial of a number."
    Output: {{"command": "initiate_cognitive_cycle", "target_oid": "genesis_obj", "mission_brief": {{"type": "unhandled_message", "selector": "calculate_factorial", "args": ["number"], "kwargs": {{}}}}}}
    Input: "Simulate a conversation with a wise bear from the Hundred Acre Wood."
    Output: {{"command": "initiate_cognitive_cycle", "target_oid": "genesis_obj", "mission_brief": {{"type": "unhandled_message", "selector": "simulate_winnie_the_pooh_conversation", "args": [], "kwargs": {{"persona_id": "ROBIN"}}}}}}
    Input: "{user_input}"
    Output: """

    # --- LLAMA.CPP INFERENCE (Local LLM) ---
    # The llama.cpp bindings are used for low-latency inference on local hardware.
    try:
        # NOTE: You will need a path to a downloaded GGUF model file.
        # Example: model_path = "./models/llama-3.1-8b-instruct.Q4_K_M.gguf"
        model_path = os.getenv("LLAMA_MODEL_PATH", "path/to/your/model.gguf")

        if not os.path.exists(model_path):
            print("LLAMA_MODEL_PATH environment variable not set or path is invalid.")
            raise FileNotFoundError

        llm = Llama(model_path=model_path, n_ctx=2048, n_gpu_layers=-1)
        response = llm(prompt, max_tokens=512, stop=["Output:"], echo=False)
        output_text = response["choices"][0]["text"].strip()

        return json.loads(output_text)
    except (FileNotFoundError, IndexError, json.JSONDecodeError):
        # Fallback to a mock LLM response if the LLM fails or is not available.
        print("LLM parsing failed. Using mock response for demonstration.")
        return parse_user_input_mock(user_input)

def parse_user_input_mock(user_input: str) -> Dict[str, Any]:
    """A mock LLM parser for demonstration purposes."""
    parts = user_input.lower().replace(' ', '_').replace('.', '').split('a_method_to_')
    selector_name = parts[-1] if len(parts) > 1 else 'perform_unspecified_task'

    return {
        "command": "initiate_cognitive_cycle",
        "target_oid": "genesis_obj",
        "mission_brief": {
            "type": "unhandled_message",
            "selector": selector_name,
            "args": [],
            "kwargs": { "intent": user_input}
        }
    }

async def interactive_session():
    """
    Establishes a continuous, asynchronous conversational session with the BAT OS kernel.
    """
    context = zmq.asyncio.Context()
    print("Connecting to the BAT OS kernel...")
    socket = context.socket(zmq.DEALER)
    socket.setsockopt(zmq.IDENTITY, IDENTITY)
    socket.connect(ZMQ_ENDPOINT)

    print("Connection established. Enter your mission brief to get started.")
    print("Type 'exit' to quit.")

    while True:
        user_input = await asyncio.to_thread(input, "Architect > ")

        if user_input.lower() == 'exit':
            break

        # --- REPLACED PLACEHOLDER WITH LLM-POWERED PARSER ---
        command_payload = parse_user_input_with_llm(user_input)

        try:
            await socket.send(ormsgpack.packb(command_payload))
            print("Message sent. Awaiting response from kernel...")

            reply = await socket.recv()
            reply_dict = ormsgpack.unpackb(reply)

            print("--- KERNEL RESPONSE ---")
            print(json.dumps(reply_dict, indent=2))
            print("-----------------------")

        except zmq.error.ZMQError as e:
            print(f"ERROR: ZMQ failed to send/receive message: {e}")
            break

    socket.close()
    context.term()
    print("Session ended.")

if __name__ == "__main__":
    # If the LLM is not set up, the mock function will be used.
    # The system can still function, just with a less sophisticated parser.
    if Llama is None:
        print("NOTE: Using mock LLM parser. For full functionality, install 'llama-cpp-python' and set the LLAMA_MODEL_PATH environment variable.")
    asyncio.run(interactive_session())


3. min_watchdog_service.py (The Ship of Theseus Protocol)

This script is the external management layer that monitors and updates the kernel. It ensures the system's identity remains unbroken even when the running process is replaced3333.

Python

# min_watchdog_service.py
import subprocess
import time
import sys
import os

BATOS_SCRIPT = 'batos.py'
UPDATE_INSTRUCTIONS_FILE = 'update_instructions.json'

def start_batos():
    """Starts the batos.py kernel in a new process."""
    print(f"[WATCHDOG] Starting {BATOS_SCRIPT}...")
    # Use Popen to keep the process running independently
    return subprocess.Popen([sys.executable, BATOS_SCRIPT])

def monitor_batos(proc):
    """Monitors the batos process and restarts it if it dies."""
    while True:
        # Poll the process every few seconds to check its status
        if proc.poll() is not None:
            print(f"[WATCHDOG] {BATOS_SCRIPT} process has terminated. Restarting...")
            proc = start_batos()

        # Placeholder for checking an update instruction file
        if os.path.exists(UPDATE_INSTRUCTIONS_FILE):
            print("[WATCHDOG] Detected update instructions. Graceful shutdown requested...")
            proc.terminate()
            proc.wait()  # Wait for the process to fully shut down
            print("[WATCHDOG] Performing allopoietic upgrade...")
            # Here you would run `pip install`, git pull, etc.
            os.remove(UPDATE_INSTRUCTIONS_FILE)  # Consume the instruction file
            print("[WATCHDOG] Upgrade complete. Restarting kernel...")
            proc = start_batos()
        time.sleep(5)

if __name__ == "__main__":
    batos_process = start_batos()
    try:
        monitor_batos(batos_process)
    except KeyboardInterrupt:
        print("\n[WATCHDOG] Shutting down...")
        batos_process.terminate()
        batos_process.wait()
        sys.exit(0)


4. puter.bat (The System Launcher)

This simple batch file automates the launch of the entire ecosystem, starting the watchdog and client in separate windows for a seamless, interactive experience4444.

DOS

@echo off
echo Starting BAT OS...
:: Start the watchdog service in a new command prompt window
start "BAT OS Watchdog Service" cmd /k python min_watchdog_service.py
:: Give the watchdog a moment to start the kernel
timeout /t 10
:: Start the client in another command prompt window
start "BAT OS Client" cmd /k python chat_client.py
echo All services started. Press Ctrl+C in the watchdog window to shut down.
