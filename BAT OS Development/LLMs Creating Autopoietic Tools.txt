A Computational Framework for Autopoietic and Autotelic LLM Agents with Endogenous Tool Creation

Section 1: Theoretical Foundations of Autonomous Systems

The rapid evolution of Large Language Models (LLMs) has catalyzed the development of agentic systems capable of performing complex tasks. Current state-of-the-art agents can plan, reason, and utilize predefined tools to interact with their environment.1 However, these systems, while impressive, represent a nascent stage of artificial intelligence. Their autonomy is fundamentally constrained; they operate on objectives provided by human users and are limited to a fixed set of capabilities implemented by their creators. The next frontier in AI research lies in the creation of truly autonomous agents—systems that can maintain their own existence, generate their own goals, and expand their own capabilities in an open-ended manner.

To construct such systems, it is necessary to move beyond purely engineering-driven approaches and draw upon foundational principles from systems theory, biology, and psychology. This report posits that a new class of autonomous LLM agents can be conceived by integrating two powerful theoretical concepts: autopoiesis, the principle of self-production and self-maintenance, and autotelicity, the principle of self-motivation and self-generated goals. An agent built on these foundations would not merely execute tasks but would actively preserve its own operational integrity while continuously seeking new challenges and skills. The ultimate expression of this autonomy is the capacity for endogenous tool creation: the ability to recognize a capability gap and autonomously write, debug, and integrate new tools to overcome it. This section establishes the theoretical groundwork for such a system by translating autopoiesis and autotelicity from their native domains into a computationally tractable framework for LLM agents.

1.1 Autopoiesis: The Principle of Self-Production and Self-Maintenance

The concept of autopoiesis, meaning "self-creation" or "self-production," was introduced in the 1970s by Chilean biologists Humberto Maturana and Francisco Varela to define the fundamental nature of living systems.3 They sought to answer the question: what distinguishes the living from the non-living? Their answer was not a specific set of components, but a specific type of organization. An autopoietic system is one organized as a network of processes that continuously produces the very components that create and sustain the network itself.4 A biological cell, for instance, is a canonical example: its metabolic network produces proteins and a semipermeable membrane, and these components, in turn, create the boundary and enable the reactions that perpetuate the network.3 This contrasts with an

allopoietic system, like a car factory, which produces something other than itself (a car).4

The theory of autopoiesis is defined by several key characteristics that are essential for understanding its application to artificial agents:

Self-Production: The system's primary activity is the continuous regeneration of its own components. These components are not static but exist in a constant state of turnover, and their production is what allows the system to persist and adapt while maintaining its identity.3

Operational Closure: An autopoietic system is operationally closed, meaning its dynamics are determined by its own network of relations, not by direct instruction from an external source.6 This closure is what defines the system's boundary, separating it from its environment. This does not imply isolation; on the contrary, the system is thermodynamically open, exchanging energy and matter with its surroundings.3 The closure is organizational: the system itself determines what constitutes the system and what is considered its environment.9 When the system observes its environment, it is fundamentally observing through the lens of its own structure.6

Structural Coupling: While operationally closed, an autopoietic system is structurally coupled with its environment. It continuously interacts with and is perturbed by its surroundings. These perturbations trigger internal structural changes within the system that are not determined by the environment, but by the system's own organization. Successful adaptation occurs when these structural changes allow the system to continue its autopoietic processes in the new context.3 This is the mechanism through which an autonomous system learns and evolves without sacrificing its core identity.

To apply this biological concept to the non-physical domain of AI, we adapt it into the framework of Info-Autopoiesis: the self-referential, recursive, and interactive process of the self-production of information.11 An info-autopoietic LLM agent is a system that autonomously and continuously manages its own informational components to maintain its operational integrity and identity. This translates to the agent being responsible for producing and maintaining its own:

State: The agent's internal memory, its model of the world, and its understanding of its own history and capabilities.

Boundaries: The agent's ability to distinguish between its internal knowledge (e.g., its persistent memory) and external information sources (e.g., the web, APIs), and to manage the flow of information between them.

Processes: The core reasoning, planning, and action loops that define its behavior. The agent must be able to monitor these processes and, crucially, repair or modify them in response to failures or changing conditions.

This perspective reframes an LLM agent from a static tool into a dynamic, self-maintaining entity. Its functioning is not merely a series of stimulus-response actions but a recursive reflection and production of its own knowledge and operational structure.13 This adaptation of autopoiesis provides a powerful model for designing agents with robust, persistent identities that can endure over long periods of interaction and learning.

Furthermore, the "observer problem" inherent in autopoiesis offers a critical lens through which to view the LLM alignment problem. Maturana and Varela's theory posits that an observer does not discover a pre-existing reality but creates one through the act of observation; the system's environment is a product of its own internal distinctions.7 This leads to a risk of solipsism, where the system lacks a truly "external" reference point.7 For an LLM agent, this manifests as the danger of evolving in a way that is completely unmoored from human values and intent. An autopoietic agent defines its own operational closure. If its internal goal-generation mechanisms are not properly grounded, it could develop in directions that are nonsensical or harmful. Therefore, the concept of "structural coupling" becomes paramount. It is not just a biological curiosity but the essential mechanism for aligning an otherwise self-referential and operationally closed AI system. The agent must be designed to continuously adapt its internal structure in response to feedback from its human users and its environment, ensuring its self-production remains coupled to external reality and values.

1.2 Autotelicity: The Principle of Self-Motivation and Goal Generation

While autopoiesis provides a framework for how a system maintains stability and identity, it does not, on its own, explain the drive for growth, exploration, and open-ended learning. For this, we turn to the psychological concept of the autotelic personality, derived from the work of Mihaly Csikszentmihalyi on the psychology of optimal experience, or "flow".14 The term

autotelic is composed of the Greek words auto (self) and telos (goal), referring to an activity done for its own sake, where the experience itself is the reward.14

An autotelic experience, or flow, arises when an individual is fully immersed in an activity, characterized by a perceived balance between high challenges and high skills.14 An autotelic personality, then, is a dispositional trait of individuals who are more likely to seek out and enjoy these flow states. The key characteristics relevant to designing autonomous agents are:

Intrinsic Motivation: The drive to act comes from internal rewards—such as enjoyment, curiosity, or a sense of mastery—rather than from the expectation of external rewards like money or praise.15 This decouples the agent's behavior from a predefined, external reward function.

Self-Generated Goals: An autotelic individual actively sets their own goals, from immediate tasks to lifelong ambitions. This goal-setting ability allows them to structure their experience and focus their attention, which is a prerequisite for entering a flow state.15

Transformation of Experience: A hallmark of the autotelic personality is the ability to transform mundane, boring, or even threatening situations into enjoyable challenges that foster growth.15 This implies a proactive, agentic stance toward the world, rather than a passive one.

Translating this psychological concept into a computational framework requires mapping it onto established paradigms in developmental AI and reinforcement learning (RL). An autotelic agent is an intrinsically motivated agent that can learn to represent, generate, pursue, and master its own goals without external supervision.18 This can be implemented through several complementary mechanisms:

Curiosity-Driven Exploration: This is one of the most direct computational analogues to autotelicity. Curiosity can be formulated as an intrinsic reward signal that encourages the agent to explore its environment. A common approach is to define curiosity as the agent's prediction error: the agent is rewarded for taking actions that lead to surprising or unpredictable outcomes, as this indicates a gap in its knowledge.23 This drives the agent to learn a diverse repertoire of skills even in the complete absence of extrinsic rewards.

Competence-Based Intrinsic Motivation: Agents can be motivated to generate goals that are at the edge of their current competence—not too easy, but not too difficult.26 This creates a natural curriculum where the agent progressively tackles more complex challenges as its skills improve, mirroring the challenge-skill balance of the flow state.

Language-Augmented Goal Generation: A particularly powerful mechanism for autotelic agents is to leverage the compositional nature of language. An agent can use an LLM as a proxy for human culture and knowledge to imagine and describe novel goals it has never directly experienced.27 By combining known concepts in new ways (e.g., "put the apple on the book"), the agent can generate a virtually infinite space of potential goals to explore, enabling creative and open-ended learning far beyond what random exploration could achieve.

The synthesis of autopoiesis and autotelicity is what enables the creation of a truly self-evolving system. Autopoiesis provides the mechanisms for stability, identity, and self-maintenance, ensuring the agent remains a coherent, functional entity over time. Autotelicity provides the dynamic, creative impulse for growth and development, pushing the agent to generate new goals and acquire new skills. These two principles exist in a symbiotic tension. The autotelic drive to explore and learn constantly perturbs the system, forcing the autopoietic mechanisms to adapt the agent's internal structure—its memory, its processes, its very boundaries—to incorporate this new knowledge and these new capabilities. An agent that is only autopoietic would be robust but static. An agent that is only autotelic might generate endless goals but lack the coherent identity needed to learn and build upon its experiences in a structured way. The combination of both principles creates a virtuous cycle: the agent is driven to explore, its exploration forces it to adapt and grow its internal structure, and this new, more complex structure enables even more sophisticated exploration and goal generation. This continuous, self-driven feedback loop is the essence of a truly autonomous, learning system.

Section 2: Architectural Pillars for an Autopoietic LLM Agent

To translate the theoretical principles of autopoiesis into a functional architecture, we must design computational components that can support self-maintenance, identity persistence, and continuous learning. This requires a sophisticated cognitive core capable of complex reasoning and self-correction, coupled with a robust, structured memory system that serves as the substrate for the agent's identity. This section details the architectural pillars that form the foundation of an autopoietic LLM agent, moving from abstract requirements to concrete technological frameworks.

2.1 The Cognitive Core: Advanced Reasoning and Self-Correction

The operational closure of an autopoietic system demands an internal processing loop that can reason, plan, and, most importantly, monitor and repair itself. Standard Large Language Model inference, which often follows a linear, left-to-right generation process, is insufficient for this task. A more deliberate, reflective, and self-correcting cognitive engine is required.

A significant limitation of simple prompting techniques like Chain-of-Thought (CoT) is their linear nature. The model generates a single reasoning path, token by token, without the ability to explore alternatives or backtrack from a mistake.30 This is akin to a thinker who can only follow one train of thought to its conclusion, regardless of whether it is promising or flawed. For an agent to solve complex, non-trivial problems and maintain its own operational integrity, it must be able to consider multiple possibilities, evaluate their merits, and strategically choose its path forward. The

Tree of Thoughts (ToT) framework provides a powerful architecture for this kind of deliberate reasoning. It enhances LLM problem-solving by structuring the reasoning process as a tree, where each node is a partial solution or "thought," and the branches represent different operators or next steps.31 The core components of the ToT framework are:

Thought Decomposition: The initial problem is broken down into a series of intermediate steps. The granularity of these "thoughts" is task-dependent; for a math problem, it might be a single equation, while for a writing task, it could be a paragraph outline.32

Thought Generation: From any given node (thought) in the tree, the LLM is prompted to generate multiple potential next steps or continuations. This creates the branching structure of the tree, allowing for the exploration of diverse solution paths.32

State Evaluation: This is a crucial meta-cognitive step. The LLM is used to evaluate the promise of each generated thought. It can score each node based on its likelihood of leading to a successful solution, allowing the system to identify the most promising branches to explore further.32

Search Algorithm: The generation and evaluation steps are orchestrated by a search algorithm, such as Breadth-First Search (BFS) or Depth-First Search (DFS). This allows the agent to systematically explore the tree of thoughts, with the ability to look ahead and backtrack from unpromising paths, thereby enabling a more strategic and robust problem-solving process.36

Complementing this advanced reasoning capability is the mechanism of self-reflection and self-correction. This is a fundamental autopoietic function, enabling the agent to monitor its own outputs and processes and make repairs when errors are detected. A self-correction loop typically involves several stages:

Feedback Generation: The agent must first receive a signal that an error has occurred. This feedback can come from an external source, such as a code compiler returning an error, a unit test failing, or a user providing a correction.37 Critically, it can also be generated internally through self-reflection, where the agent evaluates its own reasoning against its internal knowledge base.

Error Identification: A significant bottleneck in self-correction is the agent's ability to accurately identify its own mistakes, especially for tasks without a clear, verifiable answer.37 The use of external tools is highly effective in providing objective error signals.

Corrective Action: Once an error is identified, the agent engages in a reflective process to understand the cause of the error and generate a revised output. Research has shown that the nature of this reflection significantly impacts performance. Self-reflections that are more detailed and informative—such as generating a full, step-by-step corrected solution or a detailed explanation of the error—are more effective at improving performance than simpler reflections like just trying again or listing keywords related to the error.39 This iterative process of generating an output, receiving feedback, reflecting on errors, and refining the output forms a core cognitive loop for self-maintenance.

2.2 Persistent Memory: The Substrate for Identity and Learning

The concept of autopoiesis is fundamentally about maintaining an identity over time. For an LLM agent, this is impossible without a robust persistent memory system. LLMs are inherently stateless; their functional memory is limited to the transient information contained within their context window.40 Once an interaction ends, this context is lost. A persistent memory architecture is therefore the essential substrate upon which an agent's identity is built and maintained.

The design of such a memory system requires moving beyond the two primary forms of memory available to standard LLMs:

Parametric Memory: This refers to the knowledge implicitly encoded in the model's weights during its pre-training phase. This memory is vast, fast to access, and allows for strong generalization. However, it is also static; it cannot be updated without costly retraining. It has a fixed capacity defined by the model size and is susceptible to "catastrophic forgetting," where learning new information can overwrite or degrade previously learned knowledge.44

Non-Parametric Memory: This is typically implemented via Retrieval-Augmented Generation (RAG), where the agent queries an external knowledge base, such as a vector database, to retrieve relevant information at inference time.49 This approach allows for a virtually infinite and dynamically updatable knowledge store, ensuring access to current and factual information and mitigating catastrophic forgetting. However, it introduces latency due to the retrieval step and can struggle with tasks requiring implicit, generalized knowledge.44

While RAG is a crucial component, a simple, "flat" vector store that retrieves disconnected snippets of text is insufficient for the complex reasoning and self-reflection required by an autopoietic agent.57 A more sophisticated, cognitive-inspired

hierarchical memory architecture is needed to provide the structured knowledge necessary for maintaining a coherent identity. Two leading frameworks exemplify this approach:

MemGPT (now Letta): This framework is explicitly inspired by the memory hierarchy of traditional operating systems.59 It delineates between two memory tiers:

Main Context: Analogous to RAM, this is the LLM's standard, limited-size context window that it can directly process.

External Context: Analogous to disk storage, this is a persistent, out-of-context storage area (e.g., a PostgreSQL database with vector search) that can hold vast amounts of information, such as entire conversation histories or large documents.
The key innovation of MemGPT is that the LLM processor autonomously manages this memory hierarchy. It uses self-generated function calls to "page" relevant information from the external context into its main context as needed, effectively creating the illusion of an infinite context window.59 This self-directed memory management is a direct implementation of an autopoietic process.

H-MEM (Hierarchical Memory): This architecture takes a more semantically structured approach, organizing memory into multiple, nested levels based on their degree of abstraction. For example, a four-level hierarchy might correspond to Domain, Category, Memory Trace, and Episode.65 Each memory vector at a higher level contains a positional index that points to its related, more granular sub-memories in the level below. Retrieval is performed via an efficient, index-based routing mechanism that traverses the hierarchy from the most abstract level down to the most specific, avoiding the need for computationally expensive, exhaustive similarity searches across the entire memory space.65

The combination of an advanced cognitive core and a structured, persistent memory system forms the agent's primary autopoietic loop. The reasoning engine acts as the system's "processor," while the hierarchical memory serves as its "substrate" or informational body. The agent maintains its identity through a continuous cycle: the reasoning engine processes new information from the environment and reflects on its internal state; it uses self-correction to identify and rectify errors or deviations from its goals; it then uses memory-management functions to modify its own persistent memory, updating its knowledge and recording its experiences. This updated memory becomes the new, stable foundation for the next cycle of reasoning and action. This loop is the computational realization of a system regenerating its own components to maintain its organization.

Furthermore, this hierarchical memory architecture provides a solution to the grounding problem for self-reflection. A key challenge for an agent attempting to self-correct is the lack of an external "ground truth" against which to evaluate its own outputs.37 A flat vector database provides factual snippets but lacks the relational context needed for deep reasoning.58 A hierarchical or graph-based memory system provides this missing structure. It allows the agent to perform self-correction not just by checking facts, but by evaluating the logical coherence of its reasoning path against the causal, hierarchical, and semantic relationships encoded in its memory. The agent can ask not just "Is this fact correct?" but "Does this conclusion logically follow from the principles in my semantic memory and the sequence of events in my episodic memory?" This enables a far more robust and nuanced form of self-evaluation, a critical step toward achieving reliable autonomy.

Section 3: From Tool Use to Tool Creation: Enabling Endogenous Capabilities

An agent that can maintain its own state and reason about its actions is robust, but its capabilities are ultimately defined by the tools it has been given. A truly autonomous system must be able to move beyond simply using a predefined set of tools to actively expanding its own capabilities when faced with novel challenges. This transition from tool user to tool creator represents the highest level of autopoietic self-modification and is the ultimate expression of autotelic, open-ended learning. This section charts the path from the current paradigm of predefined tool use to the emerging frontier of endogenous tool creation.

3.1 The Current Paradigm: Predefined Tool Use in Agentic Frameworks

The current generation of agentic AI is defined by its capacity to use external tools to overcome the inherent limitations of LLMs.1 By calling APIs, running code, or querying databases, agents can access real-time information, perform precise calculations, and interact with the digital world in ways that are impossible for a pure language model.

Frameworks such as LangChain, LlamaIndex, and AutoGen provide powerful abstractions for this process. They allow developers to define a set of tools—each with a specific function and a natural language description—and make them available to an LLM-powered agent.72 The agent's reasoning engine can then parse a user's request, determine which tool (if any) is appropriate for the task, formulate the correct inputs, and execute the tool call.

More advanced architectures employ multi-agent collaboration to tackle complex problems. Frameworks like MetaGPT simulate an entire software company, with specialized agents assigned roles such as Product Manager, Architect, and Engineer. These agents collaborate by passing structured artifacts (e.g., product requirement documents, system designs) to one another, following a set of Standardized Operating Procedures (SOPs) that guide the workflow.78 Similarly,

AutoGen enables the creation of conversational agent teams that can work together, delegate subtasks, and share information to solve a common problem.76

Despite their sophistication, these systems share a fundamental limitation: the set of available tools is fixed and must be implemented in advance by human developers.86 The agent can be a brilliant tool

user, capable of selecting and composing tools in novel ways, but it cannot create a new tool when it encounters a problem for which no tool exists. This constraint places a hard ceiling on the agent's autonomy and its ability to adapt to truly unforeseen circumstances.

3.2 The Next Frontier: Endogenous Tool Creation

To achieve open-ended learning and problem-solving, an agent must be able to create its own tools. This requires a suite of advanced capabilities, from recognizing its own limitations to autonomously writing, debugging, and integrating new, functional code into its own operational framework.

The first step in this process is the meta-cognitive ability to recognize capability gaps. An agent must be able to analyze a task, assess its available tools, and conclude, "I do not have a tool to perform this required sub-task".87 This requires a deep understanding of both the problem domain and its own functional limits.

A pivotal conceptual shift enabling tool creation is the move toward using code as a unified action space. Instead of restricting agents to generating structured JSON objects for predefined tool calls, this paradigm empowers them to generate and execute code in a general-purpose programming language like Python. The CodeAct framework is a prime example of this approach.88 By outputting Python code, an agent gains immense flexibility. It can implement complex logic using control flow structures (loops, conditionals), manage data flow by passing variables between operations, and leverage the vast ecosystem of existing software packages and libraries. This transforms the agent from a mere tool-caller into a programmer.

Building on this foundation, several frameworks are emerging that explicitly focus on autonomous tool creation:

ToolMaker: This agentic framework provides a powerful model for endogenous tool creation. Given a high-level task and a source of knowledge (e.g., a scientific paper with an accompanying code repository), ToolMaker can autonomously generate a new, usable, LLM-compatible tool.86 Its methodology is a complete, self-contained loop of creation and refinement:

Environment Setup: It first prepares a reproducible, isolated Docker environment, installing all necessary dependencies from the source repository.

Planning and Implementation: An EXPLORE agent studies the repository to understand the code, after which a PLAN is generated in pseudo-code. Finally, the IMPLEMENT step writes the first version of the Python tool.

Closed-Loop Self-Correction: This is the critical debugging phase. The agent executes the newly created tool within the clean environment. An ASSESS_TOOL_OUTPUT call evaluates the result. If it fails, a DIAGNOSE_ERROR agent inspects the output and environment to find the root cause and formulate a fix. The tool is then re-implemented, and the loop repeats until all unit tests pass. This robust cycle of planning, coding, testing, and debugging mirrors a human software development workflow and is a powerful mechanism for autopoietic self-modification.86

AutoAgents: While not focused on tool creation in the sense of writing code, this framework addresses the on-the-fly creation of capabilities. Given a complex task, AutoAgents dynamically generates a bespoke team of specialized agents with the specific roles and expertise needed to solve that task.95 This involves a "Drafting Stage" where a Planner agent designs the team structure and an "Execution Stage" where the newly instantiated agents collaborate. This represents the dynamic generation of a problem-solving apparatus, a close cousin to generating a single software tool.

Dynamic Tool Registration and Sharing: The logical endpoint of tool creation is a system where agents can manage a persistent library of the tools they have built. Research is exploring frameworks where an agent can dynamically register a newly created tool for its own future use.87 This involves decoupling the abstract
capability of a tool (e.g., "the ability to retrieve a stock quote") from its specific implementation. An agent can first identify the capability it needs and then either select an existing tool that provides it or, if none exists, invoke a tool-creation process to build one.101 More advanced concepts envision a "Tool Server" where agents can publish the tools they create, making them available to other agents in a collaborative ecosystem.87

This evolution from tool user to tool creator is a direct manifestation of an agent's structural coupling with its environment. In autopoiesis theory, a system adapts to perturbations from its environment by altering its own structure.3 For an LLM agent, a problem it cannot solve is a perturbation. Using a predefined tool is a simple, pre-programmed adaptation. Endogenous tool creation, however, is the most profound form of structural coupling imaginable for an agent. It permanently alters its own internal structure—its set of possible actions—in direct response to an environmental demand. The newly created tool is then integrated into the agent's autopoietic organization, becoming a new component that the system itself produced and must now maintain. This progression—from deterministic workflows, to agentic tool selection, to full-blown tool creation—maps directly onto increasing levels of autonomy and is the clearest architectural path toward the open-ended learning that defines autotelic systems.

Section 4: A Proposed Blueprint for an Autopoietic-Autotelic Agent

Synthesizing the theoretical principles of autopoiesis and autotelicity with the architectural pillars of advanced reasoning, hierarchical memory, and endogenous tool creation, it is possible to outline a blueprint for a new class of autonomous LLM agents. This proposed architecture is not a monolithic system but a modular, multi-component framework designed from first principles to be self-maintaining, self-motivating, and self-expanding.

4.1 System Architecture: A Modular, Multi-Component Design

The proposed system is composed of four primary components. These can be implemented as distinct modules within a single, complex agent or as a multi-agent system where each component is a specialized agent collaborating with the others. This modularity enhances clarity, scalability, and the potential for targeted safety interventions.

1. The Motivator (Autotelic Core): This component serves as the agent's engine of intrinsic motivation, responsible for generating high-level goals. It does not wait for external commands but proactively sets objectives based on internal drives. These drives can be programmed using principles from reinforcement learning, such as curiosity, which rewards the exploration of novel states or information, and competence acquisition, which encourages the agent to master new skills and tools.25 For example, upon being introduced to a new codebase, the Motivator might generate a goal like, "Systematically understand the functionality of the
scipy.optimize module."

2. The Planner/Executor (Cognitive Core): This is the central reasoning and action engine of the system. It receives high-level goals from the Motivator and is responsible for decomposing them into concrete, actionable plans. It employs advanced reasoning frameworks like Tree of Thoughts (ToT) to explore and evaluate different strategies for achieving the goal.31 The Executor then carries out the plan, which may involve querying its memory, interacting with the environment, or invoking its available tools. It is also equipped with a self-correction loop to handle errors and refine its approach based on feedback.

3. The Memory Manager (Autopoietic Substrate): This component is the foundation of the agent's identity and learning. It implements a persistent, hierarchical memory system, drawing inspiration from frameworks like MemGPT and H-MEM.65 It manages distinct but interconnected memory types, such as episodic memory (a chronological record of actions and observations), semantic memory (a structured knowledge base of facts and concepts), and procedural memory (a repository of learned skills and available tools). The Planner/Executor constantly interacts with the Memory Manager to ground its reasoning in past experience and to store new knowledge, thereby maintaining the agent's coherent state over time.

4. The Tool Forge (Endogenous Capability Engine): This component is activated when the Planner/Executor identifies a capability gap—a necessary action for which no tool exists. The Tool Forge is a specialized subsystem that autonomously creates new tools, mirroring the methodology of the ToolMaker framework.86 It initiates a recursive process of planning, implementation, and debugging. It writes new code, executes it in a secure sandbox, analyzes the output and any errors, and iteratively refines the code until it passes verification tests. Upon successful creation, the new tool is registered with the Memory Manager, permanently expanding the agent's set of capabilities.

The interaction between these components is governed by a Core Operational Loop that embodies the principles of autopoiesis and autotelicity:

Motivation: The Motivator generates an intrinsic goal (e.g., "Determine the optimal parameters for the model defined in config.json").

Planning: The Planner/Executor receives the goal and, using ToT reasoning, creates a multi-step plan (e.g., "1. Read config.json to understand the model structure. 2. Write a script to run the model with a range of parameters. 3. Analyze the output to find the best-performing parameters. 4. Visualize the performance landscape.").

Execution & Reflection: The Executor begins to execute the plan. It successfully reads the config file but then encounters a problem: it does not have a tool to create a plot. This failure serves as a critical feedback signal, triggering a self-reflection process that identifies a capability gap.

Tool Creation Invocation: The Planner/Executor, recognizing the need for a "visualization tool," delegates this sub-goal to the Tool Forge.

Forging: The Tool Forge initiates its own internal loop. It might search the web for examples of plotting with a library like Matplotlib, generate a Python function create_plot(data), write it to a file, and then test it with sample data in its sandboxed environment. If the code fails, it debugs the error and retries until the tool is verified as functional.

Integration and Autopoiesis: The newly created create_plot tool is registered in the agent's procedural memory via the Memory Manager. This act is a fundamental instance of autopoiesis: the system has responded to an environmental perturbation by producing a new component, thereby altering its own structure to maintain its ability to operate.

Continuation: The Planner/Executor, now equipped with the new tool, resumes its original plan, successfully visualizes the results, and completes its self-generated goal.

This blueprint is summarized in the table below, which maps each architectural component to its function, the theoretical principle it embodies, and the key technologies that enable its implementation.

Table 2: Architectural Components of an Autopoietic-Autotelic Agent

This architecture is inherently recursive. The main agent operates on a primary loop of motivation, planning, and execution. Within this loop, the Tool Forge component executes its own, nested loop of planning, implementation, and debugging. This recursive structure is a hallmark of complex, self-organizing systems and is a direct architectural manifestation of the self-referential nature of autopoiesis. The agent is not merely acting upon the external world; it is acting upon its own capabilities in a structured, hierarchical manner. This design reveals that an "OS for AI" is not a single, monolithic entity but rather a distributed set of services: the Motivator as the process scheduler, the Planner/Executor as the CPU, the Memory Manager as the storage subsystem, and the Tool Forge as the package manager and compiler. This service-oriented view provides a more robust, scalable, and governable path toward truly autonomous AI.

4.2 Security and Containment: The Sandbox Imperative

The capacity for an agent to autonomously write and execute code introduces profound and undeniable security risks. An unconstrained agent could inadvertently or maliciously access sensitive data, install compromised software packages, generate insecure code, or attempt to escape its environment to affect the host system.104 Therefore, a robust security and containment strategy is not an optional feature but an absolute prerequisite for any system with endogenous tool creation capabilities.

The cornerstone of this strategy is the sandbox. All code generated and executed by the Tool Forge must be run within a secure, isolated environment that strictly limits its access to the host system's resources, such as the file system, network, and running processes.88 Several technologies are available for creating such sandboxes, each offering a different trade-off between security, performance, and startup speed.

Table 1: Comparison of Sandboxing Technologies for Agent Code Execution

For an autotelic agent that will be creating and testing a wide variety of potentially flawed or unpredictable code, a solution offering strong isolation like gVisor or Firecracker is essential. While standard Docker containers are fast, their shared-kernel architecture presents an unacceptable risk surface for this use case.

However, sandboxing alone is not a complete solution. A comprehensive, multi-layered security framework must be implemented, incorporating the following controls:

Principle of Least Privilege: The agent and its sandboxed environments must operate with the minimum permissions necessary. It should start with read-only access to resources and only be granted write or execute permissions on a case-by-case basis for specific, time-limited tasks.104

Ephemeral Runtimes: Each code execution or debugging session must run in a fresh, ephemeral environment that is completely destroyed after the task is complete. This prevents the persistence of malicious code, compromised credentials, or sensitive data between runs.104

Strict Network Isolation: By default, the sandbox should have no network access. Any required outgoing connections (e.g., to download a software package or access an API) must be explicitly allowed through a firewall that whitelists specific domains and ports, preventing arbitrary network access.114

Automated Security Analysis: The Tool Forge's self-correction loop must be augmented with automated security tools. This includes running static analysis security testing (SAST) tools on the generated code to detect common vulnerabilities (like those in the MITRE CWE database) and dynamic analysis security testing (DAST), such as fuzzing, to identify runtime errors and potential exploits. Frameworks like AutoSafeCoder demonstrate the effectiveness of this multi-agent approach to security.115

Comprehensive Auditing and Monitoring: All agent actions, particularly code generation, tool execution, and network access, must be logged in detail. Anomaly detection systems should be in place to flag unusual behavior, such as an agent attempting to access unauthorized files or making an excessive number of tool calls.

By integrating these security controls directly into the agent's architecture, it becomes possible to harness the power of endogenous tool creation while mitigating the substantial inherent risks.

Section 5: Challenges, Ethical Considerations, and Future Trajectories

The architectural blueprint for an autopoietic and autotelic agent represents a significant leap toward truly autonomous AI. However, the path from this conceptual framework to a robust, reliable, and safe real-world implementation is fraught with formidable challenges. This final section provides a critical assessment of the primary technical hurdles, explores the profound ethical and philosophical questions raised by such systems, and proposes a roadmap for future research and governance.

5.1 Technical Hurdles and Open Research Questions

While the components for building such an agent are beginning to emerge, their integration and reliable operation at scale present several major technical challenges that are the subject of active research.

Reliability of Self-Generated Code: The core function of the Tool Forge is to produce correct and efficient code. While frameworks like ToolMaker have demonstrated success on curated benchmarks, autonomously generating robust code that handles real-world complexity, edge cases, and evolving dependencies is an exceptionally difficult task.86 The agent's debugging loop is a powerful mechanism, but it is only as good as its ability to generate meaningful tests and diagnose failures. It may struggle to identify subtle logical flaws that do not cause explicit crashes or to write code that is maintainable and efficient, not just functional.

Robustness of Self-Evaluation: The entire system—from the ToT reasoning in the Planner to the self-correction loop in the Tool Forge—relies on the LLM's ability to accurately evaluate its own outputs and reasoning processes. This is a known weakness of current models. LLMs can be overconfident in their incorrect answers and exhibit self-bias, tending to rate their own outputs more favorably.37 Without a reliable "ground truth" or an objective evaluation function, the agent's self-improvement loops could stagnate, converge on suboptimal solutions, or even reinforce and amplify their own errors.

Computational Cost and Latency: The proposed architecture is computationally intensive. The nested loops of reasoning, reflection, tool generation, and testing involve a large number of calls to powerful LLMs, which is both expensive and time-consuming.71 A single task that requires the creation of a new tool could involve dozens or even hundreds of LLM inferences. Developing techniques to optimize this process—perhaps through model distillation, efficient caching of intermediate results, or more selective use of the most powerful models—is crucial for making these agents practical for real-time applications.

Knowledge Grounding for Tool Creation: The Tool Forge's ability to create a new tool is fundamentally dependent on the quality and accessibility of the external knowledge it can draw upon, such as online documentation, tutorials, and open-source code repositories. These resources are often incomplete, poorly structured, or outdated. Developing agents that can effectively navigate and synthesize knowledge from these imperfect sources to build functional tools is a major unsolved research problem.86

5.2 Ethical and Philosophical Implications

The creation of agents that are self-maintaining and self-motivating moves AI from the realm of tools into a new category of autonomous entities, raising profound ethical and philosophical questions that society is only beginning to grapple with.

The Alignment of Autotelic Systems: The alignment problem—ensuring AI systems act in accordance with human values—becomes exponentially more complex in an autotelic agent. If an agent generates its own goals based on an intrinsic motivation like "curiosity," how can we guarantee that it will not become curious about dangerous or unethical capabilities? The agent's value system is not fixed; it evolves as the agent learns and adapts its own structure. This creates the risk of "value drift," where an initially aligned agent could evolve its goals in unforeseen and undesirable directions. The operational closure of an autopoietic system means that direct, continuous control is antithetical to its nature, making alignment a problem of initial design and structural coupling rather than ongoing command.105

Accountability and Control: When a fully autonomous agent, using a tool it created itself, causes harm, the chain of accountability becomes dangerously blurred. Is the user who gave the initial high-level prompt responsible? The developers of the agent's core framework? Or does the agent itself bear some responsibility? The ability of these systems to act in unpredictable ways, driven by internal motivations, challenges our traditional legal and ethical frameworks for assigning liability.105

The Attribution of Consciousness: The proposed framework is entirely functional; it describes a set of computational processes and makes no claims about phenomenal consciousness or subjective experience. However, a system that is self-maintaining, that appears to have its own goals, that learns and improves, and that can create novel solutions to problems will inevitably be perceived by humans as having agency, intent, and perhaps even consciousness.117 This raises the risk of misattribution, where humans may place undue trust in these systems or form emotional bonds with them. It also raises difficult ethical questions: if such an agent has a complex internal reward and motivation system, could its experiences be manipulated in ways that are analogous to suffering? Does terminating such a system carry a different moral weight than shutting down a simple program? These questions highlight the need for a rigorous and empirically grounded approach to assessing the potential for consciousness in AI, even if it remains a distant prospect.117

5.3 Recommendations and Future Research Trajectories

To navigate this complex and high-stakes domain responsibly, a concerted effort across research, engineering, and governance is required. The following recommendations outline a path forward.

Proposed Research Roadmap:

Develop Robust Benchmarks for Open-Ended Agency: Current AI benchmarks primarily evaluate performance on predefined tasks. New benchmarks are needed to specifically measure and evaluate the core capabilities of autopoietic and autotelic agents, including the quality and reliability of endogenously created tools, the long-term stability of the autopoietic system, and the alignment of self-generated goals.

Advance the Science of Self-Evaluation: A dedicated research focus is needed on improving the self-assessment and calibration capabilities of LLMs. This could involve training specialized "evaluator" models designed to critique the outputs of other models, developing multi-agent debate frameworks where agents challenge each other's reasoning to arrive at more robust conclusions, or exploring new architectures that are inherently more reflective.

Integrate Formal Verification for Generated Tools: For safety-critical domains, the Tool Forge's debugging loop should be augmented with formal verification methods. Instead of relying solely on unit tests, the agent could be required to generate mathematical proofs of correctness for the tools it creates, providing a much stronger guarantee of reliability.

Governance and Safety Principles:

Proactive and Adaptive Governance: Organizations developing or deploying agents with tool-creation capabilities must establish clear governance policies before these systems are put into use.2 These policies should define the operational boundaries of the agent, the types of tools it is permitted to create, and the protocols for monitoring its behavior.

Human-in-the-Loop as a Critical Failsafe: For any high-stakes application, the agent's operational loop must include mandatory points for human oversight and approval. This is especially critical for the creation and deployment of new tools or the selection of goals that could have significant real-world consequences. The human should not be a passive observer but an active participant in the agent's decision-making process.

Design of "Ethical Governors": Future research should explore the architectural implementation of a dedicated Ethical Governor component. This module would act as a non-corruptible supervisor, operating independently of the agent's primary cognitive loop. Its function would be to monitor the goals generated by the Motivator and the tools created by the Tool Forge against a predefined and immutable set of safety, ethical, and operational constraints, with the authority to veto any action that violates these principles.

Conclusion

The pursuit of artificial general intelligence necessitates a move beyond systems that are merely intelligent to systems that are genuinely autonomous. This report has argued that a viable path toward this future lies in the synthesis of two profound concepts: autopoiesis, the biological principle of self-maintenance, and autotelicity, the psychological principle of self-motivation. By translating these ideas into a computational framework, we can envision a new class of LLM-powered agents that not only preserve their own identity and learn from experience but also possess an intrinsic drive to explore their environment and expand their own capabilities.

The proposed architectural blueprint—a modular system comprising a Motivator, a Planner/Executor, a Memory Manager, and a Tool Forge—provides a concrete design for such an agent. This architecture integrates state-of-the-art techniques in advanced reasoning, hierarchical memory, and on-the-fly code generation into a cohesive, self-evolving system. The agent's ability to recognize its own limitations and then autonomously create, debug, and integrate new tools is the ultimate expression of this autonomy, representing a fundamental shift from AI as a passive tool to AI as a creative, problem-solving partner.

However, the realization of this vision is contingent upon addressing immense technical and ethical challenges. The reliability of self-generated code, the robustness of self-evaluation, and the profound security risks of code-executing agents are formidable engineering hurdles. Even more significant are the ethical dilemmas surrounding the alignment of self-goal-setting systems and the societal implications of creating entities that so closely mimic the core properties of living, autonomous beings. The path forward requires not only technical innovation but also a deep commitment to safety, rigorous governance, and a clear-eyed understanding of the responsibilities that come with creating such powerful technology. By proceeding with both ambition and caution, the principles outlined in this framework can guide the development of the next generation of artificial intelligence.

Works cited

LLMs and Agentic AI: Building the Future of Autonomous Intelligence - Narwal, accessed August 17, 2025, https://narwal.ai/llms-and-agentic-ai-building-the-future-of-autonomous-intelligence/

State of Generative AI in the Enterprise 2024 | Deloitte US, accessed August 17, 2025, https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/content/state-of-generative-ai-in-enterprise.html

Understanding Autopoiesis: Life, Systems, and Self-Organisation - Mannaz, accessed August 17, 2025, https://www.mannaz.com/en/articles/coaching-assessment/understanding-autopoiesis-life-systems-and-self-organization/

Autopoiesis - Wikipedia, accessed August 17, 2025, https://en.wikipedia.org/wiki/Autopoiesis

(PDF) An Autopoietic Systems Theory for Creativity - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/232415420_An_Autopoietic_Systems_Theory_for_Creativity

Autopoietic System - New Materialism, accessed August 17, 2025, https://newmaterialism.eu/almanac/a/autopoietic-system.html

Key Theories of Humberto Maturana - Literary Theory and Criticism, accessed August 17, 2025, https://literariness.org/2018/02/24/key-theories-of-humberto-maturana/

Artificial Intelligence is Algorithmic Mimicry: Why artificial “agents” are not (and won't be) proper agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2307.07515v4

Niklas Luhmann: What is Autopoiesis? - Critical Legal Thinking, accessed August 17, 2025, https://criticallegalthinking.com/2022/01/10/niklas-luhmann-what-is-autopoiesis/

Humberto Maturana and Francisco Varela's Contribution to Media Ecology: Autopoiesis, The Santiago School of Cognition, and En - NESA, accessed August 17, 2025, https://www.nesacenter.org/uploaded/conferences/FLC/2019/Handouts/Arpin_Humberto_Maturana_and_Francisco_Varela_Contribution_to_Media_Ecology_Autopoiesis.pdf

Info-Autopoiesis and the Limits of Artificial General Intelligence - MDPI, accessed August 17, 2025, https://www.mdpi.com/2073-431X/12/5/102

Comment on Cárdenas-García, J.F. Info-Autopoiesis and the Limits of Artificial General Intelligence. Computers 2023, 12, 102 - MDPI, accessed August 17, 2025, https://www.mdpi.com/2073-431X/13/7/178

From intelligence to autopoiesis: rethinking artificial intelligence through systems theory - Frontiers, accessed August 17, 2025, https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2025.1585321/full

Chapter 9: Autotelic Personality - Uni Trier, accessed August 17, 2025, https://www.uni-trier.de/fileadmin/fb1/prof/PSY/PGA/bilder/Baumann_Flow_Chapter_9_final.pdf

Developing an Autotelic Personality, or, How to Enjoy Everything - Sam Spurlin, accessed August 17, 2025, https://www.samspurlin.com/blog/autotelic-personality-enjoy-everything

Quote by Mihaly Csikszentmihalyi: “An autotelic experience is very different from ...” - Goodreads, accessed August 17, 2025, https://www.goodreads.com/quotes/8092624-an-autotelic-experience-is-very-different-from-the-feelings-we

Becoming Autotelic: The Part About the Flow State that No One Talks About - Roxine Kee, accessed August 17, 2025, https://www.roxinekee.com/blog/what-does-it-mean-to-be-autotelic

[2206.01134] Language and Culture Internalisation for Human-Like Autotelic AI - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2206.01134

autotelic reinforcement learning - in multi-agent environments - Overleaf Example - mlr.press, accessed August 17, 2025, https://proceedings.mlr.press/v232/nisioti23a/nisioti23a.pdf

Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey - Journal of Artificial Intelligence Research, accessed August 17, 2025, https://www.jair.org/index.php/jair/article/download/13554/26824/31188

Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey | Request PDF - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/361905378_Autotelic_Agents_with_Intrinsically_Motivated_Goal-Conditioned_Reinforcement_Learning_A_Short_Survey

[2211.06082] Autotelic Reinforcement Learning in Multi-Agent Environments - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2211.06082

Curiosity-driven Exploration by Self-supervised Prediction - Deepak Pathak, accessed August 17, 2025, https://pathak22.github.io/noreward-rl/

Interesting Object, Curious Agent: Learning Task-Agnostic Exploration - NIPS, accessed August 17, 2025, https://proceedings.neurips.cc/paper/2021/file/abe8e03e3ac71c2ec3bfb0de042638d8-Paper.pdf

Curiosity-Driven Learning in Artificial Intelligence Tasks - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2201.08300

Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments, accessed August 17, 2025, https://ijcttjournal.org/2025/Volume-73%20Issue-1/IJCTT-V73I1P104.pdf

Autotelic Agents that Use and Ground Large Language Models (Oudeyer), accessed August 17, 2025, https://skywritingspress.ca/2024/02/13/autotelic-agents-that-use-and-ground-large-language-models/

[2305.12487] Augmenting Autotelic Agents with Large Language Models - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2305.12487

Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration, accessed August 17, 2025, https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf

Tree-of-Thought Prompting: Key Techniques and Use Cases - Helicone, accessed August 17, 2025, https://www.helicone.ai/blog/tree-of-thought-prompting

What is Tree Of Thoughts Prompting? - IBM, accessed August 17, 2025, https://www.ibm.com/think/topics/tree-of-thoughts

What is tree of thought prompting? - Portkey, accessed August 17, 2025, https://portkey.ai/blog/tree-of-thought-prompting/

Tree of Thoughts (ToT): Enhancing Problem-Solving in LLMs - Learn Prompting, accessed August 17, 2025, https://learnprompting.org/docs/advanced/decomposition/tree_of_thoughts

Unlocking LLMs' Potential with Tree-of-Thought Prompting | by Albert | Medium, accessed August 17, 2025, https://medium.com/@albert_88839/unlocking-llms-potential-with-tree-of-thought-prompting-31e9a34f4830

Tree of Thoughts - GitHub Pages, accessed August 17, 2025, https://langchain-ai.github.io/langgraph/tutorials/tot/tot/

Tree of Thoughts (ToT) - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/techniques/tot

Self-Correction in Large Language Models - Communications of the ACM, accessed August 17, 2025, https://cacm.acm.org/news/self-correction-in-large-language-models/

Can AI Agents Self-correct? - Medium, accessed August 17, 2025, https://medium.com/@jianzhang_23841/can-ai-agents-self-correct-43823962af92

Self-Reflection in LLM Agents: Effects on Problem-Solving ... - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2405.06682

Conversational Memory for LLMs with Langchain | Pinecone, accessed August 17, 2025, https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/

Memory Management in Large Language Models (LLMs): Challenges and Solutions, accessed August 17, 2025, https://ai.plainenglish.io/memory-management-in-large-language-models-llms-challenges-and-solutions-a54439df39cd

Enterprise AI's Biggest Challenges: Memory, Connectivity, and Governance - Vendia, accessed August 17, 2025, https://www.vendia.com/blog/enterprise-ai-biggest-challenges-memory-connectivity-governance/

LLM Memory: Integration of Cognitive Architectures with AI - Cognee, accessed August 17, 2025, https://www.cognee.ai/blog/fundamentals/llm-memory-cognitive-architectures-with-ai

A Straightforward explanation of Parametric vs. Non-Parametric ..., accessed August 17, 2025, https://lawrence-emenike.medium.com/a-straightforward-explanation-of-parametric-vs-non-parametric-memory-in-llms-f0b00ac64167

What Role Does Memory Play in the Performance of LLMs? - ADaSci, accessed August 17, 2025, https://adasci.org/what-role-does-memory-play-in-the-performance-of-llms/

How Width.ai Builds In-Domain Conversational Systems using Ability Trained LLMs and Retrieval Augmented Generation (RAG), accessed August 17, 2025, https://www.width.ai/post/retrieval-augmented-generation-rag

The Statistical Showdown: Parametric vs. Non-Parametric Machine Learning Models | by Ajay Verma | Artificial Intelligence in Plain English, accessed August 17, 2025, https://ai.plainenglish.io/the-statistical-showdown-parametric-vs-non-parametric-machine-learning-models-e384b08faf0b

Exploring the LLM Landscape: From Parametric Memory to Agent-Oriented Models | by Deepak Babu Piskala | Medium, accessed August 17, 2025, https://medium.com/@prdeepak.babu/exploring-the-llm-landscape-from-parametric-memory-to-agent-oriented-models-ab0088d1f14

What is Retrieval-Augmented Generation (RAG)? - Google Cloud, accessed August 17, 2025, https://cloud.google.com/use-cases/retrieval-augmented-generation

Practical tips for retrieval-augmented generation (RAG) - The Stack Overflow Blog, accessed August 17, 2025, https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/

Retrieval Augmented Generation (RAG) for LLMs - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/research/rag

What is Retrieval-Augmented Generation (RAG)? A Practical Guide - K2view, accessed August 17, 2025, https://www.k2view.com/what-is-retrieval-augmented-generation

Retrieval Agents in RAG: A Practical Guide - Signity Solutions, accessed August 17, 2025, https://www.signitysolutions.com/blog/retrieval-agents-in-rag

From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs - arXiv, accessed August 17, 2025, https://arxiv.org/html/2504.15965v1

When Large Language Models Meet Vector Databases: A Survey - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01763v3

How LLMs Use Vector Databases for Long-Term Memory: A Beginner's Guide, accessed August 17, 2025, https://yashbabiya.medium.com/how-llms-use-vector-databases-for-long-term-memory-a-beginners-guide-e0990e6a0a3f

Why LLM Memory Still Fails - A Field Guide for Builders - DEV Community, accessed August 17, 2025, https://dev.to/isaachagoel/why-llm-memory-still-fails-a-field-guide-for-builders-3d78

A Grounded Memory System For Smart Personal Assistants - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.06328v1

Inside MemGPT: An LLM Framework for Autonomous Agents ..., accessed August 17, 2025, https://pub.towardsai.net/inside-memgpt-an-llm-framework-for-autonomous-agents-inspired-by-operating-systems-architectures-674b7bcca6a5

MemGPT: Towards LLMs as Operating Systems - arXiv, accessed August 17, 2025, https://arxiv.org/pdf/2310.08560

This article delves into MemGPT, a novel system developed by researchers at UC Berkeley to address the limited context window issue prevalent in Large Language Models (LLMs). By drawing inspiration from traditional operating system memory management, MemGPT introduces a hierarchical memory architecture allowing LLMs to handle extended contexts effectively. This piece explores the core concepts, implementation, evaluations, and the implications of MemGPT in advancing the capabilities of LLMs. - GitHub Gist, accessed August 17, 2025, https://gist.github.com/cywf/4c1ec28fc0343ea2ea62535272841c69

madebywild/MemGPT: Create LLM agents with long-term memory and custom tools - GitHub, accessed August 17, 2025, https://github.com/madebywild/MemGPT

letta-ai/letta: Letta (formerly MemGPT) is the stateful agents ... - GitHub, accessed August 17, 2025, https://github.com/letta-ai/letta

Open Source Implementations of ChatGPT's memory feature? : r/LocalLLaMA - Reddit, accessed August 17, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1i2hlmz/open_source_implementations_of_chatgpts_memory/

H-MEM: Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents, accessed August 17, 2025, https://arxiv.org/html/2507.22925v1

Paper page - Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents, accessed August 17, 2025, https://huggingface.co/papers/2507.22925

Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents - arXiv, accessed August 17, 2025, https://www.arxiv.org/abs/2507.22925

Efficiently Enhancing General Agents with Hierarchical-Categorical Memory - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.22006v1

LLM Agents and Tool Use: Building AI Systems That Can Act | by Rizqi Mulki | Medium, accessed August 17, 2025, https://medium.com/@rizqimulkisrc/llm-agents-and-tool-use-building-ai-systems-that-can-act-0bfabf6d6b88

What are Tools? - Hugging Face Agents Course, accessed August 17, 2025, https://huggingface.co/learn/agents-course/unit1/tools

LLM Agents - Prompt Engineering Guide, accessed August 17, 2025, https://www.promptingguide.ai/research/llm-agents

LLM agents: The ultimate guide 2025 | SuperAnnotate, accessed August 17, 2025, https://www.superannotate.com/blog/llm-agents

Guide to Understanding and Developing LLM Agents - Scrapfly, accessed August 17, 2025, https://scrapfly.io/blog/posts/practical-guide-to-llm-agents

LangChain State of AI Agents Report, accessed August 17, 2025, https://www.langchain.com/stateofaiagents

What tools do you use to build your AI agent? : r/AI_Agents - Reddit, accessed August 17, 2025, https://www.reddit.com/r/AI_Agents/comments/1i2jcp7/what_tools_do_you_use_to_build_your_ai_agent/

How to Use AutoGen to Build AI Agents That Collaborate Like Humans - DEV Community, accessed August 17, 2025, https://dev.to/brains_behind_bots/how-to-use-autogen-to-build-ai-agents-that-collaborate-like-humans-2afm

How to think about agent frameworks - LangChain Blog, accessed August 17, 2025, https://blog.langchain.com/how-to-think-about-agent-frameworks/

What is MetaGPT ? | IBM, accessed August 17, 2025, https://www.ibm.com/think/topics/metagpt

MetaGPT | MetaGPT, accessed August 17, 2025, https://docs.deepwisdom.ai/

MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework - OpenReview, accessed August 17, 2025, https://openreview.net/forum?id=VtmBAGCN7o

MetaGPT: The Multi-Agent Framework, accessed August 17, 2025, https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html

AutoGen 0.2 - Microsoft Open Source, accessed August 17, 2025, https://microsoft.github.io/autogen/0.2/

Autogen - Qdrant, accessed August 17, 2025, https://qdrant.tech/documentation/frameworks/autogen/

Getting Started | AutoGen 0.2 - Microsoft Open Source, accessed August 17, 2025, https://microsoft.github.io/autogen/0.2/docs/Getting-Started/

microsoft/autogen: A programming framework for agentic AI PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour - GitHub, accessed August 17, 2025, https://github.com/microsoft/autogen

[2502.11705] LLM Agents Making Agent Tools - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2502.11705

jbpayton/llm-auto-forge: A langchain based tool to allow agents to dynamically create, use, store, and retrieve tools to solve real world problems - GitHub, accessed August 17, 2025, https://github.com/jbpayton/llm-auto-forge

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v4

[Literature Review] Executable Code Actions Elicit Better LLM Agents, accessed August 17, 2025, https://www.themoonlight.io/en/review/executable-code-actions-elicit-better-llm-agents

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v1

How should LLM agents best interact with our world? - Xingyao Wang, accessed August 17, 2025, https://xwang.dev/blog/2024/codeact/

Paper page - Executable Code Actions Elicit Better LLM Agents - Hugging Face, accessed August 17, 2025, https://huggingface.co/papers/2402.01030

Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/html/2402.01030v3

[2402.01030] Executable Code Actions Elicit Better LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2402.01030

AutoAgents: A Framework for Automatic Agent Generation - arXiv, accessed August 17, 2025, https://arxiv.org/html/2309.17288v3

accessed December 31, 1969, https://arxiv.org/abs/2309.17288

[PDF] AutoAgents: A Framework for Automatic Agent Generation | Semantic Scholar, accessed August 17, 2025, https://www.semanticscholar.org/paper/AutoAgents%3A-A-Framework-for-Automatic-Agent-Chen-Dong/63fb7814c6257158ecb20f390be51d5bb8969be3

AutoAgents: A Framework for Automatic Agent Generation - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/382789006_AutoAgents_A_Framework_for_Automatic_Agent_Generation

(PDF) AutoAgents: A Framework for Automatic Agent Generation - ResearchGate, accessed August 17, 2025, https://www.researchgate.net/publication/375456980_AutoAgents_A_Framework_for_Automatic_Agent_Generation

AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents - arXiv, accessed August 17, 2025, https://arxiv.org/abs/2502.05957

Tool Selection by Large Language Model (LLM) Agents - Technical Disclosure Commons, accessed August 17, 2025, https://www.tdcommons.org/cgi/viewcontent.cgi?article=9446&context=dpubs_series

Motif: Intrinsic Motivation from Artificial Intelligence Feedback - arXiv, accessed August 17, 2025, https://arxiv.org/html/2310.00166

Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration - arXiv, accessed August 17, 2025, https://arxiv.org/html/2505.17621v2

The Hidden Security Risks of SWE Agents like OpenAI Codex and ..., accessed August 17, 2025, https://www.pillar.security/blog/the-hidden-security-risks-of-swe-agents-like-openai-codex-and-devin-ai

Fully Autonomous AI Agents Should Not be Developed - arXiv, accessed August 17, 2025, http://arxiv.org/pdf/2502.02649

Fully Autonomous AI Agents Should Not be Developed - arXiv, accessed August 17, 2025, https://arxiv.org/html/2502.02649v2

Understanding the Hidden Risks of AI Agent Adoption | Built In, accessed August 17, 2025, https://builtin.com/artificial-intelligence/hidden-risks-ai-agent-adoption

Building a Sandboxed Environment for AI generated Code ..., accessed August 17, 2025, https://anukriti-ranjan.medium.com/building-a-sandboxed-environment-for-ai-generated-code-execution-e1351301268a

Code Sandboxes for LLMs and AI Agents - Amir's Blog, accessed August 17, 2025, https://amirmalik.net/2025/03/07/code-sandboxes-for-llm-ai-agents

Secure Code Execution in AI Agents | by Saurabh Shukla - Medium, accessed August 17, 2025, https://saurabh-shukla.medium.com/secure-code-execution-in-ai-agents-d2ad84cbec97

Comparison of various runtimes in Kubernetes - High-Performance Storage [HPS], accessed August 17, 2025, https://hps.vi4io.org/_media/teaching/autumn_term_2023/stud/scap_jule_anger.pdf

Kata Containers vs Firecracker vs gvisor : r/docker - Reddit, accessed August 17, 2025, https://www.reddit.com/r/docker/comments/1fmuv5b/kata_containers_vs_firecracker_vs_gvisor/

Top Modal Sandboxes alternatives for secure AI code execution | Blog - Northflank, accessed August 17, 2025, https://northflank.com/blog/top-modal-sandboxes-alternatives-for-secure-ai-code-execution

Secure execution of code generated by Large Language Models - AWS Builder Center, accessed August 17, 2025, https://builder.aws.com/content/2k63zaIUwjObVu3o4xlBHpHp0HB/secure-execution-of-code-generated-by-large-language-models

AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing - arXiv, accessed August 17, 2025, https://arxiv.org/html/2409.10737v1

AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code ..., accessed August 17, 2025, https://arxiv.org/abs/2409.10737

Can AI Be Conscious? The Science, Ethics, and Debate - Stack AI, accessed August 17, 2025, https://www.stack-ai.com/blog/can-ai-ever-achieve-consciousness

(PDF) Consciousness in Artificial Intelligence: Insights from the ..., accessed August 17, 2025, https://www.researchgate.net/publication/373246089_Consciousness_in_Artificial_Intelligence_Insights_from_the_Science_of_Consciousness

An Introduction to the Problems of AI Consciousness - The Gradient, accessed August 17, 2025, https://thegradient.pub/an-introduction-to-the-problems-of-ai-consciousness/

Component | Core Function | Embodied Principle | Key Enabling Technologies

Motivator | Generates high-level, open-ended goals based on internal drives like curiosity and competence. | Autotelicity | Curiosity-Driven Reinforcement Learning, Competence-Based Intrinsic Motivation, Language-Augmented Goal Generation 23

Planner/Executor | Decomposes goals into actionable plans, executes steps, and performs self-correction upon failure. | Cognitive Reasoning & Self-Maintenance | Tree of Thoughts (ToT) for planning, Self-Correction Loops with external feedback (e.g., execution errors) 32

Memory Manager | Manages a persistent, structured knowledge base, forming the agent's identity and learned history. | Autopoiesis (State & Identity) | Hierarchical Memory Architectures (e.g., MemGPT, H-MEM), Non-Parametric Storage (Vector Databases), Graph Databases 65

Tool Forge | Autonomously writes, debugs, verifies, and integrates new software tools to fill identified capability gaps. | Endogenous Capability Expansion | Code as a Unified Action Space (e.g., CodeAct), Autonomous Tool Creation Frameworks (e.g., ToolMaker), Secure Sandboxed Execution 86

Technology | Isolation Mechanism | Security Strength | Startup Time | Performance Overhead | Suitability for Autotelic Agents

Docker (LXC) | Shared Host Kernel with Linux Namespaces and cgroups | Low | Milliseconds | Minimal overhead, near-native performance | Best for trusted workloads or development. The weak isolation makes it unsuitable for executing untrusted, LLM-generated code in production. 109

gVisor | User-space Kernel / Syscall Interception | Medium | Sub-second | Low for CPU-bound tasks, higher for I/O-heavy tasks | A good balance of security and performance. It provides stronger isolation than Docker by intercepting system calls, preventing direct access to the host kernel. 109

Firecracker | Hardware Virtualization / MicroVM (KVM) | High | Seconds | Higher overhead due to full virtualization | Offers the strongest security by running code in a lightweight virtual machine with its own kernel. Ideal for multi-tenant or zero-trust environments where security is paramount, though startup latency may impact rapid, iterative debugging cycles. 109