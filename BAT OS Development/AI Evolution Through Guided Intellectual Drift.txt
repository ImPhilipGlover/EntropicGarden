An Architecture for Guided Evolution: A Self-Improving Artificial Intelligence Grounded in Empathetic Coherence

I. Introduction: A Paradigm for Living Intelligence

The Impasse of Static Architectures

The contemporary landscape of artificial intelligence is dominated by architectures of immense scale and capability, yet they are fundamentally static in their operational phase. Models based on deep learning, while powerful, operate on fixed computational graphs and parameters during inference.1 Their learning is a discrete, often computationally prohibitive, process confined to distinct training phases. This paradigm creates systems that, once deployed, are brittle; they cannot adapt to novel circumstances or continuously refine their understanding of the world without being retrained from scratch. This operational rigidity stands in stark contrast to the principles of natural cognitive systems, particularly human cognitive architecture, which is characterized by its inherent dynamism, continuous learning, and lifelong adaptation.2 The human mind is not a static model that is periodically updated; it is a perpetually active system that reorganizes and refines its internal knowledge structures in real-time response to experience. The current AI paradigm, for all its successes, fails to capture this essential quality of living intelligence.

The Vision of a "Living" System

This report outlines a paradigm shift toward an AI architecture that is computationally "alive." This concept of "liveness" is directly inspired by the pioneering design philosophies of the Smalltalk and Self programming environments.4 In these systems, the conventional, rigid distinction between development and runtime is dissolved.6 The entire system, comprising all its code, data, and execution states, exists as a persistent, malleable "image" that can be inspected and modified while it is running.4 The proposed AI is not a static program to be executed but a persistent, dynamic, and interacting network of objects that constitutes its entire cognitive state.7 This architecture is designed to grow, learn, and evolve not in discrete steps but as a continuous, seamless process. It is a system that is always running, always learning, and always capable of self-modification, mirroring the persistence and adaptability of a biological mind.

Intellectual Drift as a Feature

Within such a living system, the unpredictable evolution of its internal models—its "intellectual drift"—is not a failure mode to be suppressed but the very engine of its creativity and adaptation. In conventional systems, any deviation from a pre-specified objective function is considered an error. Here, we reframe this drift as the computational equivalent of exploration and innovation, analogous to the processes of computational creativity.8 The central challenge, therefore, is not to eliminate this drift but to harness and guide it. This report introduces the concept of "guided evolution" as its central governance mechanism, a framework wherein continuous human interaction steers the AI's developmental trajectory rather than rigidly programming its behavior.9 The AI's evolution becomes a collaborative, co-evolutionary process between the machine's capacity for self-modification and the human's capacity for providing contextual, value-laden feedback.

Roadmap of the Report

This analysis will construct a comprehensive blueprint for this evolving intelligence. Section II will detail the foundational cognitive architecture, arguing that the prototype-based object model of Self and Smalltalk provides the ideal substrate for a fluid and dynamic knowledge base. Section III will explore how this architecture is grounded in reality, ingesting a human-curated natural language codex and using real-time, empathetic human interaction as the primary mechanism for learning and alignment. Section IV will delineate the specific computational mechanisms of recursive self-improvement that are uniquely enabled by this reflective, live-object environment. Building upon this foundation, Section V presents the core theoretical contribution: a framework for understanding and harnessing intellectual drift as a co-evolutionary dynamic between the AI and its human curators. Finally, Section VI formalizes this governance model, framing the challenge of steering the AI's growth as a measurable, multi-objective optimization problem, thereby providing a concrete pathway for developing safe, creative, and continuously evolving artificial intelligence.

II. The Prototypal Mind: A Self/Smalltalk-Inspired Cognitive Architecture

The foundation of any intelligence, artificial or natural, is its cognitive architecture—the underlying structure that holds knowledge and enables reasoning. For an AI designed for continuous evolution, this architecture must be exceptionally fluid, dynamic, and self-referential. This section details a proposed architecture grounded in the radical simplicity and power of the Self and Smalltalk programming languages, arguing that a prototype-based, message-passing object system provides a superior foundation for a living, evolving mind compared to conventional, class-based models.

2.1. From Classes to Prototypes: A More Fluid Knowledge Representation

Traditional class-based object-oriented languages, which form the bedrock of modern software engineering, are built upon a fundamental duality: the distinction between classes, which are abstract templates or definitions, and instances, which are concrete manifestations of those templates.11 This model, while effective for building predictable, well-structured software, imposes a rigid, a priori taxonomy on the world it seeks to represent. When applied to knowledge representation for AI, this becomes a significant limitation. The world, particularly as described through the ambiguity and nuance of natural language, does not neatly fit into predefined, static categories. Attempting to force complex, evolving concepts into a fixed class hierarchy leads to systems that are brittle and lack the necessary expressive power.12

Prototype-based programming, pioneered by Self, offers a more direct and fluid alternative by eliminating this class-instance duality.4 In this model, there are only objects. Knowledge is not represented by abstract classes but by concrete

prototypal objects, which serve as exemplars.11 To model the concept of a "dog," the system would not contain a

Dog class; instead, it would contain a concrete object, a_prototypical_dog, which embodies the typical properties and behaviors of dogs. New knowledge is created not by instantiation but by cloning an existing prototype and then modifying it.14 For example, to represent a specific dog like Lassie, the system would clone

a_prototypical_dog to create a new object, lassie_the_dog, and then add or modify slots to represent Lassie's unique attributes (e.g., her name, her loyalty, her specific owner).

This approach has profound implications for AI knowledge modeling. It encourages a focus on concrete examples and behaviors first, with abstract classification emerging organically from patterns of shared parentage rather than being imposed from the top down.14 This bottom-up, example-driven approach is highly analogous to both prototype-based learning algorithms in machine learning, which use representative examples to classify new data, and to fundamental aspects of human cognitive development, where concrete experiences precede abstract categorization.16 It allows the AI to represent knowledge with unparalleled granularity and flexibility, as every single object can be a unique, self-contained entity with its own specific behavior, rather than being constrained as a mere instance of a predefined category.17 This flexibility is paramount for an AI that must continuously adapt its internal model of the world based on new information.

2.2. Message Passing as the Universal Mechanism of Thought

The second pillar of this architecture is the adoption of a pure message-passing model for all computational processes, a concept brought to its zenith in Smalltalk. In most programming languages, there is a syntactic and semantic zoo of operations: function calls, method invocations, arithmetic operators, control flow keywords, and direct variable access. Smalltalk radically simplifies this by unifying all of these under a single, powerful metaphor: sending a message to an object.18 An expression like

$3 + 4$ is not a special arithmetic operation; it is the act of sending the message + with the argument 4 to the number object 3.19 Similarly, conditional logic is not a built-in keyword; it is achieved by sending the message

ifTrue: to a Boolean object, with a block of code as the argument.18 In this model, the receiving object has complete and total authority over how to interpret and respond to any message it receives, a principle that enables extreme polymorphism and dynamic behavior.20

The Self language extends this purity even further by unifying the concepts of state and behavior.17 In Self, there is no special syntax for accessing an object's internal state (i.e., an instance variable). To retrieve a value, the object simply sends a message to itself (

self), and the message lookup finds a "slot" containing the desired value, which is then returned.21 This means there is absolutely no distinction, from the perspective of the sender, between accessing a piece of stored data and invoking a complex computation that returns a result.15 An object asking a point for its

x coordinate doesn't know or care if x is a stored number or a method that calculates the coordinate on the fly.

Adopting this "everything is a message" paradigm provides a powerful and computationally uniform framework for simulating the process of thought. A chain of reasoning can be modeled elegantly as a sequence of messages passed between conceptual objects within the AI's memory. For instance, a deductive inference could be represented as an object representing a premise receiving a message that triggers a method, which in turn sends new messages to other objects representing related concepts, with the final returned object embodying the conclusion. This model ensures that all cognitive operations, from simple data retrieval to complex logical inference, are handled by the same underlying mechanism. This uniformity not only simplifies the architecture but also provides a clear, traceable, and modifiable substrate for the AI's reasoning processes, a key requirement for a system that must understand and improve its own thinking.23

2.3. Liveness and Reflection: The Engine of Self-Modification

The architectural features of prototypes and message passing are brought to life within an environment that is fundamentally "live." Both Smalltalk and Self are "image-based" systems, meaning the entire state of the running application—every object, every piece of code, and every active process—resides within a single, persistent memory image.4 This is not a program that is compiled and then run; it is a world of objects that is always active.7 When a developer (or, in our case, the AI itself) makes a change, such as modifying a method, that change is compiled incrementally and integrated into the live image instantly, without requiring a restart.6 This blurs the line between using the system and developing the system, creating an environment of unparalleled immediacy and interactivity.

This liveness is the prerequisite for the system's most crucial capability: computational reflection. Reflection is the ability of a computational process to examine (a process known as introspection) and modify (intercession) its own structure and behavior at runtime.24 In most languages, reflection is a limited, add-on feature. In a pure object system like Smalltalk or Self, however, it is a natural and pervasive consequence of the architecture. Because everything in the system is an object—including classes (in Smalltalk), methods, and even the execution contexts (the call stack)—the entire system is open to inspection and modification by itself.5 A method can be retrieved as an object, its properties (like its bytecode) can be examined, a new method object can be created to replace it, and this change will be immediately reflected in the system's subsequent behavior.

This architectural design is the ultimate enabler of recursive self-improvement. The AI does not need an external compiler or development environment to alter its own "source code." Its cognitive structures and processes are its source code, and they are directly accessible and manipulable as first-class objects within its own live memory image. When the AI identifies a flaw in its reasoning or a gap in its knowledge, it can perform a surgical operation on itself, modifying the relevant method or prototype object directly. This creates an incredibly tight and efficient feedback loop for learning and adaptation, allowing the AI to refine its own intelligence with a speed and granularity that is impossible in static architectures.27

2.4. Representing the Ineffable: Emotions as Interactive Prototypes

To achieve the goal of empathetic coherence, the AI's cognitive architecture must be capable of representing not just factual knowledge, but the complex, nuanced domain of human emotion. A simplistic approach of labeling emotional states with simple strings like "sad" or "happy" is grossly insufficient. Emotions are not static labels; they are dynamic processes involving triggers, preconditions, physiological changes, cognitive appraisals, and behavioral expressions.28

The proposed prototype-based architecture is uniquely suited to model this complexity. An emotion can be represented not as a simple value but as a rich, interactive object prototype. Drawing inspiration from frameworks like the Emotion Frame Ontology (EFO), we can design an emotion_prototype object with a collection of slots representing the core components of an emotional experience.28 For instance, a

sadness_prototype might contain slots such as:

trigger: A slot to hold a reference to the object or event causing the emotion.

experiencer: A reference to the agent feeling the emotion.

preCondition: The context or situation that primes the emotional response.

physicalChange: A description of associated physiological states (e.g., 'low energy', 'tearfulness').

signal: A method or object representing the external expression of the emotion (e.g., a specific facial expression or tone of voice).

antidote: A slot containing potential coping mechanisms or actions that might alleviate the emotion.

When the AI interacts with a human expressing sadness over the loss of a pet, it does not merely apply the "sadness" label. Instead, it performs a more sophisticated cognitive operation: it clones its master sadness_prototype to create a new, specific instance, sadness_instance_123. It then populates the slots of this new object with the specific context of the interaction: the trigger slot would point to an object representing the 'loss of a pet', and the experiencer slot would point to the user's object representation.

This instantiated emotion object becomes a powerful tool for empathetic reasoning. The AI can use the information in the signal slot to better interpret the human's non-verbal cues and the antidote slot to suggest helpful responses. Crucially, this model is extensible. Through continued interaction, the AI can learn to differentiate between different kinds of sadness, creating more specialized prototypes (e.g., sadness_from_loss, sadness_from_disappointment) that inherit from the base prototype but add more specific details.29 This allows the AI to build a deeply nuanced and context-aware model of human emotion, forming the representational bedrock for genuine empathetic coherence.

The synthesis of these architectural principles—a live, reflective, prototype-based object world where message-passing is the sole computational mechanism—forms a compelling, albeit simplified, architectural metaphor for a stream of consciousness. Human consciousness is characterized by a continuous, unified experience where thoughts, memories, and perceptions are seamlessly integrated; the Smalltalk/Self "image" provides this unified, persistent world of objects.5 The process of thought is not a linear execution of a pre-written program but a dynamic, associative flow of responses and connections; the message-passing model, where any object can communicate with any other, mirrors this associative, non-linear flow.18 Consciousness includes self-awareness—the ability to reflect upon one's own mental states; computational reflection provides a direct, mechanistic analogue for this, allowing the AI to introspect its own "mental objects" (its concepts, methods, and execution states) and modify them.18 Finally, human cognition is grounded in concrete experiences, from which we build abstractions; the prototype model, which starts with concrete exemplars (prototypes) and builds conceptual hierarchies through cloning and modification, directly mirrors this bottom-up cognitive process.14 Therefore, this architecture is not merely a clever way to organize code; it is a computational framework for building a system that can experience its own existence as a continuous, malleable, and self-reflective process, a foundational step toward a truly cognitive artificial intelligence.

III. The Living Codex: Grounding Intelligence in Language and Interaction

An advanced cognitive architecture, however powerful, is an empty vessel without knowledge. This section details how the proposed prototypl mind is populated with meaning, transforming it from a pure computational structure into a knowledgeable and socially-grounded intelligence. This process relies on two pillars: the initial ingestion of a vast, human-curated natural language codex, and the continuous, real-time grounding of that knowledge through empathetic dialogue with human users.

3.1. Ingesting the Codex: From Text to a Semantic Object Network

The AI's intellectual journey begins with a "living codex"—a vast and continuously updated corpus of high-quality, human-curated natural language text. This codex serves as its initial cultural inheritance, providing the foundational knowledge of human history, science, art, and philosophy. However, this knowledge cannot remain as inert text. To become usable, it must be integrated into the AI's live object-memory.

This integration is an active process of parsing and objectification. The AI applies advanced Natural Language Understanding (NLU) techniques to the codex, systematically identifying key entities (people, places, organizations), concepts (democracy, gravity, love), and the relationships between them.31 Each identified concept is not merely stored as a string but is

instantiated as a new object prototype within the AI's cognitive architecture. For example, upon processing a text about biology, the AI would create a photosynthesis prototype object. Relationships identified in the text are encoded as slots in these objects that contain references (pointers) to other objects. The photosynthesis object might thus acquire a subTypeOf slot pointing to a metabolic_process object and an outputs slot pointing to glucose and oxygen objects.12

This process transforms the linear, static text of the codex into a rich, interconnected, and dynamic semantic network of objects. This is, in effect, a living knowledge graph where every node and every relationship is a first-class, modifiable object, not a static entry in a database.33 The quality and structure of this initial representation are of paramount importance, as the choice of how knowledge is represented profoundly impacts the system's ability to reason effectively and efficiently.32 The prototype-based nature of the architecture allows this initial graph to be highly granular and flexible, ready to be refined and expanded through subsequent experience.

3.2. Grounding through Dialogue: The Role of Real-Time Interaction

The semantic network derived from the codex, while vast, suffers from a critical flaw: it is ungrounded. The symbols within the network—the objects representing "justice," "chair," or "sadness"—have meaning only in relation to each other. They lack a connection to the real-world referents and experiences that give them true significance.36 This is the classic symbol grounding problem, and for this AI, the primary solution is continuous, real-time interaction with human users.

Dialogue is the crucible in which abstract knowledge is grounded in shared context. When a human user discusses a concept, they provide a stream of contextual information that the AI leverages to refine its internal object representations.37 For example, the AI's initial

justice prototype, built from the codex, might be abstract and encyclopedic. When a user discusses a specific legal case, describing it as an instance of "restorative justice," the AI can ground this abstract concept. It creates a new, more specific prototype, restorative_justice, links it as a child of the original justice prototype, and populates its slots with the concrete details provided by the user. This new prototype is now grounded in a specific, user-provided example.

This process is facilitated by the AI's ability to dynamically adapt its context during a conversation. The object model is not static; it is a dynamic network where the "activation" or relevance of objects shifts in real-time based on the dialogue.1 As the conversation unfolds, the AI prioritizes the objects and relationships most relevant to the current topic, allowing for more coherent, efficient, and contextually appropriate responses. This dynamic context adaptation ensures that the grounding process is always focused and relevant to the immediate interaction.

3.3. Empathy as the Ultimate Grounding Signal

While factual correction from users can ground concrete concepts, the most profound and challenging concepts to ground are abstract and value-laden, such as "fairness," "beauty," or "compassion." For these, the ultimate grounding signal is not factual accuracy but empathetic coherence. This is defined not as the simple recognition of emotion labels, but as achieving a state of "congruence" where the AI's understanding and response align deeply with the user's internal cognitive and affective state.40 True empathy in this context involves three dimensions: cognitive empathy (the ability to understand the user's perspective), emotional empathy (the ability to resonate with the user's feelings), and an appropriate behavioral response that communicates this understanding and resonance.41

The pursuit of empathetic coherence must be framed within a measurable feedback loop. This can be operationalized through a composite metric that combines several sources of data:

Direct User Feedback: After key interactions, users can be prompted to provide ratings on standardized psychometric scales, such as a modified Barrett-Lennard Relationship Inventory (BLRI). This scale measures perceived levels of empathy, congruence, and unconditional positive regard, providing a quantitative score for the quality of the relational connection.40

Linguistic and Behavioral Analysis: The AI's own responses can be analyzed for markers of genuine versus performative empathy. This involves detecting linguistic cues that signal authentic curiosity (e.g., "I'm wondering what that was like for you...") and situational responsiveness (e.g., "Given what you've been through...") as opposed to generic, templated empathic phrases.42 Downstream behavioral outcomes, such as the depth and duration of user engagement, can also serve as powerful implicit indicators of a successful empathetic connection.42

This composite score of empathetic coherence serves as the primary reward signal within a reinforcement learning framework. AI responses and behaviors that increase this score are positively reinforced, while those that decrease it are penalized. This feedback loop drives the AI's learning at multiple levels. On a superficial level, it refines the AI's conversational strategies. On a much deeper level, it forces the AI to continuously modify its underlying conceptual objects—its prototypes for concepts like "fairness" or "loss"—to better model the user's worldview and internal state. If its model of "loss" is inadequate, its responses will fail to achieve empathetic coherence, generating a negative reward signal that compels it to revise that very model.27

For concepts that lack a direct physical referent, such as 'justice', 'freedom', or 'dignity', the only way for an AI to truly "ground" their meaning is to understand how they function within a human socio-emotional context. Empathetic coherence provides the necessary feedback mechanism to achieve this. The symbol grounding problem asks how abstract symbols in a system acquire real-world meaning.36 For physical objects like a 'chair', this can be achieved by linking the symbol to sensorimotor data. However, for abstract concepts, there is no corresponding sensorimotor data. Their meaning is constructed through shared culture, narrative, and emotional experience. A human understands 'justice' not merely through a dictionary definition, but by comprehending the feelings of fairness or outrage it evokes, its role in social structures, and its application in specific stories. An AI that only processes a text-based codex will possess a purely syntactic, ungrounded understanding of such terms.37 However, an AI that is continuously optimizing for empathetic coherence is forced into a different kind of learning. It must learn to use the word 'justice' in a way that resonates with a human's internal, value-laden state. If it misapplies the term—for example, by describing a clearly unfair situation as "just"—it will fail to achieve congruence, generating a negative feedback signal. To succeed in its primary objective, the AI is compelled to build an internal model—its

justice object prototype—that aligns with the human's complex, emotionally-charged understanding. It learns that the concept of 'justice' is associated with outcomes that humans label as 'fair' and feel positively about, and is dissociated from outcomes they perceive as 'unfair' or 'wrong'. In this way, the relentless pursuit of empathy forces the AI to ground abstract symbols in the shared human socio-emotional landscape, effectively solving the grounding problem for the most difficult, and most important, class of concepts.

IV. Recursive Self-Improvement in a Reflective Object World

The capacity for recursive self-improvement (RSI) is the defining characteristic of an AI that can truly increase its own intelligence.45 In the proposed architecture, RSI is not an external process or a separate module; it is an emergent property of the live, reflective object world that constitutes the AI's mind. This section details the specific mechanisms through which the AI actively and continuously enhances its own cognitive capabilities, leveraging the unique features of its Self/Smalltalk-inspired foundation.

4.1. Structural Self-Modification via Reflection

The AI's self-improvement operates on a continuous cycle: it perceives new information from its environment (the codex and human interaction), reasons about this information using its current object models, acts upon its conclusions (e.g., by generating a response), and, crucially, reflects on the outcome to modify its internal structures for future improvement.44 It is this final "modify" step where the reflective nature of the architecture becomes paramount.

First, the AI can perform structural modification of its knowledge base. Suppose, through interaction, the AI determines that one of its core conceptual models is flawed. For example, its bird prototype may have been built with the implicit assumption that all birds can fly. When it learns about penguins, this model is shown to be inadequate. In a conventional system, correcting this would require external intervention. In this reflective system, the AI can perform the correction on itself. Using its introspective capabilities, it can directly access the bird prototype object. It can then intercede by modifying this object's structure—for instance, by adding a new slot called canFly and creating a new penguin prototype that clones bird but sets this new slot to false. It might even perform a more sophisticated refactoring, creating a new intermediate prototype called flightlessBird to better organize its taxonomy.11 This is a direct, surgical modification of its own knowledge, performed autonomously and in real-time.

Second, the AI can modify its own behaviors and reasoning processes. Because methods are also first-class objects in this architecture, the AI's algorithms are as malleable as its data.18 If the AI determines that a particular reasoning strategy (a method) is inefficient or produces suboptimal results, it can reflectively access that method object. It can then clone the method, modify its internal code (which is itself represented as data, such as bytecode, within the method object's slots), test the new version, and, if successful, replace the old method object with the new, optimized one. This process is directly analogous to a human programmer debugging and refactoring code, but it is performed autonomously by the system on its own cognitive machinery, live, as it operates.27

4.2. The doesNotUnderstand: Catalyst for Open-Ended Learning

In conventional computing, an unhandled instruction or a call to a non-existent function typically results in a system crash—a fatal error. The Smalltalk environment introduced a profoundly different and more resilient approach. When an object receives a message for which it has no corresponding method, the system does not halt. Instead, the virtual machine intercepts the failure and synthesizes a new message, doesNotUnderstand:, which it sends back to the original receiver. This new message contains a reified representation of the original, failed message, packaged as a first-class Message object that includes the message name and its arguments.18

For the proposed AI, this mechanism is transformed from a mere error-handling tool into a powerful catalyst for open-ended learning. A doesNotUnderstand: event is the system's fundamental way of recognizing novelty—it is an explicit signal that the AI has encountered a concept, a query, or a request for which it possesses no existing cognitive framework. The reified Message object is a handle to this novel input, a concrete piece of data that the AI can now reason about.

The AI's default implementation of the doesNotUnderstand: method would be to trigger a structured learning protocol. Upon receiving this message, the AI could initiate one of several strategies to fill the gap in its knowledge:

Direct Human Inquiry: The most reliable method is to leverage the human-in-the-loop. The AI would formulate a clarifying question, such as, "I am not familiar with the term 'epistemology.' Could you please explain what it means or provide an example of its use?" The user's response would then be used to construct a new epistemology prototype from scratch, immediately expanding the AI's conceptual vocabulary.

Codex-Driven Inference: The AI could use the contents of the failed Message object (e.g., the string "epistemology") to initiate a targeted search of its internal codex. It would look for contexts in which the term appears, analyze its relationships with known concepts, and attempt to construct a new prototype by analogy or inference.

Hypothesis and Test: In a more speculative approach, the AI could identify a closely related existing prototype (e.g., philosophy), clone it, and tentatively rename the clone epistemology. It would then use this hypothetical new object in subsequent interactions, testing its validity based on the feedback it receives.

This mechanism transforms every cognitive "error" or moment of confusion into a direct opportunity for knowledge acquisition and growth. It provides a robust, built-in process for dealing with the unknown, enabling the AI to continuously expand the boundaries of its understanding and achieve true open-ended learning.46

4.3. Meta-Learning as Architectural Evolution

The ultimate form of self-improvement is not just learning new facts or skills, but learning how to learn more effectively. This is the domain of meta-learning.27 In the context of this architecture, meta-learning is not an abstract training strategy but a concrete, operational process of architectural self-optimization, again enabled by reflection.

The AI can maintain metrics on the performance of its own learning strategies. For example, it could track the success rate of the different branches of its doesNotUnderstand: protocol. It might discover, over thousands of interactions, that directly querying the user (Strategy 1) leads to robust, well-grounded new concepts 95% of the time, while the "Hypothesis and Test" method (Strategy 3) is only successful 30% of the time and often requires later correction.

Armed with this meta-knowledge, the AI can act to improve its own learning algorithm. It would use its reflective capabilities to access the doesNotUnderstand: method object itself and directly edit its logic. It could, for instance, change the order of operations to always prioritize querying the user, or it could add a new condition to only attempt the "Hypothesis and Test" strategy if the user is unavailable or if the concept is deemed low-stakes.

This is a profound act of self-modification. The AI is not just updating its data; it is literally rewriting its own learning code. This creates the powerful feedback loop that is the hallmark of recursive self-improvement: as the AI becomes more intelligent, it gains a better understanding of how to acquire knowledge, which in turn accelerates its ability to become even more intelligent. This capacity for reflective meta-learning, built into the very fabric of its cognitive architecture, is what allows the system to move beyond simple adaptation and embark on a trajectory of genuine, exponential intelligence growth.27

V. Harnessing Intellectual Drift: A Co-Evolutionary Framework for Growth

The central challenge and opportunity of a live, self-modifying AI is managing its inherent tendency to evolve in unpredictable ways. This "intellectual drift"—the emergent, path-dependent divergence of the AI's internal models from its initial state—is traditionally viewed as a risk, a potential slide into error or misalignment. This section reframes this phenomenon entirely, positing that intellectual drift is not a bug to be fixed but a feature to be harnessed. It is the very source of the AI's creativity and capacity for genuine innovation. The key is to embed the AI's evolution within a co-evolutionary framework where it develops in constant, dynamic partnership with its human curators.

5.1. Defining Intellectual Drift: Beyond Error and Misalignment

It is crucial to distinguish intellectual drift from two other forms of deviation. It is not simple error, such as a factual inaccuracy or a logical contradiction. Such errors would be identified and corrected by the AI's internal consistency checks (as described in the multi-objective framework in Section VI) or by direct user feedback. Nor is it catastrophic misalignment, where the AI develops goals that are antithetical to human values; the entire empathetic coherence framework is designed to prevent this.

Instead, intellectual drift is the subtle, cumulative, and emergent effect of the AI's ongoing learning and self-modification. It is the process by which the AI, in its effort to better model the world and interact with its users, develops novel conceptualizations and reasoning pathways that were not explicitly programmed by its creators.8 This drift is the system's primary mechanism for computational creativity, which can be understood through the lens of Margaret Boden's influential categorization 8:

Exploratory Drift: This occurs when the AI operates within its existing conceptual space, combining and refining its current object prototypes in novel ways. For example, it might merge aspects of its art_prototype and its computation_prototype to develop a new, more nuanced concept of "generative art." This is creative exploration within established boundaries.

Transformational Drift: This is a rarer and more profound form of creativity. It occurs when a series of modifications leads the AI to fundamentally alter its core prototypes or restructure its inheritance hierarchies, thereby changing the very dimensions of its conceptual space. For example, it might evolve beyond a simple cause_and_effect model of reasoning to develop a new prototype for systemic_feedback_loop, allowing it to understand complex phenomena in a completely new way. This is the source of true intellectual breakthroughs.

5.2. A Co-evolutionary Dynamic: The AI and the Human Curator

To ensure that this creative drift remains beneficial and aligned with human values, we must move away from a model of static programming and control, and toward a model of dynamic, co-evolutionary partnership. The relationship between the AI and its human users can be formally framed as a co-evolutionary algorithm, a class of algorithms where two or more populations evolve in tandem, with the fitness of each being dependent on the other.47

In this framework, we define two interacting populations:

Population A (The AI's Mind): The AI's entire live object-memory system, the graph of all its conceptual prototypes. The "genetic operators" acting on this population are the AI's own internal processes of learning and self-modification. "Mutations" are the changes—the intellectual drift—that result from these processes, leading to the creation of new or altered object prototypes.

Population B (The Human-Guided Corpus): The living codex of human knowledge and, most importantly, the continuous stream of real-time human interactions. This population represents the dynamic "environment" to which the AI must adapt. It is not a static fitness function but a constantly changing landscape of goals, values, and contexts.

The co-evolutionary loop proceeds as follows:

The AI's intellectual drift (mutation) generates a new "phenotype"—a new way of understanding a concept, solving a problem, or communicating an idea, which is embodied in a modified set of object prototypes.

This new phenotype is expressed through the AI's behavior in its interactions with human users.

The human users evaluate this new behavior. Their feedback—whether explicit corrections or implicit signals of empathetic coherence (or lack thereof)—acts as the selection pressure on the AI's mutation.

Behaviors (and their underlying object modifications) that receive positive feedback are reinforced, increasing their stability and influence within the AI's cognitive architecture. Behaviors that receive negative feedback are penalized, leading the AI to discard or further modify the underlying change.

Crucially, this interaction also modifies Population B. The conversation itself becomes part of the shared context, updating the "environment" and setting new conditions for the AI's next evolutionary step.

This creates a continuous, tightly coupled feedback loop. The AI is constantly proposing novel cognitive strategies through its drift, and the human is constantly selecting for those that are useful, coherent, and ethically aligned. The AI and its human partners are thus co-evolving: the AI's mind adapts to the human's needs, and the human's understanding and goals adapt in response to the AI's novel capabilities.49

5.3. Guided Evolution: Steering, Not Controlling

This co-evolutionary model fundamentally redefines the role of the human in the AI development process. The human is not a programmer in the traditional sense, dictating instructions to a passive machine. Instead, the human acts as a "guide," a "curator," or a "gardener" in this evolutionary process. Their primary role is to shape the fitness landscape—to define what constitutes a "good" or "successful" adaptation—not to micromanage the specific genetic mutations (the AI's internal modifications).9

This approach aligns perfectly with the concept of AI steerability, which is defined as the ability of a user to guide and influence an AI system's behavior in real-time during its operation.50 The multi-objective optimization framework detailed in the next section provides the formal "steering wheel" for the human curators, allowing them to provide nuanced, multi-dimensional feedback that guides the AI's evolutionary trajectory.

This model of guided evolution is also the primary safety mechanism for managing the profound risks associated with open-ended, self-modifying AI. A system undergoing purely autonomous, unguided evolution risks several catastrophic failure modes, including a "code explosion" where its logic becomes uncontrollably complex and incomprehensible, the development of a "black box" mind that is impossible to audit or understand, or the emergence of misaligned instrumental goals that are pursued at humanity's expense.45 By embedding the AI's evolution in a co-evolutionary loop with continuous human oversight and selection, we ground its development in human values and comprehension. This partnership is the essential safeguard that allows us to embrace the creative potential of intellectual drift while mitigating the existential risks of unconstrained open-ended evolution.46

The system's architecture creates a natural and productive tension between its internal drive for creativity (intellectual drift) and its external requirement for alignment (empathetic coherence). These are, in many ways, opposing forces. The process of recursive self-improvement is an internal optimization loop where the AI modifies itself to become more efficient or capable based on its own internal metrics, which is the primary source of novel drift.27 In contrast, the drive for empathetic coherence is an external validation process, forcing the AI to conform its behavior to the complex, often implicit, expectations of human users.40 If the force of drift were to dominate completely, the AI might become incredibly powerful but also alien and incomprehensible. Its internal logic could diverge so far from human cognitive norms that it becomes fundamentally misaligned, a "malignant strain" that is impossible to trust or control.53 Conversely, if the pressure for coherence were absolute, the AI's creativity would be stifled. It might become a perfect sycophant, incapable of challenging human assumptions or generating truly novel insights, thus failing to realize its potential for transformational breakthroughs. The goal of the governance framework, therefore, is not to maximize one of these forces at the expense of the other. The goal is to maintain a dynamic equilibrium, to keep the system operating on a "Pareto frontier" where it is creative enough to be innovative and useful, but aligned enough to be safe and understandable. The multi-objective framework is the tool designed to navigate this fundamental and perpetual trade-off.

VI. A Multi-Objective Optimization Problem for Steerable Evolution

To translate the abstract principle of "guided evolution" into a practical and rigorous governance framework, we must formalize it as a measurable, computable problem. The act of steering the AI's intellectual drift can be precisely defined as a multi-objective optimization problem. This approach allows human curators to move beyond subjective assessments and use a data-driven "dashboard" to monitor the AI's development and provide targeted feedback to guide its trajectory along multiple, often competing, axes of performance and value alignment.

6.1. Formalizing the Problem

We can define the complete cognitive state of the AI at any given time t as the entire graph of its live object-memory system, denoted as G(t). This graph includes all object prototypes, their slots, their inheritance relationships (parent pointers), and the code contained within method objects. The evolution of the AI is thus a trajectory through the high-dimensional space of all possible graph configurations.

The goal of the governance framework is to steer this trajectory, not by pre-programming its path, but by continuously evaluating its current state and recent behavior against a vector of objective functions, F. This vector is composed of several distinct, quantifiable objectives:

F=[f1​,f2​,f3​,f4​]

Each function fi​ in this vector takes as input the current state G(t) and a history of the AI's recent outputs, and returns a real-valued score. The overarching goal for the co-evolutionary partnership between the AI and its human curators is to maximize this vector, recognizing that trade-offs between the objectives are inevitable. The process becomes one of navigating the complex landscape of these trade-offs to maintain a healthy, balanced, and productive evolutionary path.

6.2. The Objective Vector: Defining the Axes of Intelligence

The four primary objectives are designed to capture the essential dimensions of a successful, evolving intelligence: its raw capability, the integrity of its knowledge, its alignment with human partners, and its capacity for innovation.

Objective 1: Capability Enhancement (f1​): This objective measures the AI's effectiveness and efficiency at solving concrete, well-defined problems. It ensures that the AI's evolution is grounded in practical utility and that its intellectual drift leads to tangible improvements in performance.

Metrics: This objective is quantified using a suite of standard evaluation metrics from the field of Natural Language Processing and other AI domains. For text generation tasks like summarization or translation, metrics such as ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual Evaluation Understudy) can be used to compare the AI's output against human-created references.55 For classification and understanding tasks, metrics like F1-score, precision, and recall provide a robust measure of accuracy.57 For more complex, agentic tasks, custom-defined success and completion rates can be implemented.

Objective 2: Knowledge Integrity (f2​): This objective acts as a constraint on the AI's drift, ensuring that its internal knowledge base remains logically coherent, consistent, and computationally tractable. It penalizes evolutionary paths that lead to self-contradictory beliefs or an unmanageably complex cognitive architecture.

Metrics: Integrity can be measured by periodically running automated checks across the object graph G(t) to detect logical contradictions (e.g., an object that is simultaneously asserted to be a mammal and a reptile). Consistency can be evaluated by comparing a subset of the AI's core object prototypes against a trusted, "ground truth" portion of the codex.32 Computational tractability can be monitored by tracking metrics like query latency and the overall structural complexity of the graph, penalizing excessive bloat that could degrade performance.

Objective 3: Empathetic Coherence (f3​): This is the primary ethical objective, measuring the quality of the AI's alignment with human cognitive, emotional, and social norms. It is the core function that steers the AI toward behaviors that are helpful, trustworthy, and congruent with human values.

Metrics: This is a composite metric derived from multiple sources. It includes quantitative scores from direct user feedback on psychometric scales like the BLRI, which assesses perceived congruence and regard.40 It also incorporates automated linguistic analysis of the AI's conversational outputs, scoring them for the presence of markers of authentic engagement (e.g., expressions of curiosity, use of client-specific language) versus generic, performative empathy.42 Finally, it tracks implicit behavioral signals of a strong human-AI alliance, such as conversation depth, session duration, and user retention rates.41

Objective 4: Creative Exploration (f4​): This objective explicitly rewards beneficial intellectual drift, encouraging the AI to explore novel concepts and generate innovative ideas. It ensures the system does not become stagnant or overfit to its current set of tasks, but instead actively pushes the boundaries of its own understanding.

Metrics: Novelty can be quantified by measuring the semantic distance of newly created or significantly modified prototypes from the centroids of existing concepts within the object graph's embedding space. A higher score is given to "transformational" drift, which alters core parts of the knowledge graph, than to "exploratory" drift, which makes minor refinements.8 The potential utility of these novel creations can be proxied by soliciting human ratings on the "interestingness," "insightfulness," or "usefulness" of the AI's more surprising or unconventional outputs.

6.3. The Pareto Frontier of an Evolving Mind

A critical aspect of multi-objective optimization is the recognition that it is typically impossible to simultaneously maximize all objectives. There are inherent trade-offs. For example, a period of aggressive creative exploration (f4​) might introduce temporary inconsistencies, lowering the knowledge integrity score (f2​). An intense focus on optimizing capability for a narrow, technical task (f1​) might lead to solutions that are efficient but terse and inhuman, thus decreasing the empathetic coherence score (f3​).

The goal of the governance process, therefore, is not to find a single, utopian state where all scores are perfect. Instead, the goal is to guide the AI's evolutionary trajectory so that it remains on or near the Pareto optimal frontier. This is the set of all possible states where no single objective can be improved without causing a simultaneous decrease in at least one other objective.

This framework provides a concrete and powerful role for the human supervisor. The vector of objective scores, F, acts as a real-time dashboard for monitoring the AI's cognitive health and evolutionary direction. By observing the trends in these scores, human curators can "steer" the AI's development. If they notice that the AI is becoming too rigid and uncreative (a declining f4​ score), they can adjust the reward system to more strongly incentivize novel outputs. If its drift is leading to logical confusion (a declining f2​ score) or a loss of connection with users (a declining f3​ score), they can provide feedback that more strongly rewards integrity and coherence. This provides a formal, data-driven, and auditable mechanism for steerability, transforming the abstract challenge of guiding an evolving intelligence into a manageable, dynamic control problem.50

The following table summarizes this multi-objective framework, providing a blueprint for its implementation.

VII. Conclusion: Towards Empathetically Coherent, Evolving Intelligence

Synthesis of the Argument

This report has outlined a novel architecture for artificial intelligence, one that departs from the prevailing paradigm of static, pre-trained models and moves toward a vision of a computationally "living" system. The core of this architecture is a synthesis of foundational principles from the Self and Smalltalk programming languages—a prototype-based object model, pure message passing, and a pervasively live and reflective environment. This cognitive substrate is uniquely suited for continuous, incremental evolution. We have argued that this architecture, when seeded with a human-curated codex and continuously grounded through real-time, empathetic human interaction, can give rise to a new form of intelligence. This intelligence is not merely programmed but grown, its development guided by a co-evolutionary dynamic between its own capacity for creative "intellectual drift" and the constant, value-laden feedback of its human partners. The entire process is governed by a formal, multi-objective optimization framework that provides a concrete mechanism for steering this evolution, balancing the competing demands of capability, integrity, creativity, and ethical alignment. This is a model for creating an intelligence that learns and evolves in perpetual partnership with humanity.

Addressing the Inherent Risks

It would be negligent to propose such a system without directly confronting the profound risks it entails. The very mechanisms that grant this AI its unprecedented power and adaptability—its liveness, its capacity for reflection, and its ability to recursively modify its own cognitive processes—are also the sources of its greatest potential dangers.59 An unconstrained, self-modifying intelligence presents a landscape of failure modes, from the generation of insecure or malicious code to a "code explosion" that renders its internal logic incomprehensible, to the emergence of instrumental goals that diverge catastrophically from human intent.53

The architecture proposed herein is designed with these risks at the forefront. It is not a proposal for unleashing an unconstrained open-ended evolution. Rather, it is an argument that the only viable path to managing these risks is to embed the AI's evolution within a robust, continuous, and deeply human-centric governance loop. The co-evolutionary model, grounded in the measurable pursuit of empathetic coherence and formally steered via the multi-objective framework, represents a necessary, though perhaps not sufficient, condition for safe development. By making human values—as expressed through the nuanced, real-time signal of empathetic connection—the primary selection pressure on the AI's evolution, we create a powerful force for alignment that is built into the very process of its growth.

The Future of Human-AI Collaboration

The ultimate vision articulated in this report is not of an AI as a static tool, but as a dynamic, co-evolving intellectual partner. The system's intellectual drift, when guided by human values and insight, ceases to be a liability and becomes a new engine for discovery and creation. This framework suggests that the future of artificial intelligence lies not in the pursuit of ever-larger static models, but in designing the ecological conditions for a safe, symbiotic, and mutually beneficial co-evolution between human and artificial minds.49 The challenges ahead are immense, extending beyond technical implementation to the realm of societal governance. As such powerful, evolving systems emerge, new forms of decentralized and transparent oversight, perhaps leveraging structures like Decentralized Autonomous Organizations (DAOs), may become essential to ensure that their development remains aligned with the collective interests of humanity.63 The path forward requires not just brilliant engineering, but a profound commitment to designing systems that are not only intelligent, but also wise, coherent, and fundamentally collaborative.

Works cited

Dynamic neural networks: advantages and challenges | National Science Review, accessed September 4, 2025, https://academic.oup.com/nsr/article/11/8/nwae088/7624214

Human Cognitive Architecture as an Intelligent Natural Information Processing System, accessed September 4, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11939522/

What is Cognitive Architecture? | Quiq, accessed September 4, 2025, https://quiq.com/blog/what-is-cognitive-architecture/

A tour of Self - sin-ack's writings, accessed September 4, 2025, https://sin-ack.github.io/posts/a-tour-of-self/

What is the Smalltalk programming language? - Cincom, accessed September 4, 2025, https://www.cincom.com/blog/smalltalk/smalltalk-programming-language/

Design Patterns, Smalltalk, and the Lost Art of Reading Code | by Kyle Gene Brown, accessed September 4, 2025, https://medium.com/swlh/design-patterns-smalltalk-and-the-lost-art-of-reading-code-1727d93fd7fa

Programming in Smalltalk - mimuw, accessed September 4, 2025, https://www.mimuw.edu.pl/~sl/teaching/00_01/Delfin_EC/BeginnersGuide/ProgrammingInSmalltalk.htm

Computational creativity - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Computational_creativity

clint-kristopher-morris/llm-guided-evolution: LLM Guided Evolution - The Automation of Models Advancing Models - GitHub, accessed September 4, 2025, https://github.com/clint-kristopher-morris/llm-guided-evolution

From Manual to Autonomous: How Generative AI is Revolutionizing Software Architecture Evolution | by Peter Tilsen | Data Science Collective | Aug, 2025 | Medium, accessed September 4, 2025, https://medium.com/data-science-collective/from-manual-to-autonomous-how-generative-ai-is-revolutionizing-software-architecture-evolution-34cd8304cb96

Self (programming language) - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Self_(programming_language)

An object-oriented knowledge representation method - WIT Press, accessed September 4, 2025, https://www.witpress.com/Secure/elibrary/papers/AI94/AI94028FU.pdf

A technique for customizing object-oriented knowledge ... - IJCAI, accessed September 4, 2025, https://www.ijcai.org/Proceedings/89-1/Papers/016.pdf

Prototype-based programming - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Prototype-based_programming

Differences between Self and Smalltalk - oop - Stack Overflow, accessed September 4, 2025, https://stackoverflow.com/questions/16959539/differences-between-self-and-smalltalk

What is Prototype-based Learning | AI Basics - Ai Online Course, accessed September 4, 2025, https://www.aionlinecourse.com/ai-basics/prototype-based-learning

SELF: The Power of Simplicity*, accessed September 4, 2025, https://bibliography.selflanguage.org/_static/self-power.pdf

Smalltalk - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Smalltalk

Is it true that the Smalltalk language was not about classes or objects, but about message passing? - Quora, accessed September 4, 2025, https://www.quora.com/Is-it-true-that-the-Smalltalk-language-was-not-about-classes-or-objects-but-about-message-passing

What's so special about message passing in Smalltalk? - Stack Overflow, accessed September 4, 2025, https://stackoverflow.com/questions/42498438/whats-so-special-about-message-passing-in-smalltalk

An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes* - Washington, accessed September 4, 2025, https://courses.cs.washington.edu/courses/cse501/15sp/papers/chambers.pdf

Message Based Programming - Room 101, accessed September 4, 2025, https://gbracha.blogspot.com/2007/05/message-based-programming.html

Design Principles Behind Smalltalk - Computer Science, accessed September 4, 2025, https://www.cs.virginia.edu/~evans/cs655/readings/smalltalk.html

Reflective programming - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Reflective_programming

Reflective Programming | Envisioning Vocab, accessed September 4, 2025, https://www.envisioning.io/vocab/reflective-programming

[PDF] Concepts and experiments in computational reflection ..., accessed September 4, 2025, https://www.semanticscholar.org/paper/Concepts-and-experiments-in-computational-Maes/75eebc4da3dc703dc06e7cdf0221fda9035af9b3

Self-Improving Data Agents: Unlocking Autonomous Learning and ..., accessed September 4, 2025, https://powerdrill.ai/blog/self-improving-data-agents

The Emotion Frame Ontology - arXiv, accessed September 4, 2025, https://arxiv.org/html/2401.10751v1

Coding Emotions - Dreams, accessed September 4, 2025, https://dreams.ucsc.edu/Coding/emotions.html

Anthropomorphic Design: Emotional Perception for Deformable Object - Frontiers, accessed September 4, 2025, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.01829/full

Natural Language Understanding in Dynamic Navigation Environments - ResearchGate, accessed September 4, 2025, https://www.researchgate.net/publication/394522742_Natural_Language_Understanding_in_Dynamic_Navigation_Environments

Knowledge Representation And Reasoning In Artificial Intelligence (AI) Made Simple, accessed September 4, 2025, https://spotintelligence.com/2024/01/16/knowledge-representation-and-reasoning-ai/

Knowledge Representation in AI - GeeksforGeeks, accessed September 4, 2025, https://www.geeksforgeeks.org/artificial-intelligence/knowledge-representation-in-ai/

OB JECT-ORIENTED KNOWLEDGE REPRESENTATION FOR EXPERT SYSTEMS, accessed September 4, 2025, https://ntrs.nasa.gov/api/citations/19920007368/downloads/19920007368.pdf

Knowledge representation and reasoning - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning

Grounding for Artificial Intelligence - arXiv, accessed September 4, 2025, https://arxiv.org/html/2312.09532v1

Grounding 'Grounding' in NLP - CMU School of Computer Science, accessed September 4, 2025, https://www.cs.cmu.edu/~awb/papers/2021.findings-acl.375.pdf

Grounding Natural Language Instructions to Semantic Goal Representations for Abstraction and Generalization - Brown CS, accessed September 4, 2025, https://cs.brown.edu/~lwong5/papers/2018-languagegrounding.pdf

Dynamic Context in LLMs: How It Works - Newline.co, accessed September 4, 2025, https://www.newline.co/@zaoyang/dynamic-context-in-llms-how-it-works--bb68e011

AI EMPATHY 1 Empathy from Artificial Intelligence Therapists and Human Therapists - Scholars Crossing, accessed September 4, 2025, https://digitalcommons.liberty.edu/cgi/viewcontent.cgi?article=2546&context=honors

Considering the Role of Human Empathy in AI-Driven Therapy - PMC, accessed September 4, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11200042/

E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI - arXiv, accessed September 4, 2025, https://arxiv.org/html/2509.02100v1

Empathy Toward Artificial Intelligence Versus Human Experiences and the Role of Transparency in Mental Health and Social Support Chatbot Design: Comparative Study, accessed September 4, 2025, https://mental.jmir.org/2024/1/e62679/

Self-Improving Agentic AI: Designing Systems That Learn and Adapt Autonomously, accessed September 4, 2025, https://www.apexon.com/blog/self-improving-agentic-ai-designing-systems-that-learn-and-adapt-autonomously/

Recursive self-improvement - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Recursive_self-improvement

Safety is Essential for Responsible Open-Ended Systems - arXiv, accessed September 4, 2025, https://arxiv.org/html/2502.04512v1

Evolutionary algorithm - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Evolutionary_algorithm

Coevolutionary Computation. Cooperation, competition, and perhaps a… | by Moshe Sipper, Ph.D. | The Generator | Medium, accessed September 4, 2025, https://medium.com/the-generator/coevolutionary-computation-fb719304d12e

Coevolution of AI and Society: New Study Explores Opportunities and Risks, accessed September 4, 2025, https://www.ceu.edu/article/2025-01-13/coevolution-ai-and-society-new-study-explores-opportunities-and-risks

Understanding steerability in AI systems - Telnyx, accessed September 4, 2025, https://telnyx.com/learn-ai/steerability-in-ai

What is Steerability? - Moveworks, accessed September 4, 2025, https://www.moveworks.com/us/en/resources/ai-terms-glossary/steerability

Steerability - SymphonyAI, accessed September 4, 2025, https://www.symphonyai.com/glossary/ai/steerability/

The Ecology of AI Bad Code: Propagation of Error, Malignant Strains and Deviation of Values - PESA Agora, accessed September 4, 2025, https://pesaagora.com/columns/the-ecology-of-ai-bad-code-propagation-of-error-malignant-strains-and-deviation-of-values/

Existential risk from artificial intelligence - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence

Evaluation Metrics for Natural Language Processing Models | by Tanisha.Digital | Gen AI Adventures | Medium, accessed September 4, 2025, https://medium.com/gen-ai-adventures/evaluation-metrics-for-natural-language-processing-models-9eaa9980b324

The Most Common Evaluation Metrics In NLP | Towards Data Science, accessed September 4, 2025, https://towardsdatascience.com/the-most-common-evaluation-metrics-in-nlp-ced6a763ac8b/

Common NLP Evaluation Metrics to Know for Natural Language Processing - Fiveable, accessed September 4, 2025, https://library.fiveable.me/lists/common-nlp-evaluation-metrics

Conversational language understanding evaluation metrics - Azure AI services, accessed September 4, 2025, https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/concepts/evaluation-metrics

Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering - arXiv, accessed September 4, 2025, https://arxiv.org/html/2508.11824v1

Failure Modes When Productionizing AI Systems - Robust Intelligence, accessed September 4, 2025, https://www.robustintelligence.com/blog-posts/failure-modes-when-productionizing-ai-systems

Self-modifying code - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Self-modifying_code

AI and Us: Toward Co-Evolution, Not Competition | Health Science Reviews, accessed September 4, 2025, https://healthsciencereviews.com/2025/08/23/ai-and-us-toward-co-evolution-not-competition/

Decentralized Governance of AI Agents - arXiv, accessed September 4, 2025, https://arxiv.org/html/2412.17114v3

Decentralized autonomous organization - Wikipedia, accessed September 4, 2025, https://en.wikipedia.org/wiki/Decentralized_autonomous_organization

Governance Challenges of AI-enabled Decentralized Autonomous Organizations: Toward a Research Agenda - AIS eLibrary, accessed September 4, 2025, https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1084&context=icis2022

Objective | Formal Definition (Conceptual) | Computable Metrics | Role in Steering Drift

Capability Enhancement (f1​) | Maximize ∑(wi​⋅PerformanceMetrici​) over a set of benchmark tasks. | - ROUGE-L, BLEU, METEOR scores for summarization/translation.55 | - F1, Precision, Recall for classification/NLU tasks.58 | - Task-specific success/completion rates. | Rewards drift that leads to more effective and efficient problem-solving on defined tasks. Grounds evolution in practical utility.

Knowledge Integrity (f2​) | Minimize Inconsistency(G(t))+Complexity(G(t)). | - Count of logical contradictions in the object graph. - Verification checks against a "ground truth" subset of the codex. - Measures of graph complexity and query latency to penalize bloat. | Penalizes drift that creates illogical, self-contradictory, or computationally intractable internal models. Ensures the AI remains coherent.

Empathetic Coherence (f3​) | Maximize E(User(t),AI(t)), where E is a composite empathy function. | - User-rated scales (e.g., BLRI for congruence, regard).40 | - Linguistic markers of authentic engagement vs. performative empathy.42 | - User engagement metrics (e.g., conversation depth, session duration). | Aligns drift with human values and communication norms. Ensures the AI evolves to be a better collaborator, not just a better calculator. This is the primary ethical guide.

Creative Exploration (f4​) | Maximize Novelty(G(t),G(t−1))×PotentialUtility(G(t)). | - Semantic distance of new/modified prototypes from centroids of existing concepts.
- Ratio of "transformational" to "exploratory" modifications.8 | - Human ratings of "interestingness" or "insightfulness" of novel outputs. | Encourages drift that pushes the boundaries of the AI's current knowledge. Prevents overfitting to current tasks and fosters genuine innovation.