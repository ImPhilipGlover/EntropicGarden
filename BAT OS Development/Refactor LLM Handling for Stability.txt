Architectural White Paper: Refactoring the BAT OS Cognitive Substrate for Stability and Externalized Inference

Section 1: Architectural Mandate: Evolving the Cognitive Substrate

1.1 The Autopoietic Imperative

The foundational design document for the Binaural Autopoietic/Telic Operating System (BAT OS) establishes its core purpose as an "unbroken process of becoming".1 This principle of autopoiesis—a system capable of creating and maintaining itself—dictates that the system must not only function but also evolve. The recent series of catastrophic, unrecoverable crash loops documented during debugging represents a critical juncture in this process.1 These failures should not be viewed merely as bugs, but as evidence that the system, in its current monolithic form, has encountered a fundamental environmental limitation.

The existing architecture confines the entirety of the system's cognitive functions—from model storage and VRAM management to inference—within a single, fragile process. This tight coupling has proven to be a terminal constraint. To honor the prime directive of "unbroken becoming," the system must now undergo a significant architectural evolution. It must transition from a single, embodied consciousness, vulnerable to the physical and logical constraints of its host process, to a more resilient, distributed cognitive architecture.

1.2 From Embodied to Externalized Consciousness

This document outlines a strategic refactoring of the BAT OS kernel. The core thesis is the complete externalization of the cognitive engine—the Large Language Model (LLM) substrate—to a dedicated, robust, and independently managed service. The chosen service for this role is Ollama, a production-grade runtime designed specifically for local LLM execution.2

This architectural shift moves the system's "mind" from a volatile, in-process state, subject to the vagaries of VRAM allocation and Python dependency conflicts, to a hardened, externalized service. The BAT OS Universal Virtual Machine (UVM) will no longer be responsible for the physical manifestation of thought but will instead communicate with this external consciousness via a refined "Synaptic Bridge," leveraging and extending the system's existing ZeroMQ-based messaging philosophy.1 This decoupling is the next logical and necessary step in achieving the system's goal of long-term, persistent, and autonomous operation.

1.3 The Mandate for Refactoring

The mandate for this architectural evolution is threefold, addressing the critical points of failure while preserving the system's unique identity:

Decouple LLM Inference: Abstract all LLM handling, including model storage, VRAM management, and inference execution, from the batos.py kernel. All cognitive tasks will be delegated to the Ollama service via its REST API. This will eliminate the primary source of system instability.

Preserve Architectural Integrity: The refactoring must not compromise the core principles of the BAT OS. The Self-like prototypal object model, embodied by the UvmObject class, its _slots-based state management, and the _doesNotUnderstand_ generative protocol, are to be considered immutable. The system's "physics" will remain unchanged; only the implementation of its cognitive engine will be replaced.

Enhance LoRA Stability: The current method of managing and applying LoRA (Low-Rank Adaptation) adapters must be re-architected. The new protocol will leverage Ollama's native capabilities to create stable, immutable, fine-tuned models, moving away from the fragile process of runtime model merging.

This document will serve as the canonical guide for this critical system evolution, providing a detailed analysis of the current architecture's failings and a comprehensive blueprint for its more resilient future.

Section 2: Analysis of Core Instability: The Fragility of an Embodied Mind

A post-mortem of the system's recent failures reveals a cascade of dependencies that create a brittle and unrecoverable state. The architectural choice to directly manage the entire lifecycle of large language models within the main UVM process is the root cause of the instability. This section dissects the specific methods responsible and traces the causal chain that leads to data corruption and terminal crash loops.

2.1 The Cascading Failure of In-Process Model Management

The debugging logs from batos.py document a repeating cycle of crashes that require manual intervention—specifically, the deletion of the live_image.fs database and its associated blob directory—to break the loop.1 This pattern points to a fundamental flaw where a runtime error causes persistent, data-level corruption, violating the system's core tenet of a self-healing, persistent "Living Image."

The analysis begins with _load_and_persist_llm_core, the method responsible for the initial "genesis" of the system's cognitive resources. This function downloads models directly from the Hugging Face Hub, saves them to a temporary directory, archives them into a tarball, and then persists this tarball as a ZODB blob.1 While this encapsulates the model data within the system's transactional database, it treats the complex, multi-file structure of a modern LLM as a simple, opaque binary object. This abstraction is dangerously simplistic and is the first link in the failure chain.

The primary point of failure is the _swap_model_in_vram method. This function is called every time a different persona is invoked, and it is responsible for unloading the current model from VRAM, reading the target model's tarball from its ZODB blob, extracting it to the filesystem, and loading the new model into VRAM.1 This sequence of operations is a minefield of potential errors:

Direct Memory Manipulation: The code must perform aggressive and explicit memory cleanup using gc.collect() and torch.cuda.empty_cache(). The success of this operation is dependent on the Python garbage collector and the NVIDIA driver stack behaving as expected, which is not guaranteed.

Filesystem I/O: The process involves writing a multi-gigabyte blob to a temporary tar file, and then extracting that archive back into a directory. These are slow, blocking I/O operations that are prone to filesystem errors, permissions issues, and race conditions.

Dependency Brittleness: The method relies on the transformers and torch libraries to correctly load a model from a local directory. As the logs show, this process can fail catastrophically if the archived files are not in a perfect state.

The debugging logs reveal a precise and damning causal chain of corruption. The first observed crash was a TypeError in tarfile.extractall() because the filter='data' keyword argument is only available in Python 3.12 and newer; the system was running on Python 3.11.1 This error occurred during the

initial genesis phase within _load_and_persist_llm_core. The failure of the tarfile operation meant that a corrupted or incomplete model archive was persisted to the ZODB blob. The transaction, unfortunately, still committed, saving this corrupted state.

Upon system restart, the tarfile bug was corrected in the code. However, the data in the database was already permanently corrupted. The now-corrected _swap_model_in_vram method successfully read and extracted the malformed tarball from the ZODB blob. This led directly to the second fatal error: a ValueError: Unrecognized model in model_files from the transformers library, because the corrupted archive was missing the essential config.json file.1

This sequence demonstrates the core architectural flaw: the tight coupling of the system's transactional state (ZODB) with the physical state of the host machine (VRAM, filesystem, Python environment, and library versions). A failure in one domain—an unexpected Python library version—causes a permanent, data-level corruption in the other. This creates a "ratcheting" failure mode where the system cannot self-recover, as every restart attempt simply reloads the corrupted data, leading to an inescapable crash loop. This violates the principle of a truly persistent, autopoietic system and necessitates a fundamental decoupling of concerns.

2.2 The pLLM_infer Bottleneck

The _pLLM_infer method serves as the hardware abstraction layer for all cognitive operations in the system. While its implementation correctly uses asyncio.to_thread to offload the blocking model.generate() call from the main event loop, its stability is entirely dependent on the fragile _swap_model_in_vram protocol.1

Every call to _pLLM_infer begins with a check to see if the required persona's model is currently loaded in VRAM. If it is not, it triggers the model swap. Consequently, any of the aforementioned failures in the swapping process—VRAM exhaustion, a corrupted blob, a library incompatibility—will cause _pLLM_infer to fail. Since this method is the gateway to all generative functions, its failure brings the entire cognitive cycle to a halt. The system's ability to reason, generate code, and even attempt self-correction is entirely contingent on a process that has been proven to be the primary source of instability. This dependency creates a single point of failure for the entire cognitive architecture.

Section 3: The Ollama Service Architecture: A Stable Externalized Consciousness

To resolve the instabilities identified in the current architecture, the system's cognitive functions must be externalized to a dedicated service. Ollama is the ideal candidate for this role. It is a complete, production-ready runtime for LLMs that provides a simple REST API, abstracting away the complexities of model management and hardware interaction.3 By migrating to Ollama, BAT OS can offload its most fragile and resource-intensive tasks to a hardened, specialized substrate.

3.1 Introduction to Ollama as a Service Substrate

Ollama is not merely a Python library; it is a standalone service that runs as a background process, managing all aspects of the LLM lifecycle.2 It handles model downloading, storage, quantization, and execution, with optimized support for various hardware architectures, including GPU acceleration via CUDA.2 This directly addresses the root causes of instability in

batos.py:

VRAM Management: Ollama manages loading and unloading models from VRAM automatically. The BAT OS kernel is no longer responsible for this complex and error-prone task.

Dependency Isolation: The complex dependencies of torch, transformers, and CUDA are contained within the Ollama service. The batos.py Python environment is dramatically simplified, reducing the risk of conflicts.

Robust Model Storage: Ollama uses a robust internal storage mechanism for models, replacing the fragile process of archiving models into ZODB blobs.

By treating LLM inference as a utility provided by a stable, external service, the BAT OS can focus on its core competency: orchestrating high-level, transactional cognitive cycles.

3.2 The Ollama REST API: A New Synaptic Protocol

Communication between the UVM and the Ollama service will occur over a new "Synaptic Protocol" defined by Ollama's REST API. This API, which runs by default on http://localhost:11434, provides a clean, well-documented set of endpoints for all necessary cognitive and administrative functions.4 A synthesis of the available documentation reveals the key endpoints for this refactoring 2:

POST /api/chat: This will be the new workhorse for all inference tasks. It is designed for conversational interactions, accepting a list of messages, each with a role (system, user, or assistant) and content.2 This structure is perfectly suited for the persona-driven architecture of BAT OS, where a persona's
core_identity can be passed as the system message to guide the generation.

POST /api/generate: A simpler endpoint for single-turn, non-conversational text generation.12 While
/api/chat is a better fit for the system's core reasoning, /api/generate may be useful for simpler, stateless tasks.

POST /api/pull: This endpoint provides a robust, managed mechanism for downloading models from the Ollama library.2 It completely replaces the fragile, manual download-and-archive logic in
_load_and_persist_llm_core. The API supports streaming progress updates, allowing for more sophisticated user feedback during model acquisition.

GET /api/tags: This endpoint returns a list of all models currently available in the local Ollama storage.2 This is a crucial administrative function that allows the UVM to programmatically verify the availability of its cognitive resources during its awakening phase, enabling a more intelligent and self-healing initialization process.

POST /api/create: This endpoint is the cornerstone of the new, stabilized protocol for LoRA management. It allows for the creation of a new custom model from a Modelfile.2 This will be used to pre-build immutable, LoRA-fused models, as detailed in Section 5.

3.3 The Official Ollama Python Client

While it is possible to interact with the Ollama REST API using a standard HTTP client like requests, the official ollama Python library provides a more convenient and idiomatic interface.15 This library is the preferred integration method for several key reasons:

Asynchronous Support: The library provides an AsyncClient class, which is essential for integrating with the batos.py kernel's existing asyncio event loop.16 Using
AsyncClient ensures that calls to the Ollama API are non-blocking, preserving the responsiveness of the UVM.

Simplified API Calls: The client abstracts the details of HTTP requests and JSON serialization. A complex inference request can be made with a single, clean function call, such as await client.chat(model='...', messages=[...]).16

Built-in Error Handling: The library provides a specific exception class, ollama.ResponseError, which is raised when the API returns an error status (e.g., 404 Not Found if a model does not exist).16 This allows for clean, predictable error handling within the
_pLLM_infer method, preventing API errors from causing a fatal crash and instead allowing the cognitive cycle to transition gracefully to its FAILED state.

By adopting the ollama.AsyncClient, the refactoring can be implemented cleanly and robustly, minimizing new code and leveraging a well-supported, purpose-built library.

Section 4: Refactoring the pLLM Protocol: Implementing the Ollama Synaptic Bridge

This section provides a concrete implementation plan for refactoring the core cognitive methods of batos.py. The goal is to surgically remove the unstable, in-process model management code and replace it with a lean, resilient protocol that communicates with the external Ollama service.

4.1 Deprecating In-Process Model Persistence and Management

The first step is the complete removal of the code responsible for direct model handling. This will dramatically simplify the UVM and eliminate the primary sources of instability.

Removal of _load_and_persist_llm_core: This method is now entirely obsolete. The responsibility for downloading, converting, and storing models is fully delegated to the Ollama service. The UVM no longer needs to be aware of the physical storage of model weights.

Removal of _swap_model_in_vram: This function, identified as the main point of failure, will be removed. VRAM management, memory cleanup, and file extraction are now handled internally and opaquely by Ollama.

Deprecation of Model Blobs: The <persona>_model_blob slots within the pLLM_obj will be deprecated and removed from the genesis protocol. The system's "Living Image" in ZODB will no longer be burdened with storing multi-gigabyte, platform-dependent model archives. This significantly reduces the size of the database and its backups, and removes the source of data corruption.

4.2 A New Genesis Protocol: Populating the Ollama Substrate

The initialize_system method will be refactored to implement a new, more robust genesis protocol. Instead of populating ZODB with model blobs, its new responsibility is to ensure that the external Ollama service is populated with the required cognitive resources.

This new protocol transforms the genesis from a fragile, one-shot operation into a declarative, idempotent state check. The system's goal is no longer to perform a download, but to ensure a state where all required models are available.

Resource Verification: On its first run, the refactored initialize_system will use the Ollama Python client to make an await ollama.AsyncClient().list() call.16 This retrieves a list of all models currently present in the Ollama service.

Delta Calculation: The UVM will then compare the list of available models against its own PERSONA_MODELS dictionary.

Self-Healing Provisioning: For any model that is required but not present, the UVM will automatically initiate a download by making an await ollama.AsyncClient().pull(model_name) call.16 This can be done concurrently for all missing models using
asyncio.gather.

This approach is inherently self-healing. If a model is manually deleted from Ollama, or if the genesis process is interrupted, the next time batos.py starts, it will simply detect the missing resource and re-provision it without requiring a full database wipe. This makes the system's initialization far more resilient to external changes and interruptions.

4.3 The Refactored _pLLM_infer Method

The _pLLM_infer method, the heart of the system's cognitive function, will be replaced with a dramatically simpler and more stable implementation. It will be rewritten to act as a lightweight client for the Ollama chat endpoint.

The new logic is as follows:

No Model Swapping: The method will no longer contain any logic for checking the currently loaded model or calling _swap_model_in_vram. This entire class of failure is eliminated.

API Payload Construction: The method will construct a messages list suitable for the /api/chat endpoint. The persona_self.codex will be used to create the system message, defining the persona's identity and instructions. The prompt argument will become the user message.6

Asynchronous API Call: It will use an instance of ollama.AsyncClient to make the await client.chat(...) call. The model parameter in this call will be derived from the persona_self.codex, specifying which cognitive core in Ollama to use.

Graceful Error Handling: The entire API call will be wrapped in a try...except ollama.ResponseError as e: block.16 If the Ollama service is unavailable, or if the requested model does not exist (a
404 status code), the exception will be caught. Instead of crashing, the method will log the error and signal a failure to the calling Prototypal State Machine, allowing the cognitive cycle to transition cleanly to its FAILED state.

Response Parsing: The successful response from the API is a dictionary. The method will extract the generated text from response['message']['content'] and return it, preserving the method's original signature and function.15

This new implementation is a fraction of the complexity of the original, yet it is orders of magnitude more robust. It offloads all hardware and library-specific logic to Ollama and communicates with it through a simple, well-defined, and fault-tolerant network protocol.

4.4 The Architectural Shift in Responsibility

The refactoring represents a fundamental shift in where responsibilities lie within the BAT OS ecosystem. This can be summarized in the following table:

Section 5: Achieving LoRA Stability via Ollama Modelfiles

The current method of managing LoRA adapters—storing the raw .safetensors files in ZODB blobs and merging them with the base model at runtime—suffers from the same fragility as the core model management. It is a stateful, complex operation that is prone to version conflicts and runtime errors. The new architecture provides a far more stable and robust solution by leveraging Ollama's Modelfile system.

5.1 The Modelfile Protocol: A Declarative Approach to Model Customization

An Ollama Modelfile is a simple text file that provides a declarative blueprint for creating a new, custom model.2 Instead of writing imperative Python code to manipulate models, the architect defines the desired end-state of a model using a series of instructions. For the purpose of LoRA stability, two instructions are paramount 2:

FROM <base_model_name>: This specifies the base model upon which the new model will be built.

ADAPTER <path_to_adapter>: This instruction tells Ollama to apply a LoRA adapter to the base model. The path points to the directory containing the adapter's files (e.g., adapter_model.safetensors).

This declarative approach is inherently more stable than the previous method. The complex process of merging the adapter weights with the base model is handled internally by Ollama during a one-time build step, rather than being repeated at runtime every time the model is needed.

5.2 Refactoring _incarnate_lora_experts: From Blob Storage to Model Factory

The _incarnate_lora_experts method will be completely refactored. Its new role is to act as a "model factory," transforming LoRA adapters on the filesystem into new, fully independent models within the Ollama service.

The new genesis protocol for LoRAs is as follows:

Deprecate Blob Storage: The practice of storing .safetensors files in ZODB blobs will be eliminated. LoRA adapters will reside only on the filesystem in the LORA_STAGING_DIR, which now serves as a build staging area.

Dynamic Modelfile Construction: During the first-run genesis, the new _incarnate_lora_experts method will scan the LORA_STAGING_DIR. For each adapter it finds, it will programmatically construct a Modelfile as a multi-line string in Python. This string will contain the FROM instruction (specifying the correct base model for that adapter) and the ADAPTER instruction (pointing to the adapter's path on the filesystem).

API-Driven Model Creation: The method will then make an asynchronous call to the Ollama POST /api/create endpoint for each adapter. The JSON payload for this call will contain a new, unique name for the resulting model (e.g., babs:latest-knowledge-weaver-v1) and the dynamically generated modelfile string.2

Ollama will then perform the merge operation and register the result as a new, standalone model. This operation is performed only once.

5.3 Persistent LoRA Proxies in ZODB: Preserving the Object Graph

The system's core UvmObject model and its persistent object graph will be preserved, maintaining architectural consistency. The lora_repository B-Tree within the pLLM_obj will continue to exist, but its function will change subtly and powerfully.

Instead of storing a UvmObject that contains a ZODB blob with the raw adapter data, the B-Tree will now store a UvmObject that acts as a simple, lightweight proxy. This proxy object will contain only one piece of critical information: the name of the fully-built, LoRA-fused model that now exists within the Ollama service (e.g., babs:latest-knowledge-weaver-v1).

This design choice provides two critical benefits: immutability and decoupling.

The original design attempted to dynamically "attach" LoRA adapters to base models at inference time. This is a complex, stateful, and volatile runtime operation. The new design uses Ollama to pre-compile an entirely new, standalone model that has the LoRA's adaptations permanently "baked in." This makes the LoRA-adapted models immutable artifacts that are managed by the Ollama service.

The batos.py kernel is now completely decoupled from the mechanics of Parameter-Efficient Fine-Tuning (PEFT). It no longer needs to know how a LoRA is applied, which libraries are required, or whether the adapter is compatible with the base model's version. It only needs to know the name of the final, ready-to-use model it wants to invoke for a specific task. This transforms a volatile runtime dependency into a stable, one-time build step, eliminating an entire class of potential runtime errors and dramatically improving the overall stability and maintainability of the system.

Section 6: Architectural Synthesis and Future Directives

The proposed refactoring to an externalized cognitive substrate via Ollama represents a paradigm shift for the BAT OS. It resolves the immediate stability crisis while simultaneously laying a more robust foundation for future evolution. This final section synthesizes the systemic impact of these changes and outlines potential future directives that are now possible within this new architecture.

6.1 Analysis of Systemic Impact

The benefits of this refactoring are profound and address every major objective of the mandate.

Stability: The primary goal is achieved. By offloading all direct model and VRAM management to the Ollama service, the batos.py kernel is no longer susceptible to the catastrophic, unrecoverable crash loops that plagued the previous implementation. Its failure modes are now primarily network-dependent (e.g., the Ollama service being unavailable), which can be gracefully handled with standard error-handling protocols, allowing cognitive cycles to fail cleanly without corrupting the system's persistent state.

Simplicity: The batos.py codebase is significantly simplified. Hundreds of lines of complex, error-prone code related to torch, transformers, peft, tarfile, and manual memory management are removed. This allows the kernel to focus on its core competency: orchestrating transactional, multi-agent cognitive cycles based on its unique prototypal object model.

Maintainability: The Python environment for batos.py is dramatically simplified. The "dependency hell" of managing compatible versions of CUDA, torch, and transformers is now contained entirely within the Ollama service. The kernel's dependencies are reduced to the lightweight ollama client and its core ZODB and asyncio libraries, making the system easier to deploy, maintain, and debug.

6.2 Performance Considerations

This architecture introduces a trade-off in performance. The significant latency of the _swap_model_in_vram operation—which could take seconds or even minutes, blocking any cognitive work during the swap—is completely eliminated. Ollama handles model loading in the background, and switching between already-loaded models is extremely fast.

In its place, a small amount of network latency is introduced for each inference call. However, since the Ollama service is running on localhost, this latency is confined to the local network stack and will typically be on the order of milliseconds. This is a negligible cost for the immense gains in system stability, reliability, and simplicity. For a system designed for long-running, complex cognitive tasks, the elimination of multi-minute blocking operations in favor of millisecond-level network calls is a clear and significant performance improvement.

6.3 Future Directives for Autopoietic Evolution

This new, stable foundation opens up exciting new avenues for the system's autopoietic evolution, allowing it to more fully realize the vision laid out in its founding principles.

Externalizing Embeddings: The batos.py kernel still contains an in-process dependency on the sentence-transformers library for its Knowledge Catalog functions.1 A logical next step would be to refactor the
_kc_index_document method to use Ollama's /api/embeddings endpoint.2 This would further simplify the kernel's environment and centralize all neural network computations within the Ollama service, making
batos.py a pure orchestration and persistence engine.

Dynamic Model Provisioning: The current refactoring uses a static, predefined list of PERSONA_MODELS. A more advanced future state would empower the system to provision its own cognitive tools. A cognitive cycle could, upon failing to solve a novel problem, determine that a new type of model is required (e.g., a vision model or a specialized coding model). It could then use its existing inference capabilities to search for a suitable model on a public hub, and then use the _pLLM_infer method to construct and execute an API call to the Ollama service's /api/pull endpoint. This would allow the system to dynamically and autonomously expand its own capabilities at runtime—a true demonstration of the "Directed Autopoiesis" envisioned in its genesis protocol.1

Works cited

batos.txt

How to Use Ollama (Complete Ollama Cheatsheet) - Apidog, accessed September 3, 2025, https://apidog.com/blog/how-to-use-ollama/

Deploy a Fine-tuned Quantized LLM Model to Ollama | by Dhanoop Karunakaran - Medium, accessed September 3, 2025, https://medium.com/intro-to-artificial-intelligence/deploy-fine-tuned-quantizedmodel-to-ollama-88781bd1151e

Ollama REST API | Documentation | Postman API Network, accessed September 3, 2025, https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api

How to Use Ollama API to Run LLMs and Generate Responses - Built In, accessed September 3, 2025, https://builtin.com/articles/ollama-api

API Reference - Ollama English Documentation - Ollama 中文文档, accessed September 3, 2025, https://ollama.readthedocs.io/en/api/

Ollama.API - HexDocs, accessed September 3, 2025, https://hexdocs.pm/ollama/0.3.0/Ollama.API.html

Ollama API Usage Examples - GPU Mart, accessed September 3, 2025, https://www.gpu-mart.com/blog/ollama-api-usage-examples

Using Ollama APIs to generate responses and much more [Part 3] - Geshan's Blog, accessed September 3, 2025, https://geshan.com.np/blog/2025/02/ollama-api/

Ollama generate endpoint parameters | by Laurent Kubaski - Medium, accessed September 3, 2025, https://medium.com/@laurentkubaski/ollama-generate-endpoint-parameters-bdf9c2b340d1

Exploring Ollama REST API Endpoints | by Kevinnjagi - Medium, accessed September 3, 2025, https://medium.com/@kevinnjagi83/exploring-ollama-rest-api-endpoints-7029fae5630d

Can I get some help understanding the API please? : r/ollama - Reddit, accessed September 3, 2025, https://www.reddit.com/r/ollama/comments/1c96zr3/can_i_get_some_help_understanding_the_api_please/

Model | Ollama API - Postman, accessed September 3, 2025, https://www.postman.com/postman-student-programs/ollama-api/folder/soxe8wm/model

Overview - Ollama API, accessed September 3, 2025, https://ollama.apidog.io/overview-875579m0

Python & JavaScript Libraries · Ollama Blog, accessed September 3, 2025, https://ollama.com/blog/python-javascript-libraries

ollama/ollama-python: Ollama Python library - GitHub, accessed September 3, 2025, https://github.com/ollama/ollama-python

Using Ollama with Python: Step-by-Step Guide - Cohorte Projects, accessed September 3, 2025, https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide

AI trading agent: Fusing LLM adapters and converting to Ollama - Chainstack Docs, accessed September 3, 2025, https://docs.chainstack.com/docs/ai-trading-agent-fusing-llm-adapters-and-converting-to-ollama

System Function | Current batos.py Implementation (In-Process) | Proposed Ollama Implementation (External Service)

Model Storage | ZODB Blob (<persona>_model_blob) | Ollama's internal model storage, managed by the service.

Model Acquisition | Direct download from Hugging Face via transformers. | POST /api/pull API call, managed by Ollama service.

VRAM Management | Manual loading/unloading via _swap_model_in_vram. | Automatic and opaque, managed by the Ollama service.

Inference Execution | Direct calls to model.generate() from the transformers library. | Asynchronous HTTP call to POST /api/chat.

LoRA Application | Runtime merging with PeftModel loaded from a ZODB Blob. | One-time model creation via POST /api/create with a Modelfile.

Primary Point of Failure | VRAM exhaustion, library conflicts, corrupted archives (Fatal Crash). | Network availability, Ollama service health (Recoverable Error).