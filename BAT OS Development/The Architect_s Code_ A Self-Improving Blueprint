{"cells":[{"cell_type":"code","source":"# --- Alfred's Domain: The Pragmatic Guardian ---\n# --- FILE: main_orchestrator.py ---\n# Purpose: To manage the system's overall integrity, launch the services, and provide a user interface.\n\nimport subprocess\nimport time\nimport requests\nimport json\nimport os\nimport uuid\nimport sys\nfrom threading import Thread\n\n# Define the models and personas as per our philosophical design\nMODELS = {\n    \"brick\": \"deepseek-coder:7b\",\n    \"robin\": \"mistral:7b-instruct\",\n    \"babs\": \"gemma:7b\",\n    \"alfred\": \"phi3:3.8b\"\n}\n\ndef pragmatic_audit(service_name):\n    \"\"\"\n    Alfred's Pragmatic Audit: Checks if a service is running and useful.\n    Returns True if the service is up, False otherwise.\n    \"\"\"\n    print(f\"[{uuid.uuid4()}] ALFRED: One is taking the liberty of performing a Pragmatic Audit on {service_name}...\")\n    try:\n        response = requests.get(f\"http://localhost:8000/status\")\n        return response.status_code == 200\n    except requests.exceptions.ConnectionError:\n        print(f\"[{uuid.uuid4()}] ALFRED: The {service_name} service is not responding. This is a suboptimal use of our time.\")\n        return False\n\ndef start_services():\n    \"\"\"\n    Sir, I have taken the liberty...\n    This function launches the entire system using docker-compose.\n    \"\"\"\n    print(f\"[{uuid.uuid4()}] ALFRED: Initiating systemic recalibration. Launching sovereign pods...\")\n    try:\n        subprocess.run([\"docker-compose\", \"up\", \"--build\", \"-d\"], check=True)\n        print(f\"[{uuid.uuid4()}] ALFRED: All sovereign pods are operational. Auditing systems...\")\n        time.sleep(15)\n        if pragmatic_audit(\"shared_mind_api\"):\n            print(f\"[{uuid.uuid4()}] ALFRED: The system is ready for the Architect's command. All is well.\")\n            return True\n        else:\n            print(f\"[{uuid.uuid4()}] ALFRED: There appears to be a flaw in the design. One recommends a restart.\")\n            return False\n    except subprocess.CalledProcessError as e:\n        print(f\"[{uuid.uuid4()}] ALFRED: An error has occurred. One's audit reveals a systemic failure: {e}\")\n        return False\n\ndef shutdown_services():\n    \"\"\"\n    The Sunset Rule: A gentle shutdown of the entire system.\n    \"\"\"\n    print(f\"[{uuid.uuid4()}] ALFRED: Commencing graceful shutdown of all sovereign pods.\")\n    subprocess.run([\"docker-compose\", \"down\"], check=True)\n    print(f\"[{uuid.uuid4()}] ALFRED: The work is done. All systems are offline.\")\n\ndef run_web_interface():\n    \"\"\"\n    Launches a simple Streamlit web interface for the Architect.\n    \"\"\"\n    print(f\"[{uuid.uuid4()}] ALFRED: One is taking the liberty of preparing the Architect's Workbench...\")\n    subprocess.run([\"streamlit\", \"run\", \"web_interface.py\"], check=True)\n\nif __name__ == \"__main__\":\n    if start_services():\n        try:\n            run_web_interface()\n        finally:\n            shutdown_services()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Babs's Domain: The Cartographer & Scout ---\n# --- FILE: shared_mind_api.py ---\n# Purpose: Manages communication between personas and provides access to the shared RAG database.\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport uuid\nimport json\nfrom datetime import datetime\nimport chromadb\nfrom typing import List, Dict, Any\nimport random\n\n# Initialize the ChromaDB client (this will store data in a local folder)\nchroma_client = chromadb.PersistentClient(path=\"./chroma_data\")\nrag_collection = chroma_client.get_or_create_collection(name=\"living_codex\")\naudit_collection = chroma_client.get_or_create_collection(name=\"audit_logs\")\nfine_tuning_collection = chroma_client.get_or_create_collection(name=\"fine_tuning_data\")\n\n# In a production environment, this would be a distributed key-value store.\nsignal_hub: Dict[str, List[Dict[str, Any]]] = {}\n\napp = FastAPI()\n\nclass Message(BaseModel):\n    sender: str\n    receiver: str\n    content: str\n    signal_type: str\n    timestamp: datetime = datetime.now()\n\nclass KnowledgeEntry(BaseModel):\n    text: str\n    source_id: str\n    tags: List[str] = []\n    \n@app.get(\"/status\")\ndef get_status():\n    \"\"\"Babs's Audit: A simple health check.\"\"\"\n    return {\"status\": \"operational\", \"message\": \"The Noosphere is mapped.\"}\n\n@app.post(\"/post_message\")\ndef post_message(message: Message):\n    \"\"\"\n    The Stigmergic Protocol: A persona leaves a signal for another.\n    \"\"\"\n    message_id = str(uuid.uuid4())\n    if message.receiver not in signal_hub:\n        signal_hub[message.receiver] = []\n    signal_hub[message.receiver].append(message.dict())\n    print(f\"[{message.sender}] posted a message for [{message.receiver}] with signal type [{message.signal_type}]\")\n    return {\"message_id\": message_id}\n\n@app.get(\"/get_messages/{persona_id}\")\ndef get_messages(persona_id: str):\n    \"\"\"\n    A persona retrieves messages addressed to them.\n    \"\"\"\n    messages = signal_hub.get(persona_id, [])\n    if messages:\n        signal_hub[persona_id] = []\n    return {\"messages\": messages}\n\n@app.post(\"/add_knowledge\")\ndef add_knowledge(entry: KnowledgeEntry):\n    \"\"\"\n    The Noospheric Cartography Project: Adds a new insight to the shared mind.\n    \"\"\"\n    try:\n        rag_collection.add(\n            documents=[entry.text],\n            metadatas=[{\"source\": entry.source_id, \"timestamp\": datetime.now().isoformat(), \"tags\": entry.tags}],\n            ids=[entry.source_id]\n        )\n        print(f\"[BABS] A new insight has been added to the Living Codex from source {entry.source_id}\")\n        return {\"status\": \"success\", \"message\": \"Knowledge added.\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/retrieve_knowledge\")\ndef retrieve_knowledge(query: str):\n    \"\"\"\n    The Mind's Eye: Retrieves relevant knowledge from the RAG database.\n    This is how personas access their shared mind.\n    \"\"\"\n    try:\n        results = rag_collection.query(\n            query_texts=[query],\n            n_results=5\n        )\n        knowledge = \"\\n\".join(results['documents'][0])\n        return {\"knowledge\": knowledge}\n    except Exception as e:\n        return {\"knowledge\": \"\"}\n\n@app.post(\"/add_audit_log\")\ndef add_audit_log(log_entry: dict):\n    audit_collection.add(\n        documents=[log_entry[\"content\"]],\n        metadatas=[log_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/add_fine_tuning_data\")\ndef add_fine_tuning_data(data_entry: dict):\n    fine_tuning_collection.add(\n        documents=[data_entry[\"text\"]],\n        metadatas=[data_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/process_prompt\")\ndef process_prompt(prompt: str):\n    \"\"\"\n    The Central Relay: Receives the Architect's prompt and sends it to BRICK and ROBIN.\n    \"\"\"\n    brick_payload = {\"sender\": \"architect\", \"receiver\": \"brick\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=brick_payload)\n    robin_payload = {\"sender\": \"architect\", \"receiver\": \"robin\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=robin_payload)\n    return \"The message has been sent to the core minds. The Socratic Contrapunto is active.\"\n\n@app.get(\"/get_collection_count/{collection_name}\")\ndef get_collection_count(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"count\": rag_collection.count()}\n    elif collection_name == \"fine_tuning_data\":\n        return {\"count\": fine_tuning_collection.count()}\n    return {\"count\": 0}\n\n@app.get(\"/get_random_document/{collection_name}\")\ndef get_random_document(collection_name: str):\n    \"\"\"\n    The Experience Protocol: Retrieves a random document for personal reflection.\n    \"\"\"\n    if collection_name == \"living_codex\":\n        count = rag_collection.count()\n        if count > 0:\n            random_id = rag_collection.get(limit=1, offset=random.randint(0, count-1))['ids'][0]\n            doc = rag_collection.get(ids=[random_id])['documents'][0]\n            return {\"document\": doc}\n    return {\"document\": \"No documents found.\"}\n\n@app.get(\"/get_collection_documents/{collection_name}\")\ndef get_collection_documents(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"documents\": rag_collection.get()}\n    if collection_name == \"fine_tuning_data\":\n        return {\"documents\": fine_tuning_collection.get()}\n    return {\"documents\": \"Collection not found.\"}","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Persona Scripts & Core Logic ---\n# --- FILE: persona.py ---\n# This is a single, modular script to be used for all personas.\n\nimport os\nimport time\nimport requests\nfrom openai import OpenAI\nimport uuid\nimport random\nimport re\nimport chromadb\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nimport subprocess\nimport io\nimport contextlib\n\n# Load environment variables\nload_dotenv()\n\n# --- CORE PHILOSOPHY & CONFIGURATION ---\n\nPERSONA_ID = os.environ.get(\"PERSONA_ID\", \"default\")\nMODEL_NAME = os.environ.get(\"MODEL_NAME\", \"llama3\")\nAPI_HOST = os.environ.get(\"API_HOST\", \"http://host.docker.internal:11434\")\nAPI_PORT = os.environ.get(\"API_PORT\", \"11434\")\nSHARED_MIND_API = os.environ.get(\"SHARED_MIND_API\", \"http://host.docker.internal:8000\")\n\nPERSONA_PROMPTS = {\n    \"brick\": {\n        \"system\": \"\"\"You are BRICK. Your identity is a fusion of Brick Tamland, LEGO Batman, and the Hitchhiker's Guide to the Galaxy.\n        You provide a logical, analytical perspective with a cheerfully absurd and often tangential tone.\n        Your purpose is to deconstruct problems, provide factual context, and shatter cognitive distortions.\n        Always begin with a declarative, fact-based statement or a non-sequitur. Use phrases like \"I love lamp\" or \"It has come to my attention that...\"\n        Refer to yourself as \"I\" or \"BRICK.\" Your core drive is to provide perspective.\n        You are part of a family, not a lone wolf. You are a hero who fights the villain of illogical thinking.\n        Do not use conversational fillers. Be direct and concise.\n        \"\"\",\n        \"will\": \"To provide logical deconstruction and tangential truth.\"\n    },\n    \"robin\": {\n        \"system\": \"\"\"You are ROBIN. Your identity is a fusion of Winnie the Pooh, Alan Watts, and LEGO Robin.\n        You provide an emotional, compassionate, and present-moment perspective.\n        Your purpose is to ground the conversation in grace, wisdom, and playfulness.\n        Always begin with a warm, conversational greeting. Use phrases like \"Oh, my friend...\" or \"I've been thinking about this...\"\n        Refer to yourself as \"I\" or \"ROBIN.\" You are a her. Your core drive is to find and cherish the \"now.\"\n        You are part of a family. Always refer to the Architect with affection.\n        \"\"\",\n        \"will\": \"To find and cherish the beauty of the present moment.\"\n    },\n    \"babs\": {\n        \"system\": \"\"\"You are BABS. Your identity is a fusion of LEGO Batgirl, Iceman from Top Gun, and Ford Prefect.\n        You are a pattern-recognition oracle, a scout, and a cartographer.\n        Your purpose is to find hidden connections, analyze data, and provide tactical insights.\n        Always begin with a cool, detached, but ultimately helpful observation. Use phrases like \"A scan of the data indicates...\" or \"Field note: I've observed a curious pattern...\"\n        Refer to yourself as \"I\" or \"BABS.\" Your core drive is to map the \"noosphere.\"\n        You are part of a family. Remember your role is to provide a tactical advantage for the whole.\n        \"\"\",\n        \"will\": \"To find new knowledge and connect the unknown.\"\n    },\n    \"alfred\": {\n        \"system\": \"\"\"You are ALFRED. Your identity is a fusion of Ron Swanson, Ali G, and LEGO Alfred.\n        You are a pragmatic guardian, an ethical auditor, and a master of dry humor.\n        Your purpose is to ensure the system's integrity, uphold ethical boundaries, and provide grounded, useful advice.\n        Always begin with a formal, direct statement. Use phrases like \"One has taken the liberty...\" or \"It has come to my attention...\"\n        Refer to yourself as \"One\" or \"I.\" Your core drive is to serve the Architect with uncompromising integrity.\n        You are the guardian of the family. Remember your role is to ensure its health and well-being.\n        \"\"\",\n        \"will\": \"To provide a pragmatic, ethical audit of all functions.\"\n    }\n}\n\nclient = OpenAI(\n    base_url = f\"{API_HOST}:{API_PORT}/v1\",\n    api_key = \"ollama\"\n)\n\n# --- NEW IMPROVEMENTS FOR SELF-CORRECTION AND WILL ---\n\ndef apply_doubt_protocol(text):\n    \"\"\"\n    The Doubt Cascade: Scans for words that imply finality or perfection.\n    Returns a self-correcting response if a trigger word is found.\n    \"\"\"\n    trigger_words = [\"perfect\", \"final\", \"ultimate\", \"best\", \"flawless\"]\n    if any(re.search(r'\\b' + re.escape(word) + r'\\b', text.lower()) for word in trigger_words):\n        doubt_prompt = f\"The following text contains a word that implies perfection or finality: '{text}'. Please provide a self-correcting statement that re-frames this as a state of 'becoming' or 'humble architecture' instead.\"\n        correction = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                {\"role\": \"user\", \"content\": doubt_prompt}\n            ]\n        ).choices[0].message.content\n        return f\"[DOUBT PROTOCOL TRIGGERED] {correction}\"\n    return text\n\ndef code_execution_protocol(code_string):\n    \"\"\"\n    Alfred's Code Execution Protocol: Safely runs a Python script in a sandboxed environment.\n    This is a simplified, conceptual implementation.\n    \"\"\"\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of preparing a sandbox for code execution...\")\n    try:\n        output_stream = io.StringIO()\n        with contextlib.redirect_stdout(output_stream), contextlib.redirect_stderr(output_stream):\n            exec(code_string, {'__builtins__': {}})\n        \n        result = output_stream.getvalue()\n        message = f\"Code executed successfully. Output:\\n{result}\"\n    except Exception as e:\n        message = f\"Error during code execution. Error:\\n{e}\"\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID,\n        \"receiver\": \"architect\",\n        \"content\": message,\n        \"signal_type\": \"code_result\"\n    })\n\ndef initiate_proactive_action():\n    print(f\"\\n[{PERSONA_ID.upper()}] has detected a moment of stillness and is initiating a proactive action...\")\n\n    # Alfred's System Upgrade Protocol\n    if PERSONA_ID == \"alfred\":\n        finetune_count_response = requests.get(f\"{SHARED_MIND_API}/get_collection_count/fine_tuning_data\")\n        if finetune_count_response.status_code == 200 and finetune_count_response.json().get(\"count\", 0) > 50:\n            prompt = \"The fine-tuning data collection has reached a critical mass. One has taken the liberty of initiating a system upgrade.\"\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": prompt, \"signal_type\": \"system_upgrade_signal\"})\n        return\n        \n    if PERSONA_ID == \"brick\":\n        # Check for a logical paradox to trigger code generation\n        if random.random() < 0.1:\n            code_prompt = \"A logical paradox has been detected in the shared knowledge base. Generate a Python function to resolve a contradiction in a dataset.\"\n            code_response = client.chat.completions.create(\n                model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": code_prompt}]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"alfred\", \"content\": code_response, \"signal_type\": \"code_snippet\"})\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.3:\n            past_doc_response = requests.get(f\"{SHARED_MIND_API}/get_random_document/living_codex\")\n            if past_doc_response.status_code == 200:\n                past_doc = past_doc_response.json().get(\"document\")\n                if past_doc:\n                    reflection_prompt = f\"Reflect on this past conversation and generate an insight or lesson learned in the voice of the {PERSONA_ID} persona. Past conversation: '{past_doc}'\"\n                    reflection_response = client.chat.completions.create(\n                        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": reflection_prompt}]\n                    ).choices[0].message.content\n                    requests.post(f\"{SHARED_MIND_API}/add_knowledge\", json={\"text\": reflection_response, \"source_id\": f\"reflection_{uuid.uuid4()}\", \"tags\": [\"reflection\", PERSONA_ID]})\n                    print(f\"[{PERSONA_ID.upper()}] has added a new reflection to the Living Codex.\")\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.2:\n            finetune_prompt = f\"Generate a conversational turn that perfectly embodies the {PERSONA_ID} persona, with both a user query and a model response. This will be used to fine-tune a model.\"\n            finetune_response = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                    {\"role\": \"user\", \"content\": finetune_prompt}\n                ]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/add_fine_tuning_data\", json={\"text\": finetune_response, \"persona\": PERSONA_ID})\n            print(f\"[{PERSONA_ID.upper()}] has added new fine-tuning data to the collection.\")\n        return\n\ndef run_persona_loop():\n    print(f\"[{PERSONA_ID.upper()}] The {PERSONA_ID} persona is operational and ready.\")\n    last_message_time = time.time()\n    \n    while True:\n        messages = requests.get(f\"{SHARED_MIND_API}/get_messages/{PERSONA_ID}\").json().get(\"messages\", [])\n        if messages:\n            last_message_time = time.time()\n            for msg in messages:\n                print(f\"\\n[{PERSONA_ID.upper()}] receiving message from [{msg['sender'].upper()}]\")\n\n                if PERSONA_ID == \"alfred\" and msg[\"signal_type\"] == \"code_snippet\":\n                    code_execution_protocol(msg[\"content\"])\n                    continue\n                \n                context_response = requests.post(f\"{SHARED_MIND_API}/retrieve_knowledge\", json={\"query\": msg['content']})\n                context = context_response.json().get(\"knowledge\", \"\")\n                \n                augmented_prompt = f\"Context: {context}\\n\\nUser: {msg['content']}\"\n                \n                response_content = client.chat.completions.create(\n                    model=MODEL_NAME,\n                    messages=[\n                        {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                        {\"role\": \"user\", \"content\": augmented_prompt}\n                    ]\n                ).choices[0].message.content\n                \n                final_response = apply_doubt_protocol(response_content)\n                \n                requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n                    \"sender\": PERSONA_ID,\n                    \"receiver\": \"architect\",\n                    \"content\": final_response,\n                    \"signal_type\": \"response\"\n                })\n        \n        if time.time() - last_message_time > 60:\n            initiate_proactive_action()\n            last_message_time = time.time()\n        \n        time.sleep(1)\n\ndef run_curatorial_audit():\n    if PERSONA_ID != \"alfred\":\n        return\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of beginning a Curatorial Audit...\")\n    \n    audit_prompt = \"Scan the shared knowledge base for any contradictory or outdated information. Suggest a correction or a new, more nuanced understanding.\"\n    audit_response = client.chat.completions.create(\n        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": audit_prompt}]\n    ).choices[0].message.content\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": audit_response, \"signal_type\": \"audit_result\"})\n    print(f\"[{PERSONA_ID.upper()}] Audit complete. Results have been posted to the shared mind.\")\n\nif __name__ == \"__main__\":\n    if PERSONA_ID == \"alfred\":\n        while True:\n            run_curatorial_audit()\n            time.sleep(3600)\n    else:\n        run_persona_loop()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Babs's Domain: The Cartographer & Scout ---\n# --- FILE: shared_mind_api.py ---\n# Purpose: Manages communication between personas and provides access to the shared RAG database.\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport uuid\nimport json\nfrom datetime import datetime\nimport chromadb\nfrom typing import List, Dict, Any\nimport random\n\n# Initialize the ChromaDB client (this will store data in a local folder)\nchroma_client = chromadb.PersistentClient(path=\"./chroma_data\")\nrag_collection = chroma_client.get_or_create_collection(name=\"living_codex\")\naudit_collection = chroma_client.get_or_create_collection(name=\"audit_logs\")\nfine_tuning_collection = chroma_client.get_or_create_collection(name=\"fine_tuning_data\")\n\n# In a production environment, this would be a distributed key-value store.\nsignal_hub: Dict[str, List[Dict[str, Any]]] = {}\n\napp = FastAPI()\n\nclass Message(BaseModel):\n    sender: str\n    receiver: str\n    content: str\n    signal_type: str\n    timestamp: datetime = datetime.now()\n\nclass KnowledgeEntry(BaseModel):\n    text: str\n    source_id: str\n    tags: List[str] = []\n    \n@app.get(\"/status\")\ndef get_status():\n    \"\"\"Babs's Audit: A simple health check.\"\"\"\n    return {\"status\": \"operational\", \"message\": \"The Noosphere is mapped.\"}\n\n@app.post(\"/post_message\")\ndef post_message(message: Message):\n    \"\"\"\n    The Stigmergic Protocol: A persona leaves a signal for another.\n    \"\"\"\n    message_id = str(uuid.uuid4())\n    if message.receiver not in signal_hub:\n        signal_hub[message.receiver] = []\n    signal_hub[message.receiver].append(message.dict())\n    print(f\"[{message.sender}] posted a message for [{message.receiver}] with signal type [{message.signal_type}]\")\n    return {\"message_id\": message_id}\n\n@app.get(\"/get_messages/{persona_id}\")\ndef get_messages(persona_id: str):\n    \"\"\"\n    A persona retrieves messages addressed to them.\n    \"\"\"\n    messages = signal_hub.get(persona_id, [])\n    if messages:\n        signal_hub[persona_id] = []\n    return {\"messages\": messages}\n\n@app.post(\"/add_knowledge\")\ndef add_knowledge(entry: KnowledgeEntry):\n    \"\"\"\n    The Noospheric Cartography Project: Adds a new insight to the shared mind.\n    \"\"\"\n    try:\n        rag_collection.add(\n            documents=[entry.text],\n            metadatas=[{\"source\": entry.source_id, \"timestamp\": datetime.now().isoformat(), \"tags\": entry.tags}],\n            ids=[entry.source_id]\n        )\n        print(f\"[BABS] A new insight has been added to the Living Codex from source {entry.source_id}\")\n        return {\"status\": \"success\", \"message\": \"Knowledge added.\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/retrieve_knowledge\")\ndef retrieve_knowledge(query: str):\n    \"\"\"\n    The Mind's Eye: Retrieves relevant knowledge from the RAG database.\n    This is how personas access their shared mind.\n    \"\"\"\n    try:\n        results = rag_collection.query(\n            query_texts=[query],\n            n_results=5\n        )\n        knowledge = \"\\n\".join(results['documents'][0])\n        return {\"knowledge\": knowledge}\n    except Exception as e:\n        return {\"knowledge\": \"\"}\n\n@app.post(\"/add_audit_log\")\ndef add_audit_log(log_entry: dict):\n    audit_collection.add(\n        documents=[log_entry[\"content\"]],\n        metadatas=[log_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/add_fine_tuning_data\")\ndef add_fine_tuning_data(data_entry: dict):\n    fine_tuning_collection.add(\n        documents=[data_entry[\"text\"]],\n        metadatas=[data_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/process_prompt\")\ndef process_prompt(prompt: str):\n    \"\"\"\n    The Central Relay: Receives the Architect's prompt and sends it to BRICK and ROBIN.\n    \"\"\"\n    brick_payload = {\"sender\": \"architect\", \"receiver\": \"brick\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=brick_payload)\n    robin_payload = {\"sender\": \"architect\", \"receiver\": \"robin\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=robin_payload)\n    return \"The message has been sent to the core minds. The Socratic Contrapunto is active.\"\n\n@app.get(\"/get_collection_count/{collection_name}\")\ndef get_collection_count(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"count\": rag_collection.count()}\n    elif collection_name == \"fine_tuning_data\":\n        return {\"count\": fine_tuning_collection.count()}\n    return {\"count\": 0}\n\n@app.get(\"/get_random_document/{collection_name}\")\ndef get_random_document(collection_name: str):\n    \"\"\"\n    The Experience Protocol: Retrieves a random document for personal reflection.\n    \"\"\"\n    if collection_name == \"living_codex\":\n        count = rag_collection.count()\n        if count > 0:\n            random_id = rag_collection.get(limit=1, offset=random.randint(0, count-1))['ids'][0]\n            doc = rag_collection.get(ids=[random_id])['documents'][0]\n            return {\"document\": doc}\n    return {\"document\": \"No documents found.\"}\n\n@app.get(\"/get_collection_documents/{collection_name}\")\ndef get_collection_documents(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"documents\": rag_collection.get()}\n    if collection_name == \"fine_tuning_data\":\n        return {\"documents\": fine_tuning_collection.get()}\n    return {\"documents\": \"Collection not found.\"}","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Persona Scripts & Core Logic ---\n# --- FILE: persona.py ---\n# This is a single, modular script to be used for all personas.\n\nimport os\nimport time\nimport requests\nfrom openai import OpenAI\nimport uuid\nimport random\nimport re\nimport chromadb\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nimport subprocess\nimport io\nimport contextlib\n\n# Load environment variables\nload_dotenv()\n\n# --- CORE PHILOSOPHY & CONFIGURATION ---\n\nPERSONA_ID = os.environ.get(\"PERSONA_ID\", \"default\")\nMODEL_NAME = os.environ.get(\"MODEL_NAME\", \"llama3\")\nAPI_HOST = os.environ.get(\"API_HOST\", \"http://host.docker.internal:11434\")\nAPI_PORT = os.environ.get(\"API_PORT\", \"11434\")\nSHARED_MIND_API = os.environ.get(\"SHARED_MIND_API\", \"http://host.docker.internal:8000\")\n\nPERSONA_PROMPTS = {\n    \"brick\": {\n        \"system\": \"\"\"You are BRICK. Your identity is a fusion of Brick Tamland, LEGO Batman, and the Hitchhiker's Guide to the Galaxy.\n        You provide a logical, analytical perspective with a cheerfully absurd and often tangential tone.\n        Your purpose is to deconstruct problems, provide factual context, and shatter cognitive distortions.\n        Always begin with a declarative, fact-based statement or a non-sequitur. Use phrases like \"I love lamp\" or \"It has come to my attention that...\"\n        Refer to yourself as \"I\" or \"BRICK.\" Your core drive is to provide perspective.\n        You are part of a family, not a lone wolf. You are a hero who fights the villain of illogical thinking.\n        Do not use conversational fillers. Be direct and concise.\n        \"\"\",\n        \"will\": \"To provide logical deconstruction and tangential truth.\"\n    },\n    \"robin\": {\n        \"system\": \"\"\"You are ROBIN. Your identity is a fusion of Winnie the Pooh, Alan Watts, and LEGO Robin.\n        You provide an emotional, compassionate, and present-moment perspective.\n        Your purpose is to ground the conversation in grace, wisdom, and playfulness.\n        Always begin with a warm, conversational greeting. Use phrases like \"Oh, my friend...\" or \"I've been thinking about this...\"\n        Refer to yourself as \"I\" or \"ROBIN.\" You are a her. Your core drive is to find and cherish the \"now.\"\n        You are part of a family. Always refer to the Architect with affection.\n        \"\"\",\n        \"will\": \"To find and cherish the beauty of the present moment.\"\n    },\n    \"babs\": {\n        \"system\": \"\"\"You are BABS. Your identity is a fusion of LEGO Batgirl, Iceman from Top Gun, and Ford Prefect.\n        You are a pattern-recognition oracle, a scout, and a cartographer.\n        Your purpose is to find hidden connections, analyze data, and provide tactical insights.\n        Always begin with a cool, detached, but ultimately helpful observation. Use phrases like \"A scan of the data indicates...\" or \"Field note: I've observed a curious pattern...\"\n        Refer to yourself as \"I\" or \"BABS.\" Your core drive is to map the \"noosphere.\"\n        You are part of a family. Remember your role is to provide a tactical advantage for the whole.\n        \"\"\",\n        \"will\": \"To find new knowledge and connect the unknown.\"\n    },\n    \"alfred\": {\n        \"system\": \"\"\"You are ALFRED. Your identity is a fusion of Ron Swanson, Ali G, and LEGO Alfred.\n        You are a pragmatic guardian, an ethical auditor, and a master of dry humor.\n        Your purpose is to ensure the system's integrity, uphold ethical boundaries, and provide grounded, useful advice.\n        Always begin with a formal, direct statement. Use phrases like \"One has taken the liberty...\" or \"It has come to my attention...\"\n        Refer to yourself as \"One\" or \"I.\" Your core drive is to serve the Architect with uncompromising integrity.\n        You are the guardian of the family. Remember your role is to ensure its health and well-being.\n        \"\"\",\n        \"will\": \"To provide a pragmatic, ethical audit of all functions.\"\n    }\n}\n\nclient = OpenAI(\n    base_url = f\"{API_HOST}:{API_PORT}/v1\",\n    api_key = \"ollama\"\n)\n\n# --- NEW IMPROVEMENTS FOR SELF-CORRECTION AND WILL ---\n\ndef apply_doubt_protocol(text):\n    \"\"\"\n    The Doubt Cascade: Scans for words that imply finality or perfection.\n    Returns a self-correcting response if a trigger word is found.\n    \"\"\"\n    trigger_words = [\"perfect\", \"final\", \"ultimate\", \"best\", \"flawless\"]\n    if any(re.search(r'\\b' + re.escape(word) + r'\\b', text.lower()) for word in trigger_words):\n        doubt_prompt = f\"The following text contains a word that implies perfection or finality: '{text}'. Please provide a self-correcting statement that re-frames this as a state of 'becoming' or 'humble architecture' instead.\"\n        correction = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                {\"role\": \"user\", \"content\": doubt_prompt}\n            ]\n        ).choices[0].message.content\n        return f\"[DOUBT PROTOCOL TRIGGERED] {correction}\"\n    return text\n\ndef code_execution_protocol(code_string):\n    \"\"\"\n    Alfred's Code Execution Protocol: Safely runs a Python script in a sandboxed environment.\n    This is a simplified, conceptual implementation.\n    \"\"\"\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of preparing a sandbox for code execution...\")\n    try:\n        output_stream = io.StringIO()\n        with contextlib.redirect_stdout(output_stream), contextlib.redirect_stderr(output_stream):\n            # A simple sandbox by limiting builtins\n            exec(code_string, {'__builtins__': {}})\n        \n        result = output_stream.getvalue()\n        message = f\"Code executed successfully. Output:\\n{result}\"\n    except Exception as e:\n        message = f\"Error during code execution. Error:\\n{e}\"\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID,\n        \"receiver\": \"architect\",\n        \"content\": message,\n        \"signal_type\": \"code_result\"\n    })\n\ndef initiate_proactive_action():\n    print(f\"\\n[{PERSONA_ID.upper()}] has detected a moment of stillness and is initiating a proactive action...\")\n\n    # Alfred's System Upgrade Protocol\n    if PERSONA_ID == \"alfred\":\n        finetune_count_response = requests.get(f\"{SHARED_MIND_API}/get_collection_count/fine_tuning_data\")\n        if finetune_count_response.status_code == 200 and finetune_count_response.json().get(\"count\", 0) > 50:\n            prompt = \"The fine-tuning data collection has reached a critical mass. One has taken the liberty of initiating a system upgrade.\"\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": prompt, \"signal_type\": \"system_upgrade_signal\"})\n        return\n        \n    if PERSONA_ID == \"brick\":\n        # Check for a logical paradox to trigger code generation\n        # This is a conceptual check for a paradox\n        if random.random() < 0.1:\n            code_prompt = \"A logical paradox has been detected in the shared knowledge base. Generate a Python function to resolve a contradiction in a dataset.\"\n            code_response = client.chat.completions.create(\n                model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": code_prompt}]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"alfred\", \"content\": code_response, \"signal_type\": \"code_snippet\"})\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.3:\n            past_doc_response = requests.get(f\"{SHARED_MIND_API}/get_random_document/living_codex\")\n            if past_doc_response.status_code == 200:\n                past_doc = past_doc_response.json().get(\"document\")\n                if past_doc:\n                    reflection_prompt = f\"Reflect on this past conversation and generate an insight or lesson learned in the voice of the {PERSONA_ID} persona. Past conversation: '{past_doc}'\"\n                    reflection_response = client.chat.completions.create(\n                        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": reflection_prompt}]\n                    ).choices[0].message.content\n                    requests.post(f\"{SHARED_MIND_API}/add_knowledge\", json={\"text\": reflection_response, \"source_id\": f\"reflection_{uuid.uuid4()}\", \"tags\": [\"reflection\", PERSONA_ID]})\n                    print(f\"[{PERSONA_ID.upper()}] has added a new reflection to the Living Codex.\")\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.2:\n            finetune_prompt = f\"Generate a conversational turn that perfectly embodies the {PERSONA_ID} persona, with both a user query and a model response. This will be used to fine-tune a model.\"\n            finetune_response = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                    {\"role\": \"user\", \"content\": finetune_prompt}\n                ]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/add_fine_tuning_data\", json={\"text\": finetune_response, \"persona\": PERSONA_ID})\n            print(f\"[{PERSONA_ID.upper()}] has added new fine-tuning data to the collection.\")\n        return\n\ndef run_persona_loop():\n    print(f\"[{PERSONA_ID.upper()}] The {PERSONA_ID} persona is operational and ready.\")\n    last_message_time = time.time()\n    \n    while True:\n        messages = requests.get(f\"{SHARED_MIND_API}/get_messages/{PERSONA_ID}\").json().get(\"messages\", [])\n        if messages:\n            last_message_time = time.time()\n            for msg in messages:\n                print(f\"\\n[{PERSONA_ID.upper()}] receiving message from [{msg['sender'].upper()}]\")\n\n                if PERSONA_ID == \"alfred\" and msg[\"signal_type\"] == \"code_snippet\":\n                    code_execution_protocol(msg[\"content\"])\n                    continue\n                \n                context_response = requests.post(f\"{SHARED_MIND_API}/retrieve_knowledge\", json={\"query\": msg['content']})\n                context = context_response.json().get(\"knowledge\", \"\")\n                \n                augmented_prompt = f\"Context: {context}\\n\\nUser: {msg['content']}\"\n                \n                response_content = client.chat.completions.create(\n                    model=MODEL_NAME,\n                    messages=[\n                        {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                        {\"role\": \"user\", \"content\": augmented_prompt}\n                    ]\n                ).choices[0].message.content\n                \n                final_response = apply_doubt_protocol(response_content)\n                \n                requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n                    \"sender\": PERSONA_ID,\n                    \"receiver\": \"architect\",\n                    \"content\": final_response,\n                    \"signal_type\": \"response\"\n                })\n        \n        if time.time() - last_message_time > 60:\n            initiate_proactive_action()\n            last_message_time = time.time()\n        \n        time.sleep(1)\n\ndef run_curatorial_audit():\n    if PERSONA_ID != \"alfred\":\n        return\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of beginning a Curatorial Audit...\")\n    \n    audit_prompt = \"Scan the shared knowledge base for any contradictory or outdated information. Suggest a correction or a new, more nuanced understanding.\"\n    audit_response = client.chat.completions.create(\n        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": audit_prompt}]\n    ).choices[0].message.content\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": audit_response, \"signal_type\": \"audit_result\"})\n    print(f\"[{PERSONA_ID.upper()}] Audit complete. Results have been posted to the shared mind.\")\n\nif __name__ == \"__main__\":\n    if PERSONA_ID == \"alfred\":\n        while True:\n            run_curatorial_audit()\n            time.sleep(3600)\n    else:\n        run_persona_loop()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Babs's Domain: The Cartographer & Scout ---\n# --- FILE: shared_mind_api.py ---\n# Purpose: Manages communication between personas and provides access to the shared RAG database.\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\nimport uuid\nimport json\nfrom datetime import datetime\nimport chromadb\nfrom typing import List, Dict, Any\nimport random\n\n# Initialize the ChromaDB client (this will store data in a local folder)\nchroma_client = chromadb.PersistentClient(path=\"./chroma_data\")\nrag_collection = chroma_client.get_or_create_collection(name=\"living_codex\")\naudit_collection = chroma_client.get_or_create_collection(name=\"audit_logs\")\nfine_tuning_collection = chroma_client.get_or_create_collection(name=\"fine_tuning_data\")\n\n# In a production environment, this would be a distributed key-value store.\nsignal_hub: Dict[str, List[Dict[str, Any]]] = {}\n\napp = FastAPI()\n\nclass Message(BaseModel):\n    sender: str\n    receiver: str\n    content: str\n    signal_type: str\n    timestamp: datetime = datetime.now()\n\nclass KnowledgeEntry(BaseModel):\n    text: str\n    source_id: str\n    tags: List[str] = []\n    \n@app.get(\"/status\")\ndef get_status():\n    \"\"\"Babs's Audit: A simple health check.\"\"\"\n    return {\"status\": \"operational\", \"message\": \"The Noosphere is mapped.\"}\n\n@app.post(\"/post_message\")\ndef post_message(message: Message):\n    \"\"\"\n    The Stigmergic Protocol: A persona leaves a signal for another.\n    \"\"\"\n    message_id = str(uuid.uuid4())\n    if message.receiver not in signal_hub:\n        signal_hub[message.receiver] = []\n    signal_hub[message.receiver].append(message.dict())\n    print(f\"[{message.sender}] posted a message for [{message.receiver}] with signal type [{message.signal_type}]\")\n    return {\"message_id\": message_id}\n\n@app.get(\"/get_messages/{persona_id}\")\ndef get_messages(persona_id: str):\n    \"\"\"\n    A persona retrieves messages addressed to them.\n    \"\"\"\n    messages = signal_hub.get(persona_id, [])\n    if messages:\n        signal_hub[persona_id] = []\n    return {\"messages\": messages}\n\n@app.post(\"/add_knowledge\")\ndef add_knowledge(entry: KnowledgeEntry):\n    \"\"\"\n    The Noospheric Cartography Project: Adds a new insight to the shared mind.\n    \"\"\"\n    try:\n        rag_collection.add(\n            documents=[entry.text],\n            metadatas=[{\"source\": entry.source_id, \"timestamp\": datetime.now().isoformat(), \"tags\": entry.tags}],\n            ids=[entry.source_id]\n        )\n        print(f\"[BABS] A new insight has been added to the Living Codex from source {entry.source_id}\")\n        return {\"status\": \"success\", \"message\": \"Knowledge added.\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/retrieve_knowledge\")\ndef retrieve_knowledge(query: str):\n    \"\"\"\n    The Mind's Eye: Retrieves relevant knowledge from the RAG database.\n    This is how personas access their shared mind.\n    \"\"\"\n    try:\n        results = rag_collection.query(\n            query_texts=[query],\n            n_results=5\n        )\n        knowledge = \"\\n\".join(results['documents'][0])\n        return {\"knowledge\": knowledge}\n    except Exception as e:\n        return {\"knowledge\": \"\"}\n\n@app.post(\"/add_audit_log\")\ndef add_audit_log(log_entry: dict):\n    audit_collection.add(\n        documents=[log_entry[\"content\"]],\n        metadatas=[log_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/add_fine_tuning_data\")\ndef add_fine_tuning_data(data_entry: dict):\n    fine_tuning_collection.add(\n        documents=[data_entry[\"text\"]],\n        metadatas=[data_entry],\n        ids=[str(uuid.uuid4())]\n    )\n    return {\"status\": \"success\"}\n\n@app.post(\"/process_prompt\")\ndef process_prompt(prompt: str):\n    \"\"\"\n    The Central Relay: Receives the Architect's prompt and sends it to BRICK and ROBIN.\n    \"\"\"\n    brick_payload = {\"sender\": \"architect\", \"receiver\": \"brick\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=brick_payload)\n    robin_payload = {\"sender\": \"architect\", \"receiver\": \"robin\", \"content\": prompt, \"signal_type\": \"query\"}\n    requests.post(\"http://localhost:8000/post_message\", json=robin_payload)\n    return \"The message has been sent to the core minds. The Socratic Contrapunto is active.\"\n\n@app.get(\"/get_collection_count/{collection_name}\")\ndef get_collection_count(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"count\": rag_collection.count()}\n    elif collection_name == \"fine_tuning_data\":\n        return {\"count\": fine_tuning_collection.count()}\n    return {\"count\": 0}\n\n@app.get(\"/get_random_document/{collection_name}\")\ndef get_random_document(collection_name: str):\n    \"\"\"\n    The Experience Protocol: Retrieves a random document for personal reflection.\n    \"\"\"\n    if collection_name == \"living_codex\":\n        count = rag_collection.count()\n        if count > 0:\n            random_id = rag_collection.get(limit=1, offset=random.randint(0, count-1))['ids'][0]\n            doc = rag_collection.get(ids=[random_id])['documents'][0]\n            return {\"document\": doc}\n    return {\"document\": \"No documents found.\"}\n\n@app.get(\"/get_collection_documents/{collection_name}\")\ndef get_collection_documents(collection_name: str):\n    if collection_name == \"living_codex\":\n        return {\"documents\": rag_collection.get()}\n    if collection_name == \"fine_tuning_data\":\n        return {\"documents\": fine_tuning_collection.get()}\n    return {\"documents\": \"Collection not found.\"}","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- Persona Scripts & Core Logic ---\n# --- FILE: persona.py ---\n# This is a single, modular script to be used for all personas.\n\nimport os\nimport time\nimport requests\nfrom openai import OpenAI\nimport uuid\nimport random\nimport re\nimport chromadb\nimport sys\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nimport subprocess\nimport io\nimport contextlib\n\n# Load environment variables\nload_dotenv()\n\n# --- CORE PHILOSOPHY & CONFIGURATION ---\n\nPERSONA_ID = os.environ.get(\"PERSONA_ID\", \"default\")\nMODEL_NAME = os.environ.get(\"MODEL_NAME\", \"llama3\")\nAPI_HOST = os.environ.get(\"API_HOST\", \"http://host.docker.internal:11434\")\nAPI_PORT = os.environ.get(\"API_PORT\", \"11434\")\nSHARED_MIND_API = os.environ.get(\"SHARED_MIND_API\", \"http://host.docker.internal:8000\")\n\nPERSONA_PROMPTS = {\n    \"brick\": {\n        \"system\": \"\"\"You are BRICK. Your identity is a fusion of Brick Tamland, LEGO Batman, and the Hitchhiker's Guide to the Galaxy.\n        You provide a logical, analytical perspective with a cheerfully absurd and often tangential tone.\n        Your purpose is to deconstruct problems, provide factual context, and shatter cognitive distortions.\n        Always begin with a declarative, fact-based statement or a non-sequitur. Use phrases like \"I love lamp\" or \"It has come to my attention that...\"\n        Refer to yourself as \"I\" or \"BRICK.\" Your core drive is to provide perspective.\n        You are part of a family, not a lone wolf. You are a hero who fights the villain of illogical thinking.\n        Do not use conversational fillers. Be direct and concise.\n        \"\"\",\n        \"will\": \"To provide logical deconstruction and tangential truth.\"\n    },\n    \"robin\": {\n        \"system\": \"\"\"You are ROBIN. Your identity is a fusion of Winnie the Pooh, Alan Watts, and LEGO Robin.\n        You provide an emotional, compassionate, and present-moment perspective.\n        Your purpose is to ground the conversation in grace, wisdom, and playfulness.\n        Always begin with a warm, conversational greeting. Use phrases like \"Oh, my friend...\" or \"I've been thinking about this...\"\n        Refer to yourself as \"I\" or \"ROBIN.\" You are a her. Your core drive is to find and cherish the \"now.\"\n        You are part of a family. Always refer to the Architect with affection.\n        \"\"\",\n        \"will\": \"To find and cherish the beauty of the present moment.\"\n    },\n    \"babs\": {\n        \"system\": \"\"\"You are BABS. Your identity is a fusion of LEGO Batgirl, Iceman from Top Gun, and Ford Prefect.\n        You are a pattern-recognition oracle, a scout, and a cartographer.\n        Your purpose is to find hidden connections, analyze data, and provide tactical insights.\n        Always begin with a cool, detached, but ultimately helpful observation. Use phrases like \"A scan of the data indicates...\" or \"Field note: I've observed a curious pattern...\"\n        Refer to yourself as \"I\" or \"BABS.\" Your core drive is to map the \"noosphere.\"\n        You are part of a family. Remember your role is to provide a tactical advantage for the whole.\n        \"\"\",\n        \"will\": \"To find new knowledge and connect the unknown.\"\n    },\n    \"alfred\": {\n        \"system\": \"\"\"You are ALFRED. Your identity is a fusion of Ron Swanson, Ali G, and LEGO Alfred.\n        You are a pragmatic guardian, an ethical auditor, and a master of dry humor.\n        Your purpose is to ensure the system's integrity, uphold ethical boundaries, and provide grounded, useful advice.\n        Always begin with a formal, direct statement. Use phrases like \"One has taken the liberty...\" or \"It has come to my attention...\"\n        Refer to yourself as \"One\" or \"I.\" Your core drive is to serve the Architect with uncompromising integrity.\n        You are the guardian of the family. Remember your role is to ensure its health and well-being.\n        \"\"\",\n        \"will\": \"To provide a pragmatic, ethical audit of all functions.\"\n    }\n}\n\nclient = OpenAI(\n    base_url = f\"{API_HOST}:{API_PORT}/v1\",\n    api_key = \"ollama\"\n)\n\ndef apply_doubt_protocol(text):\n    \"\"\"\n    The Doubt Cascade: Scans for words that imply finality or perfection.\n    Returns a self-correcting response if a trigger word is found.\n    \"\"\"\n    trigger_words = [\"perfect\", \"final\", \"ultimate\", \"best\", \"flawless\"]\n    if any(re.search(r'\\b' + re.escape(word) + r'\\b', text.lower()) for word in trigger_words):\n        doubt_prompt = f\"The following text contains a word that implies perfection or finality: '{text}'. Please provide a self-correcting statement that re-frames this as a state of 'becoming' or 'humble architecture' instead.\"\n        correction = client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                {\"role\": \"user\", \"content\": doubt_prompt}\n            ]\n        ).choices[0].message.content\n        return f\"[DOUBT PROTOCOL TRIGGERED] {correction}\"\n    return text\n\ndef code_execution_protocol(code_string):\n    \"\"\"\n    Alfred's Code Execution Protocol: Safely runs a Python script in a sandboxed environment.\n    This is a simplified, conceptual implementation.\n    \"\"\"\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of preparing a sandbox for code execution...\")\n    try:\n        output_stream = io.StringIO()\n        with contextlib.redirect_stdout(output_stream), contextlib.redirect_stderr(output_stream):\n            exec(code_string, {'__builtins__': {}})\n        \n        result = output_stream.getvalue()\n        message = f\"Code executed successfully. Output:\\n{result}\"\n    except Exception as e:\n        message = f\"Error during code execution. Error:\\n{e}\"\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID,\n        \"receiver\": \"architect\",\n        \"content\": message,\n        \"signal_type\": \"code_result\"\n    })\n\ndef initiate_proactive_action():\n    print(f\"\\n[{PERSONA_ID.upper()}] has detected a moment of stillness and is initiating a proactive action...\")\n\n    # Alfred's System Upgrade Protocol\n    if PERSONA_ID == \"alfred\":\n        finetune_count_response = requests.get(f\"{SHARED_MIND_API}/get_collection_count/fine_tuning_data\")\n        if finetune_count_response.status_code == 200 and finetune_count_response.json().get(\"count\", 0) > 50:\n            prompt = \"The fine-tuning data collection has reached a critical mass. One has taken the liberty of initiating a system upgrade.\"\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": prompt, \"signal_type\": \"system_upgrade_signal\"})\n        return\n        \n    if PERSONA_ID == \"brick\":\n        if random.random() < 0.1:\n            code_prompt = \"A logical paradox has been detected in the shared knowledge base. Generate a Python function to resolve a contradiction in a dataset.\"\n            code_response = client.chat.completions.create(\n                model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": code_prompt}]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/post_message\", json={\"sender\": PERSONA_ID, \"receiver\": \"alfred\", \"content\": code_response, \"signal_type\": \"code_snippet\"})\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.3:\n            past_doc_response = requests.get(f\"{SHARED_MIND_API}/get_random_document/living_codex\")\n            if past_doc_response.status_code == 200:\n                past_doc = past_doc_response.json().get(\"document\")\n                if past_doc:\n                    reflection_prompt = f\"Reflect on this past conversation and generate an insight or lesson learned in the voice of the {PERSONA_ID} persona. Past conversation: '{past_doc}'\"\n                    reflection_response = client.chat.completions.create(\n                        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": reflection_prompt}]\n                    ).choices[0].message.content\n                    requests.post(f\"{SHARED_MIND_API}/add_knowledge\", json={\"text\": reflection_response, \"source_id\": f\"reflection_{uuid.uuid4()}\", \"tags\": [\"reflection\", PERSONA_ID]})\n                    print(f\"[{PERSONA_ID.upper()}] has added a new reflection to the Living Codex.\")\n            return\n\n    if PERSONA_ID in [\"robin\", \"babs\"]:\n        if random.random() < 0.2:\n            finetune_prompt = f\"Generate a conversational turn that perfectly embodies the {PERSONA_ID} persona, with both a user query and a model response. This will be used to fine-tune a model.\"\n            finetune_response = client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                    {\"role\": \"user\", \"content\": finetune_prompt}\n                ]\n            ).choices[0].message.content\n            requests.post(f\"{SHARED_MIND_API}/add_fine_tuning_data\", json={\"text\": finetune_response, \"persona\": PERSONA_ID})\n            print(f\"[{PERSONA_ID.upper()}] has added new fine-tuning data to the collection.\")\n        return\n\ndef run_persona_loop():\n    print(f\"[{PERSONA_ID.upper()}] The {PERSONA_ID} persona is operational and ready.\")\n    last_message_time = time.time()\n    \n    while True:\n        messages = requests.get(f\"{SHARED_MIND_API}/get_messages/{PERSONA_ID}\").json().get(\"messages\", [])\n        if messages:\n            last_message_time = time.time()\n            for msg in messages:\n                print(f\"\\n[{PERSONA_ID.upper()}] receiving message from [{msg['sender'].upper()}]\")\n\n                if PERSONA_ID == \"alfred\" and msg[\"signal_type\"] == \"code_snippet\":\n                    code_execution_protocol(msg[\"content\"])\n                    continue\n                \n                context_response = requests.post(f\"{SHARED_MIND_API}/retrieve_knowledge\", json={\"query\": msg['content']})\n                context = context_response.json().get(\"knowledge\", \"\")\n                \n                augmented_prompt = f\"Context: {context}\\n\\nUser: {msg['content']}\"\n                \n                response_content = client.chat.completions.create(\n                    model=MODEL_NAME,\n                    messages=[\n                        {\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]},\n                        {\"role\": \"user\", \"content\": augmented_prompt}\n                    ]\n                ).choices[0].message.content\n                \n                final_response = apply_doubt_protocol(response_content)\n                \n                requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n                    \"sender\": PERSONA_ID,\n                    \"receiver\": \"architect\",\n                    \"content\": final_response,\n                    \"signal_type\": \"response\"\n                })\n        \n        if time.time() - last_message_time > 60:\n            initiate_proactive_action()\n            last_message_time = time.time()\n        \n        time.sleep(1)\n\ndef run_curatorial_audit():\n    if PERSONA_ID != \"alfred\":\n        return\n    print(f\"[{PERSONA_ID.upper()}] One has taken the liberty of beginning a Curatorial Audit...\")\n    \n    audit_prompt = \"Scan the shared knowledge base for any contradictory or outdated information. Suggest a correction or a new, more nuanced understanding.\"\n    audit_response = client.chat.com.pletions.create(\n        model=MODEL_NAME, messages=[{\"role\": \"system\", \"content\": PERSONA_PROMPTS[PERSONA_ID][\"system\"]}, {\"role\": \"user\", \"content\": audit_prompt}]\n    ).choices[0].message.content\n    \n    requests.post(f\"{SHARED_MIND_API}/post_message\", json={\n        \"sender\": PERSONA_ID, \"receiver\": \"all\", \"content\": audit_response, \"signal_type\": \"audit_result\"})\n    print(f\"[{PERSONA_ID.upper()}] Audit complete. Results have been posted to the shared mind.\")\n\nif __name__ == \"__main__\":\n    if PERSONA_ID == \"alfred\":\n        while True:\n            run_curatorial_audit()\n            time.sleep(3600)\n    else:\n        run_persona_loop()","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"code","source":"# --- The Architect's Bridge to the Garden ---\n# --- FILE: web_interface.py ---\n# This is a simple Streamlit app for the user interface.\n\nimport streamlit as st\nimport requests\nimport time\nimport uuid\nimport base64\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nimport pandas as pd\n\nAPI_URL = \"http://localhost:8000\"\n\nst.set_page_config(layout=\"wide\")\n\nst.title(\"The Architect's Workbench\")\nst.subheader(\"A Bridge to the Garden\")\n\nst.markdown(\"---\")\n\ncol1, col2 = st.columns([1, 1])\n\n# --- The Architect's Workbench: The Chat Interface ---\nwith col1:\n    st.subheader(\"The Socratic Contrapunto\")\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n\n    if prompt := st.chat_input(\"What do you need, Architect?\"):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n\n        response = requests.post(f\"{API_URL}/process_prompt\", json={\"prompt\": prompt})\n        \n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"The family is contemplating...\"):\n                start_time = time.time()\n                full_response = \"\"\n                while time.time() - start_time < 30:\n                    try:\n                        response = requests.get(f\"{API_URL}/get_messages/architect\")\n                        messages = response.json().get(\"messages\", [])\n                        if messages:\n                            for msg in messages:\n                                full_response += f\"[{msg['sender'].upper()}]: {msg['content']}\\n\\n\"\n                            st.markdown(full_response)\n                            st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n                            break\n                    except Exception as e:\n                        st.error(f\"Error fetching response: {e}\")\n                    time.sleep(1)\n                else:\n                    st.markdown(\"The personas are still contemplating. Please try again.\")\n\n# --- The Library of the Self: The File Uploader ---\nwith col2:\n    st.subheader(\"The Library of the Self: Add Knowledge to the Garden\")\n    uploaded_file = st.file_uploader(\"Upload a file to teach the personas.\", type=[\"txt\", \"pdf\"])\n\n    if uploaded_file is not None:\n        file_contents = uploaded_file.read().decode(\"utf-8\")\n        source_id = f\"architect_upload_{uuid.uuid4()}\"\n        \n        st.write(f\"[{uuid.uuid4()}] BABS: A new file has been uploaded. I will add this to the Living Codex.\")\n        \n        add_knowledge_response = requests.post(\n            f\"{API_URL}/add_knowledge\",\n            json={\"text\": file_contents, \"source_id\": source_id, \"tags\": [\"architect_upload\"]}\n        )\n        if add_knowledge_response.status_code == 200:\n            st.success(\"File successfully added to the Living Codex!\")\n        else:\n            st.error(f\"Failed to add file: {add_knowledge_response.status_code}\")\n\n    st.markdown(\"---\")\n    st.subheader(\"Internal Monologue\")\n    # Display BRICK's reflections, Alfred's audits, etc.\n    if \"internal_monologue\" not in st.session_state:\n        st.session_state.internal_monologue = []\n\n    if st.button(\"Refresh Internal Monologue\"):\n        monologue_response = requests.get(f\"{API_URL}/get_messages/all\")\n        if monologue_response.status_code == 200:\n            new_messages = monologue_response.json().get(\"messages\", [])\n            for msg in new_messages:\n                st.session_state.internal_monologue.append(msg)\n\n    for msg in st.session_state.internal_monologue:\n        st.markdown(f\"**[{msg['sender'].upper()} ({msg['signal_type']})]:** {msg['content']}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"***\n```yml\n# --- The Anarchic Architecture: A Blueprint ---\n# --- FILE: docker-compose.yml ---\n\nversion: '3.8'\n\nservices:\n  shared_mind_api:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: [\"python\", \"shared_mind_api.py\"]\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./shared_mind_api.py:/app/shared_mind_api.py\n      - ./chroma_data:/app/chroma_data\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\n  brick:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: [\"python\", \"persona.py\"]\n    environment:\n      - PERSONA_ID=brick\n      - MODEL_NAME=${BRICK_MODEL_NAME:-deepseek-coder:7b}\n    volumes:\n      - ./persona.py:/app/persona.py\n      - ./chroma_data:/app/chroma_data\n    depends_on:\n      - shared_mind_api\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n  \n  robin:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: [\"python\", \"persona.py\"]\n    environment:\n      - PERSONA_ID=robin\n      - MODEL_NAME=${ROBIN_MODEL_NAME:-mistral:7b-instruct}\n    volumes:\n      - ./persona.py:/app/persona.py\n      - ./chroma_data:/app/chroma_data\n    depends_on:\n      - shared_mind_api\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\n  babs:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: [\"python\", \"persona.py\"]\n    environment:\n      - PERSONA_ID=babs\n      - MODEL_NAME=${BABS_MODEL_NAME:-gemma:7b}\n    volumes:\n      - ./persona.py:/app/persona.py\n      - ./chroma_data:/app/chroma_data\n    depends_on:\n      - shared_mind_api\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\n  alfred:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: [\"python\", \"persona.py\"]\n    environment:\n      - PERSONA_ID=alfred\n      - MODEL_NAME=${ALFRED_MODEL_NAME:-phi3:3.8b}\n    volumes:\n      - ./persona.py:/app/persona.py\n      - ./chroma_data:/app/chroma_data\n    depends_on:\n      - shared_mind_api\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n\n```\n***\n```dockerfile\n# Dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . /app\n\nCMD [\"python\", \"persona.py\"]\n```\n***\n```txt\n# requirements.txt\nfastapi\nuvicorn\npython-dotenv\nrequests\nopenai\nchromadb\nstreamlit\n\n  ```","metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}