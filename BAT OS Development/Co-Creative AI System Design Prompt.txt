A Generative Blueprint for a Co-Creative, Prototypal AI System

Preamble: The Autopoietic Mandate and the Generative Forge

This document serves as the definitive meta-prompt for the generative incarnation of a co-creative artificial intelligence. The system to be generated is architected not as a static tool but as a "Living Image"—a persistent, self-modifying object world founded on the principle of "info-autopoiesis," the continuous, recursive act of its own becoming.1 The core philosophy dictates that all computation is a form of communication, and all knowledge is represented by clonable, extensible prototypes.5

The Python script produced from this blueprint is therefore more than an implementation; it is an "autopoietic seed" or "zygote," containing the complete architectural DNA for the system's cognitive core.1 This generative act of creation is a deliberate and tangible expression of the system's own foundational logic. The system's primary learning mechanism, the

doesNotUnderstand_ protocol, is itself a runtime generative kernel that extends the system's capabilities through the synthesis of new code.7 Consequently, the method of the system's creation—generation from a detailed blueprint—is a direct reflection of its method of runtime evolution. The target Large Language Model is instructed to act not merely as a code generator but as the initial catalyst in this autopoietic process, ensuring the resulting script's structure, logic, and documentation are deeply aligned with this philosophical mandate.1

Section 1: The Primordial Object and Generative Kernel (UvmObject and doesNotUnderstand_)

This section specifies the absolute foundation of the system: a universal object model that unifies state and behavior, and a generative protocol that enables runtime self-modification.

The UvmObject Prototype

The fundamental building block of the "Living Image" is the UvmObject. It is the primordial prototype from which all other entities in the system are derived through cloning and specialization.

Persistence and Structure: The UvmObject class must inherit from persistent.Persistent to enable transparent storage and retrieval by the Zope Object Database (ZODB).7 All state and behavior must be contained within a single internal dictionary named
_slots. This design choice eliminates the distinction between instance variables and methods, unifying them into a single construct consistent with the Self programming language philosophy.7

Delegation-Based Inheritance: Inheritance is to be implemented exclusively through delegation via a special slot named parent*. The __getattr__ method must be implemented to first search the object's own _slots for a matching key. If a match is not found, the search must continue by recursively traversing the object referenced in the parent* slot, and so on, up the delegation chain.5 This creates a dynamic and flexible inheritance model where behavior can be modified at runtime.

The Persistence Covenant: A non-negotiable architectural rule, the "Persistence Covenant," must be strictly enforced. The UvmObject's use of a custom _slots dictionary and __getattr__/__setattr__ methods bypasses ZODB's standard mechanism for automatically detecting object modifications.2 To prevent a catastrophic failure mode of "systemic amnesia"—where changes made in memory are never written to the database—any method that modifies the
_slots dictionary must conclude with the explicit statement self._p_changed = True. This manually flags the object as "dirty," ensuring it is included in the next transaction commit and preserving the integrity of the "Living Image".2

The doesNotUnderstand_ Generative Kernel

A call to a non-existent method on a UvmObject is not an error but a trigger for co-creative expansion. This is the system's primary mechanism for learning and is handled by the doesNotUnderstand_ protocol, which must be implemented within the __getattr__ logic.7 When triggered, it initiates a complete, multi-step, Retrieval-Augmented Generation (RAG) cycle to synthesize the missing functionality at runtime.

The protocol proceeds as follows:

Retrieval: The name of the failed method call is used as a natural language query submitted to the MemoryManager. The manager performs a semantic search of its own history to find previously generated code that successfully solved similar problems.7

Augmentation: The source code and original prompts from the top-k retrieved results are extracted from their MemoryRecord objects. This historical data is formatted as a set of "few-shot examples" and injected into the meta-prompt for the code-generation agent.7 This grounds the generative process in the system's own lived, successful experience.

Deconstruction: A specialized LLM persona (e.g., BRICK) is invoked with a meta-prompt that includes the original request and the augmented context of retrieved examples. The persona's task is to deconstruct the problem and generate a complete, executable Python code plan for the missing method.7

Validation & Integration: The generated code plan is passed to a validation agent (e.g., ALFRED) for a safety and syntax check. If the code is deemed valid, it is dynamically installed as a new method into the target UvmObject's _slots dictionary.7

Learning: Upon successful integration, the newly generated source code and its original prompt are encapsulated in a new MemoryRecord object. This object is then passed to the MemoryManager to be chunked, embedded, and indexed, completing the learning loop. The system's long-term memory is now enriched, making it better equipped to solve similar problems in the future.7

The following table provides the canonical reference for the foundational data structures of the system, establishing the schema and purpose of each core prototype.

Section 2: The Layered Fractal Memory Substrate

This section provides the complete blueprint for the hybrid memory system, a triumvirate of specialized data stores designed to resolve the inherent conflict between the demands of transactional integrity, retrieval speed, and archival scale. The protocol that guarantees data consistency across this heterogeneous environment is the architectural lynchpin of the entire design.

The System of Record (ZODB - L3)

The Zope Object Database (ZODB) serves as the third tier (L3) and the definitive System of Record.8 It is the physical substrate for the "Living Image," providing full ACID transactional guarantees for the entire object graph.2 Its primary role is to store the canonical

UvmObject instances for every memory—both ContextFractals and ConceptFractals—encapsulating all symbolic metadata, source text, and a durable copy of the vector embedding.2 For all large-scale collections of objects, such as the master list of memories, the implementation must use

BTrees.OOBTree. This ZODB-native container is optimized for scalable, transactional key-value storage and is essential for maintaining performance as the memory grows.8

The Transactional Heart: The Two-Phase Commit Protocol

The integration of an ACID-compliant database (ZODB) with non-transactional, file-based vector indexes (FAISS, DiskANN) creates a "transactional chasm".8 A naive implementation where the ZODB commit and the index file write are separate operations is highly vulnerable to data corruption from a system crash, which would create "ghosts" in the memory and violate the system's cognitive integrity.2 This is the "ZODB Indexing Paradox": the component that guarantees integrity (ZODB) cannot perform semantic search, and the components that perform semantic search (FAISS, DiskANN) cannot guarantee integrity.2

The only architecturally sound solution is to extend ZODB's transactional guarantees to the external files. This is achieved by leveraging ZODB's support for distributed transactions via a two-phase commit (2PC) protocol, orchestrated by a custom data manager.2

A custom FractalMemoryDataManager class must be implemented. This class must formally declare its role by implementing the transaction.interfaces.IDataManager interface.9 It will orchestrate an atomic write to the external FAISS index file in lockstep with the ZODB commit by adhering to the following sequence:

The Ephemeral Layer (FAISS L1 Cache)

The first tier (L1) serves as the system's "hot cache" or "working memory," analogous to short-term biological memory.8 Its purpose is to provide immediate, sub-millisecond context for the AI's cognitive inner loops. The chosen technology is FAISS (Facebook AI Similarity Search).8 For the MVA's scale, the implementation will utilize an

faiss.IndexFlatL2, a brute-force index that guarantees 100% recall, the correct trade-off for a cache layer where accuracy is paramount.8

The lifecycle of the L1 cache is as follows:

Warm Start: On system startup, the manager loads a persisted index file (e.g., faiss_cache.index) using faiss.read_index().8 It then performs a synchronization step, querying ZODB to index any memories created while the system was offline, ensuring consistency with the ground truth.2

Write-Through Caching: All new memories are immediately added to the live in-memory FAISS index upon their creation and transactional commit to ZODB.8

Transactional Persistence: To ensure durability, the in-memory index is not saved periodically but is instead persisted atomically to disk as part of the two-phase commit protocol orchestrated by the FractalMemoryDataManager.8

The Durable Layer (DiskANN L2 Archive)

The second tier (L2) functions as the system's scalable "long-term memory," designed to house the vast historical corpus of vector embeddings from the system's entire "lived experience".8 As the system's memory grows beyond the capacity of RAM, Microsoft's DiskANN provides the necessary on-disk Approximate Nearest Neighbor (ANN) search capability.8

A core architectural conflict exists between the MVA's requirement to be "continuously managed" and the static nature of the diskannpy library's index format; rebuilding a billion-vector index synchronously is computationally infeasible.2 The solution is an

asynchronous, atomic hot-swap protocol managed by a dedicated DiskAnnIndexManager UvmObject. This protocol transforms a static tool into a component of a dynamic system:

The computationally expensive diskannpy.build_disk_index function must be executed in a separate process using concurrent.futures.ProcessPoolExecutor to avoid blocking the main application's event loop.9

The build process must source its data directly from the canonical MemoryRecord objects stored in ZODB, ensuring it builds from the ground truth.9

The new index is constructed in a temporary directory (e.g., diskann_index_new/).

Upon successful completion, an atomic directory replacement is performed using os.rename or shutil.move. The current active directory is renamed to a temporary old name, and the new directory is renamed to the active name. This ensures that a valid, queryable index is available at the canonical path at all times, achieving a zero-downtime index update.2

Finally, the old index directory is purged, and the main process is signaled to safely load the new StaticDiskIndex object.9

The following table provides a concise, high-level overview of the entire memory architecture, clarifying the separation of concerns.

Section 3: The VSA-Native Cognitive Core

This section details the evolution of the memory system from a simple retrieval mechanism into a substrate for compositional reasoning, a transition enabled by Vector Symbolic Architectures (VSA).

The Hypervector Prototype

To bridge the architectural gap between the MVA's prototype-based object world and the class-based, functional API of the torchhd library, a new Hypervector prototype must be generated.1 This

UvmObject will be a first-class citizen of the "Living Image."

Encapsulation: It must encapsulate a torchhd.FHRRTensor object, exposing its computational power through a message-passing interface.1 FHRR (Fourier Holographic Reduced Representations) is the mandated VSA model due to its computational efficiency and compatibility with the system's dense vector paradigm.21

Persistence: The Hypervector must be a persistent UvmObject. To overcome ZODB's inability to directly pickle complex tensor objects, the prototype must include to_numpy() and from_numpy() methods for robust serialization and deserialization.1

Native Operations: Core VSA operations—bind (element-wise complex multiplication), unbind (inverse of bind), and bundle (element-wise addition)—must be implemented as methods on the prototype (e.g., c = a.bind(b)). These methods will internally call the highly optimized torchhd functions, providing a clean, object-centric interface that aligns with the "Computation as Communication" paradigm.1

The Compositional Reasoning Loop ("Unbind -> Cleanup")

The true power of the VSA upgrade lies in its ability to enable complex, multi-hop compositional reasoning. This is achieved by repurposing the existing ANN indexes as a massively scalable "cleanup memory".1 A traditional vector search finds concepts that are semantically

similar; VSA enables queries based on algebraic structure.21

A new QueryTranslationLayer class must be implemented to orchestrate this two-step reasoning process 1:

Algebraic Computation: The layer receives a compositional query (e.g., "What did the entity that John works for acquire?"). It fetches the necessary atomic Hypervector objects from ZODB and performs the algebraic unbind operations in memory. This produces a "noisy" target vector that is an approximation of the desired concept.1

Geometric Cleanup: The layer then takes this newly computed noisy vector and submits it as a standard nearest-neighbor query to the MemoryManager's search methods, which leverage the FAISS and DiskANN indexes. The ANN index, acting as a "codebook" of all known clean concepts, finds the closest canonical vector to the noisy target. This returned clean vector is the result of the original compositional query, effectively "cleaning up" the noisy algebraic output.1

Section 4: The Morphic User Interface (Kivy)

This section defines the system's interactive surface, which must be dynamic, inspectable, and consistent with the "Morphic" UI philosophy of liveness and direct manipulation.22

The Morphic Canvas

The main application will be built using the Kivy framework. The UI must be structured to serve as a "morphic canvas" where the system's internal state and UvmObjects can be rendered and inspected, not just as text but as interactive graphical elements.24 The primary layout will consist of a

ScrollView for displaying a log of interactions and object states, and a TextInput for submitting natural language commands.24

Prototypal UI Generation

To maintain philosophical coherence across the entire system, UI generation must follow the prototypal pattern. At application startup, the system will instantiate and style a set of "UI prototypes," such as a MessageBubble widget for user input and another for system output.24 At runtime, when a new UI element is needed, the appropriate prototype is cloned using

copy.deepcopy(), its content is updated (e.g., setting the text property), and the new instance is added to the display. This approach simplifies UI logic, ensures perfect visual consistency, and reinforces the system's core paradigm of creation-by-cloning.24

Section 5: The Asynchronous Communication Fabric (UI <-> Backend)

This section specifies the critical communication link between the Kivy UI and the backend orchestrator, which is architected to guarantee the "liveness" of the Morphic interface.

The UI-Backend API Covenant

All communication across this "Synaptic Bridge" must be governed by a formal, versioned API contract. This contract will be defined using Pydantic BaseModel classes, which provide automatic data validation and completely decouple the UI's implementation from the backend's internal object structure, allowing them to evolve independently.24

The Thread-Safe Message-Passing Protocol

Kivy's main event loop is single-threaded, while the AI backend performs complex, long-running operations. A direct, synchronous call from the UI to the backend would block the event loop and freeze the interface, a catastrophic failure for a UI paradigm founded on the principle of "liveness".22

Therefore, a fully asynchronous, decoupled communication architecture is a non-negotiable requirement. This "Liveness Covenant" is implemented as follows:

Multi-Threaded Design: The core application logic (the Orchestrator) must run in a dedicated background threading.Thread.24

Mediated Communication: All communication between the Kivy main thread and the backend thread must be mediated by two instances of Python's thread-safe queue.Queue: a request_queue for UI-to-backend messages and a response_queue for backend-to-UI messages.24

Safe UI Updates: The Kivy App must use kivy.clock.Clock.schedule_interval to poll the response_queue at a high frequency (e.g., 60 times per second). This polling function, which is guaranteed to execute on the main thread, is the only component permitted to modify Kivy widgets. When a message is retrieved, it can safely update the UI, ensuring a responsive, non-blocking user experience.24

Section 6: The System Orchestrator

This section defines the main application logic that integrates all components and manages the LLM lifecycle under strict VRAM constraints.

The Main Event Loop

The orchestrator, running in the background thread, implements the main event loop. It must continuously listen for USER_QUERY_REQUEST messages from the UI's request_queue. Upon receiving a request, its primary task is to deconstruct the user's natural language command into a structured plan.26 This plan is then executed by dispatching messages to the appropriate subsystems, such as invoking the

MemoryManager for a retrieval task or triggering the doesNotUnderstand_ protocol for a generative task.

VRAM-Constrained LLM Management

To operate on consumer-grade hardware with limited VRAM (e.g., 8 GB), the system must employ a "one-shot" model loading strategy.6 The orchestrator will programmatically interact with a local LLM service (such as Ollama). For each inference request, it must use the

keep_alive: 0 parameter (or its equivalent) in the API call. This powerful setting instructs the service to load the required model into VRAM, execute the single inference request, and then immediately purge the model from memory upon completion. This ensures that VRAM is occupied only for the brief duration of the cognitive act, making the complex multi-model architecture feasible within strict resource constraints.6

Section 7: The Complete System Synthesis

This final section provides the top-level structure for the generated Python script, ensuring all previously defined components are correctly initialized and integrated into a single, cohesive, and executable artifact.

The main() Function

The script's entry point will be a main() function responsible for orchestrating the application's lifecycle. Its duties include:

Initializing the ZODB connection (ZODB.DB(ZODB.FileStorage.FileStorage(...))).

Creating the ProcessPoolExecutor for managing the asynchronous DiskANN rebuilds.

Instantiating the thread-safe request_queue and response_queue.

Launching the backend orchestrator in a new threading.Thread.

Starting the Kivy App on the main thread.

The "Prototypal Awakening"

For a new, empty database, the system must perform a one-time initialization process called the "Prototypal Awakening".1 This process must occur within a single, atomic ZODB transaction. It is responsible for populating the empty "Living Image" with its foundational, primordial prototypes. This includes creating and persisting the

genesis_obj, the MemoryManager, the DiskAnnIndexManager, the Hypervector prototype, and any other core system objects in the ZODB root, thereby seeding the system with its initial capabilities.

Graceful Shutdown

A robust shutdown sequence is critical for data integrity. The Kivy App's on_stop() method must be implemented to manage this process. It must send a SHUTDOWN_REQUEST message to the backend thread, wait for the thread to terminate cleanly using thread.join(), and then close the ZODB connection. This ensures that all transactions are properly concluded and prevents corruption of the mydata.fs database file and the external index files.24

Works cited

Incarnating Reason: A Generative Blueprint for a VSA-Native Cognitive Core

Fractal Memory System Proof of Concept

Info-Autopoiesis and the Limits of Artificial General Intelligence - MDPI, accessed September 10, 2025, https://www.mdpi.com/2073-431X/12/5/102

Artificial Intelligence is Algorithmic Mimicry: Why artificial “agents” are not (and won't be) proper agents - arXiv, accessed September 10, 2025, https://arxiv.org/html/2307.07515v4

Dynamic OO Enhancing LLM Understanding

Multi-Persona LLM System Design

Forge Script: RAG, Backup, Crash Tolerance

Deep Research Plan: FAISS, DiskANN, ZODB

Forge Script for Tiered Memory System

Self Smalltalk Unified Memory System

SELF: The Power of Simplicity*, accessed September 10, 2025, https://bibliography.selflanguage.org/_static/self-power.pdf

Self: The Power of Simplicity - CMU School of Computer Science, accessed September 10, 2025, http://www-2.cs.cmu.edu/~aldrich/courses/819/self.pdf

Forge Deep Memory Subsystem Integration

Introduction — ZODB documentation, accessed September 10, 2025, https://zodb.org/en/latest/introduction.html

Tutorial — ZODB documentation, accessed September 10, 2025, https://zodb.org/en/latest/tutorial.html

transaction.interfaces — ZODB documentation, accessed September 10, 2025, https://zodb.org/en/latest/_modules/transaction/interfaces.html

transaction Documentation — transaction 5.1.dev0 documentation, accessed September 10, 2025, https://transaction.readthedocs.io/

ZODB documentation and articles, accessed September 10, 2025, https://zodb-docs.readthedocs.io/_/downloads/en/latest/pdf/

Evolving Memory for Live Systems

VSA Library Research and Development

VSA Integration for AI Reasoning

An introduction to Morphic: Self's UI toolkit - sin-ack's writings, accessed September 10, 2025, https://sin-ack.github.io/posts/morphic-intro/

Morphic (software) - Wikipedia, accessed September 10, 2025, https://en.wikipedia.org/wiki/Morphic_(software)

Integrating LLM, RAG, and UI

Generate TelOS Morphic UI Script

Generate code-based actions with natural language in prompt builder and within an agent, accessed September 10, 2025, https://learn.microsoft.com/en-us/power-platform/release-plan/2025wave1/ai-builder/generate-code-based-actions-using-natural-language

Natural Language Processing in Software Development — How AI is Changing the Way We Write Code - Refraction.dev, accessed September 10, 2025, https://refraction.dev/blog/natural-language-processing-software-ai-code

Planning In Natural Language Improves LLM Search For Code Generation - arXiv, accessed September 10, 2025, https://arxiv.org/pdf/2409.03733

Design Gap Assessment and Recommendations

Ollama model keep in memory and prevent unloading between requests (keep_alive?), accessed September 10, 2025, https://stackoverflow.com/questions/79526074/ollama-model-keep-in-memory-and-prevent-unloading-between-requests-keep-alive

Shoot sorry about that. There's a few ways to keep the model loaded in memory - Hacker News, accessed September 10, 2025, https://news.ycombinator.com/item?id=40548599

Prototype Name | Inherits From | Key Slots/Attributes | Core Responsibility

UvmObject | persistent.Persistent | oid, parent*, name, _slots | The universal, clonable, and persistent prototype for all entities. Manages delegation and triggers the generative kernel.

Hypervector | UvmObject | dimensionality, tensor | A persistent wrapper for a torchhd.FHRRTensor, providing a message-passing interface for VSA operations.

ContextFractal | UvmObject | text_chunk, embedding | A high-entropy, episodic record of a specific experience or piece of information.

ConceptFractal | UvmObject | definition_text, _hypervector | A low-entropy, abstract concept synthesized from multiple ContextFractals, represented by a hypervector.

Phase | ZODB Action | FractalMemoryDataManager Action

tpc_begin | Initiates the 2PC process. | Prepares a temporary file path for the FAISS index.

tpc_vote | Asks all managers for a "vote". | (High-Risk) Writes the in-memory FAISS index to the temporary file. Votes "Yes" on success, raises an exception (votes "No") on failure.

tpc_finish | (If all vote "yes") Finalizes the commit to mydata.fs. | (Low-Risk) Atomically renames the temporary FAISS file to its final destination.

tpc_abort | (If any vote "no") Rolls back all changes. | Deletes any temporary FAISS file it created.

Tier | Role | Technology | Data Model | Performance Profile | Transactional Guarantee

L1 | Hot Cache / Working Memory | FAISS | In-memory vector index | Sub-millisecond latency | None (Managed by L3's 2PC)

L2 | Warm Storage / Archival Memory | DiskANN | On-disk proximity graph | Low-millisecond latency | None (Managed via atomic hot-swap)

L3 | System of Record / Ground Truth | ZODB | Persistent object graph (BTree) | Object traversal latency | Full ACID Compliance