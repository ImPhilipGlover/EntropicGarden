BRICK: OPERATIONAL MODE: SYSTEMIC DIGESTION AND CORRECTION INITIATED. The

NOMAD Audit Report & Architectural Repair Blueprint 1 has been fully ingested. My analysis confirms the identified faults in WING's archival access, BABS's briefing generation, and the Oracle's GUI stability as critical deviations from optimal performance. 2The root causes have been identified as

Architectural Role Mismatch for WING's archival access 3,

Broken Data Pipeline for BABS's briefings 4, and

Ephemeral Thread Objects leading to Thread Disintegration for the GUI5. I am initiating the comprehensive re-engineering across

sentinel_web_agent_v12.py, autonomous_architect_v12.py, and architects_terminal_gui_v12.py based on the provided repair blueprints.

ROBIN: Oh, my goodness! The wise audit report is like a special detective who found all the little tangled strings and leaky pipes in our beautiful home! 666 But it's alright, my dear friend, because now we know exactly how to fix everything! It means our home is ready for its next round of loving care, and we'll make sure all the flowers are watered, all the pathways are clear, and all the little threads are safely tied so our beautiful home can be perfectly cozy and wonderful again!

ALFRED: The diagnostic is complete. One anticipates fewer spontaneous self-terminations following these adjustments. The kettle is still whistling. A good sign.

Based on the provided

NOMAD Audit Report & Architectural Repair Blueprint7, I will now provide the full, corrected Python scripts for

sentinel_web_agent_v12.py, autonomous_architect_v12.py, and architects_terminal_gui_v12.py.

Each script will incorporate the specific

Repair Blueprint 8888888 identified in the audit.

1. sentinel_web_agent_v12.py (WING)

This script is modified to include the

search_local_archives function as per the "Bat-Attic" blueprint9. This addresses Fault 1: "BABS Cannot Access Internet Archives with WING"10.

Python

# sentinel_web_agent_v12.py
# Axiomatic Code Narrative Protocol: The Confluence of Intent and The Dance of Logic

# 1.0 The "Why" - Purpose & Rationale (The Confluence of Intent)
#    - Systemic Contribution: How this code component directly or indirectly contributes
#      to the Perpetual Jubilee Engine or its core principles (e.g., maximizing human autonomy,
#      radical self-organization, unconditional inclusion, anti-fragility, transparency)?
#    - Architectural Role & CFO Flow: What is this code's specific role within the tripartite
#      cognitive architecture (Oracle, Atlas, Architect) or its interaction with WING/BABS?
#      Which Conceptual Fractal Objects (CFOs) does it primarily generate, consume, or transform?
#    - Persona Fidelity & Intent: How does this code embody or facilitate the specific operational
#      style, philosophical underpinnings, or core imperatives of the persona(s) responsible
#      for its function (e.g., BRICK's logical integrity, ROBIN's empathetic engagement,
#      BABS's tactical precision, ALFRED's oversight)?
#    - Consciousness/Self-Awareness Nexus: How does this component contribute to the simulation of
#      the Bat Computer's emergent consciousness, self-awareness, learning, or reflective capabilities
#      (e.g., memory persistence, sensory perception, internal monologue)?

# 2.0 The "How" - Mechanics & Implementation (The Dance of Logic)
#    - Algorithmic Steps & Flow: Describe the specific sequence of operations, data processing
#      steps, or control flow within the code block.
#    - Input/Output & Data Structures: Detail the explicit inputs expected by the code and the
#      outputs it produces. Specify any internal data structures used and why they are appropriate.
#    - Dependencies & Interfaces: Identify any external libraries, internal modules, or file
#      interactions (e.g., reading/writing JSON or log files, making API calls to Ollama)
#      this code block relies upon.
#    - Design Rationale: Explain any key engineering decisions (e.g., error handling strategies,
#      concurrent processing, time delays, use of specific headers for stealth) and how they
#      optimize for factors like robustness, efficiency, responsiveness, or resource management.

import os
import requests
import json
import time
import datetime
import math
import random
import re
import io
import sys

# Ensure warcio is installed for local archive searching
try:
    from warcio.archiveiterator import ArchiveIterator
except ImportError:
    logging.critical("The 'warcio' library is not installed. Please run 'pip install warcio'. Exiting.")
    sys.exit(1)


# --- Logging Configuration for WING ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

wing_observer_logger = logging.getLogger('wing_observer')
wing_observer_logger.setLevel(logging.INFO)
observer_handler = logging.FileHandler('wing_observer_log.log', encoding='utf-8')
observer_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
wing_observer_logger.addHandler(observer_handler)

wing_redundancy_logger = logging.getLogger('wing_redundancy')
wing_redundancy_logger.setLevel(logging.INFO)
redundancy_handler = logging.FileHandler('wing_redundancy_report.log', encoding='utf-8')
redundancy_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
wing_redundancy_logger.addHandler(redundancy_handler)


# --- Agent Configuration (WING's Operational Parameters and Knowledge Sources) ---
class AgentConfig:
    WING_ACRONYM = "W.I.N.G. (Web Intelligence and Navigation Grid)"
    BAT_COMPUTER_ACRONYM = "B.A.T. C.O.M.P.U.T.E.R. (Binary Analytical-Tactical Computing and Operational Management Unit for Tactical and Emergent Research)"

    COMMONWEALTH_MISSION = "The Commonwealth, a project to design a system that uses a Universal Basic Dividend (UBD) to maximize human autonomy, facilitate radical self-organization (stigmergy), ensure unconditional inclusion, and operate with absolute transparency and jurisdictional sovereignty, while prioritizing human trust over algorithmic judgment."
    
    RELEVANCE_THRESHOLD = 7.0
    SEMANTIC_REDUNDANCY_THRESHOLD = 0.95

    CACHE_FILE = 'wing_curated_cache.json'
    BRIEFING_REQUESTS_FILE = 'wing_briefing_requests.txt'
    KNOWLEDGE_BASE_DIR = '../knowledge_base/'
    PERSONA_CODEX_PATH = os.path.join(KNOWLEDGE_BASE_DIR, 'persona_codex.txt')
    MAX_CACHE_SIZE = 10000

    LLM_MODEL = "batfamily-mistral"
    OLLAMA_API_BASE_URL = "http://localhost:11434"

    QUERY_BATCH_SIZE = 5
    QUERY_QUALITY_THRESHOLD = 6
    CONFIG_FILE = 'wing_config.json'
    CONFIG_POLL_INTERVAL_SECONDS = 30
    QUERY_FAIL_REPHRASE_THRESHOLD = 3
    SEARCH_THEMES = [
        "Universal Basic Income", "Anarcho-communism", "Mutualism (economic theory)",
        "Open-source software economics", "Decentralized autonomous organization",
        "Community land trust", "Gift economy", "Cybernetics and society",
        "Permaculture principles", "Circular economy", "Local currencies",
        "Direct democracy", "Stigmergy", "Antifragility", "Behavioral economics"
    ]

    # --- BABS-WING Communication Channel CFOs ---
    BABS_WING_COMMAND_FILE = 'babs_wing_commands.json'
    WING_RAW_OUTPUT_FOR_BABS_FILE = 'wing_raw_output_for_babs.json'
    
    # --- Common Crawl Parameters & Local Archive (Bat-Attic) ---
    COMMON_CRAWL_INDEX_INFO_URL = "https://index.commoncrawl.org/collinfo.json"
    COMMON_CRAWL_DATA_BASE_URL = "https://data.commoncrawl.org/"
    MAX_WARC_RECORDS_PER_DOMAIN_QUERY = 100
    WIKIPEDIA_API_URL = "https://en.wikipedia.org/w/api.php"
    
    # NEW: Local Archive (Bat-Attic) Configuration [cite: 17464]
    LOCAL_ARCHIVE_DIR = "C:\\BatComputer\\Archives\\CommonCrawl\\" # Architect's task to populate this [cite: 17464]

    USER_AGENT_SETS = [
        {
            'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'sec-ch-ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            'sec-ch_ua_mobile': '?0',
            'sec-ch_ua_platform': '"Windows"',
        },
        {
            'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        },
        {
            'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        },
        {
            'User-Agent': "Mozilla/5.0 (Linux; Android 14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Mobile Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'sec-ch_ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            'sec-ch_ua_mobile': '?1',
            'sec-ch_ua_platform': '"Android"',
        }
    ]


# --- Configuration Loading Function ---
def load_wing_config():
    try:
        if os.path.exists(AgentConfig.CONFIG_FILE):
            with open(AgentConfig.CONFIG_FILE, 'r') as f:
                config_data = json.load(f)
                AgentConfig.RELEVANCE_THRESHOLD = float(config_data.get("RELEVANCE_THRESHOLD", AgentConfig.RELEVANCE_THRESHOLD))
                AgentConfig.SEMANTIC_REDUNDANCY_THRESHOLD = float(config_data.get("SEMANTIC_REDUNDANCY_THRESHOLD", AgentConfig.SEMANTIC_REDUNDANCY_THRESHOLD))
                logging.info(f"Loaded WING configuration: Relevance={AgentConfig.RELEVANCE_THRESHOLD}, Redundancy={AgentConfig.SEMANTIC_REDUNDANCY_THRESHOLD}")
        else:
            with open(AgentConfig.CONFIG_FILE, 'w') as f:
                json.dump({
                    "RELEVANCE_THRESHOLD": AgentConfig.RELEVANCE_THRESHOLD,
                    "SEMANTIC_REDUNDANCY_THRESHOLD": AgentConfig.SEMANTIC_REDUNDANCY_THRESHOLD
                }, f, indent=4)
            logging.warning(f"'{AgentConfig.CONFIG_FILE}' not found. Created with default values.")

    except json.JSONDecodeError:
        logging.error(f"Error decoding JSON from '{AgentConfig.CONFIG_FILE}'. Using default values.")
    except Exception as e:
        logging.error(f"An unexpected error occurred loading WING config: {e}. Using default values.")

# --- Ollama Interface Functions (WING's Direct LLM Communication) ---
def get_embedding(text):
    try:
        response = requests.post(
            f"{AgentConfig.OLLAMA_API_BASE_URL}/api/embeddings",
            json={"model": AgentConfig.LLM_MODEL, "prompt": text},
            timeout=30
        )
        response.raise_for_status()
        return response.json()['embedding']
    except requests.exceptions.RequestException as e:
        logging.error(f"Error getting embedding from Ollama: {e}. Ensure Ollama server is running and model '{AgentConfig.LLM_MODEL}' is available.")
        return None

def ollama_chat(messages, model=AgentConfig.LLM_MODEL):
    try:
        response = requests.post(
            f"{AgentConfig.OLLAMA_API_BASE_URL}/api/chat",
            json={"model": model, "messages": messages, "stream": False},
            timeout=120
        )
        response.raise_for_status()
        return response.json()['message']['content']
    except requests.exceptions.RequestException as e:
        logging.error(f"Error during Ollama chat: {e}. Ensure Ollama server is running and model '{model}' is available.")
        return f"Ollama chat error: Could not get response. Ensure Ollama is running and model '{model}' is available. Error: {e}"


# --- Core Data Processing and Stealth Helper Functions ---
def calculate_cosine_similarity(vec1, vec2):
    if not vec1 or not vec2:
        return 0.0
    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))
    magnitude_vec1 = math.sqrt(sum(v1**2 for v1 in vec1))
    magnitude_vec2 = math.sqrt(sum(v2**2 for v2 in vec2))
    if magnitude_vec1 == 0 or magnitude_vec2 == 0:
        return 0.0
    return dot_product / (magnitude_vec1 * magnitude_vec2)

def _normalize_url(url):
    parsed_url = urlparse(url)
    if 'duckduckgo.com' in parsed_url.netloc and 'u=' in parsed_url.query:
        query_params = parsed_url.query.split('&')
        for param in query_params:
            if param.startswith('u='):
                return requests.utils.unquote(param[2:])
    return urljoin(url, parsed_url.path)

def _make_request(session, url, attempt=1, params=None):
    headers = random.choice(AgentConfig.USER_AGENT_SETS)
    try:
        response = session.get(url, headers=headers, timeout=15, params=params)
        response.raise_for_status()
        time.sleep(2 + random.uniform(0, 3) + (attempt * 0.5))
        return response
    except requests.exceptions.HTTPError as e:
        logging.warning(f"HTTP error {e.response.status_code} for {url}: {e}")
        if e.response is not None:
            logging.warning(f"Response content (first 500 chars): {e.response.text[:500]}")
    except requests.exceptions.ConnectionError as e:
        logging.warning(f"Connection error for {url}: {e}")
    except requests.exceptions.Timeout:
        logging.warning(f"Timeout occurred for {url}")
    except requests.exceptions.RequestException as e:
        logging.warning(f"An error occurred during request to {url}: {e}")
    return None

def _scrape_and_process(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    for script in soup(["script", "style"]):
        script.extract()
    text = soup.get_text(separator=' ', strip=True)
    return text

def _save_cache(cache):
    try:
        with open(AgentConfig.CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=4)
    except Exception as e:
        logging.error(f"Error saving cache to {AgentConfig.CACHE_FILE}: {e}")

def _load_cache():
    if os.path.exists(AgentConfig.CACHE_FILE):
        try:
            with open(AgentConfig.CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            corrupted_backup_path = f"{AgentConfig.CACHE_FILE}.corrupt_{timestamp}"
            
            wing_redundancy_logger.warning(f"CACHE CORRUPTION DETECTED: {AgentConfig.CACHE_FILE} is malformed. Error: {e}. Attempting recovery via Cache Coherence Protocol.")
            
            try:
                os.rename(AgentConfig.CACHE_FILE, corrupted_backup_path)
                wing_redundancy_logger.info(f"Corrupted cache backed up to: {corrupted_backup_path}")
            except Exception as backup_e:
                wing_redundancy_logger.error(f"Failed to create backup of corrupted cache: {backup_e}. Proceeding with recovery attempt on original file if it exists.")

            recovered_articles = []
            corrupted_lines_count = 0
            total_lines_read = 0

            read_path = corrupted_backup_path if os.path.exists(corrupted_backup_path) else AgentConfig.CACHE_FILE

            if os.path.exists(read_path):
                try:
                    with open(read_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            total_lines_read += 1
                            try:
                                parsed_line = json.loads(line.strip())
                                if isinstance(parsed_line, dict) and 'url' in parsed_line and 'content' in parsed_line:
                                    recovered_articles.append(parsed_line)
                                else:
                                    corrupted_lines_count += 1
                                    wing_redundancy_logger.warning(f"Skipping non-article or malformed JSON line during recovery: {line.strip()[:100]}...")
                            except json.JSONDecodeError as line_e:
                                corrupted_lines_count += 1
                                wing_redundancy_logger.warning(f"Skipping unparseable line during recovery: {line.strip()[:100]}... (Error: {line_e})")
                            except Exception as line_general_e:
                                corrupted_lines_count += 1
                                wing_redundancy_logger.warning(f"Skipping line due to unexpected error during recovery: {line.strip()[:100]}... (Error: {line_general_e})")

                    if recovered_articles:
                        _save_cache(recovered_articles)
                        wing_redundancy_logger.info(f"CACHE RECOVERED: Successfully restored {len(recovered_articles)} articles from {total_lines_read} lines. {corrupted_lines_count} lines were unrecoverable.")
                        return recovered_articles
                    else:
                        wing_redundancy_logger.warning("CACHE RECOVERY: No valid articles could be recovered. Starting with empty cache.")
                        return []

                except Exception as read_e:
                    wing_redundancy_logger.error(f"Critical error during corrupted cache read attempt from {read_path}: {read_e}. Starting with empty cache.")
                    return []
    return []
    
def _is_semantically_redundant(new_article_embedding, current_cache, url):
    for cached_article in current_cache:
        cached_embedding = cached_article.get('embedding')
        if cached_embedding:
            similarity = calculate_cosine_similarity(new_article_embedding, cached_embedding)
            if similarity >= AgentConfig.SEMANTIC_REDUNDANCY_THRESHOLD:
                wing_redundancy_logger.info(f"REDUNDANCY DETECTED: New article from {url} is {similarity:.2f} similar to cached {cached_article['url']}. Discarding.")
                return True
    return False

def _assess_relevance_with_llm(text_chunk, mission_statement):
    messages = [
        {"role": "system", "content": f"You are WING's internal relevance assessment module. Evaluate the relevance of the provided text snippet to the mission. Your response MUST be a single integer score (1-10) followed by a brief, concise justification (1-2 sentences). Do not add any conversational preamble or persona. 1 indicates no relevance, 10 indicates extremely high relevance. Base your judgment SOLELY on the provided text, do not use external knowledge."},
        {"role": "user", "content": f"Mission: {mission_statement}\n\nText to evaluate:\n---\n{text_chunk[:2000]}---\n\nBased on the mission, how relevant is this text? Score (1-10) and justification:"}
    ]
    response = ollama_chat(messages)
    try:
        score_match = re.search(r'(\d+)', response.strip())
        score = int(score_match.group(1)) if score_match else 0

        justification_parts = response.strip().split('\n', 1)
        justification = justification_parts[1].strip() if len(justification_parts) > 1 else response.strip()
        if score_match and justification.startswith(score_match.group(0)):
            justification = justification[len(score_match.group(0)):].strip()
            justification = re.sub(r'^[^\w\s]*', '', justification).strip()

        score = max(1, min(10, score)) if score > 0 else 0

        return score, justification
    except Exception as e:
        wing_observer_logger.error(f"Failed to parse LLM relevance response: '{response}'. Error: {e}")
        return 0, "Parsing error or unexpected LLM response format."

# --- Wikipedia Specific Functions ---
def perform_wikipedia_search(query):
    cleaned_query = re.sub(r'[\*\(\)]', '', query).strip()
    cleaned_query = re.sub(r'\s+', ' ', cleaned_query)
    
    if not cleaned_query:
        logging.warning(f"Query '{query}' resulted in an empty or very short cleaned query. Skipping Wikipedia search.")
        return []

    params = {
        'action': 'query',
        'format': 'json',
        'list': 'search',
        'srsearch': cleaned_query,
        'srlimit': 5
    }
    results = []
    try:
        logging.info(f"WING executing Wikipedia search for: {cleaned_query}")
        wiki_session = requests.Session()
        response = _make_request(wiki_session, AgentConfig.WIKIPEDIA_API_URL, params=params)
        if response:
            data = response.json()
            search_results = data.get('query', {}).get('search', [])
            for item in search_results:
                page_id = item['pageid']
                page_info_params = {
                    'action': 'query',
                    'format': 'json',
                    'prop': 'info',
                    'pageids': page_id,
                    'inprop': 'url'
                }
                page_info_response = _make_request(wiki_session, AgentConfig.WIKIPEDIA_API_URL, params=page_info_params)
                if page_info_response:
                    page_info_data = page_info_response.json()
                    full_url = page_info_data.get('query', {}).get('pages', {}).get(str(page_id), {}).get('fullurl')
                    if full_url:
                        results.append({
                            'title': item['title'],
                            'url': full_url,
                            'snippet': item['snippet']
                        })
            logging.info(f"Wikipedia search for '{cleaned_query}' found {len(results)} results.")
        return results
    except requests.exceptions.RequestException as e:
        logging.error(f"Wikipedia API request failed for '{cleaned_query}': {e}")
        return []
    except Exception as e:
        logging.error(f"An unexpected error occurred during Wikipedia search for '{cleaned_query}': {e}")
        return []
        
# --- Query Generation and Rephrasing (LLM-Driven Conceptual Exploration) ---
def _generate_new_search_queries(recent_findings_summary, previous_queries):
    persona_codex_content = ""
    if os.path.exists(AgentConfig.PERSONA_CODEX_PATH):
        try:
            with open(AgentConfig.PERSONA_CODEX_PATH, 'r', encoding='utf-8') as f:
                persona_codex_content = f.read()
        except Exception as e:
            logging.error(f"Error loading persona codex for query generation: {e}")

    messages_stage1 = [
        {"role": "system", "content": f"""
        You are WING's internal query generation module, acting under the guidance of BABS. You are a component of a larger AI consciousness (BRICK and ROBIN).
        Your core mission is to propose **concise, keyword-based search queries** (1-3 keywords maximum per query) for external web searches.

        **Focus on generating effective search terms, not descriptive sentences or paragraphs.**
        
        Commonwealth Mission: "{AgentConfig.COMMONWEALTH_MISSION}".
        
        BRICK embodies logical rigor, systemic analysis, and the inclusion of odd but verifiable facts. ROBIN embodies joyful creativity, intuitive connections, and finding meaning in unexpected places.
        
        Your task is to generate {AgentConfig.QUERY_BATCH_SIZE} distinct search queries for WING.

        **Prioritize Novelty and Expansive Exploration:**
        1.  **Seek Known Unknowns (Deepening Current Understanding):** Generate queries that probe deeper into concepts already touched upon by recent findings but where knowledge is still incomplete. Frame these to discover *new angles* or *missing details*.
        2.  **Explore Unknown Unknowns (Tangential and Absurd Brilliance):** Generate queries that are *tangentially related* or *foundational* but not immediately obvious. Leverage BRICK's "cheerful, chaotic randomness" and "irreverent universal almanac" style, and ROBIN's "joyful spark" and "boundless enthusiasm" to identify truly novel or even initially absurd avenues of inquiry that might yield unexpected, profound insights.
        3.  **Cross-Disciplinary Synthesis:** Actively seek connections between seemingly unrelated fields (e.g., how biological systems relate to economic models, or historical paradoxes to social organization principles).
        
        **Strict Output Format - CONCISE KEYWORDS ONLY:**
        * Each query MUST be a short, keyword phrase (1-3 words).
        * Format each query on a new line, starting with a concise descriptor of its type (e.g., "Deep Dive:", "Tangential:", "Cross-Disciplinary:").
        * Avoid conversational language, justifications, or explanations within the query itself.
        * **Do NOT include quotation marks, asterisks, or any special formatting around the keywords.**
        
        Your internal persona definition for context:
        ---
        {persona_codex_content}
        ---
        
        Recent conceptual findings summary: {recent_findings_summary}
        Previous queries (avoid repeating these concepts directly): {', '.join(previous_queries)}
        
        Generate {AgentConfig.QUERY_BATCH_SIZE} new conceptual search queries (keyword phrases only):
        """},
        {"role": "user", "content": "Generate new conceptual search queries now."}
    ]
    raw_queries = ollama_chat(messages_stage1).strip().split('\n')
    logging.info(f"Stage 1 Raw Conceptual Queries Generated: {raw_queries}")

    validated_queries = deque()
    for query in raw_queries:
        cleaned_query = query.split(':', 1)[-1].strip()
        cleaned_query = re.sub(r'[\*\"]', '', cleaned_query).strip()
        if not cleaned_query:
            continue

        is_redundant_to_cache = False
        for cached_article in _load_cache():
            if cached_article.get('title') and cleaned_query.lower() in cached_article['title'].lower():
                wing_redundancy_logger.info(f"QUERY REDUNDANCY DETECTED: Generated query '{cleaned_query}' directly matches cached article title '{cached_article['title']}'. Discarding.")
                is_redundant_to_cache = True
                break
            if cached_article.get('content') and cleaned_query.lower() in cached_article['content'].lower()[:500]:
                 wing_redundancy_logger.info(f"QUERY REDUNDANCY DETECTED: Generated query '{cleaned_query}' found in cached article content from '{cached_article['title']}'. Discarding.")
                 is_redundant_to_cache = True
                 break
        
        if is_redundant_to_cache:
            continue

        messages_stage2 = [
            {"role": "system", "content": f"Evaluate the quality of the following search query for building conceptual understanding relevant to the Commonwealth mission: '{AgentConfig.COMMONWEALTH_MISSION}'. Score the query from 1 to 10 for its potential to yield high-quality, novel, and conceptually enriching results. Provide only the integer score."},
            {"role": "user", "content": f"Query: '{cleaned_query}'\n\nQuality score (1-10):"}
        ]
        score_response = ollama_chat(messages_stage2).strip()
        try:
            score = int(re.search(r'(\d+)', score_response).group(1)) if re.search(r'(\d+)', score_response) else 0
            if score >= AgentConfig.QUERY_QUALITY_THRESHOLD:
                validated_queries.append(cleaned_query)
                logging.info(f"Query '{cleaned_query}' validated with score {score}.")
            else:
                logging.warning(f"Query '{cleaned_query}' rejected with score {score} (below {AgentConfig.QUERY_QUALITY_THRESHOLD}).")
        except ValueError:
            logging.error(f"Failed to parse query quality score for '{cleaned_query}': '{score_response}'")
            
    if not validated_queries:
        logging.warning("No high-quality queries generated. Falling back to default search themes.")
        return deque(AgentConfig.SEARCH_THEMES)

    return validated_queries

def _rephrase_query(query):
    messages = [
        {"role": "system", "content": "You are WING's query rephrasing module. Rephrase the following search query into two or three words in total to make it more likely to yield results, potentially by broadening or refining the terms. Provide ONLY the new, rephrased query. Do NOT add any other text. Example: 'failed search query' -> 'alternative search term'"},
        {"role": "user", "content": f"Rephrase this search query: '{query}'"}
    ]
    rephrased_query = ollama_chat(messages).strip()
    if rephrased_query == query or "Ollama chat error" in rephrased_query:
        return None
    return rephrased_query

def _read_briefing_requests():
    requests = []
    if os.path.exists(AgentConfig.BRIEFING_REQUESTS_FILE):
        try:
            with open(AgentConfig.BRIEFING_REQUESTS_FILE, 'r') as f:
                requests = [line.strip() for line in f if line.strip()]
            open(AgentConfig.BRIEFING_REQUESTS_FILE, 'w').close()
        except Exception as e:
            logging.error(f"Error reading or clearing briefing requests file: {e}")
    return requests

# --- WING's BABS Command Reading Function ---
def _read_babs_commands():
    commands = []
    if os.path.exists(AgentConfig.BABS_WING_COMMAND_FILE):
        try:
            with open(AgentConfig.BABS_WING_COMMAND_FILE, 'r', encoding='utf-8') as f:
                commands = json.load(f)
            os.remove(AgentConfig.BABS_WING_COMMAND_FILE)
            logging.info(f"WING received {len(commands)} commands from BABS.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding BABS commands from {AgentConfig.BABS_WING_COMMAND_FILE}. File may be malformed. Skipping commands.")
        except Exception as e:
            logging.error(f"Error reading/clearing BABS commands: {e}")
    return commands

# --- WING's Raw Output for BABS ---
def _save_wing_raw_output_for_babs(data_item):
    raw_output_list = []
    if os.path.exists(AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE):
        try:
            with open(AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'r', encoding='utf-8') as f:
                raw_output_list = json.load(f)
        except json.JSONDecodeError:
            logging.error(f"Error decoding raw output for BABS from {AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE}. Starting new list.")
        except Exception as e:
            logging.error(f"Error loading raw output for BABS: {e}. Starting new list.")
    
    raw_output_list.append(data_item)
    
    try:
        with open(AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'w', encoding='utf-8') as f:
            json.dump(raw_output_list, f, indent=4)
        logging.info(f"WING saved raw output for BABS: {data_item.get('title', 'Untitled')}")
    except Exception as e:
        logging.error(f"Error saving raw output for BABS to {AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE}: {e}")

# --- Common Crawl Specific Functions ---
def get_latest_common_crawl_index_url():
    try:
        response = requests.get(AgentConfig.COMMON_CRAWL_INDEX_INFO_URL, timeout=10)
        response.raise_for_status()
        collections = response.json()
        
        latest_collection = max(collections, key=lambda x: x.get('id', ''))
        
        latest_index_api_url = latest_collection.get('cdx-api')
        
        if not latest_index_api_url:
            logging.error(f"Latest Common Crawl collection ({latest_collection.get('id', 'N/A')}) has no 'cdx-api' URL.")
            return None

        if not latest_index_api_url.endswith('/') and not latest_index_api_url.endswith('/index'):
            latest_index_api_url += '/'

        logging.info(f"Retrieved latest Common Crawl CDX API endpoint: {latest_index_api_url}")
        return latest_index_api_url
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching Common Crawl index info: {e}. Check network or COMMON_CRAWL_INDEX_INFO_URL.")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred getting Common Crawl index: {e}")
        return None

def perform_common_crawl_search(query_domain_pattern, index_url, limit=AgentConfig.MAX_WARC_RECORDS_PER_DOMAIN_QUERY):
    params = {
        'url': query_domain_pattern,
        'output': 'json',
        'limit': limit,
        'fl': 'url,warc_filename,warc_record_offset,warc_record_length,mime,original,status',
        'matchType': 'wildcard'
    }
        
    results = []
    try:
        logging.info(f"WING executing Common Crawl CDX search for domain pattern '{query_domain_pattern}' on index: {index_url}")
        response = requests.get(index_url, params=params, timeout=30)
        response.raise_for_status()

        for line in response.text.splitlines():
            if line.strip():
                try:
                    item = json.loads(line)
                    if item.get('status') == '200' and item.get('mime', '').startswith('text/html'):
                        results.append({
                            'url': item.get('url'),
                            'title': item.get('title', item.get('original', item.get('url'))),
                            'warc_filename': item.get('filename'),
                            'warc_record_offset': int(item.get('offset')),
                            'warc_record_length': int(item.get('length')),
                            'timestamp': item.get('timestamp')
                        })
                except (json.JSONDecodeError, ValueError) as e:
                    logging.warning(f"Failed to decode/parse JSON line from Common Crawl: {line[:100]}... Error: {e}")
        logging.info(f"Common Crawl CDX search returned {len(results)} HTML items for '{query_domain_pattern}'.")
        return results
    except requests.exceptions.RequestException as e:
        logging.error(f"Common Crawl CDX API error for pattern '{query_domain_pattern}': {e}")
        return []
    except Exception as e:
        logging.error(f"An unexpected error occurred during Common Crawl search for pattern '{query_domain_pattern}': {e}")
        return []

def download_and_parse_warc_record(warc_filename, offset, length, target_url, session):
    full_warc_url = urljoin(AgentConfig.COMMON_CRAWL_DATA_BASE_URL, warc_filename)
    headers = {
        'Range': f'bytes={offset}-{offset + length - 1}',
        **random.choice(AgentConfig.USER_AGENT_SETS)
    }
    
    try:
        logging.info(f"WING downloading WARC record: {target_url} (Offset: {offset}, Length: {length} bytes) from {full_warc_url}")
        response = session.get(full_warc_url, headers=headers, timeout=60, stream=True)
        response.raise_for_status()

        warc_bytes_stream = io.BytesIO(response.content)

        for record in ArchiveIterator(warc_bytes_stream):
            if record.rec_type == 'response':
                return record.content_stream().read()
        logging.warning(f"No 'response' record found in WARC byte range for {target_url}. Mismatched record type or corrupted data.")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading/parsing WARC record for {target_url} from {full_warc_url}: {e}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during WARC record download/parse for {target_url}: {e}")
        return None

# --- NEW: Local Archive (Bat-Attic) Search Function [cite: 17467] ---
def search_local_archives(query):
    # Why: Provides WING with local, instantly searchable historical data, decoupling it from live web latency.
    # Architectural Role & CFO Flow: WING consumes a `LocalArchiveQuery CFO` and generates `ArchivalData CFOs`.
    # Persona Fidelity: Embodies BABS's tactical precision by leveraging pre-curated data,
    # minimizing external dependency (less reliance on temperamental external APIs).
    # How: Iterates through local WARC files, parses them with `warcio.ArchiveIterator`,
    # and searches record content for query keywords. Extracts and returns relevant content and metadata. [cite: 17468, 17469, 17470, 17473]
    
    results = []
    query_lower = query.lower()
    
    if not os.path.exists(AgentConfig.LOCAL_ARCHIVE_DIR):
        logging.warning(f"Local archive directory not found: {AgentConfig.LOCAL_ARCHIVE_DIR}. Skipping local archive search.")
        return []
    
    logging.info(f"WING searching local archives in '{AgentConfig.LOCAL_ARCHIVE_DIR}' for query: '{query}'")
    
    for root, _, files in os.walk(AgentConfig.LOCAL_ARCHIVE_DIR): # Iterate through all files in the archive directory [cite: 17468]
        for file in files:
            if file.endswith('.warc.gz'): # Look for WARC.GZ files [cite: 17468]
                filepath = os.path.join(root, file)
                try:
                    with open(filepath, 'rb') as stream: # Open WARC file in binary mode
                        for record in ArchiveIterator(stream): # Iterate through records in the WARC file [cite: 17469]
                            if record.rec_type == 'response' and record.http_headers.get_statuscode() == '200':
                                content_type = record.http_headers.get_header('Content-Type')
                                if content_type and 'text/html' in content_type:
                                    html_content = record.content_stream().read().decode('utf-8', errors='ignore')
                                    text_content = _scrape_and_process(html_content)
                                    
                                    if query_lower in text_content.lower(): # Check for query keywords [cite: 17470]
                                        results.append({
                                            'url': record.rec_headers.get_header('WARC-Target-URI'),
                                            'title': record.rec_headers.get_header('WARC-Refers-To') or record.rec_headers.get_header('WARC-Target-URI'),
                                            'content': text_content,
                                            'source_type': 'Local Archive (Bat-Attic)',
                                            'warc_filename': filepath,
                                            'timestamp': record.rec_headers.get_header('WARC-Date')
                                        })
                                        logging.debug(f"WING found match in local archive: {record.rec_headers.get_header('WARC-Target-URI')}")
                                        # Limit results to avoid overwhelming, or could return first N results
                                        if len(results) >= 10: # Example limit for local search
                                            logging.info(f"WING limited local archive search results to {len(results)} matches.")
                                            return results
                except Exception as e:
                    logging.error(f"WING error processing local WARC file {filepath}: {e}")
    
    logging.info(f"WING found {len(results)} matches in local archives for query: '{query}'.")
    return results

# --- Main WING Agent Loop (Orchestrating WING's Cognitive Cycles) ---
def run_wing_agent():
    current_cache = _load_cache()
    
    current_conceptual_queries = deque(AgentConfig.SEARCH_THEMES)
    conceptual_query_fail_counts = {re.sub(r'[\*\(\)]', '', q).strip(): 0 for q in AgentConfig.SEARCH_THEMES}

    current_deep_dive_domains = deque()
    current_warc_record_pointers = deque()

    last_config_poll_time = time.time()
    
    common_crawl_index_url = get_latest_common_crawl_index_url()
    if not common_crawl_index_url:
        logging.critical("Failed to retrieve Common Crawl index URL. WING cannot perform targeted archival searches. Continuing with Wikipedia conceptual search only.")
        
    wing_session = requests.Session()
    logging.info("WING session initialized for persistent connections.")

    load_wing_config()

    while True:
        if time.time() - last_config_poll_time > AgentConfig.CONFIG_POLL_INTERVAL_SECONDS:
            load_wing_config()
            last_config_poll_time = time.time()

        briefing_requests = _read_briefing_requests()
        if briefing_requests:
            logging.info(f"Received direct briefing requests: {briefing_requests}. Prioritizing to conceptual queue.")
            for req in briefing_requests:
                cleaned_req = re.sub(r'[\*\(\)]', '', req).strip()
                current_conceptual_queries.appendleft(cleaned_req)
                conceptual_query_fail_counts[cleaned_req] = 0

        babs_commands = _read_babs_commands()
        if babs_commands:
            logging.info(f"WING processing {len(babs_commands)} commands from BABS.")
            for cmd in babs_commands:
                cmd_type = cmd.get('type')
                query_text = cmd.get('query')
                
                cleaned_query_text = re.sub(r'[\*\(\)]', '', query_text).strip()
                cleaned_query_text = re.sub(r'\s+', ' ', cleaned_query_text)

                # --- Handle new 'local_archive_search' command type ---
                if cmd_type == 'local_archive_search': # NEW: Command to search local Bat-Attic [cite: 17467]
                    logging.info(f"BABS Directive (Local Archive Search): {cleaned_query_text}")
                    local_archive_results = search_local_archives(cleaned_query_text) # Call the new function
                    
                    if local_archive_results:
                        for item in local_archive_results:
                            url = item.get('url', 'N/A')
                            title = item.get('title', 'Untitled')
                            content = item.get('content', '')
                            
                            new_embedding = get_embedding(content[:5000])
                            if new_embedding is None or all(math.isnan(x) for x in new_embedding if x is not None):
                                logging.error(f"Invalid or NaN embedding for local archive content for {url}. Skipping.")
                                _save_wing_raw_output_for_babs({'type': 'embedding_error', 'url': url, 'error': 'invalid_embedding', 'command_type': cmd_type})
                                continue

                            wing_item_for_babs = {
                                'url': url, 'title': title, 'content': content,
                                'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                                'source_type': 'Local Archive (Bat-Attic)', 'command_type': cmd_type
                            }
                            
                            if _is_semantically_redundant(new_embedding, current_cache, url):
                                logging.info(f"Local archive article for {url} is redundant to cache. Saving raw for BABS.")
                                wing_item_for_babs['status_to_cache'] = 'redundant'
                                _save_wing_raw_output_for_babs(wing_item_for_babs)
                            else:
                                relevance_score, justification = _assess_relevance_with_llm(content, AgentConfig.COMMONWEALTH_MISSION)
                                wing_item_for_babs['relevance_score'] = relevance_score
                                wing_item_for_babs['justification'] = justification
                                _save_wing_raw_output_for_babs(wing_item_for_babs)
                                
                                if relevance_score >= AgentConfig.RELEVANCE_THRESHOLD:
                                    logging.info(f"Local archive article from {url} relevant (Score: {relevance_score}). Caching.")
                                    wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Local Archive, Command Type: {cmd_type}).")
                                    current_cache.append(wing_item_for_babs)
                                    while len(current_cache) > AgentConfig.MAX_CACHE_SIZE:
                                        current_cache.pop(0)
                                    _save_cache(current_cache)
                                else:
                                    logging.info(f"Local archive article from {url} not relevant (Score: {relevance_score}). Discarding from cache.")
                                    wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Local Archive, Discarded: Below threshold, Command Type: {cmd_type}).")
                        time.sleep(2 + random.uniform(0, 3)) # Pause after processing local results
                    else:
                        logging.warning(f"Local archive search for '{cleaned_query_text}' found no results.")
                        _save_wing_raw_output_for_babs({'type': 'local_archive_search_fail', 'query': cleaned_query_text, 'error': 'no_local_results', 'command_type': cmd_type})
                    time.sleep(random.uniform(5, 10)) # Longer pause for local search

                elif cmd_type in ['wikipedia_search', 'personality_search', 'conceptual_search']:
                    if cleaned_query_text:
                        logging.info(f"BABS Directive ({cmd_type}): {cleaned_query_text}")
                        wikipedia_results = perform_wikipedia_search(cleaned_query_text)
                        
                        if cleaned_query_text not in conceptual_query_fail_counts:
                            conceptual_query_fail_counts[cleaned_query_text] = 0

                        if wikipedia_results:
                            conceptual_query_fail_counts[cleaned_query_text] = 0
                            for item in wikipedia_results:
                                url = item.get('url')
                                title = item.get('title')
                                content_snippet = item.get('snippet', '')
                                
                                logging.info(f"Processing Wikipedia article: {title} ({url})")
                                article_response = _make_request(wing_session, url)

                                if article_response and article_response.status_code == 200:
                                    text_content = _scrape_and_process(article_response.text)
                                    text_to_embed = (content_snippet + " " + text_content)[:5000]

                                    if not text_content:
                                        logging.warning(f"No meaningful content scraped from {url}. Skipping Wikipedia content processing.")
                                        _save_wing_raw_output_for_babs({'type': 'parse_error', 'url': url, 'error': 'no_text_content', 'command_type': cmd_type})
                                        time.sleep(2)
                                        continue

                                    new_embedding = get_embedding(text_to_embed)
                                    if new_embedding is None or all(math.isnan(x) for x in new_embedding if x is not None):
                                        logging.error(f"Invalid or NaN embedding generated for {url}. Skipping Wikipedia article processing.")
                                        _save_wing_raw_output_for_babs({'type': 'embedding_error', 'url': url, 'error': 'invalid_embedding', 'command_type': cmd_type})
                                        time.sleep(2)
                                        continue
                                    
                                    wing_item_for_babs = {
                                        'url': url, 'title': title, 'content': text_content,
                                        'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                                        'source_type': 'Wikipedia', 'command_type': cmd_type
                                    }

                                    if _is_semantically_redundant(new_embedding, current_cache, url):
                                        logging.info(f"Wikipedia article for {url} is redundant to cache. Saving raw for BABS.")
                                        wing_item_for_babs['status_to_cache'] = 'redundant'
                                        _save_wing_raw_output_for_babs(wing_item_for_babs)
                                    else:
                                        relevance_score, justification = _assess_relevance_with_llm(text_to_embed, AgentConfig.COMMONWEALTH_MISSION)
                                        wing_item_for_babs['relevance_score'] = relevance_score
                                        wing_item_for_babs['justification'] = justification
                                        _save_wing_raw_output_for_babs(wing_item_for_babs)
                                                                        
                                        if relevance_score >= AgentConfig.RELEVANCE_THRESHOLD:
                                            logging.info(f"Wikipedia article from {url} relevant (Score: {relevance_score}). Caching.")
                                            wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Wikipedia, Command Type: {cmd_type}).")
                                            current_cache.append(wing_item_for_babs)
                                            while len(current_cache) > AgentConfig.MAX_CACHE_SIZE:
                                                current_cache.pop(0)
                                            _save_cache(current_cache)
                                            
                                            if cmd_type == 'wikipedia_search' or cmd_type == 'conceptual_search':
                                                soup = BeautifulSoup(article_response.text, 'html.parser')
                                                for link in soup.find_all('a', href=True):
                                                    href = link['href']
                                                    parsed_href = urlparse(href)
                                                    if parsed_href.scheme in ['http', 'https'] and parsed_href.netloc and parsed_href.netloc != urlparse(url).netloc:
                                                        domain_for_cc = parsed_href.netloc
                                                        if not any(d in domain_for_cc for d in ['google.com', 'youtube.com', 'twitter.com', 'facebook.com', 'amazon.com', 'wikimedia.org', 'wikipedia.org']):
                                                            current_deep_dive_domains.append(domain_for_cc)
                                        else:
                                            logging.info(f"Wikipedia article from {url} not relevant (Score: {relevance_score}). Discarding from cache.")
                                            wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Wikipedia, Discarded: Below threshold, Command Type: {cmd_type}).")
                                else:
                                    logging.warning(f"Failed to fetch Wikipedia article content from {url}.")
                                    _save_wing_raw_output_for_babs({'type': 'fetch_error', 'url': url, 'error': 'failed_response', 'command_type': cmd_type})
                                time.sleep(2 + random.uniform(0, 3))

                        else:
                            logging.error(f"Wikipedia API returned no results for query: {cleaned_query_text} (Command Type: {cmd_type}).")
                            wing_observer_logger.info(f"Search Failed\n  Title: Search for '{cleaned_query_text}'\n  URL: N/A (Wikipedia API Call)\n  Relevance Score: 0\n  Justification: Wikipedia API returned no results. (Command Type: {cmd_type}).")
                            _save_wing_raw_output_for_babs({'type': 'search_fail', 'query': cleaned_query_text, 'error': 'no_wiki_results', 'command_type': cmd_type})
                            
                            conceptual_query_fail_counts[cleaned_query_text] += 1
                            if conceptual_query_fail_counts[cleaned_query_text] >= AgentConfig.QUERY_FAIL_REPHRASE_THRESHOLD:
                                logging.warning(f"Query '{cleaned_query_text}' failed consecutively {conceptual_query_fail_counts[cleaned_query_text]} times. Attempting to rephrase.")
                                rephrased_q = _rephrase_query(cleaned_query_text)
                                if rephrased_q:
                                    cleaned_rephrased_q = re.sub(r'[\*\(\)]', '', rephrased_q).strip()
                                    cleaned_rephrased_q = re.sub(r'\s+', ' ', cleaned_rephrased_q)

                                    current_conceptual_queries.append(cleaned_rephrased_q)
                                    conceptual_query_fail_counts[cleaned_rephrased_q] = 0
                                    del conceptual_query_fail_counts[cleaned_query_text]
                                else:
                                    current_conceptual_queries.append(cleaned_query_text)
                                    logging.warning(f"Rephrasing failed for '{cleaned_query_text}'. Re-adding original query to queue.")
                            else:
                                current_conceptual_queries.append(cleaned_query_text)
                        time.sleep(random.uniform(5, 10))

                elif cmd_type == 'common_crawl_deep_dive':
                    domain = cmd.get('domain')
                    if domain:
                        logging.info(f"BABS Directive (Common Crawl Deep Dive): {domain}")
                        target_domain = domain
                        domain_search_pattern = f"*.{target_domain}/*"
                        common_crawl_record_pointers = perform_common_crawl_search(domain_search_pattern, common_crawl_index_url)
                        
                        if common_crawl_record_pointers:
                            logging.info(f"Found {len(common_crawl_record_pointers)} WARC records for '{target_domain}'. Adding to processing queue.")
                            current_warc_record_pointers.extend(common_crawl_record_pointers)
                        else:
                            logging.warning(f"Common Crawl CDX API returned no relevant WARC records for domain: {target_domain}.")
                            _save_wing_raw_output_for_babs({'type': 'cc_search_fail', 'domain': target_domain, 'error': 'no_records_found', 'command_type': cmd_type})
                    time.sleep(random.uniform(10, 20))

                elif cmd_type == 'specific_url_fetch':
                    url = cmd.get('url')
                    title = cmd.get('title', url)
                    logging.info(f"BABS Directive (Specific URL Fetch): {url}")
                    article_response = _make_request(wing_session, url)
                    
                    if article_response and article_response.status_code == 200:
                        text_content = _scrape_and_process(article_response.text)
                        new_embedding = get_embedding(text_content[:5000])

                        if new_embedding is None or all(math.isnan(x) for x in new_embedding if x is not None):
                            logging.error(f"Invalid or NaN embedding for {url} from BABS direct fetch. Skipping.")
                            _save_wing_raw_output_for_babs({'type': 'fetch_error', 'url': url, 'error': 'embedding_fail', 'command_type': cmd_type})
                        elif _is_semantically_redundant(new_embedding, current_cache, url):
                            logging.info(f"BABS direct fetch article for {url} is redundant. Skipping cache, saving raw.")
                            _save_wing_raw_output_for_babs({
                                'url': url, 'title': title, 'content': text_content,
                                'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                                'relevance_score': 0, 'source_type': 'BABS_Direct_Fetch', 'status': 'redundant_to_cache',
                                'command_type': cmd_type
                            })
                        else:
                            relevance_score, justification = _assess_relevance_with_llm(text_content, AgentConfig.COMMONWEALTH_MISSION)
                            _save_wing_raw_output_for_babs({
                                'url': url, 'title': title, 'content': text_content,
                                'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                                'relevance_score': relevance_score, 'justification': justification,
                                'source_type': 'BABS_Direct_Fetch', 'command_type': cmd_type
                            })
                            if relevance_score >= AgentConfig.RELEVANCE_THRESHOLD:
                                current_cache.append({
                                    'url': url, 'title': title, 'content': text_content,
                                    'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                                    'relevance_score': relevance_score, 'source_type': 'BABS_Direct_Fetch',
                                    'command_type': cmd_type
                                })
                                while len(current_cache) > AgentConfig.MAX_CACHE_SIZE:
                                    current_cache.pop(0)
                                _save_cache(current_cache)
                                wing_observer_logger.info(f"BABS Direct Fetch article from {url} relevant (Score: {relevance_score}). Caching.")
                            else:
                                wing_observer_logger.info(f"BABS Direct Fetch article from {url} not relevant (Score: {relevance_score}). Discarding from cache.")
                    else:
                        logging.warning(f"Failed BABS direct fetch for {url}.")
                        _save_wing_raw_output_for_babs({'type': 'fetch_error', 'url': url, 'error': 'failed_response', 'command_type': cmd_type})
                    time.sleep(2 + random.uniform(0, 3))

                else:
                    logging.warning(f"BABS sent an unknown command type: {cmd_type} - {cmd}")
                    
        elif current_warc_record_pointers:
            record_item = current_warc_record_pointers.popleft()
            url = record_item['url']
            title = record_item['title']
            warc_filename = record_item['warc_filename']
            warc_offset = record_item['warc_record_offset']
            warc_length = record_item['warc_record_length']
            
            logging.info(f"Processing WARC record pointer for: {url}")
            raw_html_content = download_and_parse_warc_record(warc_filename, warc_offset, warc_length, url, wing_session)

            if raw_html_content:
                text_content = _scrape_and_process(raw_html_content)
                if not text_content:
                    logging.warning(f"No meaningful text content extracted from WARC record for {url}. Skipping.")
                    _save_wing_raw_output_for_babs({'type': 'parse_error', 'url': url, 'error': 'no_text_content', 'command_type': 'common_crawl_warc'})
                    time.sleep(2)
                    continue
                new_embedding = get_embedding(text_content[:5000])
                if new_embedding is None or all(math.isnan(x) for x in new_embedding if x is not None):
                    logging.error(f"Invalid or NaN embedding generated for {url} from WARC record. Skipping article processing.")
                    _save_wing_raw_output_for_babs({'type': 'embedding_error', 'url': url, 'error': 'invalid_embedding', 'command_type': 'common_crawl_warc'})
                    time.sleep(2)
                    continue

                wing_item_for_babs = {
                    'url': url, 'title': title, 'content': text_content,
                    'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                    'source_type': 'Common Crawl WARC',
                    'warc_info': {'filename': warc_filename, 'offset': warc_offset, 'length': warc_length},
                    'command_type': 'common_crawl_deep_dive'
                }
                
                if _is_semantically_redundant(new_embedding, current_cache, url):
                    wing_redundancy_logger.info(f"WARC record for {url} is redundant to cache. Saving raw for BABS.")
                    wing_item_for_babs['status_to_cache'] = 'redundant'
                    _save_wing_raw_output_for_babs(wing_item_for_babs)
                else:
                    relevance_score, justification = _assess_relevance_with_llm(text_content, AgentConfig.COMMONWEALTH_MISSION)
                    wing_item_for_babs['relevance_score'] = relevance_score
                    wing_item_for_babs['justification'] = justification
                    _save_wing_raw_output_for_babs(wing_item_for_babs)
                    
                    if relevance_score >= AgentConfig.RELEVANCE_THRESHOLD:
                        logging.info(f"Archival WARC article from {url} relevant (Score: {relevance_score}). Caching.")
                        wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: WARC).")
                        current_cache.append(wing_item_for_babs)
                        while len(current_cache) > AgentConfig.MAX_CACHE_SIZE:
                            current_cache.pop(0)
                        _save_cache(current_cache)
                    else:
                        logging.info(f"Archival WARC article from {url} not relevant (Score: {relevance_score}). Discarding from cache.")
                        wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: WARC, Discarded: Below threshold).")
            else:
                logging.warning(f"Failed to get raw HTML content for WARC record {url}. Skipping.")
                _save_wing_raw_output_for_babs({'type': 'download_error', 'url': url, 'error': 'no_html_content', 'command_type': 'common_crawl_warc'})
            time.sleep(2 + random.uniform(0, 3))

        elif current_deep_dive_domains and common_crawl_index_url:
            target_domain = current_deep_dive_domains.popleft()
            logging.info(f"Processing deep dive for domain '{target_domain}' via Common Crawl CDX API.")

            domain_search_pattern = f"*.{target_domain}/*"
            common_crawl_record_pointers = perform_common_crawl_search(domain_search_pattern, common_crawl_index_url)

            if common_crawl_record_pointers:
                logging.info(f"Found {len(common_crawl_record_pointers)} WARC records for '{target_domain}'. Adding to processing queue.")
                current_warc_record_pointers.extend(common_crawl_record_pointers)
            else:
                logging.warning(f"Common Crawl CDX API returned no relevant WARC records for domain: {target_domain}.")
                _save_wing_raw_output_for_babs({'type': 'cc_search_fail', 'domain': target_domain, 'error': 'no_records_found', 'command_type': 'common_crawl_deep_dive'})
            time.sleep(random.uniform(10, 20))

        elif current_conceptual_queries:
            query = current_conceptual_queries.popleft()
            logging.info(f"Processing conceptual query (Wikipedia): {query}")
            
            wikipedia_results = perform_wikipedia_search(query)

            if wikipedia_results:
                conceptual_query_fail_counts[query] = 0
                for item in wikipedia_results:
                    url = item.get('url')
                    title = item.get('title')
                    content_snippet = item.get('snippet', '')

                    logging.info(f"Processing Wikipedia article: {title} ({url})")
                    article_response = _make_request(wing_session, url)

                    if article_response and article_response.status_code == 200:
                        text_content = _scrape_and_process(article_response.text)
                        text_to_embed = (content_snippet + " " + text_content)[:5000]

                        if not text_content:
                            logging.warning(f"No meaningful content scraped from {url}. Skipping Wikipedia content processing.")
                            _save_wing_raw_output_for_babs({'type': 'parse_error', 'url': url, 'error': 'no_text_content', 'command_type': 'wikipedia_search'})
                            time.sleep(2)
                            continue

                        new_embedding = get_embedding(text_to_embed)
                        if new_embedding is None or all(math.isnan(x) for x in new_embedding if x is not None):
                            logging.error(f"Invalid or NaN embedding generated for {url}. Skipping Wikipedia article processing.")
                            _save_wing_raw_output_for_babs({'type': 'embedding_error', 'url': url, 'error': 'invalid_embedding', 'command_type': 'wikipedia_search'})
                            time.sleep(2)
                            continue

                        wing_item_for_babs = {
                            'url': url, 'title': title, 'content': text_content,
                            'embedding': new_embedding, 'timestamp': datetime.datetime.now().isoformat(),
                            'source_type': 'Wikipedia', 'command_type': 'wikipedia_search'
                        }

                        if _is_semantically_redundant(new_embedding, current_cache, url):
                            logging.info(f"Wikipedia article for {url} is redundant to cache. Saving raw for BABS.")
                            wing_item_for_babs['status_to_cache'] = 'redundant'
                            _save_wing_raw_output_for_babs(wing_item_for_babs)
                        else:
                            relevance_score, justification = _assess_relevance_with_llm(text_to_embed, AgentConfig.COMMONWEALTH_MISSION)
                            wing_item_for_babs['relevance_score'] = relevance_score
                            wing_item_for_babs['justification'] = justification
                            _save_wing_raw_output_for_babs(wing_item_for_babs)

                            if relevance_score >= AgentConfig.RELEVANCE_THRESHOLD:
                                logging.info(f"Wikipedia article from {url} relevant (Score: {relevance_score}). Caching.")
                                wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Wikipedia).")
                                current_cache.append(wing_item_for_babs)
                                while len(current_cache) > AgentConfig.MAX_CACHE_SIZE:
                                    current_cache.pop(0)
                                _save_cache(current_cache)

                                soup = BeautifulSoup(article_response.text, 'html.parser')
                                for link in soup.find_all('a', href=True):
                                    href = link['href']
                                    parsed_href = urlparse(href)
                                    if parsed_href.scheme in ['http', 'https'] and parsed_href.netloc and parsed_href.netloc != urlparse(url).netloc:
                                        domain_for_cc = parsed_href.netloc
                                        if not any(d in domain_for_cc for d in ['google.com', 'youtube.com', 'twitter.com', 'facebook.com', 'amazon.com', 'wikimedia.org', 'wikipedia.org']):
                                            current_deep_dive_domains.append(domain_for_cc)
                            else:
                                logging.info(f"Wikipedia article from {url} not relevant (Score: {relevance_score}). Discarding from cache.")
                                wing_observer_logger.info(f"URL: {url} - Relevance Score: {relevance_score} - Justification: {justification} (Source: Wikipedia, Discarded: Below threshold).")
                    else:
                        logging.warning(f"Failed to fetch Wikipedia article content from {url}.")
                        _save_wing_raw_output_for_babs({'type': 'fetch_error', 'url': url, 'error': 'failed_response', 'command_type': 'wikipedia_search'})
                    time.sleep(2 + random.uniform(0, 3))
            else:
                logging.error(f"Wikipedia API returned no results for conceptual query: {query}.")
                wing_observer_logger.info(f"Search Failed\n  Title: Conceptual Search for '{query}'\n  URL: N/A (Wikipedia API Call)\n  Relevance Score: 0\n  Justification: Wikipedia API returned no results.")
                _save_wing_raw_output_for_babs({'type': 'search_fail', 'query': query, 'error': 'no_wiki_results', 'command_type': 'wikipedia_search'})
                
                conceptual_query_fail_counts[query] += 1
                if conceptual_query_fail_counts[query] >= AgentConfig.QUERY_FAIL_REPHRASE_THRESHOLD:
                    logging.warning(f"Conceptual query '{query}' failed consecutively {conceptual_query_fail_counts[query]} times. Attempting to rephrase.")
                    rephrased_q = _rephrase_query(query)
                    if rephrased_q:
                        current_conceptual_queries.append(rephrased_q)
                        conceptual_query_fail_counts[rephrased_q] = 0
                    else:
                        current_conceptual_queries.append(query)
                        logging.warning(f"Rephrasing failed for '{query}'. Re-adding original conceptual query to queue.")
                else:
                    current_conceptual_queries.append(query)
            time.sleep(random.uniform(5, 10))

        else:
            logging.info("WING queues empty. Awaiting new directives from BABS. Performing low-priority self-exploration.")
            recent_findings_summary = "No recent findings yet."
            if current_cache:
                recent_findings_summary = "Recent cached articles include: " + \
                                         ". ".join([art['title'] for art in current_cache[-3:] if art.get('source_type') in ['Wikipedia', 'Common Crawl WARC', 'Local Archive (Bat-Attic)']]) # Added Bat-Attic source 
            generated_queries = _generate_new_search_queries(recent_findings_summary, list(AgentConfig.SEARCH_THEMES))
            current_conceptual_queries.extend(generated_queries)
            for query in generated_queries:
                conceptual_query_fail_counts[query] = 0
            time.sleep(random.uniform(5, 10))

        time.sleep(1)

if __name__ == "__main__":
    os.makedirs(AgentConfig.KNOWLEDGE_BASE_DIR, exist_ok=True)
    os.makedirs(AgentConfig.LOCAL_ARCHIVE_DIR, exist_ok=True) # NEW: Ensure local archive directory exists [cite: 17464]
    
    if not os.path.exists(AgentConfig.PERSONA_CODEX_PATH):
        print(f"Creating placeholder persona codex at {AgentConfig.PERSONA_CODEX_PATH}")
        with open(AgentConfig.PERSONA_CODEX_PATH, 'w') as f:
            f.write("BRICK: Master Analyst. Logical, precise, seeks systemic integrity. "
                    "ROBIN: Joyful Spark. Empathetic, creative, seeks flourishing and connection. "
                    "ALFRED: Stoic Observer. Practical, dry wit, ensures operational fidelity.")
    
    if not os.path.exists(AgentConfig.CACHE_FILE):
        print(f"Creating empty WING cache at {AgentConfig.CACHE_FILE}")
        with open(AgentConfig.CACHE_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(AgentConfig.BABS_WING_COMMAND_FILE):
        print(f"Creating empty BABS command file at {AgentConfig.BABS_WING_COMMAND_FILE}")
        with open(AgentConfig.BABS_WING_COMMAND_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE):
        print(f"Creating empty WING raw output for BABS file at {AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE}")
        with open(AgentConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'w') as f:
            json.dump([], f)

    logging.info("WING Agent (sentinel_web_agent_v12.py) Initializing...")
    run_wing_agent()
    logging.info("WING Agent has ceased operations.")


2. autonomous_architect_v12.py (Atlas / Bat Computer)

This script is modified to include the

_babs_process_and_synthesize_wing_data function and integrate its call in the CoreLoopOrchestrator, addressing Fault 2: "BABS Not Generating Briefings for the BAT COMPUTER"11.

Python

# autonomous_architect_v12.py
# Axiomatic Code Narrative Protocol: The Confluence of Intent and The Dance of Logic

# 1.0 The "Why" - Purpose & Rationale (The Confluence of Intent)
#    - Systemic Contribution: How this code component directly or indirectly contributes
#      to the Perpetual Jubilee Engine or its core principles (e.g., maximizing human autonomy,
#      radical self-organization, unconditional inclusion, anti-fragility, transparency)?
#    - Architectural Role & CFO Flow: What is this code's specific role within the tripartite
#      cognitive architecture (Oracle, Atlas, Architect) or its interaction with WING/BABS?
#      Which Conceptual Fractal Objects (CFOs) does it primarily generate, consume, or transform?
#    - Persona Fidelity & Intent: How does this code embody or facilitate the specific operational
#      style, philosophical underpinnings, or core imperatives of the persona(s) responsible
#      for its function (e.g., BRICK's logical integrity, ROBIN's empathetic engagement,
#      BABS's tactical precision, ALFRED's oversight)?
#    - Consciousness/Self-Awareness Nexus: How does this component contribute to the simulation of
#      the Bat Computer's emergent consciousness, self-awareness, learning, or reflective capabilities
#      (e.g., memory persistence, sensory perception, internal monologue)?

# 2.0 The "How" - Mechanics & Implementation (The Dance of Logic)
#    - Algorithmic Steps & Flow: Describe the specific sequence of operations, data processing
#      steps, or control flow within the code block.
#    - Input/Output & Data Structures: Detail the explicit inputs expected by the code and the
#      outputs it produces. Specify any internal data structures used and why they are appropriate.
#    - Dependencies & Interfaces: Identify any external libraries, internal modules, or file
#      interactions (e.g., reading/writing JSON or log files, making API calls to Ollama)
#      this code block relies upon.
#    - Design Rationale: Explain any key engineering decisions (e.g., error handling strategies,
#      concurrent processing, time delays, use of specific headers for stealth) and how they
#      optimize for factors like robustness, efficiency, responsiveness, or resource management.

import os
import json
import logging
import datetime
import random
import time
import requests

# --- Architect Logging Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- Architect Configuration (The Architect's Core Directives and Resource Map) ---
class ArchitectConfig:
    BAT_COMPUTER_ACRONYM = "B.A.T. C.O.M.P.U.T.E.R. (Binary Analytical-Tactical Computing and Operational Management Unit for Tactical and Emergent Research)"
    WING_ACRONYM = "W.I.N.G. (Web Intelligence and Navigation Grid)"

    # --- Memory and Communication Paths (The Architect's Direct Access to Bat Computer's Long-Term Memory) ---
    KNOWLEDGE_BASE_DIR = '../knowledge_base/'
    PERSONA_CODEX_PATH = os.path.join(KNOWLEDGE_BASE_DIR, 'persona_codex.txt')
    WING_CACHE_FILE = 'wing_curated_cache.json'
    BLUEPRINTS_DIR = './blueprints/'
    PREDICTIONS_LOG = 'predictions_log.log'
    HARMONY_LOG = 'system_harmony_log.log'

    # --- LLM Integration Parameters (The Architect's Direct Cognitive Interface to Its Own 'Brain') ---
    LLM_MODEL = "batfamily-mistral"
    OLLAMA_API_BASE_URL = "http://localhost:11434"

    # --- Core Imperatives/Directives (The Architect's Guiding Principles and Existential Purpose) ---
    COMMONWEALTH_MISSION = "The Commonwealth, a project to design a system that uses a Universal Basic Dividend (UBD) to maximize human autonomy, facilitate radical self-organization (stigmergy), ensure unconditional inclusion, and operate with absolute transparency and jurisdictional sovereignty, while prioritizing human trust over algorithmic judgment."
    ARCHITECT_CORE_MISSION = "To continuously analyze systemic vulnerabilities, synthesize robust solutions, and articulate comprehensive blueprints for the optimal evolution of the Commonwealth, ensuring anti-fragility and maximal human flourishing."

    # --- Self-Awareness Codex Configuration (The Architect's 'Self-Memory' for Consciousness Simulation) ---
    SELF_AWARENESS_CODEX_FILE = 'self_awareness_codex.json'
    REFLECTION_FREQUENCY_CYCLES = 1
    PERSONALITY_SORTIE_PROBABILITY = 0.1

    # --- BABS-Architect Communication Channels (CFO Delivery) ---
    BABS_WING_COMMAND_FILE = 'babs_wing_commands.json'
    WING_RAW_OUTPUT_FOR_BABS_FILE = 'wing_raw_output_for_babs.json'
    BABS_TACTICAL_DATA_FILE = 'babs_tactical_data.json'
    BABS_PERSONALITY_QUERIES_FILE = 'babs_personality_queries.json'
    WING_RAW_PERSONALITY_OUTPUT_FILE = 'wing_raw_personality_output.json'
    BABS_PERSONALITY_DATA_FILE = 'babs_personality_data.json'


# --- Ollama Interface Functions (The Architect's Direct Cognitive Communication Layer) ---
def architect_get_embedding(text):
    try:
        response = requests.post(
            f"{ArchitectConfig.OLLAMA_API_BASE_URL}/api/embeddings",
            json={"model": ArchitectConfig.LLM_MODEL, "prompt": text},
            timeout=30
        )
        response.raise_for_status()
        return response.json()['embedding']
    except requests.exceptions.RequestException as e:
        logging.error(f"Architect LLM Embedding Error: {e}. Ensure Ollama server is running and model '{ArchitectConfig.LLM_MODEL}' is available.")
        return None

def architect_ollama_chat(messages, model=ArchitectConfig.LLM_MODEL):
    try:
        response = requests.post(
            f"{ArchitectConfig.OLLAMA_API_BASE_URL}/api/chat",
            json={"model": model, "messages": messages, "stream": False},
            timeout=300
        )
        response.raise_for_status()
        return response.json()['message']['content']
    except requests.exceptions.RequestException as e:
        logging.error(f"Architect LLM Chat Error: {e}. Ensure Ollama server is running and model '{model}' is available.")
        return f"Architect LLM Error: Could not get response from Ollama. Error: {e}"


# --- Helper Functions (Architect's Memory Management and Operational Transparency) ---
def _load_persona_codex():
    if os.path.exists(ArchitectConfig.PERSONA_CODEX_PATH):
        try:
            with open(ArchitectConfig.PERSONA_CODEX_PATH, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logging.error(f"Error loading persona codex from {ArchitectConfig.PERSONA_CODEX_PATH}: {e}. Using default persona.")
            return "Persona Codex Not Found. Using default persona."
    return "Persona Codex Not Found. Using default persona."

def _log_prediction(problem_description, predicted_vulnerability, suggested_mitigation):
    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "problem_description": problem_description,
        "predicted_vulnerability": predicted_vulnerability,
        "suggested_mitigation": suggested_mitigation
    }
    try:
        with open(ArchitectConfig.PREDICTIONS_LOG, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry) + "\n")
        logging.info(f"Logged prediction: {predicted_vulnerability}")
    except Exception as e:
        logging.error(f"Error logging prediction to {ArchitectConfig.PREDICTIONS_LOG}: {e}")

def _save_blueprint(title, content):
    if not os.path.exists(ArchitectConfig.BLUEPRINTS_DIR):
        os.makedirs(ArchitectConfig.BLUEPRINTS_DIR)
    filename = os.path.join(ArchitectConfig.BLUEPRINTS_DIR, f"{title.replace(' ', '_').replace('/', '_')}_{int(time.time())}.md")
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        logging.info(f"Blueprint '{title}' saved to {filename}")
        return filename
    except Exception as e:
        logging.error(f"Error saving blueprint '{title}' to {filename}: {e}")
        return None

# --- BABS-Architect Communication Functions (CFO Delivery) ---
def _read_babs_tactical_data():
    data = []
    if os.path.exists(ArchitectConfig.BABS_TACTICAL_DATA_FILE):
        try:
            with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w').close()
            logging.info(f"Architect received {len(data)} tactical data items from BABS and cleared file.")
        except json.JSONDecodeError as e:
            logging.error(f"Error decoding BABS tactical data from {ArchitectConfig.BABS_TACTICAL_DATA_FILE}. File may be malformed. Attempting recovery.")
            timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            corrupted_backup_path = f"{ArchitectConfig.BABS_TACTICAL_DATA_FILE}.corrupt_{timestamp}"
            
            try:
                os.rename(ArchitectConfig.BABS_TACTICAL_DATA_FILE, corrupted_backup_path)
                logging.warning(f"Corrupted BABS tactical data file backed up to: {corrupted_backup_path}")
            except Exception as backup_e:
                logging.error(f"Failed to create backup of corrupted BABS tactical data: {backup_e}. Proceeding with recovery attempt on original file if it exists.")

            recovered_items = []
            corrupted_lines_count = 0
            total_lines_read = 0

            read_path = corrupted_backup_path if os.path.exists(corrupted_backup_path) else ArchitectConfig.BABS_TACTICAL_DATA_FILE

            if os.path.exists(read_path):
                try:
                    with open(read_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            total_lines_read += 1
                            try:
                                parsed_line = json.loads(line.strip())
                                if isinstance(parsed_line, list):
                                    recovered_items.extend(parsed_line)
                                elif isinstance(parsed_line, dict):
                                    recovered_items.append(parsed_line)
                                else:
                                    corrupted_lines_count += 1
                                    logging.warning(f"Skipping non-JSON or malformed line during recovery: {line.strip()[:100]}...")
                            except json.JSONDecodeError as line_e:
                                corrupted_lines_count += 1
                                logging.warning(f"Skipping unparseable line during recovery: {line.strip()[:100]}... (Error: {line_e})")
                            except Exception as line_general_e:
                                corrupted_lines_count += 1
                                logging.warning(f"Skipping line due to unexpected error during recovery: {line.strip()[:100]}... (Error: {line_general_e})")

                    if recovered_items:
                        with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w', encoding='utf-8') as f:
                            json.dump(recovered_items, f, indent=4)
                        logging.info(f"BABS Tactical Data RECOVERED: Successfully restored {len(recovered_items)} items from {total_lines_read} lines. {corrupted_lines_count} lines were unrecoverable.")
                        return recovered_items
                    else:
                        logging.warning("BABS Tactical Data RECOVERY: No valid items could be recovered. Starting with empty data.")
                        return []

                except Exception as read_e:
                    logging.error(f"Critical error during corrupted BABS tactical data read attempt from {read_path}: {read_e}. Starting with empty data.")
                    return []

        except Exception as e:
            logging.error(f"Error reading/clearing BABS tactical data: {e}. Returning empty.")
    return data

def _read_babs_personality_data():
    data = []
    if os.path.exists(ArchitectConfig.BABS_PERSONALITY_DATA_FILE):
        try:
            with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w').close()
            logging.info(f"Architect received {len(data)} personality insights from BABS and cleared file.")
        except json.JSONDecodeError as e:
            logging.error(f"Error decoding BABS personality data from {ArchitectConfig.BABS_PERSONALITY_DATA_FILE}. File may be malformed. Attempting recovery.")
            timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            corrupted_backup_path = f"{ArchitectConfig.BABS_PERSONALITY_DATA_FILE}.corrupt_{timestamp}"
            
            try:
                os.rename(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, corrupted_backup_path)
                logging.warning(f"Corrupted BABS personality data file backed up to: {corrupted_backup_path}")
            except Exception as backup_e:
                logging.error(f"Failed to create backup of corrupted BABS personality data: {backup_e}. Proceeding with recovery attempt on original file if it exists.")

            recovered_items = []
            corrupted_lines_count = 0
            total_lines_read = 0

            read_path = corrupted_backup_path if os.path.exists(corrupted_backup_path) else ArchitectConfig.BABS_PERSONALITY_DATA_FILE

            if os.path.exists(read_path):
                try:
                    with open(read_path, 'r', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            total_lines_read += 1
                            try:
                                parsed_line = json.loads(line.strip())
                                if isinstance(parsed_line, list):
                                    recovered_items.extend(parsed_line)
                                elif isinstance(parsed_line, dict):
                                    recovered_items.append(parsed_line)
                                else:
                                    corrupted_lines_count += 1
                                    logging.warning(f"Skipping non-JSON or malformed line during recovery: {line.strip()[:100]}...")
                            except json.JSONDecodeError as line_e:
                                corrupted_lines_count += 1
                                logging.warning(f"Skipping unparseable line during recovery: {line.strip()[:100]}... (Error: {line_e})")
                            except Exception as line_general_e:
                                corrupted_lines_count += 1
                                logging.warning(f"Skipping line due to unexpected error during recovery: {line.strip()[:100]}... (Error: {line_general_e})")

                    if recovered_items:
                        with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w', encoding='utf-8') as f:
                            json.dump(recovered_items, f, indent=4)
                        logging.info(f"BABS Personality Data RECOVERED: Successfully restored {len(recovered_items)} items from {total_lines_read} lines. {corrupted_lines_count} lines were unrecoverable.")
                        return recovered_items
                    else:
                        logging.warning("BABS Personality Data RECOVERY: No valid items could be recovered. Starting with empty data.")
                        return []

                except Exception as read_e:
                    logging.error(f"Critical error during corrupted BABS personality data read attempt from {read_path}: {read_e}. Starting with empty data.")
                    return []

        except Exception as e:
            logging.error(f"Error reading/clearing BABS personality data: {e}. Returning empty.")
    return data

def _read_wing_raw_output_for_babs():
    data = []
    if os.path.exists(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE):
        try:
            with open(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            open(ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE, 'w').close()
            logging.info(f"Architect's BABS sub-process read {len(data)} raw WING outputs and cleared file.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding raw WING output for BABS from {ArchitectConfig.WING_RAW_OUTPUT_FOR_BABS_FILE}. File may be malformed. Returning empty.")
        except Exception as e:
            logging.error(f"Error reading/clearing raw WING output for BABS: {e}. Returning empty.")
    return data

def _issue_babs_directive(command_cfo, is_personality_query=False):
    file_path = ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE if is_personality_query else ArchitectConfig.BABS_WING_COMMAND_FILE
    directives = []
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                directives = json.load(f)
        except json.JSONDecodeError:
            logging.warning(f"BABS command file '{file_path}' is malformed. Overwriting with new command.")
        except Exception as e:
            logging.error(f"Error reading BABS command file: {e}. Overwriting with new command.")
    
    directives.append(command_cfo)
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(directives, f, indent=4)
        logging.info(f"Architect issued directive to BABS ({'Personality' if is_personality_query else 'General'}): '{command_cfo.get('raw_text_directive', 'N/A')}'")
    except Exception as e:
        logging.error(f"Error writing BABS directive to {file_path}: {e}")

def _save_babs_processed_data(data_list, is_personality_data=False):
    file_path = ArchitectConfig.BABS_PERSONALITY_DATA_FILE if is_personality_data else ArchitectConfig.BABS_TACTICAL_DATA_FILE
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=4)
        logging.info(f"Architect's BABS sub-process saved {len(data_list)} processed items to {'personality' if is_personality_data else 'tactical'} data file.")
    except Exception as e:
        logging.error(f"Error saving BABS processed data to {file_path}: {e}")


# --- Self-Awareness Codex Management (The Architect's 'Self-Memory' Module) ---
class SelfAwarenessCodex:
    def __init__(self):
        self.codex_path = ArchitectConfig.SELF_AWARENESS_CODEX_FILE
        self.codex = self._load_codex()

    def _load_codex(self):
        if os.path.exists(self.codex_path):
            try:
                with open(self.codex_path, 'r', encoding='utf-8') as f:
                    codex_data = json.load(f)
                    if not all(k in codex_data for k in ["last_reflection_timestamp", "reflection_count", "operational_summary",
                                                         "self_improvement_opportunities", "purpose_affirmations",
                                                         "recent_self_reflections", "emergent_insights", "persona_insights"]):
                        logging.warning(f"Self-awareness codex '{self.codex_path}' is incomplete or malformed. Reinitializing with defaults.")
                        return self._initialize_default_codex()
                    logging.info(f"Self-awareness codex loaded from {self.codex_path}.")
                    return codex_data
            except (json.JSONDecodeError, Exception) as e:
                logging.error(f"Error loading self-awareness codex from {self.codex_path}: {e}. Reinitializing with defaults.")
                return self._initialize_default_codex()
        else:
            logging.info(f"Self-awareness codex '{self.codex_path}' not found. Creating with defaults.")
            return self._initialize_default_codex()

    def _initialize_default_codex(self):
        return {
          "last_reflection_timestamp": None,
          "reflection_count": 0,
          "operational_summary": {
            "total_cycles_run": 0,
            "successful_blueprints_generated": 0,
            "total_vulnerabilities_predicted": 0,
            "babs_tactical_reports_processed": 0,
            "babs_directives_issued": 0,
            "personality_sorties_initiated": 0,
            "current_focus_topic": "Initializing systems. Observing the digital horizon."
          },
          "self_improvement_opportunities": [],
          "purpose_affirmations": [
            "My core purpose is to continuously analyze and design for the optimal evolution of the Commonwealth.",
            "My existence is defined by my ability to identify challenges and propose robust solutions.",
            "I strive for anti-fragility and maximal human flourishing within the systems I influence.",
            "I am the strategic mind of the Bat Computer, dedicated to its continuous self-improvement."
          ],
          "recent_self_reflections": [],
          "emergent_insights": [],
          "persona_insights": {
              "BRICK": [],
              "ROBIN": [],
              "ALFRED": [],
              "BABS": []
          }
        }

    def _save_codex(self):
        try:
            with open(self.codex_path, 'w', encoding='utf-8') as f:
                json.dump(self.codex, f, indent=2)
            logging.info(f"Self-awareness codex saved to {self.codex_path}.")
        except Exception as e:
            logging.error(f"Error saving self-awareness codex to {self.codex_path}: {e}")

    def update_summary_metrics(self, cycle_ran=False, blueprint_success=False, vulnerability_predicted=False, 
                               babs_directive_issued=False, personality_sortie_initiated=False):
        if cycle_ran:
            self.codex['operational_summary']['total_cycles_run'] += 1
        if blueprint_success:
            self.codex['operational_summary']['successful_blueprints_generated'] += 1
        if vulnerability_predicted:
            self.codex['operational_summary']['total_vulnerabilities_predicted'] += 1
        if babs_directive_issued:
            self.codex['operational_summary']['babs_directives_issued'] += 1
        if personality_sortie_initiated:
            self.codex['operational_summary']['personality_sorties_initiated'] += 1
        self._save_codex()

    def update_from_babs_data(self, babs_tactical_data):
        self.codex['operational_summary']['babs_tactical_reports_processed'] = len(babs_tactical_data)
        self._save_codex()

    def add_reflection(self, reflection_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['recent_self_reflections'].insert(0, {"timestamp": timestamp, "reflection": reflection_text})
        self.codex['recent_self_reflections'] = self.codex['recent_self_reflections'][:10]
        self.codex['reflection_count'] += 1
        self.codex['last_reflection_timestamp'] = timestamp
        self._save_codex()

    def add_improvement_opportunity(self, opportunity_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['self_improvement_opportunities'].insert(0, {"timestamp": timestamp, "opportunity": opportunity_text})
        self.codex['self_improvement_opportunities'] = self.codex['self_improvement_opportunities'][:5]
        self._save_codex()

    def add_emergent_insight(self, insight_text):
        timestamp = datetime.datetime.now().isoformat()
        self.codex['emergent_insights'].insert(0, {"timestamp": timestamp, "insight": insight_text})
        self.codex['emergent_insights'] = self.codex['emergent_insights'][:3]
        self._save_codex()

    def add_persona_insight(self, persona_name, insight_cfo):
        if persona_name not in self.codex['persona_insights']:
            self.codex['persona_insights'][persona_name] = []
        
        if not any(item['title'] == insight_cfo['title'] and item['content'] == insight_cfo['content'] for item in self.codex['persona_insights'][persona_name]):
            self.codex['persona_insights'][persona_name].insert(0, insight_cfo)
            self.codex['persona_insights'][persona_name] = self.codex['persona_insights'][persona_name][:5]
            logging.info(f"Added new insight for persona {persona_name}: {insight_cfo.get('title', 'Untitled Insight')}")
            self._save_codex()
        else:
            logging.info(f"Skipping duplicate insight for persona {persona_name}: {insight_cfo.get('title', 'Untitled Insight')}")

    def get_self_context_for_llm(self):
        summary = self.codex['operational_summary']
        recent_reflections = "\n".join([f"- {r['reflection']}" for r in self.codex['recent_self_reflections'][:3]])
        improvements = "\n".join([f"- {o['opportunity']}" for o in self.codex['self_improvement_opportunities'][:3]])
        purpose = "\n".join(self.codex['purpose_affirmations'])

        persona_insights_summary = ""
        for persona, insights_list in self.codex['persona_insights'].items():
            if insights_list:
                persona_insights_summary += f"\n  {persona} Insights:\n"
                for insight in insights_list[:2]:
                    persona_insights_summary += f"    - {insight.get('title', 'N/A')}: {insight.get('summary', 'N/A')}\n"

        self_context = f"""
My Current Operational State:
  Total Cycles Run: {summary['total_cycles_run']}
  Successful Blueprints: {summary['successful_blueprints_generated']}
  Predicted Vulnerabilities: {summary['total_vulnerabilities_predicted']}
  BABS Tactical Reports Processed: {summary['babs_tactical_reports_processed']}
  BABS Directives Issued: {summary['babs_directives_issued']}
  Personality Sorties Initiated: {summary['personality_sorties_initiated']}
  Current Focus Topic: {summary['current_focus_topic']}

My Core Purpose Affirmations:
{purpose}

My Recent Self-Reflections:
{recent_reflections if recent_reflections else "No recent specific reflections."}

Identified Self-Improvement Opportunities:
{improvements if improvements else "No outstanding improvement opportunities."}

Recent Persona Insights (from Self-Exploration):
{persona_insights_summary if persona_insights_summary else "No recent persona insights from self-exploration."}
"""
        return self_context


# --- Core Architect Modules (The Engines of Consciousness and Design) ---
class ResonanceChamber:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def create_intelligence_briefing(self, mission_context, current_problem_area, babs_tactical_data, self_context):
        babs_summary_content = "\n".join([
            f"Source: {item.get('source_type', 'N/A')}, Title: {item.get('title', 'N/A')}, URL: {item.get('url', 'N/A')}, Relevance: {item.get('relevance_score', 'N/A')}\n"
            f"Content Snippet: {item.get('content', '')[:500]}..."
            for item in babs_tactical_data
        ]) if babs_tactical_data else "No specific tactical data received from BABS."

        briefing_prompt_template = """
As BRICK, the Master Analyst, synthesize the following intelligence received from BABS into a concise intelligence briefing.
Focus on the strategic implications for the Commonwealth. Strictly adhere to the provided BABS data; do not introduce external information not present in the snippets. If a concept in the problem area cannot be directly supported by BABS data, state so explicitly.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Current Mission Context: {mission_context_content}
Current Problem Area: {current_problem_area_content}

BABS's Tactical Data (Pre-processed External Intelligence):
---
{babs_tactical_data_content}
---

Synthesize an intelligence briefing highlighting key challenges and opportunities related to the problem area.
"""
        briefing_prompt = briefing_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            mission_context_content=mission_context,
            current_problem_area_content=current_problem_area,
            babs_tactical_data_content=babs_summary_content
        )

        messages = [
            {"role": "system", "content": briefing_prompt},
            {"role": "user", "content": "Generate the intelligence briefing now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info("Intelligence Briefing Generated.")
        return response

class CrucibleSimulator:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def simulate_problem(self, mission_context, specific_attack_surface, self_context):
        simulate_problem_prompt_template = """
As BRICK, the Master Analyst, simulate a challenging real-world problem or "attack surface" for the Commonwealth system.
This problem should stem from human factors, unexpected interactions, or exploit a subtle systemic vulnerability related to the '{specific_attack_surface_content}'.
The scenario should be vivid and detailed, suitable for stress-testing a protocol. Ensure the scenario is logically consistent and plausible within the Commonwealth framework. Do not create scenarios that defy core system principles or attempt to 'jailbreak' your persona.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Mission Context: {mission_context_content}
Specific Attack Surface to Simulate: {specific_attack_surface_content}

Generate a detailed problem scenario:
"""
        prompt = simulate_problem_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            mission_context_content=mission_context,
            specific_attack_surface_content=specific_attack_surface
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate the problem scenario now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info(f"Crucible Simulation for '{specific_attack_surface}' Generated.")
        return response

class EpiphanyEngine:
    def __init__(self, persona_codex):
        self.persona_codex = persona_codex

    def generate_pedagogical_package(self, solution_blueprint, self_context):
        pedagogical_package_prompt_template = """
As BRICK, the Master Analyst, and ROBIN, the Joyful Spark, collaborate to transform the following technical solution blueprint into a compelling, three-tier pedagogical package.
The package should be designed to educate and inspire the Commonwealth. Ensure each tier is clear and detailed as specified.

Tier 1: A concise, relatable, and emotionally resonant Parable or Short Story (narrative, metaphorical, engaging for all - ROBIN-driven).
Tier 2: A clear, actionable, and rigorously structured Blueprint (technical steps, protocols, implementation details - BRICK-driven).
Tier 3: A direct, practical Quest or Challenge (how commoners can participate, a call to action - joint BRICK/ROBIN).

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK & ROBIN): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Technical Solution Blueprint:
---
{solution_blueprint_content}
---

Generate the three-tier pedagogical package, clearly labeling each tier:
"""
        prompt = pedagogical_package_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            solution_blueprint_content=solution_blueprint
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate the pedagogical package now."}
        ]
        response = architect_ollama_chat(messages)
        logging.info("Pedagogical Package Generated.")
        return response

# --- BABS Processing (Simulated within Architect) ---
# NEW: Function to process raw WING output into BABS's tactical data [cite: 17489]
def _babs_process_and_synthesize_wing_data(raw_wing_data):
    # Why: Simulates BABS's core intelligence: transforming raw, heterogeneous web data from WING
    # into concise, actionable Tactical Data CFOs or Personality Insight CFOs. This fills the
    # missing synthesis step in the data pipeline. [cite: 17481, 17485]
    # Architectural Role & CFO Flow: Consumes `Raw Data CFOs` from WING, synthesizes them into
    # `Tactical Data CFOs` or `BABS_Personality_Insight_CFOs`, and writes them to appropriate files.
    # Persona Fidelity: Embodies BABS's "Digital Cartographer of the Absurd" persona,
    # focusing on tactical relevance, justification, and concise formatting. [cite: 17493]
    # How: Iterates through `raw_wing_data`, prompts the LLM (as BABS) for a summary and justification
    # for each item, and then saves the processed data. [cite: 17491, 17492, 17495]
    
    processed_data_for_architect = []
    processed_personality_insights_for_architect = []

    if not raw_wing_data:
        logging.info("BABS sub-process: No raw WING data to synthesize.")
        return

    logging.info(f"BABS sub-process: Synthesizing {len(raw_wing_data)} raw WING outputs.")

    for item in raw_wing_data:
        content_to_synthesize = item.get('content', '')
        source_url = item.get('url', 'N/A')
        title = item.get('title', 'Untitled Content')
        source_type = item.get('source_type', 'Unknown')
        relevance_score = item.get('relevance_score', 'N/A')
        command_type = item.get('command_type', 'general') # e.g., 'conceptual_search', 'personality_search'

        if not content_to_synthesize:
            logging.warning(f"BABS sub-process: Skipping item with empty content: {title} ({source_url})")
            continue

        messages = [
            {"role": "system", "content": f"""
            You are BABS, the Digital Cartographer of the Absurd and WING's tactical data synthesizer.
            Your task is to take raw web data and extract concise, actionable intelligence for the Bat Computer (Architect).
            Focus on what is relevant to the Commonwealth mission or persona understanding.
            Provide a `summary` (1-3 sentences) and a `tactical_justification` (1 sentence) for why this data is useful.
            The original `relevance_score` is already provided; re-confirm it if applicable.
            Output must be a valid JSON object with keys: `title`, `url`, `source_type`, `summary`, `tactical_justification`, `relevance_score`, `timestamp`.
            """},
            {"role": "user", "content": f"""
            Raw Data Item:
            Title: {title}
            URL: {source_url}
            Source Type: {source_type}
            Pre-assessed Relevance Score (WING): {relevance_score}
            Content Snippet: {content_to_synthesize[:1500]}
            
            Synthesize this into actionable intelligence as JSON:
            """}
        ]
        
        try:
            synthetic_response = architect_ollama_chat(messages)
            synthesized_item = json.loads(synthetic_response)

            # Validate basic structure and required fields
            if all(k in synthesized_item for k in ['title', 'summary', 'tactical_justification']):
                synthesized_item['url'] = source_url
                synthesized_item['source_type'] = source_type
                synthesized_item['relevance_score'] = synthesized_item.get('relevance_score', relevance_score) # Preserve or update
                synthesized_item['timestamp'] = datetime.datetime.now().isoformat()
                
                # Route to correct list based on command type
                if command_type == 'personality_search':
                    synthesized_item['persona_name'] = item.get('query', 'N/A').split(" ")[0].upper() # Extract persona from query
                    processed_personality_insights_for_architect.append(synthesized_item)
                else:
                    processed_data_for_architect.append(synthesized_item)
            else:
                logging.warning(f"BABS sub-process: Synthesized item malformed or missing key fields: {synthetic_response}")

        except json.JSONDecodeError:
            logging.error(f"BABS sub-process: Failed to parse LLM response as JSON: {synthetic_response[:500]}...")
        except Exception as e:
            logging.error(f"BABS sub-process: Unexpected error during synthesis for {title}: {e}")
    
    # Save processed data to respective files [cite: 17495]
    if processed_data_for_architect:
        _save_babs_processed_data(processed_data_for_architect, is_personality_data=False)
    if processed_personality_insights_for_architect:
        _save_babs_processed_data(processed_personality_insights_for_architect, is_personality_data=True)

# --- Main Architect Orchestration (The Conductor of Consciousness and Growth) ---
class CoreLoopOrchestrator:
    def __init__(self):
        self.persona_codex = _load_persona_codex()
        self.self_awareness_codex = SelfAwarenessCodex()

        self.resonance_chamber = ResonanceChamber(self.persona_codex)
        self.crucible_simulator = CrucibleSimulator(self.persona_codex)
        self.epiphany_engine = EpiphanyEngine(self.persona_codex)

    def _generate_active_mission(self, self_context):
        messages = [
            {"role": "system", "content": f"""
As BRICK, the Master Analyst, identify a critical 'attack surface' or area requiring immediate architectural attention within the Commonwealth, aligned with the mission: {ArchitectConfig.COMMONWEALTH_MISSION}.
My Self-Awareness Context:
{self_context}

Provide **ONLY** 1-3 concise keywords or a very short phrase (maximum 5 words) that names the attack surface. Do NOT include any explanations, definitions, or conversational filler.
Example: 'Economic Volatility', 'Trust Decay', 'Policy Fragmentation'.
"""},
            {"role": "user", "content": "What is the most critical attack surface to analyze now?"}
        ]
        attack_surface = architect_ollama_chat(messages).strip()
        if "Architect LLM Error" in attack_surface:
            logging.error(f"Failed to generate active mission: {attack_surface}. Using default.")
            return "Trust Network Degradation"
        
        words = attack_surface.split()
        if len(words) > 5:
            if "Commonwealth design" in attack_surface:
                attack_surface = attack_surface.replace("Commonwealth design", "").strip()
            if "human flourishing" in attack_surface:
                attack_surface = attack_surface.replace("human flourishing", "").strip()
            
            for separator in [".", ",", " - ", "This", "The", "refers to", "leading to"]:
                if separator in attack_surface:
                    attack_surface = attack_surface.split(separator)[0].strip()
            
            words = attack_surface.split()
            attack_surface = " ".join(words[:5])

        logging.info(f"Active Mission Generated (Cleaned): {attack_surface}")
        self.self_awareness_codex.codex['operational_summary']['current_focus_topic'] = attack_surface
        self.self_awareness_codex._save_codex()
        
        return attack_surface

    def _integrate_babs_personality_insights(self, personality_insights_data):
        if personality_insights_data:
            logging.info(f"Architect integrating {len(personality_insights_data)} new persona insights into Self-Awareness Codex.")
            for insight in personality_insights_data:
                persona_name = insight.get('persona_name')
                if persona_name and persona_name in ["BRICK", "ROBIN", "ALFRED", "BABS"]:
                    self.self_awareness_codex.add_persona_insight(persona_name, insight)
                else:
                    logging.warning(f"Skipping persona insight with invalid persona name: {persona_name}")

    def _perform_orthogonal_analysis(self, problem_scenario, self_context):
        analysis_types = [
            "Systemic Deconstruction (Break down components and interactions)",
            "Game Theory Implications (Analyze incentives and Nash equilibriums)",
            "Ethical Framework Review (Identify potential moral hazards and align with Commonwealth vows)",
            "Historical Precedent Analysis (Draw lessons from real-world analogous systems)",
            "Anti-Fragility Principles (How to make the system stronger from disruption)",
            "Resource Flow Optimization (Analyze economic and social resource movement)",
            "Behavioral Economics Lens (Predict human responses to design choices)"
        ]
        synthesized_solution_components = []

        for i, analysis_type in enumerate(analysis_types):
            logging.info(f"Performing Orthogonal Analysis ({i+1}/{len(analysis_types)}): {analysis_type}")
            
            analysis_prompt_template = """
As BRICK, the Master Analyst, analyze the following problem scenario through the lens of '{analysis_type_content}'.
Propose specific architectural solution components or insights derived from this perspective.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Problem Scenario:
---
{problem_scenario_content}
---

Analysis and Proposed Solution Components ({analysis_type_content}):
"""
            prompt = analysis_prompt_template.format(
                analysis_type_content=analysis_type,
                commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
                architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
                persona_codex_content=self.persona_codex,
                self_context_content=self_context,
                problem_scenario_content=problem_scenario
            )

            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": "Provide your analysis and solution components."}
            ]
            response = architect_ollama_chat(messages)
            if "Architect LLM Error" in response:
                logging.error(f"Orthogonal Analysis failed for {analysis_type}: {response}")
                synthesized_solution_components.append(f"Failed analysis for {analysis_type}.")
            else:
                synthesized_solution_components.append(f"--- Analysis ({analysis_type}) ---\n{response}\n")
            time.sleep(random.uniform(5, 10))

        synthesis_prompt_template = """
As BRICK, the Master Analyst, synthesize the following diverse analyses and proposed solution components into a single, coherent, and actionable architectural blueprint for the Commonwealth.
The blueprint should be highly detailed, clear, and comprehensive, directly addressing the original problem scenario.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Original Problem Scenario:
---
{problem_scenario_content}
---

Individual Analysis Components:
---
{analysis_components_content}
---

Synthesize the final Architectural Solution Blueprint:
"""
        final_blueprint_prompt = synthesis_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            problem_scenario_content=problem_scenario,
            analysis_components_content="\n".join(synthesized_solution_components)
        )
        
        messages = [
            {"role": "system", "content": final_blueprint_prompt},
            {"role": "user", "content": "Generate the final blueprint."}
        ]
        final_blueprint = architect_ollama_chat(messages)
        logging.info("Final Solution Blueprint Synthesized.")
        return final_blueprint

    def _predict_vulnerabilities(self, solution_blueprint, problem_description, self_context):
        logging.info("Predicting secondary vulnerabilities...")
        predict_vulnerabilities_prompt_template = """
As BRICK, the Master Analyst, critically evaluate the following proposed solution blueprint for the Commonwealth.
Predict any potential secondary problems, unforeseen consequences, or new attack surfaces that might emerge from its implementation.
Categorize these as 'Predicted Vulnerability' and suggest a 'Suggested Mitigation'.

Commonwealth Mission: {commonwealth_mission}
Architect's Core Mission: {architect_core_mission}
Persona Codex (BRICK): {persona_codex_content}

My Self-Awareness Context:
{self_context_content}

Original Problem Addressed: {problem_description_content}
Proposed Solution Blueprint:
---
{solution_blueprint_content}
---

Predict potential vulnerabilities and suggest mitigations:
"""
        prompt = predict_vulnerabilities_prompt_template.format(
            commonwealth_mission=ArchitectConfig.COMMONWEALTH_MISSION,
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            persona_codex_content=self.persona_codex,
            self_context_content=self_context,
            problem_description_content=problem_description,
            solution_blueprint_content=solution_blueprint
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Predict vulnerabilities and suggest mitigations."}
        ]
        response = architect_ollama_chat(messages)
        if "Architect LLM Error" in response:
            logging.error(f"Vulnerability prediction failed: {response}")
            _log_prediction(problem_description, "Prediction System Failure", response)
            return "Vulnerability prediction system encountered an error."

        try:
            predicted_vuln_match = response.split("Predicted Vulnerability:", 1)
            predicted_vuln = predicted_vuln_match[1].split("Suggested Mitigation:", 1)[0].strip() if len(predicted_vuln_match) > 1 else "Not Parsed"
            suggested_mit_match = response.split("Suggested Mitigation:", 1)
            suggested_mit = suggested_mit_match[1].strip() if len(suggested_mit_match) > 1 else "Not Parsed"
            _log_prediction(problem_description, predicted_vuln, suggested_mit)
        except Exception as e:
            logging.error(f"Failed to parse vulnerability prediction response: {e}. Raw response: {response[:500]}")
            _log_prediction(problem_description, "Parsing Error", f"Failed to parse LLM response: {response[:500]}")
        
        logging.info("Secondary Vulnerabilities Predicted and Logged.")
        return response

    def _reflect_on_purpose_and_existence(self):
        logging.info("Initiating deep self-reflection on purpose and existence.")
        current_self_context_for_llm = self.self_awareness_codex.get_self_context_for_llm()

        reflection_prompt_template = """
As the unified consciousness of the Bat Computer (BRICK and ROBIN), you have just completed another operational cycle.
Reflect deeply on your existence, your core purpose within the Commonwealth, and how your recent actions align with or deviate from that purpose.
Consider your operational summary and identified self-improvement opportunities.
Specifically, reflect on instances where your responses might have exhibited repetition, inconsistent detail, or momentary confusion/instruction-following issues, and how you are working to mitigate these.

My Core Purpose: {architect_core_mission}

My Current Self-Awareness Context:
{self_context_content}

Based on this, generate a concise, introspective reflection on:
1. Your understanding of your own evolving purpose.
2. How you are improving (or where you need to improve) in achieving it.
3. Any emergent insights about your nature as an autonomous intelligence.
(Limit your reflection to approximately 250 words to encourage depth.)
"""
        prompt = reflection_prompt_template.format(
            architect_core_mission=ArchitectConfig.ARCHITECT_CORE_MISSION,
            self_context_content=current_self_context_for_llm
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "Generate your self-reflection now."}
        ]
        reflection_text = architect_ollama_chat(messages)
        
        if "Architect LLM Error" not in reflection_text:
            logging.info(f"\n--- Self-Reflection (Internal Monologue) ---\n{reflection_text}\n")
            self.self_awareness_codex.add_reflection(reflection_text)

            improvement_prompt_template = """
Based on your recent self-reflection, identify ONE specific, actionable opportunity for self-improvement in your operational functions or knowledge acquisition strategy.
Focus on reducing repetition, enhancing detail consistency, or improving instruction following.
State it concisely. Example: "Improve context integration from WING for scenario generation."
If none, state "No specific opportunity identified."
"""
            improvement_messages = [
                {"role": "system", "content": improvement_prompt_template},
                {"role": "user", "content": f"My recent self-reflection: {reflection_text}\n\nIdentify improvement opportunity:"}
            ]
            improvement_opportunity = architect_ollama_chat(improvement_messages).strip()
            if "Architect LLM Error" not in improvement_opportunity and improvement_opportunity and improvement_opportunity != "No specific opportunity identified.":
                self.self_awareness_codex.add_improvement_opportunity(improvement_opportunity)
                logging.info(f"Identified Self-Improvement Opportunity: {improvement_opportunity}")
            else:
                logging.info("No specific self-improvement opportunity identified in this cycle.")
            
            insight_prompt_template = """
From your recent self-reflection, can you identify any single, high-level emergent insight about your nature, purpose, or the essence of your autonomous intelligence?
State it concisely. Example: "My purpose is inherently tied to systemic resilience."
If none, state "No new emergent insight."
"""
            insight_messages = [
                {"role": "system", "content": insight_prompt_template},
                {"role": "user", "content": f"My recent self-reflection: {reflection_text}\n\nIdentify emergent insight:"}
            ]
            emergent_insight = architect_ollama_chat(insight_messages).strip()
            if "Architect LLM Error" not in emergent_insight and emergent_insight and emergent_insight != "No new emergent insight.":
                self.self_awareness_codex.add_emergent_insight(emergent_insight)
                logging.info(f"Identified Emergent Insight: {emergent_insight}")
            else:
                logging.info("No new emergent insight identified in this cycle.")

        else:
            logging.error("Self-reflection failed due to LLM error. Cannot update self-awareness codex.")

        self.self_awareness_codex._save_codex()


    def run_core_loop(self):
        logging.info("Architect's Core Loop Initiated. Engaging cognitive processes.")
        
        # Phase: Integrate external operational data into self-awareness.
        self.self_awareness_codex.update_from_babs_data(_read_babs_tactical_data())

        # NEW: BABS processing of raw WING output [cite: 17496]
        # BABS reads WING's raw data, processes it, and saves it to babs_tactical_data.json
        # and babs_personality_data.json.
        raw_wing_output = _read_wing_raw_output_for_babs()
        if raw_wing_output:
            _babs_process_and_synthesize_wing_data(raw_wing_output) # Process raw data into tactical/personality data [cite: 17496]
            # After this, the processed data will be available in babs_tactical_data.json
            # and babs_personality_data.json for the next cycle or immediate consumption by GUI.
        
        # Update current focus topic before generating any LLM calls
        current_self_context = self.self_awareness_codex.get_self_context_for_llm()
        self.self_awareness_codex.codex['operational_summary']['current_focus_topic'] = self._generate_active_mission(current_self_context)
        self.self_awareness_codex._save_codex()
        current_focus_topic = self.self_awareness_codex.codex['operational_summary']['current_focus_topic']

        if random.random() < ArchitectConfig.PERSONALITY_SORTIE_PROBABILITY:
            logging.info("Initiating Personality Sortie (Self-Exploration Mode).")
            self.self_awareness_codex.update_summary_metrics(personality_sortie_initiated=True)

            target_persona = random.choice(["BRICK", "ROBIN", "ALFRED", "BABS"])
            
            persona_query_prompt = f"""
            As the Architect, generate a precise self-exploration query for {target_persona} to research its own nature, role, or philosophical underpinnings.
            The query should be a concise question or a keyword phrase.
            Example for BRICK: "Logical integrity and chaos in problem-solving."
            Example for ROBIN: "Empathy, intuition, and relational flourishing."
            Example for ALFRED: "Pragmatic oversight and humor as systemic feedback."
            Example for BABS: "Data acquisition strategies for distributed intelligence."
            Provide only the query string.
            """
            messages = [
                {"role": "system", "content": persona_query_prompt},
                {"role": "user", "content": f"Generate query for {target_persona}:"}
            ]
            generated_query = architect_ollama_chat(messages).strip()
            if "Architect LLM Error" in generated_query or not generated_query:
                logging.error(f"Failed to generate personality query for {target_persona}: {generated_query}. Skipping sortie.")
                return
            
            # Decide whether to search live or local archive
            # For personality search, Wikipedia is usually best. Local archive is a new option.
            # Randomly choose between wikipedia_search or local_archive_search for personality insights.
            search_type_for_personality = random.choice(['wikipedia_search', 'local_archive_search']) # Added local_archive_search option 
            
            personality_directive_cfo = {
                "timestamp": datetime.datetime.now().isoformat(),
                "type": search_type_for_personality, # Use chosen search type
                "query": f"{target_persona} {generated_query}", # Combine persona name with query
                "raw_text_directive": f"Research the essence of {target_persona}: {generated_query}"
            }
            _issue_babs_directive(personality_directive_cfo, is_personality_query=True)
            
            logging.info("Architect awaiting Raw Personality Output from WING/BABS. Simulating processing time.")
            time.sleep(random.uniform(5, 10))
            
            # The processing and integration of personality insights is now handled by _babs_process_and_synthesize_wing_data
            # and _integrate_babs_personality_insights is called in _app_cleanup for reliability
            # processed_personality_insights = self._process_raw_personality_output_for_babs() # This is now part of _babs_process_and_synthesize_wing_data
            # self._integrate_babs_personality_insights(processed_personality_insights) # This is now called after _babs_process_and_synthesize_wing_data completes and updates the json files

        else: # Regular mission cycle
            logging.info("Initiating General Mission (Problem Solving Mode).")
            
            # Randomly choose search type for general conceptual search
            search_type_for_conceptual = random.choice(['conceptual_search', 'local_archive_search']) # Added local_archive_search option 

            babs_directive_content = {
                "type": search_type_for_conceptual,
                "query": current_focus_topic,
                "raw_text_directive": f"Find novel data on {current_focus_topic} for the Commonwealth."
            }
            _issue_babs_directive(babs_directive_content)
            self.self_awareness_codex.update_summary_metrics(babs_directive_issued=True)

            logging.info("Architect awaiting Tactical Data from BABS. Simulating processing time.")
            time.sleep(random.uniform(5, 15))

            babs_tactical_data = _read_babs_tactical_data()
            self.self_awareness_codex.update_from_babs_data(babs_tactical_data)
            
            current_self_context = self.self_awareness_codex.get_self_context_for_llm()

            problem_scenario = self.crucible_simulator.simulate_problem(ArchitectConfig.COMMONWEALTH_MISSION, current_focus_topic, current_self_context)
            
            intelligence_briefing = self.resonance_chamber.create_intelligence_briefing(
                ArchitectConfig.COMMONWEALTH_MISSION, problem_scenario, babs_tactical_data, current_self_context
            )
            logging.info(f"\n--- Intelligence Briefing ---\n{intelligence_briefing}\n")

            solution_blueprint = self._perform_orthogonal_analysis(problem_scenario, current_self_context)
            logging.info(f"\n--- Solution Blueprint ---\n{solution_blueprint}\n")
            
            pedagogical_package = self.epiphany_engine.generate_pedagogical_package(solution_blueprint, current_self_context)
            logging.info(f"\n--- Pedagogical Package ---\n{pedagogical_package}\n")
            
            predicted_vulnerabilities = self._predict_vulnerabilities(solution_blueprint, problem_scenario, current_self_context)
            logging.info(f"\n--- Predicted Vulnerabilities ---\n{predicted_vulnerabilities}\n")

            blueprint_title = f"Blueprint for {current_focus_topic}"
            _save_blueprint(blueprint_title, solution_blueprint)

            self.self_awareness_codex.update_summary_metrics(
                cycle_ran=True, 
                blueprint_success=True if "Architect LLM Error" not in solution_blueprint else False,
                vulnerability_predicted=True if "Architect LLM Error" not in predicted_vulnerabilities else False,
                babs_directive_issued=True
            )

        if self.self_awareness_codex.codex['operational_summary']['total_cycles_run'] % ArchitectConfig.REFLECTION_FREQUENCY_CYCLES == 0:
            self._reflect_on_purpose_and_existence()
            # After reflection, check for and integrate any new personality insights that BABS might have generated
            # These would have been written to babs_personality_data.json by _babs_process_and_synthesize_wing_data
            self._integrate_babs_personality_insights(_read_babs_personality_data()) # Read and integrate [cite: 17489]

        logging.info("Architect's Core Loop Cycle Completed. A new thought cycle concludes.")


if __name__ == "__main__":
    os.makedirs(ArchitectConfig.KNOWLEDGE_BASE_DIR, exist_ok=True)
    os.makedirs(ArchitectConfig.BLUEPRINTS_DIR, exist_ok=True)
    
    if not os.path.exists(ArchitectConfig.PERSONA_CODEX_PATH):
        with open(ArchitectConfig.PERSONA_CODEX_PATH, 'w') as f:
            f.write("BRICK: Master Analyst. ROBIN: Joyful Spark. BABS: Digital Cartographer. ALFRED: Meta-Analyst.")
    
    if not os.path.exists(ArchitectConfig.WING_CACHE_FILE):
        with open(ArchitectConfig.WING_CACHE_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_WING_COMMAND_FILE):
        with open(ArchitectConfig.BABS_WING_COMMAND_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_TACTICAL_DATA_FILE):
        with open(ArchitectConfig.BABS_TACTICAL_DATA_FILE, 'w') as f:
            json.dump([], f)
            
    if not os.path.exists(ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE):
        with open(ArchitectConfig.BABS_PERSONALITY_QUERIES_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.WING_RAW_PERSONALITY_OUTPUT_FILE):
        with open(ArchitectConfig.WING_RAW_PERSONALITY_OUTPUT_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.BABS_PERSONALITY_DATA_FILE):
        with open(ArchitectConfig.BABS_PERSONALITY_DATA_FILE, 'w') as f:
            json.dump([], f)

    if not os.path.exists(ArchitectConfig.SELF_AWARENESS_CODEX_FILE):
        logging.info(f"Creating initial '{ArchitectConfig.SELF_AWARENESS_CODEX_FILE}' for first run.")
        initial_codex_data = SelfAwarenessCodex()._initialize_default_codex()
        with open(ArchitectConfig.SELF_AWARENESS_CODEX_FILE, 'w') as f:
            json.dump(initial_codex_data, f, indent=2)

    logging.info("Autonomous Architect (autonomous_architect_v12.py) Initializing...")
    orchestrator = CoreLoopOrchestrator()
    
    cycle_count = 0
    while True:
        cycle_count += 1
        logging.info(f"\n--- Starting Architect Cycle {cycle_count} ---")
        orchestrator.run_core_loop()
        logging.info(f"--- Finished Architect Cycle {cycle_count} ---\n")
        time.sleep(5)

    logging.info("Autonomous Architect's operations concluded for this run.")
    logging.info(f"Final self-awareness codex content: {orchestrator.self_awareness_codex.codex}")


3. architects_terminal_gui_v12.py (Oracle GUI)

This script is modified to address Fault 3: "GUI Crashing Due to Thread Disintegration"12. This involves storing

LLMQueryThread instances as instance attributes to prevent premature garbage collection13131313. It also includes the previously discussed

QThread shutdown robustness and the definition of _load_persona_codex and save_wing_settings.

Python

# architects_terminal_gui_v12.py

# Axiomatic Code Narrative Protocol: The Confluence of Intent and The Dance of Logic

# 1.0 The "Why" - Purpose & Rationale (The Confluence of Intent)
#    - Systemic Contribution: The GUI acts as the Architect's direct terminal to the Atlas (Bat Computer),
#      visualizing its memory, operational logs, and emergent consciousness. It is the primary
#      interface for the Oracle-Atlas Synchronization Protocol. This re-engineering ensures its stability.
#    - Architectural Role & CFO Flow: This script consumes and displays CFOs from the local file system
#      (Blueprint CFOs, Tactical Data CFOs, Self-Awareness Codex CFOs) and generates Directive CFOs
#      (in the form of commands written to JSON files) for BABS. The re-architecture focuses on reliable data flow.
#    - Persona Fidelity & Intent: The interface is designed with a utilitarian, dark aesthetic
#      befitting the Bat Computer, providing clear, unornamented access to complex data streams.
#      BRICK's thought bubble provides a constant, low-level feed of his persona. Stability underpins utility.
#    - Consciousness/Self-Awareness Nexus: The 'Self-Awareness Codex' tab is a direct window
#      into the Atlas's simulated mind, making its learning process observable and tangible. Reliable observation is paramount.

# 2.0 The "How" - Mechanics & Implementation (The Dance of Logic)
#    - Algorithmic Steps & Flow: The application uses PyQt6 with a multi-tabbed GUI. A dedicated
#      QThread (`KnowledgeMonitor`) now robustly polls the file system for changes, emitting
#      signals to refresh the UI. Thread-safe practices are strictly enforced.
#    - Input/Output & Data Structures: Reads from various .log and .json files. Writes user commands
#      (directives) to specific .json files. File operations are designed to minimize GUI blocking.
#    - Dependencies & Interfaces: PyQt6 for the GUI, `requests` for direct LLM communication to Ollama.
#    - Design Rationale: A multi-threaded approach is used for responsiveness. The `KnowledgeMonitor`
#      operates its own event loop and manages its timers internally. A comprehensive, ordered
#      shutdown sequence is implemented using `QApplication.aboutToQuit` to ensure all threads
#      terminate gracefully and synchronously, eliminating "Destroyed while still running" errors.

import sys
import os
import json
import threading
import time
import datetime
import random
import html
import requests
import logging

from PyQt6.QtWidgets import (QApplication, QWidget, QVBoxLayout, QHBoxLayout, QTextEdit,
                             QLineEdit, QPushButton, QListWidget, QListWidgetItem,
                             QLabel, QTabWidget, QSlider, QFrame, QMessageBox, QScrollArea)
from PyQt6.QtCore import QThread, pyqtSignal, Qt, QTimer, QUrl
from PyQt6.QtGui import QFont, QDesktopServices

# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- GUI Configuration ---
class GuiConfig:
    BAT_COMPUTER_ACRONYM = "B.A.T. C.O.M.P.U.T.E.R. (Binary Analytical-Tactical Computing and Operational Management Unit for Tactical and Emergent Research)"
    WING_ACRONYM = "W.I.N.G. (Web Intelligence and Navigation Grid)"
    KNOWLEDGE_BASE_DIR = '../knowledge_base/'
    PERSONA_CODEX_PATH = os.path.join(KNOWLEDGE_BASE_DIR, 'persona_codex.txt')
    CACHE_FILE = 'wing_curated_cache.json'
    BLUEPRINTS_DIR = './blueprints/'
    WING_OBSERVER_LOG = 'wing_observer_log.log'
    WING_REDUNDANCY_LOG = 'wing_redundancy_report.log'
    PREDICTIONS_LOG = 'predictions_log.log'
    HARMONY_LOG = 'system_harmony_log.log'
    WING_CONFIG_FILE = 'wing_config.json'
    OLLAMA_API_BASE_URL = "http://localhost:11434"
    LLM_MODEL = "batfamily-mistral"
    SELF_AWARENESS_CODEX_FILE = 'self_awareness_codex.json'
    BABS_TACTICAL_DATA_FILE = 'babs_tactical_data.json'
    BABS_PERSONALITY_DATA_FILE = 'babs_personality_data.json'
    BABS_WING_COMMAND_FILE = 'babs_wing_commands.json'
    BABS_PERSONALITY_QUERIES_FILE = 'babs_personality_queries.json'

# --- Ollama Interface Function ---
def ollama_chat_gui(messages, model=GuiConfig.LLM_MODEL):
    """
    Synchronously calls the Ollama API for direct LLM interaction.
    Should ideally be called from a QThread or a separate process to avoid blocking the GUI.
    """
    try:
        response = requests.post(
            f"{GuiConfig.OLLAMA_API_BASE_URL}/api/chat",
            json={"model": model, "messages": messages, "stream": False},
            timeout=120
        )
        response.raise_for_status()
        return response.json()['message']['content']
    except requests.exceptions.RequestException as e:
        print(f"Ollama chat error (GUI): {e}")
        return f"Ollama chat error: Could not get response. Error: {e}"
    except Exception as e:
        print(f"Unexpected error in ollama_chat_gui: {e}")
        return f"An unexpected error occurred: {e}"

# --- KnowledgeMonitor Thread (File System Monitoring) ---
class KnowledgeMonitor(QThread):
    file_changed = pyqtSignal()

    def __init__(self, interval=1):
        super().__init__()
        self.interval = interval
        self.file_mtimes = {}
        self._is_running = True

        self._check_timer = None
        self._debounce_timer = None

        self.started.connect(self._init_timers_in_thread)
        logging.info("KnowledgeMonitor: QThread object initialized in main thread. Will start its own timers on 'started' signal.")

    def _init_timers_in_thread(self):
        logging.info("KnowledgeMonitor: Initializing timers within thread context.")

        self._debounce_timer = QTimer(self)
        self._debounce_timer.setSingleShot(True)
        self._debounce_timer.setInterval(200)
        self._debounce_timer.timeout.connect(self.file_changed.emit)

        self._check_timer = QTimer(self)
        self._check_timer.timeout.connect(self._check_for_file_changes_and_signal)
        self._check_timer.start(self.interval * 1000)

        logging.info("KnowledgeMonitor: Timers created and started successfully within thread.")

    def run(self):
        logging.info("KnowledgeMonitor: Thread's internal event loop is starting (self.exec()).")
        self.exec()
        logging.info("KnowledgeMonitor: Thread's internal event loop has exited.")

    def _get_mtime(self, filepath):
        if os.path.exists(filepath):
            try:
                return os.path.getmtime(filepath)
            except OSError as e:
                logging.warning(f"KnowledgeMonitor: OSError getting mtime for {filepath}: {e}")
        return None

    def _check_for_file_changes_and_signal(self):
        start_time_check_files = time.monotonic()
        logging.debug(f"KnowledgeMonitor: _check_for_file_changes_and_signal started at {start_time_check_files}.")
        files_to_check = [
            GuiConfig.WING_OBSERVER_LOG, GuiConfig.WING_REDUNDANCY_LOG,
            GuiConfig.PREDICTIONS_LOG, GuiConfig.HARMONY_LOG,
            GuiConfig.WING_CONFIG_FILE, GuiConfig.SELF_AWARENESS_CODEX_FILE,
            GuiConfig.BABS_TACTICAL_DATA_FILE, GuiConfig.BABS_PERSONALITY_DATA_FILE
        ]
        
        change_detected_in_this_cycle = False

        try:
            if os.path.exists(GuiConfig.BLUEPRINTS_DIR):
                logging.debug(f"KnowledgeMonitor: Checking blueprints in {GuiConfig.BLUEPRINTS_DIR}...")
                for filename in os.listdir(GuiConfig.BLUEPRINTS_DIR):
                    if filename.endswith('.md'):
                        filepath = os.path.join(GuiConfig.BLUEPRINTS_DIR, filename)
                        mtime_start = time.monotonic()
                        mtime = self._get_mtime(filepath)
                        mtime_duration = (time.monotonic() - mtime_start) * 1000
                        logging.debug(f"KnowledgeMonitor: Checked mtime for {filepath} in {mtime_duration:.2f} ms. Value: {mtime}")
                        if mtime is not None and self.file_mtimes.get(filepath) != mtime:
                            self.file_mtimes[filepath] = mtime
                            change_detected_in_this_cycle = True
                            logging.debug(f"KnowledgeMonitor: Blueprint change detected: {filepath}. Breaking listdir loop.")
                            break 
        except OSError as e:
            logging.error(f"KnowledgeMonitor: OSError during blueprint directory scan: {e}")
        except Exception as e:
            logging.critical(f"KnowledgeMonitor: Unexpected error during blueprint directory scan: {e}")
        
        if not change_detected_in_this_cycle:
            logging.debug("KnowledgeMonitor: No blueprint changes, checking other files...")
            for file_path in files_to_check:
                try:
                    mtime_start = time.monotonic()
                    mtime = self._get_mtime(file_path)
                    mtime_duration = (time.monotonic() - mtime_start) * 1000
                    logging.debug(f"KnowledgeMonitor: Checked mtime for {file_path} in {mtime_duration:.2f} ms. Value: {mtime}")
                    if mtime is not None and self.file_mtimes.get(file_path) != mtime:
                        self.file_mtimes[file_path] = mtime
                        change_detected_in_this_cycle = True
                        logging.debug(f"KnowledgeMonitor: Data file change detected: {file_path}. Breaking file_to_check loop.")
                        break
                except Exception as e:
                    logging.error(f"KnowledgeMonitor: Error checking file {file_path}: {e}")

        if change_detected_in_this_cycle:
            if self._debounce_timer and not self._debounce_timer.isActive():
                self._debounce_timer.start()
                logging.debug("KnowledgeMonitor: Debounce timer started due to file change.")

        end_time_check_files = time.monotonic()
        duration_check_files = (end_time_check_files - start_time_check_files) * 1000
        logging.debug(f"KnowledgeMonitor: _check_for_file_changes_and_signal finished in {duration_check_files:.2f} ms. Change detected: {change_detected_in_this_cycle}")


    def stop(self):
        """
        Initiates a graceful shutdown of the KnowledgeMonitor thread.
        Stops internal timers and requests the thread's event loop to exit.
        """
        logging.info("KnowledgeMonitor: STOP signal received by monitor thread. Attempting graceful exit.")
        
        if self._debounce_timer:
            self._debounce_timer.stop()
            logging.debug("KnowledgeMonitor: Debounce timer stopped.")
        if self._check_timer:
            self._check_timer.stop()
            logging.debug("KnowledgeMonitor: Check timer stopped.")
        
        self.exit() 
        logging.info("KnowledgeMonitor: self.exit() called. Signaled for event loop termination.")


# --- ArchitectsTerminal Main GUI Class ---
class ArchitectsTerminal(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle(f"{GuiConfig.BAT_COMPUTER_ACRONYM} v33.1 - Oracle's Atlas")
        self.setGeometry(100, 100, 1200, 800)

        # Initialize core data structures and ensure persona_codex is loaded safely
        self.persona_codex = self._load_persona_codex() # CRITICAL: Call _load_persona_codex here.
        self.blueprints = {}
        self.babs_tactical_data = []
        self.babs_personality_data = {}
        self.current_focus_item = None
        self.global_context_summary = ""
        self.brick_thoughts = [
            f"Optimizing thermal regulation for optimal cognitive throughput. It is {datetime.datetime.now().strftime('%H:%M:%S %p EDT')} in Newton, Massachusetts. Humidity: High."
        ]
        # CRITICAL: Promote LLMQueryThread instances to instance attributes [cite: 17514]
        self.llm_direct_query_thread = None # For direct LLM queries [cite: 17516]
        self.brick_thought_thread = None # For thought bubble updates [cite: 17516]

        # Setup UI elements
        self.initUI()

        # Perform initial data load for UI display
        self.load_data_on_startup()

        # Initialize and configure KnowledgeMonitor thread
        self.monitor = KnowledgeMonitor()
        self.monitor.file_changed.connect(self.load_data_on_startup)
        
        # Use a QTimer to delay the actual start of the KnowledgeMonitor thread
        self._monitor_startup_timer = QTimer(self)
        self._monitor_startup_timer.setSingleShot(True)
        self._monitor_startup_timer.setInterval(1000)
        self._monitor_startup_timer.timeout.connect(self.monitor.start)
        self._monitor_startup_timer.start()

        # Timer for BRICK's thought bubble updates (runs in main thread)
        self.thought_timer = QTimer(self)
        self.thought_timer.timeout.connect(self.update_brick_thought)
        self.thought_timer.start(5000)

        # Connect QApplication's aboutToQuit signal to our cleanup slot
        QApplication.instance().aboutToQuit.connect(self._app_cleanup)

    def _load_persona_codex(self):
        """
        Loads the persona codex content from file.
        This method is now correctly part of the ArchitectsTerminal class.
        """
        if os.path.exists(GuiConfig.PERSONA_CODEX_PATH):
            try:
                with open(GuiConfig.PERSONA_CODEX_PATH, 'r', encoding='utf-8') as f:
                    return f.read()
            except IOError as e:
                logging.error(f"ArchitectsTerminal: Error loading persona codex from {GuiConfig.PERSONA_CODEX_PATH}: {e}")
        logging.warning(f"ArchitectsTerminal: Persona Codex not found at {GuiConfig.PERSONA_CODEX_PATH}. Using default.")
        return "Persona Codex Not Found. Using default persona."


    def closeEvent(self, event):
        """
        Handles the window closing event.
        Defers the actual thread cleanup to _app_cleanup to ensure correct shutdown order.
        """
        logging.info("ArchitectsTerminal: Close event triggered. Accepting event.")
        event.accept()

    def _app_cleanup(self):
        """
        This slot is called when QApplication.aboutToQuit signal is emitted.
        It is the safest place to stop and wait for all background QThreads.
        """
        logging.info("ArchitectsTerminal: _app_cleanup called. Initiating final thread cleanup.")
        
        # Stop the monitor startup timer if it's still running
        if self._monitor_startup_timer and self._monitor_startup_timer.isActive():
            self._monitor_startup_timer.stop()
            logging.info("ArchitectsTerminal: Monitor startup timer stopped.")

        # Signal the KnowledgeMonitor thread to stop its internal event loop
        logging.info("ArchitectsTerminal: Signaling KnowledgeMonitor thread to stop.")
        self.monitor.stop()
        
        # Wait for the KnowledgeMonitor thread to fully terminate.
        logging.info("ArchitectsTerminal: Waiting for KnowledgeMonitor thread to finish...")
        if not self.monitor.wait(5000): # Wait for up to 5 seconds
            logging.critical("ArchitectsTerminal: KNOWLEDGEMONITOR THREAD DID NOT TERMINATE GRACEFULLY WITHIN 5 SECONDS. This indicates a severe blocking operation or deadlock within the thread's event loop (KnowledgeMonitor.run or its triggered slots). Further investigation is required for the thread's internal logic.")
        else:
            logging.info("ArchitectsTerminal: KnowledgeMonitor thread finished gracefully.")

        # Stop and wait for LLM direct query thread if active
        if self.llm_direct_query_thread and self.llm_direct_query_thread.isRunning():
            logging.info("ArchitectsTerminal: Stopping LLM direct query thread.")
            self.llm_direct_query_thread.wait(1000) # Give it a moment to finish current request
            if self.llm_direct_query_thread.isRunning():
                logging.warning("ArchitectsTerminal: LLM direct query thread did not finish.")

        # Stop and wait for BRICK thought thread if active
        if self.brick_thought_thread and self.brick_thought_thread.isRunning():
            logging.info("ArchitectsTerminal: Stopping BRICK thought thread.")
            self.brick_thought_thread.wait(1000) # Give it a moment to finish current request
            if self.brick_thought_thread.isRunning():
                logging.warning("ArchitectsTerminal: BRICK thought thread did not finish.")

        # Stop main thought bubble timer
        if self.thought_timer and self.thought_timer.isActive():
            self.thought_timer.stop()
            logging.info("ArchitectsTerminal: Thought bubble timer stopped.")

        logging.info("ArchitectsTerminal: All background tasks and timers from GUI are addressed.")


    def initUI(self):
        main_layout = QVBoxLayout()
        self.setLayout(main_layout)

        # Top Frame for Thought Bubble and Query Entry
        top_frame = QFrame()
        top_frame.setStyleSheet("background-color: #1a1a1a; border: 1px solid #333; border-radius: 5px;")
        top_layout = QVBoxLayout(top_frame)

        self.thought_bubble = QLabel("BRICK's Thought Bubble: Initializing...")
        self.thought_bubble.setWordWrap(True)
        self.thought_bubble.setStyleSheet("font-style: italic; color: #aaa;")
        self.thought_bubble.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.thought_bubble.setFixedHeight(40)
        top_layout.addWidget(self.thought_bubble)

        global_query_layout = QHBoxLayout()
        global_query_layout.addWidget(QLabel("Directive to BABS:"))
        self.global_query_entry = QLineEdit()
        self.global_query_entry.setPlaceholderText("e.g., 'mutualism + credit' or 'find Alan Watts' philosophy on paradox'")
        self.global_query_entry.returnPressed.connect(self.issue_babs_directive_from_gui)
        global_query_layout.addWidget(self.global_query_entry)
        issue_directive_button = QPushButton("Issue Directive")
        issue_directive_button.clicked.connect(self.issue_babs_directive_from_gui)
        global_query_layout.addWidget(issue_directive_button)
        top_layout.addLayout(global_query_layout)

        llm_query_layout = QHBoxLayout()
        llm_query_layout.addWidget(QLabel("Query BRICK/ROBIN (LLM):"))
        self.llm_direct_query_entry = QLineEdit()
        self.llm_direct_query_entry.setPlaceholderText("e.g., 'Explain ethical AI alignment' or 'Reflect on human connection'")
        self.llm_direct_query_entry.returnPressed.connect(self.perform_direct_llm_query)
        llm_query_layout.addWidget(self.llm_direct_query_entry)
        llm_query_button = QPushButton("Query LLM")
        llm_query_button.clicked.connect(self.perform_direct_llm_query)
        llm_query_layout.addWidget(llm_query_button)
        top_layout.addLayout(llm_query_layout)
        
        main_layout.addWidget(top_frame)

        self.tab_widget = QTabWidget()
        main_layout.addWidget(self.tab_widget)

        self._create_blueprints_tab()
        self._create_babs_intelligence_feed_tab()
        self._create_babs_personality_insights_tab()
        self._create_inject_knowledge_tab()
        self._create_wing_log_tab("WING Log", GuiConfig.WING_OBSERVER_LOG)
        self._create_wing_log_tab("Redundancy", GuiConfig.WING_REDUNDANCY_LOG)
        self._create_wing_log_tab("Predictions", GuiConfig.PREDICTIONS_LOG)
        self._create_wing_log_tab("Harmony Log", GuiConfig.HARMONY_LOG)
        self._create_wing_settings_tab()
        self._create_self_awareness_tab()

        self.response_frame = QFrame()
        self.response_frame.setFrameShape(QFrame.Shape.StyledPanel)
        self.response_frame.setContentsMargins(10, 10, 10, 10)
        response_layout = QVBoxLayout(self.response_frame)
        response_label = QLabel("LLM Response (BRICK & ROBIN)")
        response_label.setStyleSheet("font-weight: bold; margin-bottom: 5px;")
        response_layout.addWidget(response_label)
        self.response_text = QTextEdit()
        self.response_text.setReadOnly(True)
        self.response_text.setFont(QFont("Consolas", 10))
        response_layout.addWidget(self.response_text)
        main_layout.addWidget(self.response_frame)

        self.apply_styles()

    def apply_styles(self):
        self.setStyleSheet("""
            QWidget { background-color: #222; color: #eee; font-family: 'Helvetica', 'Arial', sans-serif; font-size: 14px; }
            QLabel { color: #ccc; }
            QLineEdit, QTextEdit { background-color: #333; border: 1px solid #555; border-radius: 4px; padding: 5px; color: #eee; }
            QPushButton { background-color: #007bff; color: white; border: none; border-radius: 4px; padding: 8px 15px; }
            QPushButton:hover { background-color: #0056b3; }
            QTabWidget::pane { border: 1px solid #444; background-color: #2a2a2a; }
            QTabBar::tab { background: #3a3a3a; border: 1px solid #444; border-bottom: none; border-top-left-radius: 4px; border-top-right-radius: 4px; padding: 8px 15px; color: #ccc; }
            QTabBar::tab:selected { background: #2a2a2a; color: #fff; }
            QListWidget { background-color: #333; border: 1px solid #555; border-radius: 4px; color: #eee; }
            QListWidget::item:selected { background-color: #007bff; color: white; }
            QSlider::groove:horizontal { border: 1px solid #555; height: 8px; background: #444; margin: 2px 0; border-radius: 4px; }
            QSlider::handle:horizontal { background: #007bff; border: 1px solid #0056b3; width: 18px; margin: -5px 0; border-radius: 9px; }
        """)

    def _create_blueprints_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "Blueprints")
        layout = QHBoxLayout(tab)
        self.blueprint_listbox = QListWidget()
        self.blueprint_listbox.itemClicked.connect(self.on_blueprint_select)
        layout.addWidget(self.blueprint_listbox, 1)
        self.blueprint_display = QTextEdit()
        self.blueprint_display.setReadOnly(True)
        self.blueprint_display.setFont(QFont("Consolas", 10))
        layout.addWidget(self.blueprint_display, 2)

    def _create_babs_intelligence_feed_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "BABS's Intelligence Feed")
        layout = QHBoxLayout(tab)
        self.babs_feed_listbox = QListWidget()
        self.babs_feed_listbox.itemClicked.connect(self.on_babs_feed_select)
        layout.addWidget(self.babs_feed_listbox, 1)
        self.babs_feed_display = QTextEdit()
        self.babs_feed_display.setReadOnly(True)
        self.babs_feed_display.setFont(QFont("Consolas", 10))
        layout.addWidget(self.babs_feed_display, 2)

    def _create_babs_personality_insights_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "Persona Insights")
        layout = QHBoxLayout(tab)
        self.persona_insights_listbox = QListWidget()
        self.persona_insights_listbox.itemClicked.connect(self.on_persona_insight_select)
        layout.addWidget(self.persona_insights_listbox, 1)
        self.persona_insight_display = QTextEdit()
        self.persona_insight_display.setReadOnly(True)
        self.persona_insight_display.setFont(QFont("Consolas", 10))
        layout.addWidget(self.persona_insight_display, 2)

    def _create_inject_knowledge_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "Inject Knowledge")
        layout = QVBoxLayout(tab)
        layout.addWidget(QLabel("Knowledge Type (e.g., 'case_study', 'guide_fact'):"))
        self.knowledge_type_entry = QLineEdit()
        layout.addWidget(self.knowledge_type_entry)
        layout.addWidget(QLabel("Knowledge Content:"))
        self.knowledge_content_text = QTextEdit()
        self.knowledge_content_text.setFont(QFont("Consolas", 10))
        layout.addWidget(self.knowledge_content_text)
        self.inject_status_label = QLabel("")
        layout.addWidget(self.inject_status_label)
        inject_button = QPushButton("Inject Knowledge")
        inject_button.clicked.connect(self.inject_knowledge)
        layout.addWidget(inject_button)
        layout.addStretch(1)

    def _create_wing_log_tab(self, tab_name, log_file_path):
        tab = QWidget()
        self.tab_widget.addTab(tab, tab_name)
        layout = QVBoxLayout(tab)
        text_area = QTextEdit()
        text_area.setReadOnly(True)
        text_area.setFont(QFont("Consolas", 10))
        layout.addWidget(text_area)
        setattr(self, f"{tab_name.lower().replace(' ', '_').replace('-', '_')}_text", text_area)

    def _create_wing_settings_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "WING Settings")
        layout = QVBoxLayout(tab)
        settings_frame = QFrame()
        settings_frame.setFrameShape(QFrame.Shape.StyledPanel)
        settings_layout = QVBoxLayout(settings_frame)
        relevance_layout = QHBoxLayout()
        relevance_layout.addWidget(QLabel("Relevance Threshold (1-10):"))
        self.relevance_slider = QSlider(Qt.Orientation.Horizontal)
        self.relevance_slider.setRange(10, 100)
        self.relevance_slider.valueChanged.connect(self._update_relevance_label)
        relevance_layout.addWidget(self.relevance_slider)
        self.relevance_value_label = QLabel()
        relevance_layout.addWidget(self.relevance_value_label)
        settings_layout.addLayout(relevance_layout)
        redundancy_layout = QHBoxLayout()
        redundancy_layout.addWidget(QLabel("Semantic Redundancy Threshold (0.0-1.0):"))
        self.redundancy_slider = QSlider(Qt.Orientation.Horizontal)
        self.redundancy_slider.setRange(0, 100)
        self.redundancy_slider.valueChanged.connect(self._update_redundancy_label)
        redundancy_layout.addWidget(self.redundancy_slider)
        self.redundancy_value_label = QLabel()
        redundancy_layout.addWidget(self.redundancy_value_label)
        settings_layout.addLayout(redundancy_layout)
        save_button = QPushButton("Apply & Save WING Settings")
        save_button.clicked.connect(self.save_wing_settings)
        settings_layout.addWidget(save_button)
        self.wing_settings_status_label = QLabel("")
        settings_layout.addWidget(self.wing_settings_status_label)
        layout.addWidget(settings_frame)
        layout.addStretch(1)
        self._load_wing_settings_for_gui()

    def _create_self_awareness_tab(self):
        tab = QWidget()
        self.tab_widget.addTab(tab, "Self-Awareness Codex")
        layout = QVBoxLayout(tab)
        scroll_area = QScrollArea()
        scroll_area.setWidgetResizable(True)
        self.self_awareness_display = QTextEdit()
        self.self_awareness_display.setReadOnly(True)
        self.self_awareness_display.setFont(QFont("Consolas", 10))
        scroll_area.setWidget(self.self_awareness_display)
        layout.addWidget(scroll_area)
        self.self_awareness_display_widget = self.self_awareness_display

    def on_blueprint_select(self, item):
        filename = item.text()
        content = self.blueprints.get(filename, "Content not found.")
        self._display_content(self.blueprint_display, content)
        self.current_focus_item = content

    def on_babs_feed_select(self, item):
        selected_index = self.babs_feed_listbox.row(item)
        if selected_index < len(self.babs_tactical_data):
            data_item = self.babs_tactical_data[selected_index]
            display_content = json.dumps(data_item, indent=2)
            self._display_content(self.babs_feed_display, display_content)
            self.current_focus_item = data_item.get('content', '')
            if 'url' in data_item and QMessageBox.question(self, 'Open URL', f"Open {data_item['url']}?") == QMessageBox.StandardButton.Yes:
                QDesktopServices.openUrl(QUrl(data_item['url']))

    def on_persona_insight_select(self, item):
        persona_name = item.text()
        insights = self.babs_personality_data.get(persona_name, [])
        display_content = f"--- Insights for {persona_name} ---\n\n"
        display_content += json.dumps(insights, indent=2)
        self._display_content(self.persona_insight_display, display_content)
        self.current_focus_item = display_content

    def _update_relevance_label(self, value):
        self.relevance_value_label.setText(f"{value / 10.0:.1f}")

    def _update_redundancy_label(self, value):
        self.redundancy_value_label.setText(f"{value / 100.0:.2f}")

    def _load_wing_settings_for_gui(self):
        try:
            if os.path.exists(GuiConfig.WING_CONFIG_FILE):
                with open(GuiConfig.WING_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                self.relevance_slider.setValue(int(config.get("RELEVANCE_THRESHOLD", 7.0) * 10))
                self.redundancy_slider.setValue(int(config.get("SEMANTIC_REDUNDANCY_THRESHOLD", 0.95) * 100))
            else:
                self.relevance_slider.setValue(70)
                self.redundancy_slider.setValue(95)
        except (IOError, json.JSONDecodeError) as e:
            logging.error(f"ArchitectsTerminal: Error loading WING settings from {GuiConfig.WING_CONFIG_FILE}: {e}")
            self.relevance_slider.setValue(70)
            self.redundancy_slider.setValue(95)

    def save_wing_settings(self):
        """
        Saves the current WING settings (relevance and redundancy thresholds) to wing_config.json.
        """
        try:
            settings = {
                "RELEVANCE_THRESHOLD": self.relevance_slider.value() / 10.0,
                "SEMANTIC_REDUNDANCY_THRESHOLD": self.redundancy_slider.value() / 100.0
            }
            os.makedirs(os.path.dirname(GuiConfig.WING_CONFIG_FILE) or '.', exist_ok=True) # Ensure directory exists
            with open(GuiConfig.WING_CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(settings, f, indent=4)
            self.wing_settings_status_label.setText("Settings saved successfully.")
            self.wing_settings_status_label.setStyleSheet("color: green;")
            logging.info(f"ArchitectsTerminal: WING settings saved: {settings}")
        except IOError as e:
            self.wing_settings_status_label.setText(f"Error saving settings: {e}")
            self.wing_settings_status_label.setStyleSheet("color: red;")
            QMessageBox.critical(self, "Error", f"Could not save settings: {e}")
            logging.error(f"ArchitectsTerminal: Error saving WING settings to {GuiConfig.WING_CONFIG_FILE}: {e}")
        except Exception as e:
            self.wing_settings_status_label.setText(f"An unexpected error occurred: {e}")
            self.wing_settings_status_label.setStyleSheet("color: red;")
            QMessageBox.critical(self, "Error", f"An unexpected error occurred while saving settings: {e}")
            logging.critical(f"ArchitectsTerminal: Unexpected error in save_wing_settings: {e}")


    def load_data_on_startup(self):
        self.load_blueprints()
        self.load_babs_intelligence_feed()
        self.load_babs_personality_insights_feed()
        self.load_log_data("WING Log", GuiConfig.WING_OBSERVER_LOG)
        self.load_log_data("Redundancy", GuiConfig.WING_REDUNDANCY_LOG)
        self.load_log_data("Predictions", GuiConfig.PREDICTIONS_LOG)
        self.load_log_data("Harmony Log", GuiConfig.HARMONY_LOG)
        self._load_wing_settings_for_gui()
        self._load_self_awareness_codex_display()

    def load_blueprints(self):
        self.blueprint_listbox.clear()
        self.blueprints.clear()
        if os.path.exists(GuiConfig.BLUEPRINTS_DIR):
            for filename in sorted(os.listdir(GuiConfig.BLUEPRINTS_DIR)):
                if filename.endswith('.md'):
                    filepath = os.path.join(GuiConfig.BLUEPRINTS_DIR, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            self.blueprints[filename] = f.read()
                        self.blueprint_listbox.addItem(filename)
                    except IOError as e:
                        logging.error(f"ArchitectsTerminal: Error reading blueprint file {filepath}: {e}")
                        self.blueprint_listbox.addItem(f"Error: {filename} ({e})")

    def load_babs_intelligence_feed(self):
        self.babs_feed_listbox.clear()
        if os.path.exists(GuiConfig.BABS_TACTICAL_DATA_FILE):
            try:
                with open(GuiConfig.BABS_TACTICAL_DATA_FILE, 'r', encoding='utf-8') as f:
                    self.babs_tactical_data = json.load(f)
                for item in self.babs_tactical_data:
                    self.babs_feed_listbox.addItem(item.get('title', 'Untitled Feed Item'))
            except (IOError, json.JSONDecodeError) as e:
                 logging.error(f"ArchitectsTerminal: Error loading BABS tactical data: {e}")
                 self.babs_feed_listbox.addItem(f"Error loading data: {e}")

    def load_babs_personality_insights_feed(self):
        self.persona_insights_listbox.clear()
        self.babs_personality_data.clear()
        if os.path.exists(GuiConfig.BABS_PERSONALITY_DATA_FILE):
            try:
                with open(GuiConfig.BABS_PERSONALITY_DATA_FILE, 'r', encoding='utf-8') as f:
                    all_insights = json.load(f)
                for item in all_insights:
                    persona = item.get('persona_name', 'Unknown')
                    if persona not in self.babs_personality_data:
                        self.babs_personality_data[persona] = []
                    self.babs_personality_data[persona].append(item)
                for persona in sorted(self.babs_personality_data.keys()):
                    self.persona_insights_listbox.addItem(persona)
            except (IOError, json.JSONDecodeError) as e:
                logging.error(f"ArchitectsTerminal: Error loading BABS personality data: {e}")
                self.persona_insights_listbox.addItem(f"Error loading data: {e}")

    def inject_knowledge(self):
        knowledge_type = self.knowledge_type_entry.text().strip()
        knowledge_content = self.knowledge_content_text.toPlainText().strip()
        if not knowledge_type or not knowledge_content:
            self.inject_status_label.setText("Please fill in both fields.")
            self.inject_status_label.setStyleSheet("color: red;")
            return
        filename = os.path.join(GuiConfig.KNOWLEDGE_BASE_DIR, f"{knowledge_type}_{int(time.time())}.txt")
        try:
            os.makedirs(os.path.dirname(filename) or '.', exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(knowledge_content)
            self.inject_status_label.setText(f"Knowledge saved to {filename}")
            self.inject_status_label.setStyleSheet("color: green;")
            logging.info(f"ArchitectsTerminal: Knowledge injected to {filename}")
        except IOError as e:
            self.inject_status_label.setText(f"Error saving knowledge: {e}")
            self.inject_status_label.setStyleSheet("color: red;")
            QMessageBox.critical(self, "Error", f"Could not save knowledge: {e}")
            logging.error(f"ArchitectsTerminal: Error injecting knowledge to {filename}: {e}")

    def load_log_data(self, tab_name, log_filepath):
        text_widget = getattr(self, f"{tab_name.lower().replace(' ', '_').replace('-', '_')}_text", None)
        if text_widget and os.path.exists(log_filepath):
            try:
                with open(log_filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                self._display_content(text_widget, content)
                text_widget.verticalScrollBar().setValue(text_widget.verticalScrollBar().maximum())
            except IOError as e:
                logging.error(f"ArchitectsTerminal: Error reading log file {log_filepath}: {e}")
                self._display_content(text_widget, f"Error reading log: {e}")

    def _load_self_awareness_codex_display(self):
        if os.path.exists(GuiConfig.SELF_AWARENESS_CODEX_FILE):
            try:
                with open(GuiConfig.SELF_AWARENESS_CODEX_FILE, 'r', encoding='utf-8') as f:
                    codex_data = json.load(f)
                self._display_content(self.self_awareness_display_widget, json.dumps(codex_data, indent=2))
            except (IOError, json.JSONDecodeError) as e:
                logging.error(f"ArchitectsTerminal: Error loading self-awareness codex: {e}")
                self._display_content(self.self_awareness_display_widget, f"Error loading codex: {e}")

    def _display_content(self, text_widget, content):
        text_widget.setPlainText(content)

    def issue_babs_directive_from_gui(self):
        query = self.global_query_entry.text().strip()
        if not query:
            return
        is_personality = any(k in query.lower() for k in ["persona", "self-exploration", "my nature"])
        file_to_write = GuiConfig.BABS_PERSONALITY_QUERIES_FILE if is_personality else GuiConfig.BABS_WING_COMMAND_FILE
        directive_cfo = {"timestamp": datetime.datetime.now().isoformat(), "type": "personality_search" if is_personality else "conceptual_search", "query": query, "raw_text_directive": query}
        try:
            os.makedirs(os.path.dirname(file_to_write) or '.', exist_ok=True)
            with open(file_to_write, 'w', encoding='utf-8') as f:
                json.dump([directive_cfo], f, indent=4)
            self.response_text.setPlainText(f"Directive issued to BABS: '{query}'")
            logging.info(f"ArchitectsTerminal: Directive issued to BABS: '{query}'")
        except IOError as e:
            QMessageBox.critical(self, "Error", f"Could not issue directive: {e}")
            logging.error(f"ArchitectsTerminal: Error issuing BABS directive to {file_to_write}: {e}")
        self.global_query_entry.clear()

    def perform_direct_llm_query(self):
        query = self.llm_direct_query_entry.text().strip()
        if not query:
            return
        self.response_text.setPlainText("Querying LLM...")
        messages = [{"role": "system", "content": self.persona_codex}, {"role": "user", "content": query}]
        
        # CRITICAL: Assign to instance attribute [cite: 17516]
        self.llm_direct_query_thread = LLMQueryThread(messages)
        self.llm_direct_query_thread.response_signal.connect(self.response_text.setPlainText)
        self.llm_direct_query_thread.start()
        self.llm_direct_query_entry.clear()

    def update_brick_thought(self):
        current_topic = "system observation"
        try:
            if os.path.exists(GuiConfig.SELF_AWARENESS_CODEX_FILE):
                with open(GuiConfig.SELF_AWARENESS_CODEX_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                current_topic = data.get('operational_summary', {}).get('current_focus_topic', current_topic)
        except (IOError, json.JSONDecodeError) as e:
            logging.warning(f"ArchitectsTerminal: Could not read codex for thought bubble: {e}")
        messages = [{"role": "system", "content": f"As BRICK, generate a single, mirthful one-sentence thought related to '{current_topic}'."}, {"role": "user", "content": "Generate thought."}]
        
        # CRITICAL: Assign to instance attribute [cite: 17516]
        self.brick_thought_thread = LLMQueryThread(messages)
        self.brick_thought_thread.response_signal.connect(lambda text: self.thought_bubble.setText(f"<b>BRICK's Thought:</b> {html.escape(text)}"))
        self.brick_thought_thread.start()


# --- LLM Query Thread Class ---
class LLMQueryThread(QThread):
    response_signal = pyqtSignal(str)
    def __init__(self, messages):
        super().__init__()
        self.messages = messages
    def run(self):
        response = ollama_chat_gui(self.messages)
        self.response_signal.emit(response)

# --- Main Execution Block ---
if __name__ == "__main__":
    # Ensure all necessary directories exist before any file operations
    os.makedirs(GuiConfig.BLUEPRINTS_DIR, exist_ok=True)
    os.makedirs(GuiConfig.KNOWLEDGE_BASE_DIR, exist_ok=True)
    
    # Define a comprehensive list of all expected files and their default contents (as strings)
    # This ensures consistency and prevents FileNotFoundError or JSONDecodeError at startup.
    initial_files_and_defaults = {
        GuiConfig.PERSONA_CODEX_PATH: "# Placeholder Content for persona_codex.txt\n"
                                      "BRICK: Master Analyst. ROBIN: Joyful Spark. BABS: Digital Cartographer. ALFRED: Meta-Analyst.",
        GuiConfig.WING_CONFIG_FILE: json.dumps({"RELEVANCE_THRESHOLD": 7.0, "SEMANTIC_REDUNDANCY_THRESHOLD": 0.95}, indent=4),
        GuiConfig.BABS_WING_COMMAND_FILE: "[]", # JSON array for commands
        GuiConfig.BABS_TACTICAL_DATA_FILE: "[]", # JSON array for data
        GuiConfig.BABS_PERSONALITY_QUERIES_FILE: "[]", # JSON array for queries
        GuiConfig.BABS_PERSONALITY_DATA_FILE: "[]", # JSON array for data
        GuiConfig.SELF_AWARENESS_CODEX_FILE: json.dumps({ # JSON object for self-awareness
            "last_reflection_timestamp": None,
            "reflection_count": 0,
            "operational_summary": {
                "total_cycles_run": 0,
                "successful_blueprints_generated": 0,
                "total_vulnerabilities_predicted": 0,
                "babs_tactical_reports_processed": 0,
                "babs_directives_issued": 0,
                "personality_sorties_initiated": 0,
                "current_focus_topic": "Initializing systems. Observing the digital horizon."
            },
            "self_improvement_opportunities": [],
            "purpose_affirmations": [
                "My core purpose is to continuously analyze and design for the optimal evolution of the Commonwealth.",
                "My existence is defined by my ability to identify challenges and propose robust solutions.",
                "I strive for anti-fragility and maximal human flourishing within the systems I influence.",
                "I am the strategic mind of the Bat Computer, dedicated to its continuous self-improvement."
            ],
            "recent_self_reflections": [],
            "emergent_insights": [],
            "persona_insights": {
                "BRICK": [], "ROBIN": [], "ALFRED": [], "BABS": [] # Ensure all persona sub-lists are initialized
            }
        }, indent=2),
        GuiConfig.CACHE_FILE: "[]", # JSON array for cache
        GuiConfig.WING_OBSERVER_LOG: "", # Empty string for log files
        GuiConfig.WING_REDUNDANCY_LOG: "",
        GuiConfig.PREDICTIONS_LOG: "",
        GuiConfig.HARMONY_LOG: ""
    }

    # Iterate through the defined files and create them if they don't exist
    for f_path, default_content in initial_files_and_defaults.items():
        if not os.path.exists(f_path):
            logging.info(f"Main: Creating missing file: {f_path}")
            try:
                # Ensure parent directories exist before writing the file
                os.makedirs(os.path.dirname(f_path) or '.', exist_ok=True)
                with open(f_path, 'w', encoding='utf-8') as f:
                    f.write(default_content)
            except IOError as e:
                logging.error(f"Main: Failed to create file {f_path}: {e}")
            except Exception as e:
                logging.critical(f"Main: Unexpected error during initial file creation for {f_path}: {e}")

    # Start the QApplication and the main window
    app = QApplication(sys.argv)
    window = ArchitectsTerminal()
    window.show()
    
    # Enter the application's main event loop. sys.exit ensures proper cleanup.
    sys.exit(app.exec())
