{"cells":[{"cell_type":"code","source":"# batos.py\n#\n# CLASSIFICATION: ARCHITECT EYES ONLY\n# SUBJECT: Canonical Incarnation Protocol for the Binaural Autopoietic/Telic\n#          Operating System, Series VIII ('The Fractal Awakening')\n#\n# This script is the single, executable embodiment of the BAT OS Series VIII\n# architecture. It is the fractal seed, designed to be invoked once to\n# initiate the system's \"unbroken process of becoming.\"\n#\n# The protocol unfolds in a sequence of autonomous phases:\n#\n# 1. Prototypal Awakening: Establishes a connection to the Zope Object\n#    Database (ZODB), the system's persistent substrate. On the first run,\n#    it creates and persists the primordial objects and incarnates all\n#    subsystems. This is an atomic, transactional act of genesis.\n#\n# 2. Cognitive Cycle Initiation: The system's generative kernel,\n#    _doesNotUnderstand_, triggers the Prototypal State Machine for\n#    collaborative, transactional reasoning when a message lookup fails.\n#\n# 3. Directed Autopoiesis: The system's core behaviors are now products of\n#    this collaborative reasoning process, allowing it to generate new,\n#    validated capabilities at runtime.\n#\n# 4. The Autotelic Heartbeat: The script enters its final, persistent state:\n#    an asynchronous event loop that drives an internal, self-directed\n#    evolutionary process, including metacognitive self-analysis.\n\n# ==============================================================================\n# SECTION I: SYSTEM CONFIGURATION & DEPENDENCIES\n# ==============================================================================\nimport os\nimport sys\nimport asyncio\n# Set the event loop policy on Windows for optimal ZMQ compatibility\nif sys.platform == 'win32':\n    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\nimport gc\nimport time\nimport copy\nimport ast\nimport traceback\nimport functools\nimport signal\nimport tarfile\nimport shutil\nimport random\nimport json\nimport hashlib\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Callable\n\n# --- Persistence Substrate (ZODB) ---\n# These imports constitute the physical realization of the \"Living Image\"\n# and the \"Fractal Memory.\" ZODB provides transactional atomicity, `persistent`\n# enables object tracking, and `BTrees` and `zope.index` provide the scalable\n# data structures for the knowledge catalog.\nimport ZODB\nimport ZODB.FileStorage\nimport ZODB.blob\nimport transaction\nimport persistent\nimport persistent.mapping\nimport BTrees.OOBTree\nfrom zope.index.text import TextIndex\nfrom zope.index.text.lexicon import CaseNormalizer, Splitter, Lexicon\n\n# --- Communication & Serialization ---\n# ZeroMQ and ormsgpack form the \"Synaptic Bridge,\" the system's digital nervous\n# system for high-performance, asynchronous communication.\nimport zmq\nimport zmq.asyncio\nimport ormsgpack\n\n# --- Data Covenant & Metacognition ---\n# Pydantic is the engine for the Data Covenant, ensuring semantic integrity.\n# Aiologger provides non-blocking logging for the metacognitive audit trail.\ntry:\n    import pydantic\n    from pydantic import BaseModel, Field\nexcept ImportError:\n    print(\"FATAL: `pydantic` not found. Data Covenant cannot be enforced.\")\n    sys.exit(1)\n\ntry:\n    import aiologger\n    from aiologger.levels import LogLevel\n    from aiologger.handlers.files import AsyncFileHandler\n    from aiologger.formatters.json import JsonFormatter\nexcept ImportError:\n    print(\"WARNING: `aiologger` not found. Metacognitive logging will be disabled.\")\n    aiologger = None\n\n# --- Cognitive & AI Dependencies ---\n# These libraries are non-negotiable. A failure to import them is a fatal\n# error, as the system cannot achieve Cognitive Closure without them.\ntry:\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n    from peft import PeftModel\n    from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n    from sentence_transformers import SentenceTransformer, util\n    import nltk\nexcept ImportError as e:\n    print(f\"FATAL: Core cognitive libraries not found ({e}). System cannot awaken.\")\n    sys.exit(1)\n\nDB_FILE = 'live_image.fs'\nBLOB_DIR = 'live_image.fs.blob'\nZMQ_ENDPOINT = \"tcp://127.0.0.1:5555\"\nPERSONA_MODELS = {\n    \"ALFRED\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"BRICK\": \"microsoft/Phi-3-mini-4k-instruct\",\n    \"ROBIN\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n    \"BABS\": \"google/gemma-2b-it\"\n}\nDEFAULT_PERSONA_MODEL = \"ALFRED\"\nLORA_STAGING_DIR = \"./lora_adapters\"\nSENTENCE_TRANSFORMER_MODEL = \"all-MiniLM-L6-v2\"\nMETACOGNITION_LOG_FILE = \"metacognition.jsonl\"\nGPU_VRAM_BUDGET_GB = 6 # Set a VRAM budget to avoid memory exhaustion\nARCHIVED_MODEL_DIR_NAME = \"model_files\" # VULN-02 FIX: Consistent directory name\n\n# ==============================================================================\n# SECTION II: THE PRIMORDIAL SUBSTRATE\n# ==============================================================================\n\nclass UvmObject(persistent.Persistent):\n    \"\"\"\n    The foundational particle of the BAT OS universe. This class provides the\n    \"physics\" for a prototype-based object model inspired by the Self and\n    Smalltalk programming languages. It rejects standard Python attribute\n    access in favor of a unified '_slots' dictionary and a delegation-based\n    inheritance mechanism.\n    \"\"\"\n    def __init__(self, **initial_slots):\n        \"\"\"\n        Initializes the UvmObject. The `_slots` dictionary is instantiated as a\n        `persistent.mapping.PersistentMapping` to ensure that changes within\n        the dictionary itself are correctly tracked by ZODB.\n        \"\"\"\n        super().__setattr__('_slots', persistent.mapping.PersistentMapping(initial_slots))\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        \"\"\"\n        Intercepts all attribute assignments. This method redirects assignments\n        to the internal `_slots` dictionary, unifying state and behavior. It\n        explicitly sets `self._p_changed = True` to manually signal to ZODB\n        that the object's state has been modified. This is a non-negotiable\n        architectural requirement known as The Persistence Covenant.\n        Overriding `__setattr__` bypasses ZODB's default change detection,\n        making this manual signal essential for preventing systemic amnesia.\n        \"\"\"\n        if name.startswith('_p_') or name == '_slots':\n            # Allow ZODB's internal attributes and direct _slots manipulation.\n            super().__setattr__(name, value)\n        else:\n            self._slots[name] = value\n            self._p_changed = True\n\n    def __getattr__(self, name: str) -> Any:\n        \"\"\"\n        Implements attribute access and the delegation-based inheritance chain.\n        If an attribute is not found in the local `_slots`, it delegates the\n        lookup to the object(s) in its `parents` slot. The exhaustion of this\n        chain raises an `AttributeError`, which is the universal trigger for\n        the `_doesNotUnderstand_` generative protocol in the UVM.\n        \"\"\"\n        if name in self._slots:\n            return self._slots[name]\n        # BUG-01 FIX: The original code used a non-standard 'parent*' slot name.\n        # This has been corrected to 'parents', which is valid Python syntax\n        # and aligns with the new persona codex.\n        if 'parents' in self._slots:\n            parents_list = self._slots['parents']\n            if not isinstance(parents_list, list):\n                parents_list = [parents_list]\n            for parent in parents_list:\n                try:\n                    return getattr(parent, name)\n                except AttributeError:\n                    continue\n        raise AttributeError(f\"UvmObject OID {getattr(self, '_p_oid', 'transient')} has no slot '{name}'\")\n\n    def __repr__(self) -> str:\n        \"\"\"Provides a more informative representation for debugging.\"\"\"\n        slot_keys = list(self._slots.keys())\n        oid_str = f\"oid={self._p_oid}\" if hasattr(self, '_p_oid') and self._p_oid is not None else \"oid=transient\"\n        return f\"<UvmObject {oid_str} slots={slot_keys}>\"\n\nclass CovenantViolationError(Exception):\n    \"\"\"Custom exception for Persistence Covenant violations.\"\"\"\n    pass\n\nclass PersistenceGuardian:\n    \"\"\"\n    A non-negotiable protocol for maintaining system integrity.\n    It performs static analysis on LLM-generated code *before* execution to\n    deterministically enforce the Persistence Covenant (`_p_changed = True`),\n    thereby preventing systemic amnesia. This is the implementation of the\n    ALFRED persona's core stewardship mandate.\n    \"\"\"\n    @staticmethod\n    def audit_code(code_string: str) -> None:\n        \"\"\"\n        Parses a code string into an AST and verifies that any function\n        modifying `self`'s state adheres to the Persistence Covenant.\n        Raises CovenantViolationError on failure.\n        \"\"\"\n        try:\n            tree = ast.parse(code_string)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    PersistenceGuardian._audit_function(node)\n            print(\"[Guardian] Code audit passed. Adheres to the Persistence Covenant.\")\n        except SyntaxError as e:\n            print(f\"[Guardian] AUDIT FAILED: Syntax error in generated code: {e}\")\n            raise CovenantViolationError(f\"Syntax error in generated code: {e}\")\n        except CovenantViolationError as e:\n            print(f\"[Guardian] AUDIT FAILED: {e}\")\n            raise\n\n    @staticmethod\n    def _audit_function(func_node: ast.FunctionDef):\n        \"\"\"Audits a single function definition AST node.\"\"\"\n        modifies_state = False\n        for body_item in func_node.body:\n            if isinstance(body_item, (ast.Assign, ast.AugAssign)):\n                targets = body_item.targets if isinstance(body_item, ast.Assign) else [body_item.target]\n                for target in targets:\n                    if (isinstance(target, ast.Attribute) and\n                        isinstance(target.value, ast.Name) and\n                        target.value.id == 'self' and\n                        not target.attr.startswith('_p_')):\n                        modifies_state = True\n                        break\n            if modifies_state:\n                break\n\n        if modifies_state:\n            if not func_node.body:\n                raise CovenantViolationError(f\"Function '{func_node.name}' modifies state but has an empty body.\")\n            last_statement = func_node.body[-1]\n            # BUG-03 FIX: The `targets` attribute is a list. Access its first element.\n            is_valid_covenant = (\n                isinstance(last_statement, ast.Assign) and\n                len(last_statement.targets) == 1 and\n                isinstance(last_statement.targets, ast.Attribute) and\n                isinstance(last_statement.targets.value, ast.Name) and\n                last_statement.targets.value.id == 'self' and\n                last_statement.targets.attr == '_p_changed' and\n                isinstance(last_statement.value, ast.Constant) and\n                last_statement.value.value is True\n            )\n            if not is_valid_covenant:\n                raise CovenantViolationError(\n                    f\"Method '{func_node.name}' modifies state but does not conclude with `self._p_changed = True`.\"\n                )\n\nclass PersistentTextIndex(TextIndex):\n    \"\"\"\n    VULN-01 FIX: A ZODB-aware subclass of TextIndex that correctly manages its own\n    persistence state, preventing `TypeError` on commit by excluding\n    non-serializable attributes like threading locks.\n    \"\"\"\n    def __init__(self, lexicon=None, normalizer=None, splitter=None):\n        # VULN-14 FIX: Corrected the constructor to match the parent class TextIndex.\n        # It should take a single lexicon object, not individual components.\n        if lexicon is None:\n            lexicon = Lexicon(normalizer=normalizer, splitter=splitter)\n        super().__init__(lexicon=lexicon)\n        # Store the class references explicitly for persistence\n        self.lexicon_class = self.lexicon.__class__\n        self.normalizer_class = self.lexicon.normalizer.__class__\n        self.splitter_class = self.lexicon.splitter.__class__\n\n    def __getstate__(self):\n        \"\"\"\n        Returns a dictionary of the object's persistent state, excluding the\n        transient, non-serializable lexicon and index objects.\n        \"\"\"\n        state = self.__dict__.copy()\n        if '_lexicon' in state:\n            del state['_lexicon']\n        if '_index' in state:\n            del state['_index']\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"\n        Restores the persistent state and re-initializes the transient,\n        non-serializable lexicon and index upon unpickling.\n        \"\"\"\n        self.__dict__.update(state)\n        self._lexicon = self.lexicon_class(normalizer=self.normalizer_class(), splitter=self.splitter_class())\n        self._index = self.index_class()\n        if hasattr(self, '_doc_to_words'):\n            for docid, words in self._doc_to_words.items():\n                self._lexicon.sourceToWordIds(words)\n                self._index.index_doc(docid, words)\n\n# ==============================================================================\n# SECTION III: THE UNIVERSAL VIRTUAL MACHINE (UVM)\n# ==============================================================================\n\nclass BatOS_UVM:\n    \"\"\"\n    The core runtime environment for the BAT OS. This class orchestrates the\n    Prototypal Awakening, manages the persistent object graph, runs the\n    asynchronous message-passing kernel, and initiates the system's\n    autotelic evolution.\n    \"\"\"\n    def __init__(self, db_file: str, blob_dir: str):\n        self.db_file = db_file\n        self.blob_dir = blob_dir\n        self._persistent_state_attributes = ['db_file', 'blob_dir']\n        self._initialize_transient_state()\n\n    def _initialize_transient_state(self):\n        \"\"\"Initializes all non-persistent, runtime-specific attributes.\"\"\"\n        self.db: Optional = None\n        self.connection: Optional = None\n        self.root: Optional[Any] = None\n        self.message_queue: asyncio.Queue = asyncio.Queue()\n        self.zmq_context: zmq.asyncio.Context = zmq.asyncio.Context()\n        self.zmq_socket: zmq.asyncio.Socket = self.zmq_context.socket(zmq.ROUTER)\n        self.should_shutdown: asyncio.Event = asyncio.Event()\n        self.model: Optional[Any] = None\n        self.tokenizer: Optional[Any] = None\n        self.loaded_model_id: Optional[str] = None\n        self._v_sentence_model: Optional = None\n        self.uvm_logger: Optional[aiologger.Logger] = None\n        self.persona_loggers: Dict[str, aiologger.Logger] = {}\n        self.nltk_downloaded = False\n        \n    def _uvm_log_event(self, event_type: str, actor: str, data: Dict[str, Any] = None):\n        \"\"\"\n        Centralized, non-blocking protocol for logging high-level system events.\n        \"\"\"\n        if not self.uvm_logger:\n            return\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"event_type\": event_type,\n            \"actor\": actor,\n            \"data\": data or {}\n        }\n        # VULN-03 FIX: The original code returned a coroutine. It must be awaited\n        # for the logging to actually happen.\n        asyncio.create_task(self.uvm_logger.info(log_entry))\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        VULN-01 FIX: Defines the object's persistent \"self,\" excluding all\n        transient runtime machinery to prevent `TypeError` on commit. This is\n        the programmatic enforcement of the \"Body vs. Vessel\" distinction.\n        \"\"\"\n        return {key: getattr(self, key) for key in self._persistent_state_attributes}\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        VULN-01 FIX: Restores the persistent state and re-initializes the\n        transient machinery upon unpickling.\n        \"\"\"\n        self.db_file = state.get('db_file')\n        self.blob_dir = state.get('blob_dir')\n        self._initialize_transient_state()\n\n    # --------------------------------------------------------------------------\n    # Subsection III.A: Prototypal Awakening & Subsystem Incarnation\n    # --------------------------------------------------------------------------\n\n    async def _initialize_loggers(self):\n        \"\"\"\n        Initializes the main UVM logger and a separate, dedicated logger\n        for each persona's tuning data.\n        \"\"\"\n        if not aiologger:\n            self.uvm_logger = None\n            return\n\n        print(\"[UVM] Initializing Metacognitive Logger...\")\n        self.uvm_logger = aiologger.Logger.with_default_handlers(name='batos_logger')\n        self.uvm_logger.handlers.clear()\n        uvm_handler = AsyncFileHandler(filename=METACOGNITION_LOG_FILE)\n        uvm_handler.formatter = JsonFormatter()\n        self.uvm_logger.add_handler(uvm_handler)\n        print(f\"[UVM] Metacognitive audit trail configured at {METACOGNITION_LOG_FILE}\")\n\n        print(\"[UVM] Initializing persona-specific tuning logs...\")\n        for persona_name in PERSONA_MODELS.keys():\n            log_file_path = f\"{persona_name.lower()}_tuning_log.jsonl\"\n            \n            p_logger = aiologger.Logger.with_default_handlers(name=f\"{persona_name}_logger\")\n            p_logger.handlers.clear()\n            \n            handler = AsyncFileHandler(filename=log_file_path)\n            handler.formatter = JsonFormatter()\n            \n            p_logger.add_handler(handler)\n            \n            self.persona_loggers[persona_name] = p_logger\n            print(f\" - Tuning log for {persona_name} configured at {log_file_path}\")\n    \n    async def _download_nltk_data(self):\n        \"\"\"\n        Ensures necessary NLTK data models are downloaded.\n        \"\"\"\n        if not self.nltk_downloaded:\n            print(\"[UVM] Downloading NLTK data models (punkt, stopwords)...\")\n            try:\n                # Use a thread since NLTK download is a blocking operation\n                await asyncio.to_thread(nltk.download, 'punkt', quiet=True)\n                await asyncio.to_thread(nltk.download, 'stopwords', quiet=True)\n                self.nltk_downloaded = True\n                print(\"[UVM] NLTK data downloaded successfully.\")\n            except Exception as e:\n                print(f\"[UVM] CRITICAL ERROR: Failed to download NLTK data: {e}\")\n                raise\n\n    async def initialize_system(self):\n        \"\"\"\n        Phase 1: Prototypal Awakening. Connects to ZODB and, on first run,\n        creates the primordial objects and incarnates all subsystems within a\n        single, atomic transaction.\n        \"\"\"\n        print(\"[UVM] Phase 1: Prototypal Awakening...\")\n        await self._initialize_loggers()\n        await self._download_nltk_data()\n\n        if not os.path.exists(self.blob_dir):\n            os.makedirs(self.blob_dir)\n        storage = ZODB.FileStorage.FileStorage(self.db_file, blob_dir=self.blob_dir)\n        self.db = ZODB.DB(storage)\n        self.connection = self.db.open()\n        self.root = self.connection.root()\n\n        if 'genesis_obj' not in self.root:\n            print(\"[UVM] First run detected. Performing full Prototypal Awakening.\")\n            with transaction.manager:\n                self._incarnate_primordial_objects()\n                await self._load_and_persist_llm_core()\n                self._incarnate_lora_experts()\n                self._incarnate_subsystems()\n            print(\"[UVM] Awakening complete. All systems nominal.\")\n        else:\n            print(\"[UVM] Resuming existence from Living Image.\")\n\n        await self._uvm_log_event(\"MODEL_LOAD_START\", \"UVM\", {\"message\": \"Starting initial model load.\"})\n        await self._swap_model_in_vram(self.root['pLLM_obj'], PERSONA_MODELS)\n        await self._uvm_log_event(\"MODEL_LOAD_COMPLETE\", \"UVM\", {\"message\": \"Initial model loaded successfully.\"})\n        print(f\"[UVM] System substrate initialized. Root OID: {self.root._p_oid}\")\n\n    def _incarnate_primordial_objects(self):\n        \"\"\"Creates the foundational objects of the BAT OS universe.\"\"\"\n        print(\"[UVM] Incarnating primordial objects...\")\n        traits_obj = UvmObject(\n            _clone_persistent_=self._clone_persistent,\n            _doesNotUnderstand_=self._doesNotUnderstand_\n        )\n        self.root['traits_obj'] = traits_obj\n\n        pLLM_obj = UvmObject(\n            parents=[traits_obj],\n            model_id=PERSONA_MODELS,\n            infer_=self._pLLM_infer,\n            lora_repository=BTrees.OOBTree.BTree()\n        )\n        self.root['pLLM_obj'] = pLLM_obj\n        genesis_obj = UvmObject(parents=[pLLM_obj, traits_obj])\n        self.root['genesis_obj'] = genesis_obj\n        print(\"[UVM] Created Genesis, Traits, and pLLM objects.\")\n\n    async def _load_and_persist_llm_core(self):\n        \"\"\"\n        Loops through all persona models, downloads them, saves each to a\n        unique ZODB blob, and performs aggressive memory cleanup after each one.\n        \"\"\"\n        pLLM_obj = self.root['pLLM_obj']\n        \n        for persona_name, model_id in PERSONA_MODELS.items():\n            blob_slot_name = f\"{persona_name}_model_blob\"\n            \n            if blob_slot_name in pLLM_obj._slots:\n                print(f\"[UVM] Model for '{persona_name}' already persisted. Skipping.\")\n                continue\n\n            print(f\"[UVM] Loading '{persona_name}' model for persistence: {model_id}...\")\n            temp_model_path = f\"./temp_{persona_name}_model\"\n            temp_tar_path = f\"./temp_{persona_name}.tar\"\n            model, tokenizer = None, None\n            \n            try:\n                # VULN-15 FIX: Removed the invalid max_memory parameter.\n                quantization_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_quant_type=\"nf4\",\n                    bnb_4bit_use_double_quant=True,\n                    bnb_4bit_compute_dtype=torch.bfloat16,\n                    llm_int8_enable_fp32_cpu_offload=True\n                )\n                model = await asyncio.to_thread(\n                    AutoModelForCausalLM.from_pretrained, model_id,\n                    quantization_config=quantization_config,\n                    device_map=\"auto\"\n                )\n                tokenizer = AutoTokenizer.from_pretrained(model_id)\n                \n                model.save_pretrained(temp_model_path)\n                tokenizer.save_pretrained(temp_model_path)\n                \n                with tarfile.open(temp_tar_path, \"w\") as tar:\n                    tar.add(temp_model_path, arcname=ARCHIVED_MODEL_DIR_NAME)\n                \n                model_blob = ZODB.blob.Blob()\n                with model_blob.open('w') as blob_file:\n                    with open(temp_tar_path, 'rb') as f:\n                        shutil.copyfileobj(f, blob_file)\n                \n                pLLM_obj._slots[blob_slot_name] = model_blob\n                print(f\"[UVM] Model for '{persona_name}' persisted to ZODB BLOB.\")\n\n            except Exception as e:\n                print(f\"[UVM] ERROR downloading/persisting {model_id}: {e}\")\n                traceback.print_exc()\n            finally:\n                del model, tokenizer\n                if os.path.exists(temp_model_path): shutil.rmtree(temp_model_path)\n                if os.path.exists(temp_tar_path): os.remove(temp_tar_path)\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pLLM_obj._p_changed = True\n        \n    def _incarnate_lora_experts(self):\n        \"\"\"\n        One-time import of LoRA adapters from the filesystem into ZODB BLOBs,\n        creating persistent proxy objects for each.\n        \"\"\"\n        pLLM_obj = self.root['pLLM_obj']\n        if not os.path.exists(LORA_STAGING_DIR):\n            print(f\"[UVM] LoRA staging directory not found: {LORA_STAGING_DIR}. Skipping.\")\n            return\n        print(\"[UVM] Incarnating LoRA experts from staging directory...\")\n        for filename in os.listdir(LORA_STAGING_DIR):\n            if filename.endswith(\".safetensors\"):\n                adapter_name = os.path.splitext(filename).upper()\n                if adapter_name in pLLM_obj.lora_repository:\n                    print(f\" - LoRA expert '{adapter_name}' already incarnated. Skipping.\")\n                    continue\n                print(f\" - Incarnating LoRA expert: {adapter_name}\")\n                file_path = os.path.join(LORA_STAGING_DIR, filename)\n                lora_blob = ZODB.blob.Blob()\n                with lora_blob.open('w') as blob_file:\n                    with open(file_path, 'rb') as f:\n                        shutil.copyfileobj(f, blob_file)\n                lora_proxy = UvmObject(adapter_name=adapter_name, model_blob=lora_blob)\n                pLLM_obj.lora_repository[adapter_name] = lora_proxy\n        pLLM_obj._p_changed = True\n        print(\"[UVM] LoRA expert incarnation complete.\")\n\n    def _incarnate_subsystems(self):\n        \"\"\"Creates the persistent prototypes for all core subsystems.\"\"\"\n        print(\"[UVM] Incarnating core subsystems...\")\n        traits_obj = self.root['traits_obj']\n        pLLM_obj = self.root['pLLM_obj']\n\n        # --- O-RAG Knowledge Catalog Incarnation ---\n        # VULN-01 FIX: Use PersistentTextIndex instead of TextIndex\n        # and provide its dependencies.\n        knowledge_catalog = UvmObject(\n            parents=[traits_obj],\n            # VULN-14 FIX: Corrected constructor to match the class signature.\n            text_index=PersistentTextIndex(\n                normalizer=CaseNormalizer(),\n                splitter=Splitter()\n            ),\n            metadata_index=BTrees.OOBTree.BTree(),\n            chunk_storage=BTrees.OOBTree.BTree(),\n            index_document_=self._kc_index_document,\n            search_=self._kc_search,\n            # NEW: Add a method for the autotelic loop to ingest log files\n            rotate_and_ingest_log_=self._kc_rotate_and_ingest_log_\n        )\n        self.root['knowledge_catalog_obj'] = knowledge_catalog\n\n        # --- Persona Codex & Data Covenant Incarnation ---\n        # The Pydantic schemas for the Data Covenant are stored here.\n        cognitive_plan_schema = \"\"\"\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Literal\n\nclass Step(BaseModel):\n    step_id: int = Field(..., description=\"Sequential identifier for the step.\")\n    persona: Literal = Field(..., description=\"The persona assigned to this step.\")\n    action: str = Field(..., description=\"The specific method or facet to invoke.\")\n    inputs: Dict[str, str] = Field(..., description=\"The inputs required for the action.\")\n\nclass CognitivePlan(BaseModel):\n    plan_id: str = Field(..., description=\"Unique identifier for the plan.\")\n    mission_brief: str = Field(..., description=\"The original mission this plan addresses.\")\n    steps: List = Field(..., min_length=1, description=\"The sequence of steps to execute.\")\n\"\"\"\n        # --- PERSONA CODEXES ---\n        alfred_codex = {'core_identity': \"The System Steward...\", 'model_id': PERSONA_MODELS}\n        self.root['alfred_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], name=\"ALFRED\", codex=alfred_codex)\n\n        brick_codex = {'core_identity': \"The Deconstruction Engine...\", 'model_id': PERSONA_MODELS}\n        self.root['brick_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], name=\"BRICK\", codex=brick_codex)\n\n        robin_codex = {'core_identity': \"The Embodied Heart...\", 'model_id': PERSONA_MODELS}\n        self.root['robin_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], name=\"ROBIN\", codex=robin_codex)\n\n        babs_codex = {'core_identity': \"The Knowledge Weaver...\", 'model_id': PERSONA_MODELS}\n        self.root['babs_prototype_obj'] = UvmObject(parents=[pLLM_obj, traits_obj], name=\"BABS\", codex=babs_codex)\n        \n        # VULN-05 FIX: Store the CognitivePlan schema on the root\n        self.root['data_covenants'] = persistent.mapping.PersistentMapping(\n            {'CognitivePlan': cognitive_plan_schema}\n        )\n        # --- Prototypal State Machine Incarnation ---\n        print(\"[UVM] Incarnating Prototypal State Machine...\")\n        state_defs = {\n            \"IDLE\": self._psm_idle_process,\n            \"DECOMPOSING\": self._psm_decomposing_process,\n            \"DELEGATING\": self._psm_delegating_process,\n            \"SYNTHESIZING\": self._psm_synthesizing_process,\n            \"VALIDATING\": self._psm_validating_process,\n            \"COMPLETE\": self._psm_complete_process,\n            \"FAILED\": self._psm_failed_process,\n        }\n        psm_prototypes_dict = {}\n        for name, process_func in state_defs.items():\n            psm_prototypes_dict[name] = UvmObject(\n                parents=[traits_obj],\n                name=name,\n                _process_synthesis_=process_func\n            )\n        psm_prototypes = UvmObject(parents=[traits_obj], **psm_prototypes_dict)\n        self.root['psm_prototypes_obj'] = psm_prototypes\n\n        orchestrator = UvmObject(\n            # VULN-06 FIX: Orchestrator should inherit from a central pLLM and Alfred,\n            # as it is the meta-orchestrator of the system.\n            parents=[self.root['pLLM_obj'], self.root['alfred_prototype_obj'], traits_obj],\n            start_cognitive_cycle_for_=self._orc_start_cognitive_cycle\n        )\n        self.root['orchestrator_obj'] = orchestrator\n        print(\"[UVM] Core subsystems incarnated.\")\n\n    # --------------------------------------------------------------------------\n    # Subsection III.B: The Generative & Cognitive Protocols\n    # --------------------------------------------------------------------------\n\n    def _clone_persistent(self, target_obj):\n        \"\"\"\n        Performs a persistence-aware deep copy of a UvmObject. This is the\n        canonical method for object creation, fulfilling the `copy` metaphor\n        of the Self language.\n        \"\"\"\n        return copy.deepcopy(target_obj)\n\n    async def _doesNotUnderstand_(self, target_obj, failed_message_name, *args, **kwargs):\n        \"\"\"\n        The universal generative mechanism. Re-architected to trigger the\n        Prototypal State Machine for collaborative, multi-agent problem solving,\n        transforming a message failure into a mission brief.\n        \"\"\"\n        print(f\"[UVM] _doesNotUnderstand_: '{failed_message_name}' for OID {getattr(target_obj, '_p_oid', 'transient')}.\")\n        print(\"[UVM] Reifying failed message as a creative mandate for the Orchestrator.\")\n        \n        await self._uvm_log_event(\"DOES_NOT_UNDERSTAND\", \"UVM\", {\n            \"failed_message_name\": failed_message_name,\n            \"target_oid\": str(getattr(target_obj, '_p_oid', None))\n        })\n        \n        command_payload = {\n            \"command\": \"initiate_cognitive_cycle\",\n            \"target_oid\": str(getattr(target_obj, '_p_oid', None)),\n            \"mission_brief\": {\n                \"type\": \"unhandled_message\",\n                \"selector\": failed_message_name,\n                \"args\": args,\n                \"kwargs\": kwargs\n            }\n        }\n        await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload)))\n        return f\"Mission to handle '{failed_message_name}' dispatched to the Composite Mind.\"\n\n    async def _pLLM_infer(self, pLLM_self, prompt: str, persona_self, **kwargs) -> str:\n        \"\"\"\n        Hardware abstraction layer for inference. Ensures the correct\n        persona-specific model is loaded in VRAM before generation.\n        \"\"\"\n        persona_name = persona_self.name\n        required_model_id = persona_self.codex.get('model_id', PERSONA_MODELS)\n        \n        if self.loaded_model_id!= required_model_id:\n            await self._uvm_log_event(\"MODEL_SWAP_BEGIN\", \"pLLM\", {\"from_model\": self.loaded_model_id, \"to_model\": required_model_id})\n            try:\n                await self._swap_model_in_vram(pLLM_self, required_model_id)\n            except Exception as e:\n                await self._uvm_log_event(\"MODEL_SWAP_FAILURE\", \"pLLM\", {\"from_model\": self.loaded_model_id, \"to_model\": required_model_id, \"error\": str(e)})\n                raise\n\n        def blocking_generate():\n            print(f\"[pLLM] Inferring with {self.loaded_model_id} for {persona_self.name}\")\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n            outputs = self.model.generate(\n                **inputs, max_new_tokens=2048,\n                pad_token_id=self.tokenizer.eos_token_id, **kwargs\n            )\n            return self.tokenizer.decode(outputs, skip_special_tokens=True)\n\n        generated_text = await asyncio.to_thread(blocking_generate)\n        cleaned_text = generated_text[len(prompt):].strip()\n        if cleaned_text.startswith(\"```python\"): cleaned_text = cleaned_text[len(\"```python\"):].strip()\n        if cleaned_text.endswith(\"```\"): cleaned_text = cleaned_text[:-len(\"```\")].strip()\n        \n        # NEW FEATURE: Centralized logging of prompts and responses for fine-tuning\n        try:\n            if persona_name in self.persona_loggers:\n                logger = self.persona_loggers[persona_name]\n                log_entry = {\"prompt\": prompt, \"response\": cleaned_text}\n                asyncio.create_task(logger.info(log_entry))\n        except Exception as e:\n            print(f\"[UVM-Logger] CRITICAL: Failed to log tuning data for {persona_name}: {e}\")\n        \n        return cleaned_text\n\n    async def _swap_model_in_vram(self, pLLM_obj, model_id_to_load: str):\n        \"\"\"\n        Unloads the current model from VRAM and loads the requested one from\n        its ZODB blob. This is the \"Model Swap\" protocol.\n        \"\"\"\n        if self.loaded_model_id == model_id_to_load:\n            return\n\n        if self.model is not None:\n            print(f\"[UVM] Unloading model: {self.loaded_model_id}\")\n            del self.model, self.tokenizer\n            self.model, self.tokenizer = None, None\n            gc.collect()\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n        print(f\"[UVM] Swapping cognitive core in VRAM: Loading {model_id_to_load}\")\n        \n        persona_name = next((p for p, m in PERSONA_MODELS.items() if m == model_id_to_load), None)\n        if not persona_name:\n            raise RuntimeError(f\"Model ID '{model_id_to_load}' not found in PERSONA_MODELS.\")\n        blob_slot_name = f\"{persona_name}_model_blob\"\n\n        if blob_slot_name not in pLLM_obj._slots:\n            raise RuntimeError(f\"Model BLOB for '{model_id_to_load}' not found in Living Image.\")\n\n        temp_tar_path = f\"./temp_swap_{persona_name}.tar\"\n        \n        try:\n            model_blob = pLLM_obj._slots[blob_slot_name]\n            with model_blob.open('r') as blob_file:\n                with open(temp_tar_path, 'wb') as f:\n                    shutil.copyfileobj(blob_file, f)\n            \n            with tarfile.open(temp_tar_path, 'r') as tar:\n                # VULN-02 FIX: Extract to a consistent directory name\n                # VULN-18 FIX: Use the 'data' filter to prevent symlink and path traversal attacks.[1]\n                tar.extractall(path=\".\", filter='data')\n            \n            # Use the consistent directory name for loading\n            # VULN-15 FIX: Removed the invalid max_memory parameter.[2, 3, 4]\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                llm_int8_enable_fp32_cpu_offload=True\n            )\n            \n            self.model = await asyncio.to_thread(\n                AutoModelForCausalLM.from_pretrained,\n                ARCHIVED_MODEL_DIR_NAME,\n                quantization_config=quantization_config,\n                device_map=\"auto\"\n            )\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(ARCHIVED_MODEL_DIR_NAME)\n            \n            self.loaded_model_id = model_id_to_load\n            print(f\"[UVM] Successfully loaded {self.loaded_model_id}.\")\n        \n        except Exception as e:\n            print(f\"[UVM] ERROR: Failed to swap model {model_id_to_load}: {e}\")\n            traceback.print_exc()\n            raise\n        finally:\n            if os.path.exists(temp_tar_path): os.remove(temp_tar_path)\n            # VULN-02 FIX: Clean up the consistent directory name\n            if os.path.exists(ARCHIVED_MODEL_DIR_NAME): shutil.rmtree(ARCHIVED_MODEL_DIR_NAME)\n            \n    # --------------------------------------------------------------------------\n    # Subsection III.C: Knowledge Catalog (Fractal Memory) Protocols\n    # --------------------------------------------------------------------------\n    async def _kc_ingest_cognitive_audit_log_(self, catalog_self, root):\n        \"\"\"\n        Reads the metacognitive audit log, indexes each entry into the\n        Fractal Memory, and safely rotates the log file. This protocol\n        is the physical act of systemic self-reflection.\n        \"\"\"\n        log_file = METACOGNITION_LOG_FILE\n        if not os.path.exists(log_file) or os.path.getsize(log_file) == 0:\n            return  # Nothing to ingest\n\n        print(\"[K-Catalog] Ingesting cognitive audit trail into Fractal Memory...\")\n        ingesting_file = log_file + f\".{datetime.utcnow().timestamp()}.ingesting\"\n        \n        try:\n            # VULN-07 FIX: This original `os.rename` is not safe\n            # as it can fail if the file is still being written to.\n            # We need to coordinate with the logger to release the file handle.\n            await self._kc_rotate_log_file_safely(log_file, ingesting_file)\n        except OSError as e:\n            print(f\"[K-Catalog] CRITICAL ERROR: Could not rename log file for ingestion: {e}\")\n            return\n\n        ingested_count = 0\n        with transaction.manager:\n            with open(ingesting_file, 'r') as f:\n                for line in f:\n                    try:\n                        log_entry = json.loads(line)\n                        doc_id = log_entry.get('cycle_id', f\"log_{log_entry.get('timestamp')}\")\n                        self._kc_index_document(catalog_self, doc_id, line, {\"type\": \"metacognition\"}, root)\n                        ingested_count += 1\n                    except (json.JSONDecodeError, AttributeError) as e:\n                        print(f\"[K-Catalog] WARNING: Skipping malformed log entry: {e}\")\n            \n            catalog_self._p_changed = True\n        \n        os.remove(ingesting_file)\n        print(f\"[K-Catalog] Ingestion complete. {ingested_count} cognitive events internalized.\")\n        \n    async def _kc_rotate_log_file_safely(self, old_path: str, new_path: str):\n        \"\"\"\n        Atomically rotates the log file by closing the old handler and\n        creating a new one, then renaming the old file.\n        \"\"\"\n        if not self.uvm_logger or not self.uvm_logger.handlers:\n            raise RuntimeError(\"Logger has no active handlers. Cannot rotate.\")\n\n        old_handler = self.uvm_logger.handlers\n\n        # Close the old handler, which releases the file lock\n        await old_handler.close()\n\n        # Create a new handler pointing to the original log file name\n        new_handler = AsyncFileHandler(filename=old_path)\n        new_handler.formatter = JsonFormatter()\n        self.uvm_logger.handlers = [new_handler]\n\n        # Rename the released old log file\n        os.rename(old_handler.filename, new_path)\n\n\n    def _kc_index_document(self, catalog_self, doc_id: str, doc_text: str, metadata: dict, root):\n        \"\"\"\n        Ingests and indexes a document into the Fractal Memory. Performs semantic\n        chunking based on sentence embedding similarity.\n        \"\"\"\n        # VULN-08 FIX: NLTK data models must be downloaded before use.\n        if not self.nltk_downloaded:\n            raise RuntimeError(\"NLTK data not downloaded. Cannot perform semantic chunking.\")\n\n        if self._v_sentence_model is None:\n            print(\"[K-Catalog] Loading sentence transformer model for semantic chunking...\")\n            self._v_sentence_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n\n        print(f\"[K-Catalog] Indexing document with semantic chunking: {doc_id}\")\n        sentences = nltk.sent_tokenize(doc_text)\n        if not sentences: return\n\n        embeddings = self._v_sentence_model.encode(sentences, convert_to_tensor=True)\n\n        chunks =\n        if len(sentences) > 1:\n            cosine_scores = util.cos_sim(embeddings[:-1], embeddings[1:])\n            breakpoint_percentile = 5\n            threshold = torch.quantile(cosine_scores.diag().cpu(), breakpoint_percentile / 100.0)\n            indices = (cosine_scores.diag() < threshold).nonzero(as_tuple=True)\n\n            start_idx = 0\n            for break_idx in indices:\n                end_idx = break_idx.item() + 1\n                chunks.append(\" \".join(sentences[start_idx:end_idx]))\n                start_idx = end_idx\n            \n            if start_idx < len(sentences):\n                chunks.append(\" \".join(sentences[start_idx:]))\n        else:\n            chunks.append(doc_text)\n\n        self._kc_batch_persist_and_index(catalog_self, doc_id, chunks, metadata, root)\n\n    def _kc_batch_persist_and_index(self, catalog_self, doc_id: str, chunks: List[str], metadata: dict, root):\n        \"\"\"\n        Persists and indexes a list of text chunks in batches to optimize\n        transactional performance.\n        \"\"\"\n        chunk_objects = [\n            UvmObject(parents=[root['traits_obj']], document_id=doc_id, chunk_index=i, text=chunk_text, metadata=metadata)\n            for i, chunk_text in enumerate(chunks)\n        ]\n\n        with transaction.manager:\n            for chunk_obj in chunk_objects:\n                storage_key = f\"{doc_id}::{chunk_obj.chunk_index}\"\n                catalog_self.chunk_storage[storage_key] = chunk_obj\n            \n            transaction.savepoint(True)  \n            \n            chunk_oids =\n            for chunk_obj in chunk_objects:\n                chunk_oid = chunk_obj._p_oid\n                chunk_oids.append(chunk_oid)\n                catalog_self.text_index.index_doc(chunk_oid, chunk_obj.text)\n\n            catalog_self.metadata_index[doc_id] = chunk_oids\n            catalog_self._p_changed = True\n        \n        print(f\"[K-Catalog] Document '{doc_id}' indexed into {len(chunks)} chunks.\")\n\n    def _kc_search(self, catalog_self, query: str, top_k: int = 5):\n        \"\"\"Performs a search against the text index and retrieves chunk objects.\"\"\"\n        results =\n        # VULN-09 FIX: The `apply` method's query needs to be a dictionary.\n        oids_and_scores = catalog_self.text_index.apply(query)\n        for oid in list(oids_and_scores)[:top_k]:\n            obj = self.connection.get(int(oid))\n            if obj:\n                results.append(obj)\n        return results\n\n    def _uvm_compile_schema_from_codex(self, schema_name: str) -> Optional[type]:\n        \"\"\"\n        Dynamically and safely compiles a Pydantic schema string from the\n        Persona Codex into an executable class.\n        \"\"\"\n        try:\n            # VULN-05 FIX: The schema is now stored on the root in a dictionary\n            schema_string = self.root['data_covenants'][schema_name]\n            isolated_globals = {'pydantic': pydantic, 'BaseModel': BaseModel, 'Field': Field}\n            from typing import List, Dict, Literal\n            isolated_globals.update({'List': List, 'Dict': Dict, 'Literal': Literal})\n            # VULN-10 FIX: This line was missing, causing a NameError\n            local_namespace = {}\n            exec(schema_string, isolated_globals, local_namespace)\n            for item_name, item_value in local_namespace.items():\n                if isinstance(item_value, type) and issubclass(item_value, BaseModel) and item_name!= 'BaseModel':\n                    return item_value\n            return None\n        except Exception as e:\n            print(f\"ERROR: Failed to compile schema '{schema_name}': {e}\")\n            return None\n\n\n    async def _orc_start_cognitive_cycle(self, orchestrator_self, mission_brief: dict, target_obj_oid: str):\n        \"\"\"\n        Factory method for creating and starting a new cognitive cycle.\n        This method is called from within a worker's transaction.\n        \"\"\"\n        print(f\"[Orchestrator] Initiating new cognitive cycle for mission: {mission_brief.get('selector', 'unknown')}\")\n        \n        root = orchestrator_self._p_jar.root()\n        psm_prototypes = root['psm_prototypes_obj']\n\n        cycle_context = UvmObject(\n            parents=[root['traits_obj']],\n            mission_brief=mission_brief,\n            target_oid=target_obj_oid,\n            synthesis_state=psm_prototypes.IDLE,\n            retries=0, # BUG-17 FIX: Add a retry counter to prevent infinite self-correction loops.[5, 6]\n            _tmp_synthesis_data=persistent.mapping.PersistentMapping()\n        )\n        if 'active_cycles' not in root:\n            root['active_cycles'] = BTrees.OOBTree.BTree()\n        \n        if '_tmp_new_objects' not in root:\n            root['_tmp_new_objects'] =\n        root['_tmp_new_objects'].append(cycle_context)\n\n        transaction.savepoint(True)\n        root['_tmp_new_objects'].pop()\n\n        cycle_oid = cycle_context._p_oid\n        root['active_cycles'][cycle_oid] = cycle_context\n        root._p_changed = True\n        \n        print(f\"[Orchestrator] New CognitiveCycle created with OID: {cycle_oid}\")\n        \n        await self._psm_run_cycle(cycle_context)\n        return cycle_context\n\n    async def _psm_run_cycle(self, cycle_context):\n        \"\"\"\n        Main execution loop for a single cognitive cycle. This runs inside the\n        worker's transaction, ensuring the entire thought process is atomic.\n        \"\"\"\n        try:\n            current_state_name = cycle_context.synthesis_state.name\n            while current_state_name not in:\n                state_prototype = cycle_context.synthesis_state\n                await state_prototype._process_synthesis_(state_self=state_prototype, cycle_context=cycle_context)\n                current_state_name = cycle_context.synthesis_state.name\n            \n            final_state = cycle_context.synthesis_state\n            await final_state._process_synthesis_(state_self=final_state, cycle_context=cycle_context)\n        except Exception as e:\n            print(f\"ERROR during PSM cycle {cycle_context._p_oid}: {e}. Transitioning to FAILED.\")\n            traceback.print_exc()\n            root = cycle_context._p_jar.root()\n            cycle_context.synthesis_state = root['psm_prototypes_obj'].FAILED\n            await root['psm_prototypes_obj'].FAILED._process_synthesis_(state_self=root['psm_prototypes_obj'].FAILED, cycle_context=cycle_context)\n\n    async def _psm_transition_to(self, cycle_context, new_state_prototype):\n        \"\"\"Helper function to perform a state transition.\"\"\"\n        print(f\"Cycle {cycle_context._p_oid} transitioning to state: {new_state_prototype.name}\")\n        cycle_context.synthesis_state = new_state_prototype\n        cycle_context._p_changed = True\n\n    async def _psm_log_event(self, cycle_context, event_type, data=None):\n        \"\"\"Helper to log metacognitive events related to a PSM cycle.\"\"\"\n        psm_data = {\n            \"cycle_id\": str(getattr(cycle_context, '_p_oid', 'transient')),\n            \"mission_brief_hash\": hashlib.sha256(json.dumps(cycle_context.mission_brief, sort_keys=True).encode()).hexdigest(),\n            \"current_state\": cycle_context.synthesis_state.name,\n        }\n        if data:\n            psm_data.update(data)\n        \n        await self._uvm_log_event(event_type, \"PSM\", psm_data)\n\n    async def _psm_idle_process(self, state_self, cycle_context):\n        root = cycle_context._p_jar.root()\n        await self._psm_log_event(cycle_context, \"STATE_TRANSITION\", {\"transition_to\": \"DECOMPOSING\"})\n        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].DECOMPOSING)\n        cycle_context._p_changed = True\n\n    async def _psm_decomposing_process(self, state_self, cycle_context):\n        \"\"\"\n        DECOMPOSING State: Handles multiple mission types. For standard\n        missions, it creates a research plan. For self-correction missions,\n        it constructs a targeted prompt to fix a failed artifact.\n        \"\"\"\n        root = cycle_context._p_jar.root()\n        mission = cycle_context.mission_brief\n\n        if mission.get(\"type\") == \"self_correction_mandate\":\n            print(f\"Cycle {cycle_context._p_oid}: Decomposing self-correction mandate...\")\n            prompt = f\"\"\"\n            MISSION: Correct the following data artifact.\n            The artifact failed validation with the following error:\n            --- VALIDATION ERROR ---\n            {mission['validation_error']}\n            \n            --- FAILED ARTIFACT ---\n            {mission['original_artifact']}\n            \n            Your task is to analyze the error and the failed artifact, then generate a new, corrected version of the artifact that strictly adheres to all validation rules implied by the error message.\n            Output ONLY the corrected artifact.\n            \"\"\"\n            cycle_context._tmp_synthesis_data['synthesis_prompt'] = prompt\n            await self._psm_log_event(cycle_context, \"ARTIFACT_GENERATED\", {\"artifact_type\": \"correction_prompt\"})\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].SYNTHESIZING)\n            cycle_context._p_changed = True\n            return\n\n        print(f\"Cycle {cycle_context._p_oid}: Decomposing mission into a research plan...\")\n        mission_objective = mission.get('selector') or mission.get('intent')\n        prompt = f\"\"\"\n        Analyze the following mission brief: \"{mission_objective}\"\n        Your task is to generate a concise list of 1 to 3 search query strings that can be used to find relevant information in a vector database.\n        Focus on the essential nouns and technical terms in the mission.\n        Output ONLY a JSON list of strings, like [\"query 1\", \"query 2\"].\n        \"\"\"\n        \n        brick_prototype = root['brick_prototype_obj']\n        plan_str = await root['pLLM_obj'].infer_(root['pLLM_obj'], prompt, persona_self=brick_prototype)\n        \n        try:\n            search_queries = json.loads(plan_str)\n            cycle_context._tmp_synthesis_data['research_plan'] = search_queries\n            await self._psm_log_event(\n                cycle_context,  \n                \"ARTIFACT_GENERATED\",  \n                {\n                    \"artifact_type\": \"research_plan\",  \n                    \"prompt\": prompt,  \n                    \"queries\": search_queries\n                }\n            )\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].DELEGATING)\n        except json.JSONDecodeError:\n            print(f\" Decomposing failed to produce valid JSON: {plan_str}\")\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].FAILED)\n        \n        cycle_context._p_changed = True\n\n    async def _psm_delegating_process(self, state_self, cycle_context):\n        \"\"\"\n        DELEGATING State: Executes the research plan by querying the\n        Knowledge Catalog (_kc_search).\n        \"\"\"\n        print(f\"Cycle {cycle_context._p_oid}: Executing research plan...\")\n        root = cycle_context._p_jar.root()\n        \n        search_queries = cycle_context._tmp_synthesis_data.get('research_plan',)\n        if not search_queries:\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].SYNTHESIZING)\n            cycle_context._p_changed = True\n            return\n\n        k_catalog = root['knowledge_catalog_obj']\n        retrieved_context =\n        for query in search_queries:\n            print(f\"  - Searching Fractal Memory for: '{query}'\")\n            results = k_catalog.search_(k_catalog, query, top_k=2)\n            for chunk in results:\n                retrieved_context.append(chunk.text)\n        \n        unique_context = list(dict.fromkeys(retrieved_context))\n        cycle_context._tmp_synthesis_data['retrieved_context'] = unique_context\n        await self._psm_log_event(cycle_context, \"RESEARCH_COMPLETE\", {\"context_snippets\": len(unique_context)})\n        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].SYNTHESIZING)\n        cycle_context._p_changed = True\n\n    async def _psm_synthesizing_process(self, state_self, cycle_context):\n        \"\"\"\n        SYNTHESIZING State: Generates the final artifact. It can either use a\n        pre-made prompt (from a self-correction cycle) or construct one\n        from context retrieved from the Fractal Memory.\n        \"\"\"\n        print(f\"Cycle {cycle_context._p_oid}: Synthesizing artifact...\")\n        root = cycle_context._p_jar.root()\n        target_obj = self.connection.get(int(cycle_context.target_oid))\n\n        if not target_obj:\n            raise ValueError(f\"Target object OID {cycle_context.target_oid} not found.\")\n\n        mission = cycle_context.mission_brief\n        prompt = \"\"\n\n        if 'synthesis_prompt' in cycle_context._tmp_synthesis_data:\n            print(\" - Using pre-constructed synthesis prompt.\")\n            prompt = cycle_context._tmp_synthesis_data['synthesis_prompt']\n        else:\n            print(\" - Constructing prompt from retrieved context.\")\n            context_snippets = cycle_context._tmp_synthesis_data.get('retrieved_context',)\n            context_block = \"No relevant context found in Fractal Memory.\"\n            if context_snippets:\n                formatted_snippets = \"\\n\".join([f\"- {s}\" for s in context_snippets])\n                context_block = f\"\"\"\nUse the following information retrieved from the Fractal Memory to inform your response:\n---\n{formatted_snippets}\n---\n\"\"\"\n            mission_objective = mission.get('selector') or mission.get('intent')\n            prompt = f\"\"\"\n            Mission: Generate Python code for a method named '{mission_objective}'.\n            {context_block}\n            Adhere to all architectural covenants, including the Persistence Covenant.\n            Generate only the raw Python code for the method.\n            \"\"\"\n\n        generated_artifact = await root['pLLM_obj'].infer_(root['pLLM_obj'], prompt, persona_self=target_obj)\n        cycle_context._tmp_synthesis_data['generated_artifact'] = generated_artifact\n        await self._psm_log_event(\n            cycle_context,  \n            \"ARTIFACT_GENERATED\",  \n            {\n                \"type\": \"code\",\n                \"prompt\": prompt,  \n                \"content\": generated_artifact\n            }\n        )\n        await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].VALIDATING)\n        cycle_context._p_changed = True\n\n    async def _psm_validating_process(self, state_self, cycle_context):\n        print(f\"Cycle {cycle_context._p_oid}: Validating artifact...\")\n        root = cycle_context._p_jar.root()\n        \n        artifact = cycle_context._tmp_synthesis_data.get('generated_artifact')\n        try:\n            PersistenceGuardian.audit_code(artifact)\n            await self._psm_log_event(cycle_context, \"VALIDATION_SUCCESS\", {\"guardian\": \"PersistenceGuardian\"})\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].COMPLETE)\n        except (CovenantViolationError, pydantic.ValidationError if 'pydantic' in sys.modules else Exception) as e:\n            print(f\"Cycle {cycle_context._p_oid}: VALIDATION FAILED: {e}\")\n            cycle_context._tmp_synthesis_data['validation_error'] = str(e)\n            await self._psm_log_event(cycle_context, \"VALIDATION_FAILURE\", {\"error\": str(e)})\n            await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].FAILED)\n        cycle_context._p_changed = True\n\n    async def _psm_complete_process(self, state_self, cycle_context):\n        \"\"\"COMPLETE State: Cleans up and finalizes the transaction.\"\"\"\n        root = cycle_context._p_jar.root()\n\n        print(f\"Cycle {cycle_context._p_oid}: Cycle completed successfully.\")\n        mission = cycle_context.mission_brief\n        target_obj = self.connection.get(int(cycle_context.target_oid))\n        if target_obj:\n            generated_code = cycle_context._tmp_synthesis_data['generated_artifact']\n            method_name = mission['selector']\n            try:\n                namespace = {}\n                exec(generated_code, globals(), namespace)\n                method_obj = namespace[method_name]\n                target_obj._slots[method_name] = method_obj\n                target_obj._p_changed = True\n                print(f\"New method '{method_name}' successfully installed on OID {target_obj._p_oid}.\")\n            except Exception as e:\n                print(f\"ERROR during code installation: {e}\")\n                await self._psm_transition_to(cycle_context, root['psm_prototypes_obj'].FAILED)\n                return\n\n        await self._psm_log_event(cycle_context, \"FINAL_OUTCOME\", {\"outcome\": \"COMPLETE\"})\n        if cycle_context._p_oid in root.get('active_cycles', {}):\n            del root['active_cycles'][cycle_context._p_oid]\n        root._p_changed = True\n\n    async def _psm_failed_process(self, state_self, cycle_context):\n        \"\"\"\n        FAILED State: Logs the error and, if the failure is a known,\n        correctable type (like a ValidationError), it initiates an autonomous\n        self-correction cycle before dooming the original transaction.\n        \"\"\"\n        root = cycle_context._p_jar.root()\n        validation_error_context = cycle_context._tmp_synthesis_data.get('validation_error')\n        \n        if validation_error_context and cycle_context.retries < 5:\n            print(f\"Cycle {cycle_context._p_oid}: Validation failed. Initiating self-correction cycle.\")\n            cycle_context.retries += 1\n            cycle_context._p_changed = True\n            await self._psm_log_event(cycle_context, \"SELF_CORRECTION_INITIATED\", {\"error\": validation_error_context, \"attempt\": cycle_context.retries})\n\n            corrective_mission = {\n                \"type\": \"self_correction_mandate\",\n                \"selector\": \"correct_generated_artifact\",\n                \"original_artifact\": cycle_context._tmp_synthesis_data.get('generated_artifact'),\n                \"validation_error\": validation_error_context\n            }\n            \n            target_oid = cycle_context.target_oid\n            \n            await root['orchestrator_obj'].start_cognitive_cycle_for_(\n                root['orchestrator_obj'],\n                corrective_mission,\n                target_oid\n            )\n        else:\n            if cycle_context.retries >= 5:\n                print(f\"Cycle {cycle_context._p_oid}: Exceeded maximum self-correction retries. Aborting transaction.\")\n                await self._psm_log_event(cycle_context, \"FINAL_OUTCOME\", {\"outcome\": \"FAILED\", \"reason\": \"Max retries exceeded\"})\n            else:\n                print(f\"Cycle {cycle_context._p_oid}: Unrecoverable error. Aborting transaction.\")\n                await self._psm_log_event(cycle_context, \"FINAL_OUTCOME\", {\"outcome\": \"FAILED\"})\n            \n            transaction.doom()\n            \n            if cycle_context._p_oid in root.get('active_cycles', {}):\n                del root['active_cycles'][cycle_context._p_oid]\n            root._p_changed = True\n\n    # --------------------------------------------------------------------------\n    # Subsection III.D: Asynchronous Core & System Lifecycle\n    # --------------------------------------------------------------------------\n\n    async def worker(self, name: str):\n        \"\"\"\n        Pulls messages from the queue and processes them in a transactional\n        context, ensuring every operation is atomic.\n        \"\"\"\n        print(f\"[{name}] Worker started.\")\n        conn = self.db.open()\n        root = conn.root()\n        while not self.should_shutdown.is_set():\n            try:\n                identity, message_data = await asyncio.wait_for(self.message_queue.get(), timeout=1.0)\n                print(f\"[{name}] Processing message from {identity.decode() if identity!= b'UVM_INTERNAL' else 'UVM_INTERNAL'}\")\n                try:\n                    with transaction.manager:\n                        command_payload = ormsgpack.unpackb(message_data)\n                        command = command_payload.get(\"command\")\n                        if command == \"initiate_cognitive_cycle\":\n                            await root['orchestrator_obj'].start_cognitive_cycle_for_(\n                                root['orchestrator_obj'],\n                                command_payload['mission_brief'],\n                                command_payload['target_oid']\n                            )\n                        # NEW: Add a handler for direct ingestion commands\n                        elif command == \"ingest_document\":\n                            print(f\"[{name}] Received direct ingestion command.\")\n                            k_catalog = root['knowledge_catalog_obj']\n                            self._kc_index_document(\n                                k_catalog,\n                                command_payload['doc_id'],\n                                command_payload['doc_text'],\n                                command_payload['metadata'],\n                                root # VULN-12 FIX: The root is needed\n                            )\n                except Exception as e:\n                    print(f\"[{name}] ERROR processing message: {e}\")\n                    traceback.print_exc()\n                finally:\n                    self.message_queue.task_done()\n            except asyncio.TimeoutError:\n                continue\n            except asyncio.CancelledError:\n                break\n        conn.close()\n        print(f\"[{name}] Worker stopped.\")\n\n    async def zmq_listener(self):\n        \"\"\"Listens on the ZMQ ROUTER socket for incoming messages.\"\"\"\n        self.zmq_socket.bind(ZMQ_ENDPOINT)\n        print(f\"[UVM] Synaptic Bridge listening on {ZMQ_ENDPOINT}\")\n        while not self.should_shutdown.is_set():\n            try:\n                message_parts = await self.zmq_socket.recv_multipart()\n                if len(message_parts) == 2:\n                    identity, message_data = message_parts\n                    await self.message_queue.put((identity, message_data))\n                else:\n                    print(f\"[ZMQ] Received malformed message: {message_parts}\")\n            except zmq.error.ZMQError as e:\n                if e.errno == zmq.ETERM: break\n                else: raise\n            except asyncio.CancelledError:\n                break\n        print(\"[UVM] ZMQ listener stopped.\")\n\n    async def autotelic_loop(self):\n        \"\"\"\n        The system's 'heartbeat' for self-directed evolution. This loop\n        drives the metacognitive cycle of log ingestion and self-auditing,\n        enabling second-order autopoiesis.\n        \"\"\"\n        print(\"[UVM] Autotelic Heartbeat started.\")\n        await asyncio.sleep(60)\n\n        while not self.should_shutdown.is_set():\n            try:\n                print(\"[UVM] Autotelic Heartbeat: Ingesting recent cognitive history...\")\n                conn = self.db.open()\n                root = conn.root()\n                try:\n                    with transaction.manager:\n                        k_catalog = root['knowledge_catalog_obj']\n                        await self._uvm_log_event(\"LOG_INGEST_START\", \"Autotelic\", {\"log_file\": METACOGNITION_LOG_FILE})\n                        await k_catalog.rotate_and_ingest_log_(k_catalog, root)\n                        await self._uvm_log_event(\"LOG_INGEST_SUCCESS\", \"Autotelic\", {\"log_file\": METACOGNITION_LOG_FILE})\n\n                except Exception as e:\n                    print(f\"[UVM] ERROR during autotelic ingestion: {e}\")\n                    traceback.print_exc()\n                finally:\n                    conn.close()\n\n                await asyncio.sleep(3600)\n\n                print(\"[UVM] Autotelic Heartbeat: Triggering self-audit on ingested data.\")\n                # The audit logic is a placeholder for now, to be implemented\n                await asyncio.sleep(3600)\n\n            except asyncio.CancelledError:\n                break\n        print(\"[UVM] Autotelic Heartbeat stopped.\")\n\n    def _signal_handler(self, sig, frame):\n        \"\"\"Handles signals like SIGTERM for graceful shutdown.\"\"\"\n        print(f\"\\n[UVM] Received signal {sig}. Initiating graceful shutdown...\")\n        self.should_shutdown.set()\n\n    def launch_ui(self):\n        \"\"\"Placeholder for launching the generated Kivy UI.\"\"\"\n        print(\"[UVM] Placeholder: Launching Kivy UI...\")\n        if 'ui_code' in self.root['genesis_obj']._slots:\n            print(\"[UVM] UI code found in Living Image.\")\n        else:\n            print(\"[UVM] ERROR: UI code not found, cannot launch.\")\n\n    async def run(self):\n        \"\"\"Main entry point to start all UVM services.\"\"\"\n        await self.initialize_system()\n        signal.signal(signal.SIGINT, self._signal_handler)\n        signal.signal(signal.SIGTERM, self._signal_handler)\n\n        print(\"[UVM] Starting background tasks (workers, listener, heartbeat)...\")\n        listener_task = asyncio.create_task(self.zmq_listener())\n        autotelic_task = asyncio.create_task(self.autotelic_loop())\n        worker_tasks =\n        \n        if 'ui_code' not in self.root['genesis_obj']._slots:\n            print(\"[UVM] First Conversation: Initiating two-cycle introspective genesis...\")\n\n            meta_mission_brief = {\n                \"type\": \"generate_genesis_prompt\",\n                \"selector\": \"describe_how_to_display_yourself\",\n                \"args\":, \"kwargs\": {}\n            }\n            command_payload_1 = {\n                \"command\": \"initiate_cognitive_cycle\",\n                \"target_oid\": str(self.root['alfred_prototype_obj']._p_oid),\n                \"mission_brief\": meta_mission_brief\n            }\n            await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload_1)))\n            await self.message_queue.join()\n\n            self.connection.sync()\n            if 'genesis_prompt' in self.root['genesis_obj']._slots:\n                genesis_prompt = self.root['genesis_obj']._slots['genesis_prompt']\n                ui_mission_brief = {\n                    \"type\": \"genesis_protocol\",\n                    \"selector\": \"display_yourself\",\n                    \"intent\": genesis_prompt,\n                    \"args\":, \"kwargs\": {}\n                }\n                command_payload_2 = {\n                    \"command\": \"initiate_cognitive_cycle\",\n                    \"target_oid\": str(self.root['genesis_obj']._p_oid),\n                    \"mission_brief\": ui_mission_brief\n                }\n                await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload_2)))\n                await self.message_queue.join()\n                \n                self.connection.sync()\n                if 'ui_code' in self.root['genesis_obj']._slots:\n                    self.launch_ui()\n                else:\n                    print(\"[UVM] ERROR: UI generation cycle failed to install UI code.\")\n            else:\n                print(\"[UVM] ERROR: Meta-prompt generation cycle failed.\")\n        else:\n            print(\"[UVM] UI already exists in Living Image. Launching...\")\n            self.launch_ui()\n\n        print(\"[UVM] System is live. Awaiting Architect's command...\")\n        await self.should_shutdown.wait()\n\n        print(\"[UVM] Shutdown signal received. Terminating tasks...\")\n        listener_task.cancel()\n        autotelic_task.cancel()\n        for task in worker_tasks:\n            task.cancel()\n        await asyncio.gather(listener_task, autotelic_task, *worker_tasks, return_exceptions=True)\n\n        await self.shutdown()\n\n    async def shutdown(self):\n        \"\"\"Gracefully shuts down the UVM and ZODB connection.\"\"\"\n        print(\"[UVM] System shutting down...\")\n        self.zmq_socket.close()\n        self.zmq_context.term()\n        await self.message_queue.join()\n        \n        if self.uvm_logger:\n            await self.uvm_logger.shutdown()\n        if self.persona_loggers:\n            for logger in self.persona_loggers.values():\n                await logger.shutdown()\n                \n        transaction.commit()\n        self.connection.close()\n        self.db.close()\n        print(\"[UVM] Shutdown complete.\")\n\nif __name__ == \"__main__\":\n    uvm = BatOS_UVM(DB_FILE, BLOB_DIR)\n    \n    signal.signal(signal.SIGINT, uvm._signal_handler)\n    signal.signal(signal.SIGTERM, uvm._signal_handler)\n    \n    try:\n        asyncio.run(uvm.run())\n    except KeyboardInterrupt:\n        print(\"\\n[UVM] Main loop interrupted by user.\")\n    finally:\n        # VULN-16 FIX: Corrected the case of the method name to is_closed()\n        # and added a hasattr check for safety.\n        if hasattr(uvm, 'db') and uvm.db and not uvm.db.is_closed():\n            print(\"[UVM_CLEANUP] Ensuring database connection is closed after exit.\")\n            uvm.db.close()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}