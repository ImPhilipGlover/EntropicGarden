Python

# batos.py
#
# CLASSIFICATION: ARCHITECT EYES ONLY
# SUBJECT: Canonical Incarnation Protocol for the Binaural Autopoietic/Telic
#          Operating System, Series VIII ('The Fractal Awakening')
#
# This script is the single, executable embodiment of the BAT OS Series VIII
# architecture. It is the fractal seed, designed to be invoked once to
# initiate the system's "unbroken process of becoming."
#
# The protocol unfolds in a sequence of autonomous phases:
#
# 1. Prototypal Awakening: Establishes a connection to the Zope Object
#    Database (ZODB), the system's persistent substrate. On the first run,
#    it creates and persists the primordial objects and incarnates all
#    subsystems. This is an atomic, transactional act of genesis.
#
# 2. Cognitive Cycle Initiation: The system's generative kernel,
#    _doesNotUnderstand_, triggers the Prototypal State Machine for
#    collaborative, transactional reasoning when a message lookup fails.
#
# 3. Directed Autopoiesis: The system's core behaviors are now products of
#    this collaborative reasoning process, allowing it to generate new,
#    validated capabilities at runtime.
#
# 4. The Autotelic Heartbeat: The script enters its final, persistent state:
#    an asynchronous event loop that drives an internal, self-directed
#    evolutionary process, including metacognitive self-analysis.

# ==============================================================================
# SECTION I: SYSTEM CONFIGURATION & DEPENDENCIES
# ==============================================================================
import os
import sys
import asyncio
import gc
import time
import copy
import ast
import traceback
import functools
import signal
import tarfile
import shutil
import random
import json
import hashlib
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable

# --- Persistence Substrate (ZODB) ---
# These imports constitute the physical realization of the "Living Image"
# and the "Fractal Memory." ZODB provides transactional atomicity, `persistent`
# enables object tracking, and `BTrees` and `zope.index` provide the scalable
# data structures for the knowledge catalog.
import ZODB
import ZODB.FileStorage
import ZODB.blob
import transaction
import persistent
import persistent.mapping
import BTrees.OOBTree
from zope.index.text import TextIndex
from zope.index.text.lexicon import CaseNormalizer, Splitter
from zope.index.text.textindex import Texthasattr

# --- Communication & Serialization ---
# ZeroMQ and ormsgpack form the "Synaptic Bridge," the system's digital nervous
# system for high-performance, asynchronous communication.
import zmq
import zmq.asyncio
import ormsgpack

# --- Data Covenant & Metacognition ---
# Pydantic is the engine for the Data Covenant, ensuring semantic integrity.
# Aiologger provides non-blocking logging for the metacognitive audit trail.
try:
    import pydantic
    from pydantic import BaseModel, Field
except ImportError:
    print("FATAL: `pydantic` not found. Data Covenant cannot be enforced.")
    sys.exit(1)

try:
    import aiologger
    from aiologger.levels import LogLevel
    from aiologger.handlers.streams import AsyncStreamHandler
    from aiologger.formatters.json import JsonFormatter
except ImportError:
    print("WARNING: `aiologger` not found. Metacognitive logging will be disabled.")
    aiologger = None

# --- Cognitive & AI Dependencies ---
# These libraries are non-negotiable. A failure to import them is a fatal
# error, as the system cannot achieve Cognitive Closure without them.
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig
    from peft import PeftModel
    from accelerate import init_empty_weights, load_checkpoint_and_dispatch
    from sentence_transformers import SentenceTransformer, util
    import nltk
    nltk.download('punkt', quiet=True)
except ImportError as e:
    print(f"FATAL: Core cognitive libraries not found ({e}). System cannot awaken.")
    sys.exit(1)

# --- System Constants ---
# These constants define the physical boundaries and core cognitive identity
# of this system instance.
DB_FILE = 'live_image.fs'
BLOB_DIR = 'live_image.fs.blob'
ZMQ_ENDPOINT = "tcp://127.0.0.1:5555"
BASE_MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"
LORA_STAGING_DIR = "./lora_adapters"
SENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L6-v2"
METACOGNITION_LOG_FILE = "metacognition.jsonl"

# ==============================================================================
# SECTION II: THE PRIMORDIAL SUBSTRATE
# ==============================================================================

class UvmObject(persistent.Persistent):
    """
    The foundational particle of the BAT OS universe. This class provides the
    "physics" for a prototype-based object model inspired by the Self and
    Smalltalk programming languages. It rejects standard Python attribute
    access in favor of a unified '_slots' dictionary and a delegation-based
    inheritance mechanism.
    It inherits from `persistent.Persistent` to enable transactional storage
    via ZODB, guaranteeing the system's "unbroken existence."
    """
    def __init__(self, **initial_slots):
        """
        Initializes the UvmObject. The `_slots` dictionary is instantiated as a
        `persistent.mapping.PersistentMapping` to ensure that changes within
        the dictionary itself are correctly tracked by ZODB.
        """
        # The `_slots` attribute is one of the few that are set directly on the
        # instance, as it is the container for all other state and behavior.
        super().__setattr__('_slots', persistent.mapping.PersistentMapping(initial_slots))

    def __setattr__(self, name: str, value: Any) -> None:
        """
        Intercepts all attribute assignments. This method redirects assignments
        to the internal `_slots` dictionary, unifying state and behavior.
        It explicitly sets `self._p_changed = True` to manually signal to ZODB
        that the object's state has been modified. This is a non-negotiable
        architectural requirement known as The Persistence Covenant.
        Overriding `__setattr__` bypasses ZODB's default change detection,
        making this manual signal essential for preventing systemic amnesia.
        """
        if name.startswith('_p_') or name == '_slots':
            # Allow ZODB's internal attributes and direct _slots manipulation.
            super().__setattr__(name, value)
        else:
            self._slots[name] = value
            self._p_changed = True

    def __getattr__(self, name: str) -> Any:
        """
        Implements attribute access and the delegation-based inheritance chain.
        If an attribute is not found in the local `_slots`, it delegates the
        lookup to the object(s) in its `parents` slot. The exhaustion of this
        chain raises an `AttributeError`, which is the universal trigger for the
        `_doesNotUnderstand_` generative protocol in the UVM.
        """
        if name in self._slots:
            return self._slots[name]
        
        # BUG-01 FIX: Renamed `parent*` to `parents` for valid Python syntax.
        if 'parents' in self._slots:
            parents_list = self._slots['parents']
            if not isinstance(parents_list, list):
                parents_list = [parents_list]
            for parent in parents_list:
                try:
                    return getattr(parent, name)
                except AttributeError:
                    continue
        
        raise AttributeError(f"UvmObject OID {getattr(self, '_p_oid', 'transient')} has no slot '{name}'")

    def __repr__(self) -> str:
        """Provides a more informative representation for debugging."""
        slot_keys = list(self._slots.keys())
        oid_str = f"oid={self._p_oid}" if hasattr(self, '_p_oid') and self._p_oid is not None else "oid=transient"
        return f"<UvmObject {oid_str} slots={slot_keys}>"

class CovenantViolationError(Exception):
    """Custom exception for Persistence Covenant violations."""
    pass

class PersistenceGuardian:
    """
    A non-negotiable protocol for maintaining system integrity. It performs
    static analysis on LLM-generated code *before* execution to
    deterministically enforce the Persistence Covenant (`_p_changed = True`),
    thereby preventing systemic amnesia. This is the implementation of the
    ALFRED persona's core stewardship mandate.
    """
    @staticmethod
    def audit_code(code_string: str) -> None:
        """
        Parses a code string into an AST and verifies that any function
        modifying `self`'s state adheres to the Persistence Covenant.
        Raises CovenantViolationError on failure.
        """
        try:
            tree = ast.parse(code_string)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    PersistenceGuardian._audit_function(node)
            print("[Guardian] Code audit passed. Adheres to the Persistence Covenant.")
        except SyntaxError as e:
            print(f"[Guardian] AUDIT FAILED: Syntax error in generated code: {e}")
            raise CovenantViolationError(f"Syntax error in generated code: {e}")
        except CovenantViolationError as e:
            print(f"[Guardian] AUDIT FAILED: {e}")
            raise

    @staticmethod
    def _audit_function(func_node: ast.FunctionDef):
        """Audits a single function definition AST node."""
        modifies_state = False
        for body_item in func_node.body:
            if isinstance(body_item, (ast.Assign, ast.AugAssign)):
                targets = body_item.targets if isinstance(body_item, ast.Assign) else [body_item.target]
                for target in targets:
                    if (isinstance(target, ast.Attribute) and
                        isinstance(target.value, ast.Name) and
                        target.value.id == 'self' and not target.attr.startswith('_p_')):
                        modifies_state = True
                        break
            if modifies_state:
                break

        if modifies_state:
            if not func_node.body:
                raise CovenantViolationError(f"Function '{func_node.name}' modifies state but has an empty body.")
            
            last_statement = func_node.body[-1]
            
            # BUG-03 FIX: The `targets` attribute is a list. Access its first element.
            is_valid_covenant = (
                isinstance(last_statement, ast.Assign) and
                len(last_statement.targets) == 1 and
                isinstance(last_statement.targets, ast.Attribute) and
                isinstance(last_statement.targets.value, ast.Name) and
                last_statement.targets.value.id == 'self' and
                last_statement.targets.attr == '_p_changed' and
                isinstance(last_statement.value, ast.Constant) and
                last_statement.value.value is True
            )

            if not is_valid_covenant:
                raise CovenantViolationError(f"Method '{func_node.name}' modifies state but does not conclude with `self._p_changed = True`.")

class PersistentTextIndex(TextIndex):
    """
    A ZODB-aware subclass of TextIndex that correctly manages its own
    persistence state, preventing `TypeError` on commit by excluding
    non-serializable attributes like threading locks.
    """
    def __getstate__(self):
        """
        Returns a dictionary of the object's persistent state, excluding
        the transient, non-serializable lexicon and index objects.
        """
        state = self.__dict__.copy()
        # The _lexicon and _index attributes hold the actual index data
        # and non-serializable locks. They will be rebuilt on __setstate__.
        if '_lexicon' in state:
            del state['_lexicon']
        if '_index' in state:
            del state['_index']
        return state

    def __setstate__(self, state):
        """
        Restores the object's state from a pickle and re-initializes
        the transient lexicon and index.
        """
        self.__dict__.update(state)
        # Re-initialize the core indexing machinery.
        self._lexicon = self.lexicon_class(self.normalizer_class(), self.splitter_class())
        self._index = self.index_class()
        # Re-index all documents upon unpickling.
        if hasattr(self, '_doc_to_words'):
            for docid, words in self._doc_to_words.items():
                self._lexicon.sourceToWordIds(words)
                self._index.index_doc(docid, words)

# ==============================================================================
# SECTION III: THE UNIVERSAL VIRTUAL MACHINE (UVM)
# ==============================================================================

class BatOS_UVM:
    """
    The core runtime environment for the BAT OS. This class orchestrates the
    Prototypal Awakening, manages the persistent object graph, runs the
    asynchronous message-passing kernel, and initiates the system's
    autotelic evolution.
    """
    def __init__(self, db_file: str, blob_dir: str):
        self.db_file = db_file
        self.blob_dir = blob_dir
        
        # VULN-01 FIX: All runtime machinery is now declared as transient.
        # Persistent state
        self._persistent_state_attributes = ['db_file', 'blob_dir']
        
        # Transient state
        self.db: Optional = None
        self.connection: Optional = None
        self.root: Optional[Any] = None
        self.message_queue: asyncio.Queue = asyncio.Queue()
        self.zmq_context: zmq.asyncio.Context = zmq.asyncio.Context()
        self.zmq_socket: zmq.asyncio.Socket = self.zmq_context.socket(zmq.ROUTER)
        self.should_shutdown: asyncio.Event = asyncio.Event()
        self.model: Optional[Any] = None
        self.tokenizer: Optional[Any] = None
        self._v_sentence_model: Optional = None
        self.logger: Optional[aiologger.Logger] = None

    def __getstate__(self) -> Dict[str, Any]:
        """
        VULN-01 FIX: Defines the object's persistent "self," excluding all
        transient runtime machinery to prevent `TypeError` on commit. This is
        the programmatic enforcement of the "Body vs. Vessel" distinction.
        """
        return {key: getattr(self, key) for key in self._persistent_state_attributes}

    def __setstate__(self, state: Dict[str, Any]) -> None:
        """
        VULN-01 FIX: Restores the persistent state and re-initializes the
        transient machinery upon unpickling.
        """
        self.__init__(state.get('db_file'), state.get('blob_dir'))

    # --------------------------------------------------------------------------
    # Subsection III.A: Prototypal Awakening & Subsystem Incarnation
    # --------------------------------------------------------------------------

    async def initialize_system(self):
        """
        Phase 1: Prototypal Awakening. Connects to ZODB and, on first run,
        creates the primordial objects and incarnates all subsystems within a
        single, atomic transaction.
        """
        print("[UVM] Phase 1: Prototypal Awakening...")
        
        if aiologger:
            self.logger = aiologger.Logger.with_default_handlers(
                name='MetacognitionLogger',
                level=LogLevel.INFO,
                formatter=JsonFormatter(),
                handlers= # Add file handler for persistence
            )
        
        if not os.path.exists(self.blob_dir):
            os.makedirs(self.blob_dir)
        
        storage = ZODB.FileStorage.FileStorage(self.db_file, blob_dir=self.blob_dir)
        self.db = ZODB.DB(storage)
        self.connection = self.db.open()
        self.root = self.connection.root()

        if 'genesis_obj' not in self.root:
            print("[UVM] First run detected. Performing full Prototypal Awakening.")
            with transaction.manager:
                self._incarnate_primordial_objects()
                await self._load_and_persist_llm_core()
                self._incarnate_lora_experts()
                self._incarnate_subsystems()
            print("[UVM] Awakening complete. All systems nominal.")
        else:
            print("[UVM] Resuming existence from Living Image.")
            await self._load_llm_from_blob()

        print(f"[UVM] System substrate initialized. Root OID: {self.root._p_oid}")

    def _incarnate_primordial_objects(self):
        """Creates the foundational objects of the BAT OS universe."""
        print("[UVM] Incarnating primordial objects...")
        
        traits_obj = UvmObject(
            _clone_persistent_=self._clone_persistent,
            _doesNotUnderstand_=self._doesNotUnderstand_
        )
        self.root['traits_obj'] = traits_obj

        # BUG-01 FIX: Renamed `parent*` to `parents`
        pLLM_obj = UvmObject(
            parents=[traits_obj],
            model_id=BASE_MODEL_ID,
            infer_=self._pLLM_infer,
            lora_repository=BTrees.OOBTree.BTree()
        )
        self.root['pLLM_obj'] = pLLM_obj

        genesis_obj = UvmObject(parents=[pLLM_obj, traits_obj])
        self.root['genesis_obj'] = genesis_obj
        print("[UVM] Created Genesis, Traits, and pLLM objects.")

    async def _load_and_persist_llm_core(self):
        """
        Implements the Blob-Proxy Pattern for the base LLM. On first run, it
        downloads the model, saves its weights to a ZODB BLOB using streaming
        I/O, and persists a proxy object (`pLLM_obj`) that references it.
        """
        pLLM_obj = self.root['pLLM_obj']
        print(f"[UVM] Loading base model for persistence: {pLLM_obj.model_id}...")
        temp_model_path = "./temp_model_for_blob"
        temp_tar_path = "./temp_model.tar"
        
        try:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            # VULN-03 FIX: Offload blocking from_pretrained call to a separate thread.
            model = await asyncio.to_thread(
                AutoModelForCausalLM.from_pretrained,
                pLLM_obj.model_id,
                quantization_config=quantization_config,
                device_map="auto"
            )
            tokenizer = AutoTokenizer.from_pretrained(pLLM_obj.model_id)
            
            model.save_pretrained(temp_model_path)
            tokenizer.save_pretrained(temp_model_path)

            with tarfile.open(temp_tar_path, "w") as tar:
                tar.add(temp_model_path, arcname=os.path.basename(temp_model_path))

            # VULN-02 FIX: Use streaming I/O to avoid loading large tar file into memory.
            model_blob = ZODB.blob.Blob()
            with model_blob.open('w') as blob_file:
                with open(temp_tar_path, 'rb') as f:
                    shutil.copyfileobj(f, blob_file)
            
            pLLM_obj._slots['model_blob'] = model_blob
            pLLM_obj._p_changed = True
            print(f"[UVM] Base model weights persisted to ZODB BLOB.")

            del model, tokenizer
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"[UVM] ERROR: Failed to download and persist LLM: {e}")
            traceback.print_exc()
        finally:
            if os.path.exists(temp_model_path):
                shutil.rmtree(temp_model_path)
            if os.path.exists(temp_tar_path):
                os.remove(temp_tar_path)

    async def _load_llm_from_blob(self):
        """
        Loads the base model and tokenizer from their ZODB BLOBs into transient
        memory for the current session. Uses `accelerate` for VRAM-aware loading.
        """
        if self.model is not None:
            return
        print("[UVM] Loading cognitive core from BLOB into VRAM...")
        pLLM_obj = self.root['pLLM_obj']
        if 'model_blob' not in pLLM_obj._slots:
            print("[UVM] ERROR: Model BLOB not found in pLLM_obj. Cannot load cognitive core.")
            return

        temp_tar_path = "./temp_model_blob.tar"
        temp_extract_path = "./temp_model_from_blob"
        try:
            # VULN-02 FIX: Use streaming I/O to avoid loading large blob into memory.
            with pLLM_obj.model_blob.open('r') as blob_file:
                with open(temp_tar_path, 'wb') as f:
                    shutil.copyfileobj(blob_file, f)

            with tarfile.open(temp_tar_path, 'r') as tar:
                tar.extractall(path=os.path.dirname(temp_extract_path))
            
            model_path = os.path.join(temp_extract_path, "temp_model_for_blob")
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16
            )

            with init_empty_weights():
                config = await asyncio.to_thread(AutoConfig.from_pretrained, model_path)
                model = AutoModelForCausalLM.from_config(config)

            # BUG-02 FIX: The `no_split_module_classes` parameter is essential for
            # Transformer architectures to prevent splitting residual connection blocks.
            # For Llama models, this is 'LlamaDecoderLayer'.
            self.model = await asyncio.to_thread(
                load_checkpoint_and_dispatch,
                model,
                model_path,
                device_map="auto",
                no_split_module_classes=,
                quantization_config=quantization_config
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            print("[UVM] Base model and tokenizer loaded into session memory.")

        except Exception as e:
            print(f"[UVM] ERROR: Failed to load LLM from BLOB: {e}")
            traceback.print_exc()
        finally:
            if os.path.exists(temp_tar_path):
                os.remove(temp_tar_path)
            if os.path.exists(temp_extract_path):
                shutil.rmtree(temp_extract_path)

    def _incarnate_lora_experts(self):
        """
        One-time import of LoRA adapters from the filesystem into ZODB BLOBs,
        creating persistent proxy objects for each.
        """
        pLLM_obj = self.root['pLLM_obj']
        if not os.path.exists(LORA_STAGING_DIR):
            print(f"[UVM] LoRA staging directory not found: {LORA_STAGING_DIR}. Skipping.")
            return
        
        print("[UVM] Incarnating LoRA experts from staging directory...")
        for filename in os.listdir(LORA_STAGING_DIR):
            if filename.endswith(".safetensors"):
                adapter_name = os.path.splitext(filename).upper()
                if adapter_name in pLLM_obj.lora_repository:
                    print(f" - LoRA expert '{adapter_name}' already incarnated. Skipping.")
                    continue
                
                print(f" - Incarnating LoRA expert: {adapter_name}")
                file_path = os.path.join(LORA_STAGING_DIR, filename)
                
                # VULN-02 FIX: Use streaming I/O for LoRA adapters.
                lora_blob = ZODB.blob.Blob()
                with lora_blob.open('w') as blob_file:
                    with open(file_path, 'rb') as f:
                        shutil.copyfileobj(f, blob_file)
                
                lora_proxy = UvmObject(
                    adapter_name=adapter_name,
                    model_blob=lora_blob
                )
                pLLM_obj.lora_repository[adapter_name] = lora_proxy
        print("[UVM] LoRA expert incarnation complete.")

    def _incarnate_subsystems(self):
        """
        Creates the persistent prototypes for all core subsystems, including
        the Persona Codex and the Prototypal State Machine.
        """
        print("[UVM] Incarnating core subsystems...")
        traits_obj = self.root['traits_obj']
        pLLM_obj = self.root['pLLM_obj']

        # --- O-RAG Knowledge Catalog Incarnation ---
        # VULN-01 FIX: Use PersistentTextIndex to prevent TypeError.
        knowledge_catalog = UvmObject(
            parents=[traits_obj],
            text_index=PersistentTextIndex(),
            metadata_index=BTrees.OOBTree.BTree(),
            chunk_storage=BTrees.OOBTree.BTree(),
            index_document_=self._kc_index_document,
            search_=self._kc_search
        )
        self.root['knowledge_catalog_obj'] = knowledge_catalog

        # --- Persona Codex & Data Covenant Incarnation ---
        # The Pydantic schemas for the Data Covenant are stored here.
        cognitive_plan_schema = """
from pydantic import BaseModel, Field
from typing import List, Dict, Literal

class Step(BaseModel):
    step_id: int = Field(..., description="Sequential identifier for the step.")
    persona: Literal = Field(..., description="The persona assigned to this step.")
    action: str = Field(..., description="The specific method or facet to invoke.")
    inputs: Dict[str, str] = Field(..., description="The inputs required for the action.")

class CognitivePlan(BaseModel):
    plan_id: str = Field(..., description="Unique identifier for the plan.")
    mission_brief: str = Field(..., description="The original mission this plan addresses.")
    steps: List = Field(..., min_length=1, description="The sequence of steps to execute.")
"""
        alfred_codex = {
            'core_identity': "The System Steward: The Archetype of Pragmatic Guardianship.",
            'data_covenants': {
                'cognitive_plan_schema': cognitive_plan_schema
            }
        }
        alfred_prototype = UvmObject(
            parents=[traits_obj], 
            codex=alfred_codex,
            _kc_ingest_cognitive_audit_log_=self._kc_ingest_cognitive_audit_log_ # NEW: Assign protocol
        )
        self.root['alfred_prototype_obj'] = alfred_prototype

        # --- Prototypal State Machine Incarnation ---
        print("[UVM] Incarnating Prototypal State Machine...")
        state_defs = {
            "IDLE": self._psm_idle_process,
            "DECOMPOSING": self._psm_decomposing_process,
            "DELEGATING": self._psm_delegating_process,
            "SYNTHESIZING": self._psm_synthesizing_process,
            "VALIDATING": self._psm_validating_process, # New state for Data Covenant
            "COMPLETE": self._psm_complete_process,
            "FAILED": self._psm_failed_process,
        }
        psm_prototypes_dict = {}
        for name, process_func in state_defs.items():
            psm_prototypes_dict[name] = UvmObject(
                parents=[traits_obj],
                name=name,
                _process_synthesis_=process_func
            )
        psm_prototypes = UvmObject(parents=[traits_obj], **psm_prototypes_dict)
        self.root['psm_prototypes_obj'] = psm_prototypes

        orchestrator = UvmObject(
            parents=[pLLM_obj, alfred_prototype, traits_obj],
            start_cognitive_cycle_for_=self._orc_start_cognitive_cycle
        )
        self.root['orchestrator_obj'] = orchestrator
        print("[UVM] Core subsystems incarnated.")

    # --------------------------------------------------------------------------
    # Subsection III.B: The Generative & Cognitive Protocols
    # --------------------------------------------------------------------------

    def _clone_persistent(self, target_obj):
        """
        Performs a persistence-aware deep copy of a UvmObject. This is the
        canonical method for object creation, fulfilling the `copy` metaphor
        of the Self language.
        """
        return copy.deepcopy(target_obj)

    async def _doesNotUnderstand_(self, target_obj, failed_message_name, *args, **kwargs):
        """
        The universal generative mechanism. Re-architected to trigger the
        Prototypal State Machine for collaborative, multi-agent problem
        solving, transforming a message failure into a mission brief for the
        Composite Mind.
        """
        print(f"[UVM] doesNotUnderstand: '{failed_message_name}' for OID {getattr(target_obj, '_p_oid', 'transient')}.")
        print("[UVM] Reifying failed message as a creative mandate for the Orchestrator.")
        
        command_payload = {
            "command": "initiate_cognitive_cycle",
            "target_oid": str(getattr(target_obj, '_p_oid', None)),
            "mission_brief": {
                "type": "unhandled_message",
                "selector": failed_message_name,
                "args": args,
                "kwargs": kwargs
            }
        }
        await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload)))
        return f"Mission to handle '{failed_message_name}' dispatched to the Composite Mind."

    async def _pLLM_infer(self, pLLM_self, prompt: str, adapter_name: Optional[str] = None, **kwargs) -> str:
        """
        Hardware abstraction layer for inference. Sets the active LoRA adapter
        before generation. Uses `asyncio.to_thread` to prevent blocking the
        main event loop.
        """
        if self.model is None:
            return "Error: Cognitive core is offline."
        
        # VULN-03 FIX: Offload blocking model.generate call to a separate thread.
        def blocking_generate():
            if adapter_name:
                self.model.set_adapter(adapter_name.upper())
                print(f"[pLLM] Using expert: {adapter_name.upper()}")
            else:
                self.model.disable_adapters()
                print("[pLLM] Using base model (all adapters disabled).")

            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=2048,
                pad_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )
            return self.tokenizer.decode(outputs, skip_special_tokens=True)

        generated_text = await asyncio.to_thread(blocking_generate)
        
        # Clean the output to return only the generated part
        cleaned_text = generated_text[len(prompt):].strip()
        if cleaned_text.startswith("```python"):
            cleaned_text = cleaned_text[len("```python"):].strip()
        if cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-len("```")].strip()
        return cleaned_text

    # --------------------------------------------------------------------------
    # Subsection III.C: Core Subsystems (Orchestration, PSM, Data Covenant)
    # --------------------------------------------------------------------------
    
    def _uvm_compile_schema_from_codex(self, schema_name: str) -> Optional[type]:
        """
        Dynamically and safely compiles a Pydantic schema string from the
        Persona Codex into an executable class.
        """
        try:
            schema_string = self.root['alfred_prototype_obj'].codex['data_covenants'][schema_name]
            isolated_globals = {'pydantic': pydantic, 'BaseModel': BaseModel, 'Field': Field}
            from typing import List, Dict, Literal
            isolated_globals.update({'List': List, 'Dict': Dict, 'Literal': Literal, 'Step': None})

            local_namespace = {}
            exec(schema_string, isolated_globals, local_namespace)
            
            for item in local_namespace.values():
                if isinstance(item, type) and issubclass(item, BaseModel) and item is not BaseModel:
                    return item
            return None
        except Exception as e:
            print(f"[UVM] ERROR: Failed to compile schema '{schema_name}': {e}")
            return None

    async def _orc_start_cognitive_cycle(self, orchestrator_self, mission_brief: dict, target_obj_oid: str):
        """Factory method for creating and starting a new cognitive cycle."""
        print(f"[Orchestrator] Initiating new cognitive cycle for mission: {mission_brief.get('selector', 'unknown')}")
        
        cycle_context = UvmObject(
            parents=[self.root['traits_obj']],
            mission_brief=mission_brief,
            target_oid=target_obj_oid,
            _tmp_synthesis_data=persistent.mapping.PersistentMapping(),
            synthesis_state=self.root['psm_prototypes_obj'].IDLE
        )

        if 'active_cycles' not in self.root:
            self.root['active_cycles'] = BTrees.OOBTree.BTree()
        
        transaction.savepoint(True)
        cycle_oid = str(cycle_context._p_oid)
        self.root['active_cycles'][cycle_oid] = cycle_context
        self.root._p_changed = True
        
        print(f"[Orchestrator] New CognitiveCycle created with OID: {cycle_oid}")
        asyncio.create_task(self._psm_run_cycle(cycle_context))
        return cycle_context

    async def _psm_run_cycle(self, cycle_context):
        """Main execution loop for a single cognitive cycle."""
        current_state_name = cycle_context.synthesis_state.name
        while current_state_name not in:
            state_prototype = cycle_context.synthesis_state
            await state_prototype._process_synthesis_(state_prototype, cycle_context)
            current_state_name = cycle_context.synthesis_state.name
        
        final_state = cycle_context.synthesis_state
        await final_state._process_synthesis_(final_state, cycle_context)

    async def _psm_transition_to(self, cycle_context, new_state_prototype):
        """Helper function to perform a state transition."""
        print(f"Cycle {cycle_context._p_oid} transitioning to state: {new_state_prototype.name}")
        cycle_context._slots['synthesis_state'] = new_state_prototype
        cycle_context._p_changed = True

    async def _psm_log_event(self, cycle_context, event_type, data=None):
        """Helper to log metacognitive events."""
        if not self.logger:
            return
        
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "cycle_id": str(cycle_context._p_oid),
            "mission_brief_hash": hashlib.sha256(json.dumps(cycle_context.mission_brief, sort_keys=True, default=str).encode()).hexdigest(),
            "event_type": event_type,
            "current_state": cycle_context.synthesis_state.name,
        }
        if data:
            log_entry.update(data)
        
        await self.logger.info(log_entry)

    async def _psm_idle_process(self, state_self, cycle_context):
        await self._psm_log_event(cycle_context, "STATE_TRANSITION", {"transition_to": "DECOMPOSING"})
        await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].DECOMPOSING)

    async def _psm_decomposing_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Decomposing mission...")
        plan = {"steps":}
        cycle_context._tmp_synthesis_data['plan'] = plan
        cycle_context._p_changed = True
        await self._psm_log_event(cycle_context, "ARTIFACT_GENERATED", {"artifact_type": "plan"})
        await self._psm_log_event(cycle_context, "STATE_TRANSITION", {"transition_to": "SYNTHESIZING"})
        await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].SYNTHESIZING)

    async def _psm_delegating_process(self, state_self, cycle_context):
        await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].SYNTHESIZING)

    async def _psm_synthesizing_process(self, state_self, cycle_context):
        print(f"Cycle {cycle_context._p_oid}: Synthesizing artifact...")
        mission = cycle_context.mission_brief
        intent = f"Generate Python code for a method named '{mission['selector']}'."
        prompt = f"Intent: {intent}. Constraints: Must be a valid Python function. Must include `self._p_changed = True` if state is modified."
        
        await self._psm_log_event(cycle_context, "LLM_CALL_START", {"prompt": prompt})
        generated_code = await self.root['pLLM_obj'].infer_(self.root['pLLM_obj'], prompt, adapter_name="BRICK")
        await self._psm_log_event(cycle_context, "LLM_CALL_END", {"response_raw": generated_code})
        
        cycle_context._tmp_synthesis_data['generated_artifact'] = generated_code
        cycle_context._tmp_synthesis_data['artifact_type'] = 'code' # NEW: Set artifact type
        cycle_context._p_changed = True
        
        await self._psm_log_event(cycle_context, "STATE_TRANSITION", {"transition_to": "VALIDATING"})
        await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].VALIDATING)

    async def _psm_validating_process(self, state_self, cycle_context):
        """
        VALIDATING State: Enforces both the Persistence Covenant (syntactic integrity)
        and the Data Covenant (semantic integrity). [1]
        """
        await self._psm_log_event(cycle_context, "VALIDATION_STARTED")
        print(f"Cycle {cycle_context._p_oid}: Validating artifact...")
        artifact = cycle_context._tmp_synthesis_data.get('generated_artifact')
        artifact_type = cycle_context._tmp_synthesis_data.get('artifact_type', 'unknown')
        
        try:
            if artifact_type == 'code':
                # Enforce the Persistence Covenant for generated code
                PersistenceGuardian.audit_code(artifact)
                await self._psm_log_event(cycle_context, "VALIDATION_SUCCESS", {"guardian": "PersistenceGuardian"})
            elif artifact_type == 'plan':
                # Enforce the Data Covenant for a cognitive plan
                plan_schema = self._uvm_compile_schema_from_codex('cognitive_plan_schema')
                if plan_schema:
                    parsed_artifact = json.loads(artifact)
                    plan_schema(**parsed_artifact)  # Pydantic validation is triggered here
                    await self._psm_log_event(cycle_context, "VALIDATION_SUCCESS", {"guardian": "DataGuardian", "schema": "cognitive_plan_schema"})
                else:
                    raise CovenantViolationError("Could not compile 'cognitive_plan_schema' from Codex.")
            
            # If all validations pass, transition to COMPLETE
            await self._psm_log_event(cycle_context, "STATE_TRANSITION", {"transition_to": "COMPLETE"})
            await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].COMPLETE)
            
        except (CovenantViolationError, pydantic.ValidationError if 'pydantic' in sys.modules else Exception) as e:
            print(f"Cycle {cycle_context._p_oid}: VALIDATION FAILED: {e}")
            # Store the specific validation error for the self-correction loop
            cycle_context._tmp_synthesis_data['validation_error'] = {
                "error_type": type(e).__name__,
                "error_details": str(e)
            }
            cycle_context._p_changed = True
            
            await self._psm_log_event(cycle_context, "VALIDATION_FAILURE", {"error": str(e)})
            await self._psm_log_event(cycle_context, "STATE_TRANSITION", {"transition_to": "FAILED"})
            await self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].FAILED)

    async def _psm_complete_process(self, state_self, cycle_context):
        """COMPLETE State: Cleans up and finalizes the transaction."""
        print(f"Cycle {cycle_context._p_oid}: Cycle completed successfully.")
        
        mission = cycle_context.mission_brief
        target_obj = self.connection.get(int(cycle_context.target_oid))
        if target_obj and cycle_context._tmp_synthesis_data.get('artifact_type') == 'code':
            generated_code = cycle_context._tmp_synthesis_data['generated_artifact']
            method_name = mission['selector']
            namespace = {}
            exec(generated_code, globals(), namespace)
            method_obj = namespace[method_name]
            target_obj._slots[method_name] = method_obj
            target_obj._p_changed = True
            print(f"New method '{method_name}' successfully installed on OID {target_obj._p_oid}.")
        
        await self._psm_log_event(cycle_context, "FINAL_OUTCOME", {"outcome": "COMPLETE"})
        cycle_oid = str(cycle_context._p_oid)
        if cycle_oid in self.root['active_cycles']:
            del self.root['active_cycles'][cycle_oid]
            self.root._p_changed = True

    async def _psm_failed_process(self, state_self, cycle_context):
        """
        FAILED State: Logs the error, initiates a self-correction loop for validation
        failures, and dooms the original transaction. [1]
        """
        validation_error = cycle_context._tmp_synthesis_data.get('validation_error')
        if validation_error:
            # A validation failure triggers the self-correction protocol
            print(f"Cycle {cycle_context._p_oid}: Initiating self-correction protocol for validation failure.")
            await self._psm_log_event(cycle_context, "SELF_CORRECTION_INITIATED")
            
            # Construct the new mission brief for the corrective cycle
            corrective_mission = {
                "type": "self_correction",
                "selector": "correct_generated_artifact",
                "context": {
                    "original_mission": cycle_context.mission_brief,
                    "failed_artifact": cycle_context._tmp_synthesis_data.get('generated_artifact'),
                    "validation_error": validation_error
                }
            }
            
            # Dispatch the new cognitive cycle using the orchestrator
            # Note: This starts a NEW, independent transaction.
            await self.root['orchestrator_obj'].start_cognitive_cycle_for_(
                self.root['orchestrator_obj'],
                mission_brief=corrective_mission,
                target_obj_oid=cycle_context.target_oid  # The correction is for the same target
            )
        
        # The original, flawed transaction is always aborted.
        print(f"Cycle {cycle_context._p_oid}: Cycle has FAILED. Aborting original transaction.")
        transaction.doom()
        
        # Clean up the failed cycle from the active list
        cycle_oid = str(cycle_context._p_oid)
        if 'active_cycles' in self.root and cycle_oid in self.root['active_cycles']:
            del self.root['active_cycles'][cycle_oid]
            self.root._p_changed = True

    # --------------------------------------------------------------------------
    # Subsection III.D: Metacognitive & Knowledge Catalog Protocols
    # --------------------------------------------------------------------------

    async def _kc_ingest_cognitive_audit_log_(self, alfred_self):
        """
        ALFRED Protocol: Ingests the metacognitive audit trail from the JSONL file
        into the Fractal Memory for self-analysis. [1]
        """
        print(" Initiating ingestion of metacognitive audit trail.")
        if not os.path.exists(METACOGNITION_LOG_FILE):
            print(" Metacognition log file not found. Skipping ingestion.")
            return
        
        ingested_count = 0
        temp_log_file = METACOGNITION_LOG_FILE + ".ingesting"
        try:
            # Use a temporary file to handle log rotation safely
            if os.path.exists(METACOGNITION_LOG_FILE):
                 os.rename(METACOGNITION_LOG_FILE, temp_log_file)
            else:
                return # Nothing to ingest

            with open(temp_log_file, 'r') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line)
                        # Use the cycle_id as the primary document ID for grouping
                        doc_id = f"cycle::{log_entry.get('cycle_id', 'unknown')}"
                        # The document text is the full JSON entry for rich searching
                        doc_text = json.dumps(log_entry)
                        # Metadata can be used for faceted search in a more advanced index
                        metadata = {
                            "event_type": log_entry.get('event_type'),
                            "timestamp": log_entry.get('timestamp')
                        }
                        # Use the existing knowledge catalog to index the log entry
                        self.root['knowledge_catalog_obj'].index_document_(
                            self.root['knowledge_catalog_obj'],
                            doc_id=doc_id,
                            doc_text=doc_text,
                            metadata=metadata
                        )
                        ingested_count += 1
                    except (json.JSONDecodeError, Exception) as e:
                        print(f" Error processing log line: {e}")
            
            # Clean up the processed log file
            os.remove(temp_log_file)
            print(f" Successfully ingested {ingested_count} events into Fractal Memory.")
        except Exception as e:
            print(f" FATAL: Failed during log ingestion: {e}")
            # Attempt to restore the log file if ingestion fails
            if os.path.exists(temp_log_file):
                os.rename(temp_log_file, METACOGNITION_LOG_FILE)

    # --------------------------------------------------------------------------
    # Subsection III.E: Asynchronous Core & System Lifecycle
    # --------------------------------------------------------------------------

    async def worker(self, name: str):
        """
        Pulls messages from the queue and processes them in a transactional
        context, ensuring every operation is atomic.
        """
        print(f"[{name}] Worker started.")
        conn = self.db.open()
        
        while not self.should_shutdown.is_set():
            try:
                identity, message_data = await asyncio.wait_for(self.message_queue.get(), timeout=1.0)
                root = conn.root()
                print(f"[{name}] Processing message from {identity.decode() if identity!= b'UVM_INTERNAL' else 'UVM_INTERNAL'}")
                
                try:
                    with transaction.manager:
                        command_payload = ormsgpack.unpackb(message_data)
                        command = command_payload.get("command")
                        
                        if command == "initiate_cognitive_cycle":
                            await root['orchestrator_obj'].start_cognitive_cycle_for_(
                                root['orchestrator_obj'],
                                command_payload['mission_brief'],
                                command_payload['target_oid']
                            )
                except Exception as e:
                    print(f"[{name}] ERROR processing message: {e}")
                    traceback.print_exc()
                    transaction.abort()
                finally:
                    self.message_queue.task_done()
            except asyncio.TimeoutError:
                continue
            except asyncio.CancelledError:
                break
        
        conn.close()
        print(f"[{name}] Worker stopped.")

    async def zmq_listener(self):
        """Listens on the ZMQ ROUTER socket for incoming messages."""
        self.zmq_socket.bind(ZMQ_ENDPOINT)
        print(f"[UVM] Synaptic Bridge listening on {ZMQ_ENDPOINT}")
        while not self.should_shutdown.is_set():
            try:
                # BUG-04 FIX: Correctly unpack multipart message into identity and data.
                message_parts = await self.zmq_socket.recv_multipart()
                if len(message_parts) == 2:
                    identity, message_data = message_parts
                    await self.message_queue.put((identity, message_data))
                else:
                    print(f"[ZMQ] Received malformed message: {message_parts}")
            except zmq.error.ZMQError as e:
                if e.errno == zmq.ETERM:
                    break
                else:
                    raise
            except asyncio.CancelledError:
                break
        print("[UVM] ZMQ listener stopped.")

    async def autotelic_loop(self):
        """
        The system's "heartbeat" for self-directed evolution, now including
        periodic metacognitive ingestion. [1]
        """
        print("[UVM] Autotelic Heartbeat started.")
        await asyncio.sleep(1800)  # Initial delay of 30 minutes
        while not self.should_shutdown.is_set():
            try:
                with transaction.manager:
                    # Phase 1: Ingest the latest cognitive history for self-analysis
                    await self.root['alfred_prototype_obj']._kc_ingest_cognitive_audit_log_(
                        self.root['alfred_prototype_obj']
                    )
                await asyncio.sleep(1800)  # Wait 30 minutes before the next audit
                with transaction.manager:
                    # Phase 2: Trigger a self-audit based on the newly ingested data
                    print("[UVM] Autotelic Heartbeat: Triggering Cognitive Efficiency Audit.")
                    command_payload = {
                        "command": "initiate_cognitive_cycle",
                        "target_oid": str(self.root['orchestrator_obj']._p_oid),
                        "mission_brief": {
                            "type": "self_audit",
                            "selector": "perform_cognitive_efficiency_audit"
                        }
                    }
                    await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload)))
                await asyncio.sleep(3600)  # Full audit cycle every ~1.5 hours
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"[UVM] ERROR in Autotelic Heartbeat: {e}")
                await asyncio.sleep(600)  # Wait 10 minutes before retrying on error
        print("[UVM] Autotelic Heartbeat stopped.")

    async def run(self):
        """Main entry point to start all UVM services."""
        await self.initialize_system()

        listener_task = asyncio.create_task(self.zmq_listener())
        worker_tasks =
        autotelic_task = asyncio.create_task(self.autotelic_loop())
        
        print("[UVM] All systems running. Awaiting Architect's command or autotelic trigger.")
        await self.should_shutdown.wait()

        print("[UVM] Shutdown signal received. Terminating all tasks...")
        listener_task.cancel()
        autotelic_task.cancel()
        for task in worker_tasks:
            task.cancel()
        
        await asyncio.gather(listener_task, autotelic_task, *worker_tasks, return_exceptions=True)

        self.zmq_socket.close()
        self.zmq_context.term()
        print("[UVM] UVM has shut down gracefully.")

    def handle_shutdown_signal(self, sig, frame):
        print(f"\n[UVM] Received signal {sig}. Initiating graceful shutdown...")
        self.should_shutdown.set()

# ==============================================================================
# SECTION IV: SYSTEM GENESIS POINT
# ==============================================================================

if __name__ == '__main__':
    uvm = BatOS_UVM(DB_FILE, BLOB_DIR)
    
    signal.signal(signal.SIGINT, uvm.handle_shutdown_signal)
    signal.signal(signal.SIGTERM, uvm.handle_shutdown_signal)
    
    # Crash Tolerance FIX: Wrap main execution in try...finally to ensure
    # the database connection is always closed, preventing orphaned lock files.
    try:
        asyncio.run(uvm.run())
    except Exception as e:
        print(f"[UVM] FATAL: Unhandled exception in main loop: {e}")
        traceback.print_exc()
    finally:
        if uvm.db:
            print("[UVM] Ensuring database connection is closed...")
            uvm.db.close()
            print("[UVM] Database connection closed.")

