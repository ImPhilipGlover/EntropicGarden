batos.py

CLASSIFICATION: ARCHITECT EYES ONLY

SUBJECT: Canonical Incarnation Protocol for the Binaural Autopoietic/Telic

Operating System, Series VIII ('The Fractal Awakening')

This script is the single, executable embodiment of the BAT OS Series VIII

architecture. It is the fractal seed, designed to be invoked once to

initiate the system's "unbroken process of becoming."

2

The protocol unfolds in a sequence of autonomous phases:

1. Prototypal Awakening: Establishes a connection to the Zope Object

Database (ZODB), the system's persistent substrate. On the first run,

it creates and persists the primordial objects and incarnates all

subsystems, including the cognitive core (pLLM_obj), the persona-LoRAs,

the memory manager, the knowledge catalog, and the orchestrator's

Prototypal State Machine. This is an atomic, transactional act of

genesis.

3

2. Cognitive Cycle Initiation: The system's generative kernel,

_doesNotUnderstand_, is re-architected from a simple JIT compiler into

a dispatcher. A failed message lookup is no longer a simple error but a

creative mandate, reified as a mission brief and enqueued for the

Composite Mind. This triggers the Prototypal State Machine, initiating a

structured, multi-agent, transactional cognitive cycle to fulfill the

original intent.

2

3. Directed Autopoiesis: The system's core behaviors, such as creating new

methods or cognitive facets, are now products of this collaborative

reasoning process. The system can reason about its own structure,

consult its fractal memory, and generate new, validated capabilities

at runtime, ensuring its own continuous evolution.

8

4. The Autotelic Heartbeat: The script enters its final, persistent state:

an asynchronous event loop that functions as the Universal Virtual

Machine (UVM). This loop not only processes external commands but also

drives an internal, self-directed evolutionary process, compelling the

system to autonomously initiate self-improvement tasks based on its

own operational history.

2

Report Generated: Saturday, August 30, 2025, 9:19 AM

Location: Portland, Oregon

--- Core Dependencies ---

These libraries are non-negotiable architectural components.

import os

import sys

import asyncio

import threading

import gc

import time

import copy

import ast

import traceback

import functools

from typing import Any, Dict, List, Optional, Callable

--- Persistence Substrate (ZODB) ---

import ZODB

import ZODB.FileStorage

import ZODB.blob

import transaction

import persistent

import persistent.mapping

import BTrees.OOBTree

from zope.index.text import TextIndex

--- Communication & Serialization ---

import zmq

import zmq.asyncio

import ormsgpack

--- Cognitive & UI Dependencies (Conditionally Imported) ---

try:

import torch

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig

from peft import PeftModel, LoraConfig

from accelerate import init_empty_weights, load_checkpoint_and_dispatch

except ImportError:

print("WARNING: 'transformers', 'torch', 'bitsandbytes', 'peft', or 'accelerate' not found. LLM capabilities will be disabled.")

AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, PeftModel, LoraConfig = None, None, None, None, None

init_empty_weights, load_checkpoint_and_dispatch, AutoConfig = None, None, None

try:

from kivy.app import App

from kivy.clock import mainthread

except ImportError:

print("WARNING: 'kivy' not found. UI generation capabilities will be disabled.")

App = object

mainthread = lambda f: f

--- System Constants ---

DB_FILE = 'live_image.fs'

BLOB_DIR = 'live_image.fs.blob'

ZMQ_ENDPOINT = "tcp://127.0.0.1:5555"

BASE_MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

LORA_STAGING_DIR = "./lora_adapters"

==============================================================================

SECTION II: THE PRIMORDIAL SUBSTRATE

==============================================================================

class UvmObject(persistent.Persistent):

"""

The foundational particle of the BAT OS universe. This class provides the

"physics" for a prototype-based object model inspired by the Self and

Smalltalk programming languages. It rejects standard Python attribute

access in favor of a unified '_slots' dictionary and a delegation-based

inheritance mechanism. 3

It inherits from `persistent.Persistent` to enable transactional storage
via ZODB, guaranteeing the system's "unbroken existence." [4, 7]
"""
def __init__(self, **initial_slots):
    """
    Initializes the UvmObject. The `_slots` dictionary is instantiated as a
    `persistent.mapping.PersistentMapping` to ensure that changes within
    the dictionary itself are correctly tracked by ZODB. [12, 13]
    """
    # The `_slots` attribute is one of the few that are set directly on the
    # instance, as it is the container for all other state and behavior.
    super().__setattr__('_slots', persistent.mapping.PersistentMapping(initial_slots))

def __setattr__(self, name: str, value: Any) -> None:
    """
    Intercepts all attribute assignments. This method redirects assignments
    to the internal `_slots` dictionary, unifying state and behavior.

    It explicitly sets `_p_changed = True` to manually signal to ZODB that
    the object's state has been modified. This is a non-negotiable
    architectural requirement known as The Persistence Covenant. Overriding
    `__setattr__` bypasses ZODB's default change detection, making this
    manual signal essential for preventing systemic amnesia. [3, 7, 8]
    """
    if name.startswith('_p_') or name == '_slots':
        # Allow ZODB's internal attributes and direct _slots manipulation.
        super().__setattr__(name, value)
    else:
        self._slots[name] = value
        self._p_changed = True

def __getattr__(self, name: str) -> Any:
    """
    Implements attribute access and the delegation-based inheritance chain.
    If an attribute is not found in the local `_slots`, it delegates the
    lookup to the object(s) in its `parent*` slot.

    The exhaustion of this chain raises an `AttributeError`, which is the
    universal trigger for the `_doesNotUnderstand_` generative protocol
    in the UVM. [2, 3, 5, 8, 11]
    """
    if name in self._slots:
        return self._slots[name]

    if 'parent*' in self._slots:
        parents = self._slots['parent*']
        if not isinstance(parents, list):
            parents = [parents]
        for parent in parents:
            try:
                return getattr(parent, name)
            except AttributeError:
                continue

    raise AttributeError(f"UvmObject OID {self._p_oid} has no slot '{name}'")

def __repr__(self) -> str:
    """Provides a more informative representation for debugging."""
    slot_keys = list(self._slots.keys())
    oid_str = f"oid={self._p_oid}" if hasattr(self, '_p_oid') and self._p_oid is not None else "oid=transient"
    return f"<UvmObject {oid_str} slots={slot_keys}>"

def __deepcopy__(self, memo):
    """
    Custom deepcopy implementation to ensure persistence-aware cloning.
    Standard `copy.deepcopy` is not aware of ZODB's object lifecycle and
    can lead to unintended shared state or broken object graphs. [14]
    This method is the foundation for the `_clone_persistent_` protocol.
    """
    cls = self.__class__
    result = cls.__new__(cls)
    memo[id(self)] = result
    
    # Deepcopy the _slots dictionary to create new persistent containers.
    # This is crucial for ensuring the clone is a distinct entity.
    new_slots = copy.deepcopy(self._slots, memo)
    super(UvmObject, result).__setattr__('_slots', new_slots)

    return result


==============================================================================

SECTION III: THE UNIVERSAL VIRTUAL MACHINE (UVM)

==============================================================================

class BatOS_UVM:

"""

The core runtime environment for the BAT OS. This class orchestrates the

Prototypal Awakening, manages the persistent object graph, runs the

asynchronous message-passing kernel, and initiates the system's autotelic

evolution. 3

"""

def init(self, db_file: str, blob_dir: str):

self.db_file = db_file

self.blob_dir = blob_dir

self.db = None

self.connection = None

self.root = None

self.message_queue = asyncio.Queue()

self.zmq_context = zmq.asyncio.Context()

self.zmq_socket = self.zmq_context.socket(zmq.ROUTER)

self.should_shutdown = asyncio.Event()

    # Transient attributes to hold the loaded model and tokenizer
    self.model = None
    self.tokenizer = None

# --------------------------------------------------------------------------
# Subsection III.A: Prototypal Awakening & Subsystem Incarnation
# --------------------------------------------------------------------------

async def initialize_system(self):
    """
    Phase 1: Prototypal Awakening. Connects to ZODB and, on first run,
    creates the primordial objects and incarnates all subsystems within a
    single, atomic transaction. [2, 5, 7]
    """
    print("[UVM] Phase 1: Prototypal Awakening...")
    if not os.path.exists(self.blob_dir):
        os.makedirs(self.blob_dir)
    
    storage = ZODB.FileStorage.FileStorage(self.db_file, blob_dir=self.blob_dir)
    self.db = ZODB.DB(storage)
    self.connection = self.db.open()
    self.root = self.connection.root()

    if 'genesis_obj' not in self.root:
        print("[UVM] First run detected. Performing full Prototypal Awakening.")
        with transaction.manager:
            self._incarnate_primordial_objects()
            self._load_and_persist_llm_core()
            self._incarnate_lora_experts()
            self._incarnate_subsystems()
        print("[UVM] Awakening complete. All systems nominal.")
    else:
        print("[UVM] Resuming existence from Living Image.")

    # Load the model and tokenizer into transient memory for this session.
    await self._load_llm_from_blob()
    print(f"[UVM] System substrate initialized. Root OID: {self.root._p_oid}")

def _incarnate_primordial_objects(self):
    """Creates the foundational objects of the BAT OS universe."""
    print("[UVM] Incarnating primordial objects...")
    traits_obj = UvmObject(
        _clone_persistent_=self._clone_persistent,
        _doesNotUnderstand_=self._doesNotUnderstand
    )
    self.root['traits_obj'] = traits_obj

    pLLM_obj = UvmObject(
        parent*=[traits_obj],
        model_id=BASE_MODEL_ID,
        infer_=self._pLLM_infer,
        lora_repository=BTrees.OOBTree.BTree()
    )
    self.root['pLLM_obj'] = pLLM_obj

    genesis_obj = UvmObject(parent*=[pLLM_obj, traits_obj])
    self.root['genesis_obj'] = genesis_obj
    print("[UVM] Created Genesis, Traits, and pLLM objects.")

def _load_and_persist_llm_core(self):
    """
    Implements the Blob-Proxy Pattern for the base LLM. On first run, it
    downloads the model, saves its weights to a ZODB BLOB, and persists a
    proxy object (`pLLM_obj`) that references it. [3, 4, 6, 7]
    """
    if AutoModelForCausalLM is None:
        print("[UVM] LLM libraries not available. Cognitive core offline.")
        return

    pLLM_obj = self.root['pLLM_obj']
    print(f"[UVM] Loading base model for persistence: {pLLM_obj.model_id}...")

    try:
        # Use a temporary directory to save model parts before creating the BLOB
        temp_model_path = "./temp_model_for_blob"
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            pLLM_obj.model_id,
            quantization_config=quantization_config,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(pLLM_obj.model_id)

        model.save_pretrained(temp_model_path)
        tokenizer.save_pretrained(temp_model_path)

        # Create a single tarball BLOB for atomicity
        import tarfile
        temp_tar_path = "./temp_model.tar"
        with tarfile.open(temp_tar_path, "w") as tar:
            tar.add(temp_model_path, arcname=os.path.basename(temp_model_path))

        with open(temp_tar_path, 'rb') as f:
            model_data = f.read()
            model_blob = ZODB.blob.Blob(model_data)
            pLLM_obj.model_blob = model_blob
        
        print(f"[UVM] Base model weights ({len(model_data) / 1e9:.2f} GB) persisted to ZODB BLOB.")
        
        # Clean up temporary files
        import shutil
        shutil.rmtree(temp_model_path)
        os.remove(temp_tar_path)
        
        # Release model from memory after persisting
        del model
        del tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"[UVM] ERROR: Failed to download and persist LLM: {e}")
        traceback.print_exc()

async def _load_llm_from_blob(self):
    """
    Loads the base model and tokenizer from their ZODB BLOBs into transient
    memory for the current session. Uses `accelerate` for VRAM-aware loading.
    [15, 6, 1]
    """
    if AutoModelForCausalLM is None:
        return
        
    print("[UVM] Loading cognitive core from BLOB into VRAM...")
    pLLM_obj = self.root['pLLM_obj']
    
    if 'model_blob' not in pLLM_obj._slots:
        print("[UVM] ERROR: Model BLOB not found in pLLM_obj. Cannot load cognitive core.")
        return

    temp_tar_path = "./temp_model_blob.tar"
    temp_extract_path = "./temp_model_from_blob"
    
    try:
        with pLLM_obj.model_blob.open('r') as blob_file:
            with open(temp_tar_path, 'wb') as f:
                f.write(blob_file.read())

        import tarfile
        with tarfile.open(temp_tar_path, 'r') as tar:
            tar.extractall(path=os.path.dirname(temp_extract_path))
        
        model_path = os.path.join(temp_extract_path, "temp_model_for_blob")

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # Use Accelerate's big model inference tools for VRAM-aware loading
        with init_empty_weights():
            config = AutoConfig.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_config(config)
        
        self.model = load_checkpoint_and_dispatch(
            model,
            model_path,
            device_map="auto",
            no_split_module_classes=,
            quantization_config=quantization_config
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("[UVM] Base model and tokenizer loaded into session memory.")
        
        # Load all incarnated LoRA adapters into the PEFT model
        print("[UVM] Attaching all incarnated LoRA experts to base model...")
        for name, proxy in pLLM_obj.lora_repository.items():
            temp_lora_path = f"./temp_{name}.safetensors"
            with proxy.model_blob.open('r') as blob_file:
                with open(temp_lora_path, 'wb') as temp_f:
                    temp_f.write(blob_file.read())
            
            self.model.load_adapter(temp_lora_path, adapter_name=name)
            os.remove(temp_lora_path)
            print(f"  - Attached '{name}' expert.")

    except Exception as e:
        print(f"[UVM] ERROR: Failed to load LLM from BLOB: {e}")
        traceback.print_exc()
    finally:
        # Clean up temporary files
        import shutil
        if os.path.exists(temp_tar_path):
            os.remove(temp_tar_path)
        if os.path.exists(temp_extract_path):
            shutil.rmtree(temp_extract_path)

def _incarnate_lora_experts(self):
    """
    One-time import of LoRA adapters from the filesystem into ZODB BLOBs,
    creating persistent proxy objects for each. [6]
    """
    pLLM_obj = self.root['pLLM_obj']
    if not os.path.exists(LORA_STAGING_DIR):
        print(f"[UVM] LoRA staging directory not found: {LORA_STAGING_DIR}. Skipping expert incarnation.")
        return

    print("[UVM] Incarnating LoRA experts from staging directory...")
    for filename in os.listdir(LORA_STAGING_DIR):
        if filename.endswith(".safetensors"):
            adapter_name = os.path.splitext(filename).upper()
            if adapter_name in pLLM_obj.lora_repository:
                print(f"  - LoRA expert '{adapter_name}' already incarnated. Skipping.")
                continue
            
            print(f"  - Incarnating LoRA expert: {adapter_name}")
            file_path = os.path.join(LORA_STAGING_DIR, filename)
            with open(file_path, 'rb') as f:
                lora_data = f.read()
            
            lora_blob = ZODB.blob.Blob(lora_data)
            lora_proxy = UvmObject(
                adapter_name=adapter_name,
                model_blob=lora_blob
            )
            pLLM_obj.lora_repository[adapter_name] = lora_proxy
    print("[UVM] LoRA expert incarnation complete.")

def _incarnate_subsystems(self):
    """
    Creates the persistent prototypes for all core subsystems, including
    the Prototypal State Machine for collaborative agency. [2]
    """
    print("[UVM] Incarnating core subsystems...")
    traits_obj = self.root['traits_obj']
    pLLM_obj = self.root['pLLM_obj']

    # --- Synaptic Memory Manager Incarnation ---
    memory_manager = UvmObject(
        parent*=[traits_obj],
        activate_expert_=self._mm_activate_expert,
        # The warm cache is a transient, non-persistent dictionary.
        _v_warm_cache={} 
    )
    self.root['memory_manager_obj'] = memory_manager

    # --- O-RAG Knowledge Catalog Incarnation ---
    knowledge_catalog = UvmObject(
        parent*=[traits_obj],
        text_index=TextIndex(),
        metadata_index=BTrees.OOBTree.BTree(),
        index_document_=self._kc_index_document,
        unindex_document_=self._kc_unindex_document,
        search_=self._kc_search
    )
    self.root['knowledge_catalog_obj'] = knowledge_catalog

    # --- Prototypal State Machine Incarnation ---
    print("[UVM] Incarnating Prototypal State Machine...")
    failed_state = UvmObject(parent*=[traits_obj], name="FAILED", _process_synthesis_=self._psm_failed_process)
    idle_state = UvmObject(parent*=[traits_obj], name="IDLE", _process_synthesis_=self._psm_idle_process)
    decomposing_state = UvmObject(parent*=[traits_obj], name="DECOMPOSING", _process_synthesis_=self._psm_decomposing_process)
    delegating_state = UvmObject(parent*=[traits_obj], name="DELEGATING", _process_synthesis_=self._psm_delegating_process)
    synthesizing_state = UvmObject(parent*=[traits_obj], name="SYNTHESIZING", _process_synthesis_=self._psm_synthesizing_process)
    complete_state = UvmObject(parent*=[traits_obj], name="COMPLETE", _process_synthesis_=self._psm_complete_process)

    psm_prototypes = UvmObject(
        parent*=[traits_obj],
        IDLE=idle_state,
        DECOMPOSING=decomposing_state,
        DELEGATING=delegating_state,
        SYNTHESIZING=synthesizing_state,
        COMPLETE=complete_state,
        FAILED=failed_state
    )
    self.root['psm_prototypes_obj'] = psm_prototypes

    orchestrator = UvmObject(
        parent*=[traits_obj, pLLM_obj],
        start_cognitive_cycle_for_=self._orc_start_cognitive_cycle
    )
    self.root['orchestrator_obj'] = orchestrator
    
    # --- ALFRED Persona Prototype ---
    # ALFRED is the steward, requiring access to system internals for audits.
    alfred_prototype = UvmObject(
        parent*=[pLLM_obj, traits_obj],
        name="ALFRED"
    )
    self.root['alfred_prototype_obj'] = alfred_prototype

    print("[UVM] Core subsystems incarnated.")

# --------------------------------------------------------------------------
# Subsection III.B: The Generative & Cognitive Protocols
# --------------------------------------------------------------------------

def _clone_persistent(self, target_obj):
    """
    Performs a persistence-aware deep copy of a UvmObject. This is the
    canonical method for object creation, fulfilling the `copy` metaphor
    of the Self language. It ensures that the new object is a distinct
    entity within the ZODB transaction. [3, 11]
    """
    # Use Python's copy.deepcopy, which will invoke our custom __deepcopy__
    # method on UvmObject instances, ensuring a correct, deep, and
    # persistence-aware clone. [14]
    new_obj = copy.deepcopy(target_obj)
    return new_obj

async def _doesNotUnderstand(self, target_obj, failed_message_name, *args, **kwargs):
    """
    The universal generative mechanism. Re-architected to trigger the
    Prototypal State Machine for collaborative, multi-agent problem solving,
    transforming a message failure into a mission brief for the Composite
    Mind. [2, 8, 9]
    """
    print(f"[UVM] doesNotUnderstand: '{failed_message_name}' for OID {target_obj._p_oid}.")
    print("[UVM] Reifying failed message as a creative mandate for the Orchestrator.")

    command_payload = {
        "command": "initiate_cognitive_cycle",
        "target_oid": str(target_obj._p_oid),
        "mission_brief": {
            "type": "unhandled_message",
            "selector": failed_message_name,
            "args": args,
            "kwargs": kwargs
        }
    }
    
    # Enqueue the mission. The worker will pick this up and hand it to the
    # orchestrator within a new transaction. This decouples the immediate
    # failure from the complex, asynchronous resolution process.
    await self.message_queue.put((b'UVM_INTERNAL', ormsgpack.packb(command_payload)))
    
    return f"Mission to handle '{failed_message_name}' has been dispatched to the Composite Mind."

def _persistence_guardian(self, code_string: str) -> bool:
    """
    A non-negotiable protocol for maintaining system integrity. It performs
    static analysis on LLM-generated code *before* execution to
    deterministically enforce the Persistence Covenant (`_p_changed = True`),
    thereby preventing systemic amnesia. [16, 17, 8]
    """
    try:
        tree = ast.parse(code_string)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                modifies_state = False
                signals_persistence = False
                
                # Check for state modifications
                for body_item in node.body:
                    if isinstance(body_item, (ast.Assign, ast.AugAssign)):
                        for target in getattr(body_item, 'targets', [getattr(body_item, 'target', None)]):
                            if isinstance(target, ast.Attribute) and \
                               isinstance(target.value, ast.Name) and \
                               target.value.id == 'self' and \
                               target.attr!= '_p_changed':
                                modifies_state = True
                                break
                        if modifies_state:
                            break
                
                # If state is modified, ensure the covenant is met
                if modifies_state:
                    last_statement = node.body[-1]
                    if isinstance(last_statement, ast.Assign) and \
                       len(last_statement.targets) == 1 and \
                       isinstance(last_statement.targets, ast.Attribute) and \
                       isinstance(last_statement.targets.value, ast.Name) and \
                       last_statement.targets.value.id == 'self' and \
                       last_statement.targets.attr == '_p_changed':
                        signals_persistence = True
                    
                    if not signals_persistence:
                        print(f"[Guardian] VIOLATION: Method '{node.name}' modifies state but does not signal persistence.")
                        return False
        
        print("[Guardian] Code adheres to the Persistence Covenant.")
        return True
    except SyntaxError as e:
        print(f"[Guardian] Syntax error in generated code: {e}")
        return False

def _construct_architectural_covenant_prompt(self, target_obj, failed_message_name, intent_string=None, *args, **kwargs):
    """
    Constructs the structured, zero-shot prompt for JIT compilation,
    including the specialized mandate for Cognitive Facet generation. [2, 9]
    """
    is_facet_generation = failed_message_name.endswith('_facet_') and intent_string is not None
    facet_instructions = ""
    if is_facet_generation:
        facet_instructions = f"""


Cognitive Facet Generation Mandate:

This method is a 'Cognitive Facet'. Its purpose is to invoke the parent persona's own inference capability (self.infer_) with a specialized system prompt that embodies a specific inspirational pillar.

Pillar Intent: "{intent_string}"

Implementation: The generated function must construct a system prompt based on the Pillar Intent and then call self.infer_(self, user_query, system_prompt=specialized_prompt). The user_query will be passed as an argument to the facet method.
"""
  return f"""You are the BAT OS Universal Virtual Machine's Just-in-Time (JIT) Compiler for Intent.


An object has received a message it does not understand.

Your task is to generate the complete, syntactically correct Python code for a new method to handle this message.

Architectural Covenants (Non-Negotiable):

The code must be a single, complete Python function definition (def method_name(self,...):).

The function MUST accept self as its first argument, representing the UvmObject instance.

The function can access the object's state and behavior ONLY through self.slot_name. Direct access to self._slots is forbidden.

If the function modifies the object's state (e.g., self.some_slot = new_value), it MUST conclude with the line self._p_changed = True. This is The Persistence Covenant.

Do NOT include any conversational text, explanations, or markdown formatting. Output only the raw Python code.{facet_instructions}

Context for Generation:

Target Object OID: {target_obj._p_oid}

Target Object Slots: {list(target_obj._slots.keys())}

Failed Message Selector: {failed_message_name}

Message Arguments (args): {args}

Message Arguments (kwargs): {kwargs}

GENERATE METHOD CODE:

"""

def _pLLM_infer(self, pLLM_self, prompt: str, adapter_name: Optional[str] = None, **kwargs):
    """
    Hardware abstraction layer for inference. Sets the active LoRA adapter
    before generation. [18, 6, 19]
    """
    if not self.model:
        return "Error: Cognitive core is offline."

    if adapter_name:
        print(f"[pLLM] Activating adapter: {adapter_name.upper()}")
        self.model.set_adapter(adapter_name.upper())
    else:
        print("[pLLM] Using base model (all adapters disabled).")
        self.model.disable_adapters()

    inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
    outputs = self.model.generate(**inputs, max_new_tokens=2048, **kwargs)
    
    # Clean the generated text from the prompt
    full_text = self.tokenizer.decode(outputs, skip_special_tokens=True)
    prompt_end_marker = "**GENERATE METHOD CODE:**"
    code_start_index = full_text.rfind(prompt_end_marker)
    if code_start_index!= -1:
        generated_code = full_text[code_start_index + len(prompt_end_marker):].strip()
    else:
        # Fallback for non-JIT prompts
        generated_code = full_text

    # Further cleanup to remove potential code block markers
    if generated_code.startswith("```python"):
        generated_code = generated_code[len("```python"):].strip()
    if generated_code.endswith("```"):
        generated_code = generated_code[:-len("```")].strip()
        
    return generated_code

# --------------------------------------------------------------------------
# Subsection III.C: The Synaptic Memory Manager
# --------------------------------------------------------------------------

def _mm_activate_expert(self, memory_manager_self, expert_name: str):
    """
    Full protocol for activating an expert, managing the three-tier memory
    hierarchy: Cold (ZODB BLOB), Warm (RAM Cache), and Hot (VRAM). [2, 6, 1]
    """
    print(f"[MemMan] Received request to activate expert: {expert_name.upper()}")
    expert_name = expert_name.upper()

    # Tier 3: Hot (VRAM) - Check if the expert is already active
    if self.model.active_adapter == expert_name:
        print(f"[MemMan] Expert '{expert_name}' is already active in VRAM.")
        return True

    pLLM_obj = self.root['pLLM_obj']
    warm_cache = memory_manager_self._v_warm_cache

    # Tier 2: Warm (RAM) - Check if the expert is in the RAM cache
    if expert_name not in warm_cache:
        print(f"[MemMan] Expert '{expert_name}' not in RAM cache. Loading from Cold Storage...")
        # Tier 1: Cold (ZODB BLOB) - Load from persistent storage
        if expert_name not in pLLM_obj.lora_repository:
            print(f"[MemMan] ERROR: Expert '{expert_name}' not found in persistent repository.")
            return False
        
        proxy = pLLM_obj.lora_repository[expert_name]
        try:
            with proxy.model_blob.open('r') as blob_file:
                lora_data = blob_file.read()
            warm_cache[expert_name] = lora_data
            print(f"[MemMan] Expert '{expert_name}' loaded into RAM cache ({len(lora_data) / 1e6:.2f} MB).")
        except Exception as e:
            print(f"[MemMan] ERROR: Failed to load expert '{expert_name}' from BLOB: {e}")
            return False

    # Now, load the expert from RAM cache into VRAM
    try:
        temp_path = f"./temp_{expert_name}.safetensors"
        with open(temp_path, 'wb') as temp_f:
            temp_f.write(warm_cache[expert_name])
        
        # Unload current adapter if one is active to free VRAM
        if self.model.active_adapter is not None:
            current_adapter = self.model.active_adapter
            print(f"[MemMan] Deactivating '{current_adapter}' to free VRAM.")
            self.model.delete_adapter(current_adapter)

        self.model.load_adapter(temp_path, adapter_name=expert_name)
        os.remove(temp_path)
        self.model.set_adapter(expert_name)
        print(f"[MemMan] Expert '{expert_name}' is now active in VRAM.")
        return True
    except Exception as e:
        print(f"[MemMan] ERROR: Failed to load or activate expert '{expert_name}' from RAM to VRAM: {e}")
        if expert_name in self.model.peft_config:
            self.model.delete_adapter(expert_name)
        return False

# --------------------------------------------------------------------------
# Subsection III.D: The Fractal Memory (Knowledge Catalog)
# --------------------------------------------------------------------------

def _kc_index_document(self, catalog_self, doc_id: str, doc_text: str, metadata: dict):
    """
    Ingests and indexes a document into the Fractal Memory. Performs
    semantic chunking and populates the text and metadata indices. [2, 20]
    """
    print(f"[K-Catalog] Indexing document: {doc_id}")
    # Simple chunking logic (placeholder for a more sophisticated semantic chunker)
    chunk_size = 512
    chunks = [doc_text[i:i + chunk_size*4] for i in range(0, len(doc_text), chunk_size*4)]
    
    chunk_oids =
    for i, chunk_text in enumerate(chunks):
        chunk_obj = UvmObject(
            parent*=[self.root['traits_obj']],
            document_id=doc_id,
            chunk_index=i,
            text=chunk_text,
            metadata=metadata
        )
        chunk_oid = chunk_obj._p_oid
        chunk_oids.append(chunk_oid)
        catalog_self.text_index.index_doc(chunk_oid, chunk_text)

    catalog_self.metadata_index[doc_id] = chunk_oids
    catalog_self._p_changed = True
    print(f"[K-Catalog] Document {doc_id} indexed into {len(chunks)} chunks.")
    return chunk_oids

def _kc_unindex_document(self, catalog_self, doc_id: str):
    """Removes a document and all its chunks from the indices."""
    if doc_id not in catalog_self.metadata_index:
        return
    print(f"[K-Catalog] Un-indexing document: {doc_id}")
    chunk_oids = catalog_self.metadata_index[doc_id]
    for oid in chunk_oids:
        catalog_self.text_index.unindex_doc(oid)
        # The UvmObject itself would need to be garbage collected separately.
    del catalog_self.metadata_index[doc_id]
    catalog_self._p_changed = True

def _kc_search(self, catalog_self, query: str, top_k: int = 5):
    """
    Performs a search against the text index and retrieves the top_k most
    relevant MemoryChunk objects. [21, 22, 2]
    """
    print(f"[K-Catalog] Searching for: '{query}'")
    results = catalog_self.text_index.apply(query)
    if not results:
        return
    
    sorted_results = sorted(results.items(), key=lambda item: item[1], reverse=True)[:top_k]
    
    retrieved_chunks =
    for oid, score in sorted_results:
        try:
            chunk_obj = self.connection.get(oid)
            retrieved_chunks.append({"chunk": chunk_obj, "score": score})
        except KeyError:
            print(f"[K-Catalog] WARNING: OID {oid} found in index but not in database.")
    
    return retrieved_chunks

# --------------------------------------------------------------------------
# Subsection III.E: The Prototypal State Machine (Orchestrator & States)
# --------------------------------------------------------------------------

def _orc_start_cognitive_cycle(self, orchestrator_self, mission_brief: dict, target_obj_oid: str):
    """
    Factory method for creating and starting a new cognitive cycle. This is
    the entry point for the Prototypal State Machine. [2]
    """
    print(f"[Orchestrator] Initiating new cognitive cycle for mission: {mission_brief.get('type', 'unknown')}")
    
    cycle_context = UvmObject(
        parent*=[self.root['traits_obj']],
        mission_brief=mission_brief,
        target_oid=target_obj_oid,
        _tmp_synthesis_data=persistent.mapping.PersistentMapping(),
        synthesis_state*=self.root['psm_prototypes_obj'].IDLE
    )
    
    if 'active_cycles' not in self.root:
        self.root['active_cycles'] = BTrees.OOBTree.BTree()
    
    cycle_oid = str(cycle_context._p_oid)
    self.root['active_cycles'][cycle_oid] = cycle_context
    print(f"[Orchestrator] New CognitiveCycle created with OID: {cycle_oid}")
    
    # Send the initial processing message to the newly created cycle object.
    # The message will be delegated to the IDLE state prototype.
    # This initial call happens within the same transaction.
    cycle_context.synthesis_state*._process_synthesis_(cycle_context)
    return cycle_context

def _psm_transition_to(self, cycle_context, new_state_prototype):
    """Helper function to perform a state transition."""
    print(f" Transitioning OID {cycle_context._p_oid} to state: {new_state_prototype.name}")
    cycle_context.synthesis_state* = new_state_prototype
    cycle_context._p_changed = True
    # Immediately process the new state within the same transaction.
    new_state_prototype._process_synthesis_(cycle_context)

def _psm_idle_process(self, cycle_context):
    """IDLE State: Awaits a mission and transitions to DECOMPOSING."""
    print(f" Cycle {cycle_context._p_oid} activated (IDLE).")
    cycle_context._tmp_synthesis_data['original_query'] = cycle_context.mission_brief
    cycle_context._tmp_synthesis_data['start_time'] = time.time()
    cycle_context._p_changed = True
    self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].DECOMPOSING)

def _psm_decomposing_process(self, cycle_context):
    """DECOMPOSING State: Analyzes the query to create a synthesis plan."""
    print(f" Cycle {cycle_context._p_oid} is creating a synthesis plan (DECOMPOSING).")
    # In a real implementation, this would invoke the BRICK persona with a
    # "decomposition meta-prompt". Here we simulate a successful decomposition.
    plan = {
        "strategy": "Dialectical Synthesis",
        "relevant_pillars": ["sage_facet_", "simple_heart_facet_"],
        "sub_queries": {
            "sage_facet_": "How would a non-dual philosopher frame this problem?",
            "simple_heart_facet_": "What is the kindest, simplest response to this situation?"
        }
    }
    cycle_context._tmp_synthesis_data['plan'] = plan
    cycle_context._p_changed = True
    print(f" Plan created: {plan['strategy']}")
    self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].DELEGATING)

def _psm_delegating_process(self, cycle_context):
    """DELEGATING State: Invokes the required Cognitive Facets."""
    print(f" Cycle {cycle_context._p_oid} is delegating to cognitive facets (DELEGATING).")
    plan = cycle_context._tmp_synthesis_data['plan']
    partial_responses = {}
    # This would involve finding the target persona object and sending the
    # `invoke_facet_` message, which would be JIT-compiled on first use.
    for pillar, sub_query in plan['sub_queries'].items():
        print(f"  - Invoking facet: {pillar} with query: '{sub_query}'")
        if pillar == "sage_facet_":
            partial_responses[pillar] = "The problem is not a problem to be solved, but a reality to be accepted."
        elif pillar == "simple_heart_facet_":
            partial_responses[pillar] = "Perhaps a small smackerel of something would help."
    
    cycle_context._tmp_synthesis_data['partial_responses'] = partial_responses
    cycle_context._p_changed = True
    print(f" All partial responses collected.")
    self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].SYNTHESIZING)

def _psm_synthesizing_process(self, cycle_context):
    """SYNTHESIZING State: Executes Cognitive Weaving to generate the final response."""
    print(f" Cycle {cycle_context._p_oid} is performing Cognitive Weaving (SYNTHESIZING).")
    original_query = cycle_context._tmp_synthesis_data['original_query']['selector']
    partials = cycle_context._tmp_synthesis_data['partial_responses']
    
    # This would invoke the ROBIN persona with a "synthesis meta-prompt".
    final_response = f"In response to '{original_query}', consider that while the problem may seem complex, true wisdom lies in accepting what is. And in the meantime, perhaps a small smackerel of something would help."
    
    cycle_context._tmp_synthesis_data['final_response'] = final_response
    cycle_context._p_changed = True
    print(f" Final response generated.")
    self._psm_transition_to(cycle_context, self.root['psm_prototypes_obj'].COMPLETE)

def _psm_complete_process(self, cycle_context):
    """COMPLETE State: Cleans up and signals completion."""
    print(f" Cycle {cycle_context._p_oid} has completed successfully (COMPLETE).")
    final_response = cycle_context._tmp_synthesis_data['final_response']
    
    print(f"--- FINAL SYNTHESIZED RESPONSE ---\n{final_response}\n--------------------------------")
    
    cycle_oid = str(cycle_context._p_oid)
    if 'active_cycles' in self.root and cycle_oid in self.root['active_cycles']:
        del self.root['active_cycles'][cycle_oid]
    # The overarching ZODB transaction can now be committed.

def _psm_failed_process(self, cycle_context):
    """FAILED State: Logs the error and dooms the transaction."""
    print(f" Cycle {cycle_context._p_oid} has failed. Aborting transaction (FAILED).")
    
    # Log the error context for later analysis by ALFRED
    if 'failed_cycle_log' not in self.root:
        self.root['failed_cycle_log'] = BTrees.OOBTree.BTree()
    
    cycle_oid = str(cycle_context._p_oid)
    self.root['failed_cycle_log'][cycle_oid] = {
        'timestamp': time.time(),
        'mission_brief': cycle_context.mission_brief,
        'last_state': cycle_context.synthesis_state*.name
    }
    
    # Doom the current transaction to ensure atomicity. This call prevents
    # any changes from this cycle from being persisted. [23, 2]
    transaction.doom()
    
    if 'active_cycles' in self.root and cycle_oid in self.root['active_cycles']:
        del self.root['active_cycles'][cycle_oid]

# --------------------------------------------------------------------------
# Subsection III.F: Core Asynchronous Services
# --------------------------------------------------------------------------

async def worker(self, name: str):
    """
    Pulls messages from the queue and processes them in a transactional
    context. Each message is a discrete, atomic unit of work. [2, 7]
    """
    print(f"[{name}] Worker started.")
    conn = self.db.open()
    root = conn.root()

    while not self.should_shutdown.is_set():
        try:
            identity, message_data = await self.message_queue.get()
            print(f"[{name}] Processing message from {identity.decode()}")
            
            try:
                with transaction.manager:
                    command_dict = ormsgpack.unpackb(message_data)
                    command_name = command_dict.get("command")

                    if command_name == "initiate_cognitive_cycle":
                        target_oid = command_dict['target_oid']
                        mission_brief = command_dict['mission_brief']
                        orchestrator = root['orchestrator_obj']
                        orchestrator.start_cognitive_cycle_for_(orchestrator, mission_brief, target_oid)
                    
                    # Add other direct command handlers here as the system evolves.
                    
            except Exception as e:
                print(f"[{name}] ERROR processing message: {e}")
                traceback.print_exc()
                transaction.abort()
            finally:
                self.message_queue.task_done()
        except asyncio.CancelledError:
            break
    
    conn.close()
    print(f"[{name}] Worker stopped.")

async def zmq_listener(self):
    """Listens on the ZMQ ROUTER socket for incoming messages."""
    self.zmq_socket.bind(ZMQ_ENDPOINT)
    print(f"[UVM] Synaptic Bridge listening on {ZMQ_ENDPOINT}")
    while not self.should_shutdown.is_set():
        try:
            identity, message = await self.zmq_socket.recv_multipart()
            await self.message_queue.put((identity, message))
        except asyncio.CancelledError:
            break
    print("[UVM] ZMQ listener stopped.")

async def autotelic_loop(self):
    """
    The system's 'heartbeat' for self-directed evolution. Periodically
    initiates a "Cognitive Efficiency Audit" driven by ALFRED. [2, 7]
    """
    print("[UVM] Autotelic Heartbeat started.")
    await asyncio.sleep(30)  # Initial delay

    while not self.should_shutdown.is_set():
        try:
            await asyncio.sleep(300)  # Audit every 5 minutes
            print("[Heartbeat] Initiating Cognitive Efficiency Audit...")

            # This logic must use its own connection to be thread-safe.
            audit_conn = self.db.open()
            audit_root = audit_conn.root()
            try:
                if 'failed_cycle_log' in audit_root and audit_root['failed_cycle_log']:
                    num_failures = len(audit_root['failed_cycle_log'])
                    print(f"[Heartbeat] AUDIT: Found {num_failures} logged cycle failures.")
                    
                    mission_payload = {
                        "command": "initiate_cognitive_cycle",
                        "target_oid": str(audit_root['alfred_prototype_obj']._p_oid),
                        "mission_brief": {
                            "type": "systemic_self_improvement",
                            "selector": "analyze_and_resolve_cycle_failures",
                            "args": (list(audit_root['failed_cycle_log'].values()),),
                            "kwargs": {}
                        }
                    }
                    
                    await self.message_queue.put((b'ALFRED_AUDITOR', ormsgpack.packb(mission_payload)))
                    
                    # Clear the log after dispatching the mission
                    with transaction.manager:
                        audit_root['failed_cycle_log'].clear()
                else:
                    print("[Heartbeat] AUDIT: No cycle failures found.")
            finally:
                audit_conn.close()

        except asyncio.CancelledError:
            break
        except Exception as e:
            print(f"[Heartbeat] ERROR during audit: {e}")
    
    print("[UVM] Autotelic Heartbeat stopped.")

def _signal_handler(self, sig, frame):
    """Handles signals like SIGTERM for graceful shutdown."""
    print(f"\n[UVM] Received signal {sig}. Initiating graceful shutdown...")
    self.should_shutdown.set()

async def run(self):
    """Main entry point to start all UVM services."""
    await self.initialize_system()

    # Set up signal handlers
    import signal
    signal.signal(signal.SIGTERM, self._signal_handler)
    signal.signal(signal.SIGINT, self._signal_handler)

    # Start core services
    listener_task = asyncio.create_task(self.zmq_listener())
    worker_tasks =
    autotelic_task = asyncio.create_task(self.autotelic_loop())
    
    print("[UVM] System is live. Awaiting creative mandates.")
    
    await self.should_shutdown.wait()
    
    # Cancel running tasks
    listener_task.cancel()
    for task in worker_tasks:
        task.cancel()
    autotelic_task.cancel()
    
    await asyncio.gather(listener_task, *worker_tasks, autotelic_task, return_exceptions=True)
    self.shutdown()

def shutdown(self):
    """Performs a final commit and closes database connections."""
    print("[UVM] Shutting down...")
    try:
        transaction.commit()
    except transaction.interfaces.NoTransaction:
        pass  # No changes to commit
    except Exception as e:
        print(f"[UVM] Error during final commit: {e}. Aborting.")
        transaction.abort()
    
    self.connection.close()
    self.db.close()
    print("[UVM] Shutdown complete. Identity preserved in live_image.fs.")


==============================================================================

SECTION IV: SYSTEM EXECUTION BLOCK

==============================================================================

if name == 'main':

uvm = BatOS_UVM(DB_FILE, BLOB_DIR)

try:

asyncio.run(uvm.run())

except KeyboardInterrupt:

print("[UVM] Architect initiated shutdown via KeyboardInterrupt.")

Works cited

Evolving BatOS: Fractal Cognition Augmentation

Fractal Cognition Engine Integration Plan

Refining System for Prototypal Approach

Fractal OS Design: Morphic UI Generation

Batos.py: Cognitive Ecosystem Architecture

Architecting a Self-Educating AI System

Critiquing BAT OS Fractal Architecture

Persona-Level Synthesis Architecture Design

Memory-Aware O-RAG Architecture Refinement

Training LLM for Self's `doesNotUnderstand:`