From Autopoiesis to Compositionality: A Critical Analysis and Research Agenda for the TelOS Neuro-Symbolic Architecture

Section 1: Introduction: Situating the TelOS Mandate in the Landscape of Neuro-Symbolic AI

This analysis establishes the intellectual context for the TelOS project, synthesizing its unique philosophical underpinnings and demonstrating how they mandate a specific set of technical challenges. The system's core philosophy—"info-autopoiesis"—is not merely a conceptual framework but the primary driver of its architecture, leading directly to its central research problem: the resolution of the "Cognitive-Mnemonic Impedance Mismatch." By framing this as a canonical problem in the field of neuro-symbolic AI, the proposed "Unifying Grammar" can be situated within established academic taxonomies, enabling a rigorous, externally-bolstered evaluation of its design and potential contributions.

1.1 The Autopoietic Imperative as an Architectural Driver

The TelOS architecture is distinguished by its deep commitment to a single philosophical premise: info-autopoiesis, defined as the self-referential, recursive, and interactive process of the self-production of information.1 Derived from the biological theory of Maturana and Varela, this principle posits that a living system is a network of processes that continuously regenerates the very network that produced it.3 This is not a post-hoc justification for a set of design choices but the prime directive from which the entire technical stack is logically derived in an "unbroken causal chain".1

This commitment to continuous self-production necessitates a state of "Organizational Closure," the ability for the system to modify its own structure at runtime without halting or requiring external intervention.2 This single requirement logically forbids conventional static persistence models, which breach the system's operational boundary with every restart, and forces the adoption of the "Living Image" paradigm inherited from Smalltalk.1 In this model, the system's entire state—code, data, and cognitive architecture—is persisted as a single, transactionally coherent entity within a Zope Object Database (ZODB).2 For this "Living Image" to be truly dynamic, a rigid class-instance duality is rejected in favor of a prototype-based object model inspired by the Self programming language, where new objects are created by cloning and modifying existing exemplars.1 This entire structure is animated by the

doesNotUnderstand_ protocol, which reframes runtime AttributeError exceptions not as terminal failures but as the primary trigger for creative self-modification and learning.1

This tight coupling between a philosophical foundation and its technical implementation is a significant architectural asset, resulting in a highly consistent and integrated design. It contrasts sharply with many contemporary AI systems, which are often assembled from pragmatically chosen but philosophically disjoint components. However, this coherence also introduces a systemic risk: the validity of the entire architecture rests upon the validity of its core premise. If the principles of info-autopoiesis provide a sound basis for an intelligent system, the resulting architecture is exceptionally robust; if the premise is flawed, the entire structure may prove brittle. Any evaluation of TelOS must therefore assess not only its performance on specific tasks but also the soundness of the philosophical-technical deduction that underpins its existence.

1.2 The "Cognitive-Mnemonic Impedance Mismatch" as a Neuro-Symbolic Problem

The TelOS MVA, in its current state, embodies a classic challenge in artificial intelligence: the "Cognitive-Mnemonic Impedance Mismatch".11 This refers to the fundamental disconnect between two powerful but isolated modes of representation:

A geometric, metric space of semantic embeddings, optimized for similarity-based reasoning. This is the foundation of the system's Retrieval-Augmented Generation (RAG) capabilities, where meaning is determined by proximity in a high-dimensional vector space.12

An algebraic space of symbolic hypervectors, optimized for compositional reasoning. This is the domain of Vector Symbolic Architectures (VSA), where meaning is constructed through discrete, rule-based algebraic operations.6

Without a "Unifying Grammar" to bridge these two modalities, they are relegated to a simplistic master-servant relationship, where the powerful semantic index is used merely for denoising the outputs of algebraic operations.11 This problem places the TelOS project squarely within the academic field of Neuro-Symbolic AI (NSAI), which explicitly seeks to create hybrid systems that combine the strengths of connectionist deep learning (robust pattern recognition from unstructured data) with those of classical symbolic AI (structured reasoning, interpretability, and compositionality).13

The proposed solution within the TelOS framework is the "Unifying Grammar," an architecture designed to create a formal, shared substrate upon which both representations can operate synergistically. This is achieved by formalizing the system's fractal memory as a Hierarchical Knowledge Graph (HKG) and elevating VSA to a formal algebra that operates over the typed relationships within that graph.10

1.3 Architectural Analogy to Dual-Process Theory

The design of the Unifying Grammar explicitly draws an analogy to dual-process theories of human cognition, most famously articulated by Daniel Kahneman and Amos Tversky.2 This theory posits two distinct modes of thought:

System 1: Fast, intuitive, automatic, and associative. It operates on patterns and heuristics and is responsible for the majority of our effortless cognitive tasks.20

System 2: Slow, deliberate, sequential, and rule-based. It requires conscious effort and working memory and is engaged for complex, logical reasoning.19

The TelOS architecture maps its two representational spaces directly onto this cognitive model. The geometric RAG space, with its fast, similarity-based ANN search, functions as the computational equivalent of System 1. The algebraic VSA space, with its multi-step, compositional query capabilities, functions as the equivalent of System 2.11 This framing is not merely a convenient metaphor; it aligns the TelOS architecture with a growing movement in the NSAI community that views the System 1/System 2 distinction as a powerful conceptual blueprint for designing hybrid intelligent systems.13

Furthermore, the TelOS design moves beyond a simple descriptive analogy to a generative one. Many neuro-symbolic models implement a sequential pipeline where a neural "perceptual" front-end feeds information to a symbolic "reasoning" back-end.14 The TelOS architecture, however, proposes a more dynamic and symbiotic interaction. The "Constrained Cleanup Operation," a core mechanism of the Unifying Grammar, is a concrete, algorithmic implementation of the

interaction between System 1 and System 2. In this model, the intuitive System 1 (RAG) does not simply pass a result to the deliberate System 2 (VSA); it generates a candidate space that actively constrains the search domain for System 2's reasoning operations.10 This suggests a more sophisticated cognitive model than a simple pipeline, where the two systems engage in a tight, turn-by-turn dialogue. A key hypothesis for this research agenda, therefore, is to determine whether this interactive constraint mechanism produces qualitatively superior reasoning outcomes compared to simpler sequential neuro-symbolic architectures.

Section 2: A Critical Review of the Algebraic Substrate: Vector Symbolic Architectures

This section provides a detailed analysis of the Vector Symbolic Architecture component of the TelOS system. It critically evaluates the selection of the FHRR model, the mechanics of the proposed unbind -> cleanup reasoning loop, and the novel repurposing of Approximate Nearest Neighbor indexes as a VSA cleanup memory. This evaluation is grounded in external academic surveys and technical papers on VSA and Hyperdimensional Computing (HDC) to assess the soundness, novelty, and potential risks of the proposed implementation.

2.1 VSA Model Selection: Fourier Holographic Reduced Representations (FHRR)

The initial and most critical decision in implementing a VSA is the choice of the underlying algebraic model. The TelOS plan makes a well-reasoned selection of Fourier Holographic Reduced Representations (FHRR) over alternatives such as Binary Spatter Codes (BSC) and Multiply-Add-Permute (MAP).6 The primary justification is the need for coherence with the MVA's existing architecture, which is built upon dense, real-valued vectors for its RAG component.

Binary Spatter Codes (BSC) operate on binary vectors, using bitwise XOR for binding and a majority-rule sum for bundling. While computationally efficient, especially in hardware, the binary nature creates a significant impedance mismatch with the real-valued embeddings of the RAG system.6

Multiply-Add-Permute (MAP) models typically use bipolar vectors ({-1, 1}) and element-wise multiplication for binding. Like BSC, this is not a natural fit for an architecture already leveraging the rich, continuous space of deep learning embeddings.6

Holographic Reduced Representations (HRR), and specifically its FHRR variant, are based on real or complex-valued vectors. In FHRR, the binding operation of circular convolution becomes an efficient element-wise complex multiplication in the frequency domain, while bundling is simple vector addition.6 This use of dense, continuous-valued vectors aligns perfectly with the representations used in the MVA's RAG system and modern machine learning in general.

This choice is validated by comprehensive VSA surveys, which highlight the trade-offs between binary models (simplicity, hardware-friendliness) and real/complex-valued models like HRR (representational richness, differentiability, and compatibility with neural networks).28 The plan also demonstrates a mature understanding of the chosen model by correctly identifying a key weakness of FHRR: the potential for numerical instability when used in differentiable architectures, a known issue that requires mitigation strategies like projection steps.6

2.2 The unbind -> cleanup Cognitive Loop

The core mechanism for compositional reasoning in the proposed architecture is the unbind -> cleanup cycle.2 This is a canonical pattern in VSA literature.28 The process unfolds in two stages:

Unbind (Algebraic Computation): The inverse of the binding operation is applied to a composite hypervector to retrieve a constituent part. For example, given a composite vector V=bind(A,B), the operation unbind(V,A) is performed. Due to the distributed nature of the representations and noise introduced by bundling multiple pairs, this operation does not yield the exact vector B, but rather a noisy version, B′.6

Cleanup (Geometric Search): The noisy vector B′ is used as a query to find the most similar "clean" vector from a known codebook of all atomic hypervectors. This is explicitly a nearest-neighbor search problem, where the codebook is often referred to as an "associative memory" or "cleanup memory".6

This two-step process allows the system to perform precise, multi-hop reasoning that is intractable for conventional RAG systems. For instance, a query like "What did the entity that John works for acquire?" can be decomposed into a sequence of unbind -> cleanup operations to traverse the knowledge graph algebraically.6

2.3 Innovation: ANN Indexes as a Massively Scalable Cleanup Memory

The central and most novel hypothesis of the TelOS VSA architecture is the proposal to use existing, highly-optimized Approximate Nearest Neighbor (ANN) search libraries—specifically FAISS for in-memory operations and DiskANN for on-disk archival—as the physical implementation of the VSA cleanup memory.2

This approach has the profound advantage of leveraging mature, battle-tested technology capable of performing nearest-neighbor search at the billion-vector scale, which could solve a major scalability challenge for practical VSA implementations.40 The architecture pragmatically repurposes the geometric search infrastructure, originally built for the RAG system, to serve the needs of the algebraic reasoning system.

This design choice, however, introduces a critical research question. The "cleanup" operation in VSA ideally requires an exact nearest-neighbor search to guarantee the correct denoising of the query vector. ANN algorithms, by definition, trade a degree of accuracy for significant gains in speed and scalability.41 Therefore, the primary technical risk of this approach is whether the "approximate" nature of the search is sufficient for the high-precision demands of VSA cleanup. The potential for approximation errors to compound across multiple steps of a multi-hop reasoning query must be considered a key area for empirical investigation.

The architecture's design also pragmatically sidesteps one of the most significant challenges in modern VSA research: the development of fully learnable VSA encoders and end-to-end differentiable operations.12 The

unbind operation, as implemented in libraries like torchhd, can be part of a differentiable computation graph.54 However, the proposed

cleanup step is a non-differentiable call to an external, pre-built ANN index. This creates a modular pipeline rather than a system that can be trained end-to-end via backpropagation. This is a sound engineering trade-off, sacrificing the long-term goal of end-to-end learnability for the immediate benefits of massive scalability and performance. It makes the system buildable with current technology while highlighting a clear future evolutionary path toward more deeply integrated, learnable models as the research in that area matures.

Finally, the selection of FHRR, based on circular convolution, implicitly equips the system with a powerful mechanism for analogical reasoning, a capability that is not fully exploited in the current plan. The structure of HRR/FHRR vectors has been shown in cognitive science literature to be particularly well-suited for modeling the kind of structural similarity that underpins human analogy.32 While the TelOS documents focus on deductive, path-finding queries, the underlying mathematics is also highly effective for analogical queries like "What is the dollar of Mexico?".6 This suggests an underexplored capability. The research agenda should be expanded to explicitly include benchmarks for analogical reasoning, connecting the technical choice of FHRR to the foundational cognitive theories of analogy from researchers such as Dedre Gentner (Structure-Mapping Theory) and Keith Holyoak (Multiconstraint Theory).55

Section 3: The Geometric-Algebraic Interface: A Deep Analysis of the "Unifying Grammar"

This section deconstructs the specific mechanisms proposed to bridge the geometric and algebraic worlds within the TelOS MVA. It provides a detailed analysis of the "Semantic-Weighted Bundling" and "Constrained Cleanup Operation" as the core innovations of the Unifying Grammar. These mechanisms are evaluated by grounding them in external research and assessing their potential impact on the system's reasoning performance, accuracy, and robustness.

3.1 Multi-Space Embeddings: Euclidean and Hyperbolic Geometries

A cornerstone of the Unifying Grammar is the adoption of a multi-space representation for concepts within the Hierarchical Knowledge Graph (HKG).10 This is a sophisticated architectural choice that demonstrates an awareness of the state-of-the-art in knowledge graph representation learning. The plan specifies that each

ConceptFractal will possess a portfolio of vectors:

Euclidean Embeddings: To maintain compatibility with the existing RAG system, standard Euclidean embeddings will be used to represent semantic similarity or "aboutness." Proximity in this space, measured by cosine similarity, is well-suited for finding topically related concepts.

Hyperbolic Embeddings: To represent the object's position within the conceptual hierarchy of the HKG, a secondary hyperbolic embedding will be generated. Extensive research has shown that the geometry of hyperbolic space, a Riemannian manifold with constant negative curvature, is intrinsically superior for embedding tree-like or hierarchical structures. In hyperbolic space, the volume of a hypersphere grows exponentially with its radius, providing significantly more "room" to embed child nodes under a parent without them becoming metrically close to each other, a problem that plagues Euclidean embeddings.11

The plan's proposal to adapt the Hierarchical Hyperbolic Neural Graph Embedding (H2E) model is particularly well-founded, as this model explicitly uses a dual-embedding in hyperbolic polar space to model both inter-level (parent-child) and intra-level (sibling) relationships.11 This multi-space representation makes the system "multilingual," capable of reasoning about both semantic content and structural position as distinct but complementary aspects of a concept's meaning.

3.2 Mechanism I: Semantic-Weighted Bundling

The first concrete integration mechanism, Semantic-Weighted Bundling, creates a powerful one-way bridge from the geometric space to the algebraic space.10 When the

MemoryCurator agent creates a new ConceptFractal by abstracting over a cluster of ContextFractals, the VSA bundling operation (vector addition) is not a simple, unweighted sum. Instead, the contribution of each constituent hypervector is modulated by a weight derived from the geometric space.

The algorithm is as follows: first, the geometric centroid of the RAG embeddings for all ContextFractals in the cluster is calculated in Euclidean space. This centroid represents the semantic "center of mass" of the emergent concept. Then, for each ContextFractal, its semantic relevance weight is calculated as the cosine similarity between its embedding and the cluster centroid. Finally, the hypervector for the new ConceptFractal, Hconcept​, is computed as a weighted sum: Hconcept​=∑i​si​⋅Hi​, where Hi​ is the hypervector of a constituent ContextFractal and si​ is its calculated semantic weight.11

This technique ensures that experiences more central to a concept's core meaning have a proportionally greater influence on its final symbolic representation, while down-weighting the influence of outliers or tangential information. This directly uses semantic structure to refine the construction of algebraic symbols, resulting in cleaner and more robust abstractions.

3.3 Mechanism II: The Constrained Cleanup Operation

The most significant and innovative architectural proposal is the Constrained Cleanup Operation, which forges a dynamic, bidirectional link between the two reasoning modalities.10 It transforms the simplistic

unbind -> cleanup loop into a form of context-aware query optimization. The process is executed as part of a hybrid query plan:

Semantic Subspace Definition: An initial semantic RAG search is performed. For a query like "Find the headquarters of BMW," a preliminary search might be for "information related to German car manufacturers." This search returns a set of top-k objects. Crucially, this set is not treated as the final answer but as a filter that defines a semantic subspace or a region of interest within the system's total knowledge base.

Algebraic Operation: The VSA engine performs its unbind operation (e.g., unbind(HBMW​,HHAS_HQ​)), which produces a noisy target hypervector, Hcity′​.

Constrained Cleanup Search: The final cleanup step is executed not as a global search across the entire ANN index, but as a Constrained Nearest Neighbor (CNN) search. The search for the vector closest to Hcity′​ is restricted to only consider the hypervectors of concepts that fall within the semantic subspace defined in the first step.

This mechanism creates a dynamic dialogue between the two systems. The algebraic system makes a proposal ("I'm looking for a concept structurally similar to this noisy vector"), and the geometric system provides the necessary context ("You should only look for it within this specific semantic neighborhood"). This approach offers a powerful defense against a common VSA failure mode often termed "context poisoning." A standard VSA cleanup search might find that the noisy vector for "Munich" (HMunich′​) is, due to noise, geometrically closer to the clean vector for "Paris" than it is to the clean vector for "Munich." A global search would incorrectly return "Paris." However, in the TelOS architecture, the initial semantic search for "German car manufacturers" would create a constraint set that includes "Munich" but excludes "Paris." The constrained search would therefore reject "Paris" even if it were the closest vector, and correctly return "Munich." This use of the semantic system as a "common sense" filter dramatically improves the robustness and accuracy of VSA reasoning.

However, this tight coupling also introduces a critical, unaddressed vulnerability. The performance of the entire hybrid reasoning chain becomes dependent on the quality of the initial RAG retrieval. Modern RAG systems are known to suffer from the "lost-in-the-middle" problem, where the attention mechanism of Large Language Models struggles to effectively retrieve and utilize information that is not located at the beginning or end of a long context window.67 If the initial RAG search fails to retrieve the necessary documents to form an accurate semantic subspace—perhaps because the key information was buried in the middle of a document—it will starve the subsequent VSA reasoning step of the correct candidates. This would cause the entire multi-hop query to fail, not because of a flaw in the symbolic reasoning, but because of a known fallibility in the semantic retrieval front-end. This dependency represents a potential single point of failure that the research and validation agenda must explicitly investigate and seek to mitigate.

Section 4: The Engine of Learning: Validating the Autopoietic Abstraction Pipeline

This section analyzes the proposed mechanisms for autonomous, cumulative learning, which are designed to close the "Amnesiac Abstraction" gap identified in the MVA's current state.10 The algorithms and protocols of the "Mnemonic Curation Pipeline" are evaluated by connecting them to established machine learning and software engineering patterns, assessing their potential to enable true, long-term knowledge acquisition.

4.1 Emergent Concept Discovery: Accelerated Relational Clustering

The foundational step of the learning pipeline is the identification of emergent concepts from the raw, high-entropy ContextFractals stored in the L2 archival memory.10 The plan mandates the use of DBSCAN (Density-Based Spatial Clustering of Applications with Noise), a choice well-justified by its ability to discover clusters of arbitrary shapes without requiring the number of clusters (

k) to be specified in advance.10

A naive implementation of DBSCAN, which has O(n2) complexity for its core regionQuery operation, is computationally infeasible at the scale of millions or billions of vectors. The critical innovation proposed is an accelerated DBSCAN that leverages the high-performance range_search capabilities of the underlying FAISS and DiskANN indexes.10 This offloads the most expensive part of the clustering algorithm to the highly optimized C++ backends of the ANN libraries, making large-scale density clustering a practical reality.

The plan also demonstrates a forward-looking architectural vision by identifying a future evolutionary path: framing the task not as geometric clustering but as graph-based community detection.10 Algorithms such as the Louvain method could identify concepts based not only on the semantic similarity of nodes but also on the density of explicit relational links between them in the HKG. This would allow the system to discover concepts defined by how they are structurally related to other knowledge, providing a far richer basis for abstraction.11

4.2 Concept Synthesis: LLM-Driven Abstractive Summarization

Once a cluster of ContextFractals is identified, its collective meaning must be distilled into a concise, low-entropy definition for a new ConceptFractal. This is a multi-document abstractive summarization task, which is a natural and powerful application for a Large Language Model.10 The process involves retrieving the full text content for all cluster members from the L3 ZODB ground-truth store and using a sophisticated, multi-part prompt to guide the LLM. The emphasis on engineering a prompt that encourages true abstraction and paraphrasing, rather than simple extraction and concatenation, is critical for creating high-quality, genuinely novel conceptual definitions.11

This entire learning loop can be understood as a form of self-supervised knowledge graph construction. The Mnemonic Curation Pipeline is, in effect, an unsupervised algorithm that begins with unstructured text (the content of ContextFractals) and autonomously produces structured entities (new ConceptFractal objects) and typed relationships (the ABSTRACTS_FROM edges linking the new concept to its constituents). This automates a process that, in traditional knowledge engineering, is often manual or heavily supervised. The research agenda should therefore include a validation step that compares the quality of the autonomously generated HKG against established KG construction benchmarks.

4.3 The Autopoietic Bottleneck and On-Demand Abstraction

The design astutely identifies a potential "Autopoietic Bottleneck" arising from the different timescales of its core loops: a fast, synchronous, high-priority reasoning loop (doesNotUnderstand_) and a slow, asynchronous, low-priority background curation pipeline.10 The reasoning loop may require a

ConceptFractal for a compositional query that the curation pipeline has not yet had time to create.

The proposed solution is a mechanism for "On-Demand Abstraction".10 This protocol empowers the query layer, upon failing to find a required

ConceptFractal, to trigger a high-priority, targeted execution of the Mnemonic Curation Pipeline on a small, relevant subset of ContextFractals identified by a preliminary RAG search. This approach can be framed within established software engineering patterns like the Cache-Aside pattern or Just-in-Time (JIT) data materialization. It transforms the learning process from a purely passive, background task into a dynamic, just-in-time knowledge synthesis engine that is tightly coupled with the system's immediate cognitive needs.

A final, critical aspect of this learning pipeline is its transactional integrity. The plan correctly emphasizes that the integration of a newly synthesized ConceptFractal—including its embeddings, hypervector, and graph linkages—must be performed as a single, atomic ZODB transaction.10 This is not merely a database-level detail; it is a fundamental guarantee of cognitive consistency. A system crash mid-process could leave the symbolic graph (ZODB) and the semantic search indexes (FAISS/DiskANN) in a dangerously inconsistent state. The mandated use of a two-phase commit protocol, managed by the

FractalMemoryDataManager, is the correct and only robust solution to this "ZODB Indexing Paradox".2 This ensures that the system's "thought"—the creation of a new concept—is either fully completed across all cognitive substrates or completely rolled back, preventing a state of partial or corrupted knowledge.

Section 5: An Expanded Research and Validation Agenda: The "Compositional Gauntlet"

This section transforms the internal validation plan into a rigorous, externally-grounded research agenda. It proposes a phased approach to systematically de-risk the project's core technical hypotheses and defines a comprehensive benchmark, the "Compositional Gauntlet," designed to empirically prove the superiority of the hybrid architecture by specifically targeting the known failure modes of conventional RAG systems.

5.1 Phase 1: Foundational VSA Benchmarking

Objective: To validate the core algebraic components and assess the performance and scalability of the novel ANN-based cleanup memory.

Tasks:

Implement FHRR Algebra Library: Develop the core FHRR operations (bind, bundle, unbind) using a high-performance library such as torchhd.6

Benchmark unbind -> cleanup Cycle: Create a benchmark to measure the recall@k and query latency of the unbind -> cleanup cycle using both FAISS and DiskANN as the cleanup memory.

Noise Tolerance Analysis: Critically, this benchmark must evaluate performance under increasing levels of bundling noise. This involves creating composite hypervectors by bundling an increasing number of role-filler pairs and measuring the degradation in cleanup accuracy. This will determine the practical limits of the ANN-based cleanup approach and identify its breaking point.

5.2 Phase 2: Evaluating the Neuro-Symbolic Interface

Objective: To empirically quantify the performance gains from the two core integration mechanisms of the Unifying Grammar.

Tasks:

Constrained Cleanup Efficacy: Design a controlled experiment comparing a VSA query's performance with and without the semantic pre-filtering from the RAG system. The key metrics will be the reduction in false positives (semantically irrelevant but structurally similar results) and the improvement in query latency due to the reduced search space.

Semantic-Weighted Bundling Quality: Design an experiment to measure the "cleanliness" of the learned abstractions. This will involve creating two ConceptFractal hypervectors for the same cluster of ContextFractals—one using simple bundling and one using semantic-weighted bundling. The effectiveness of these two hypervectors will then be compared in downstream cleanup tasks to determine if the weighted version provides a more accurate and robust symbolic handle.

5.3 Phase 3: The Compositional Gauntlet (Expanded Benchmark)

Objective: To provide falsifiable, quantitative evidence that the TelOS hybrid architecture can solve classes of problems that are intractable for standard, state-of-the-art RAG systems. The benchmark will be inspired by academic datasets like GrailQA and ComplexWebQuestions 11 and will be specifically designed to target the documented failure modes of RAG in multi-hop and compositional reasoning tasks.76

Benchmark Design: The Gauntlet will consist of a curated set of questions that require specific reasoning patterns. Performance will be measured against a baseline RAG-only system. The construction of a benchmark that specifically targets the known weaknesses of the baseline is crucial for a meaningful and compelling evaluation of the hybrid system's advantages. For example, multi-hop tasks must be constructed such that the answer requires retrieving an intermediate entity that is not present in the initial query, a known failure point for single-pass RAG systems.76 Similarly, the benchmark should include analogical reasoning problems known to be brittle for simple linear offset methods in word embeddings to test if the VSA-based approach is more robust.88

The following table provides a specification for the core tasks within the Compositional Gauntlet.

Table 1: Benchmark Task Specification for the Compositional Gauntlet

5.4 Phase 4: Longitudinal Evaluation of Autopoietic Learning

Objective: To assess the long-term stability, convergence, and quality of the Mnemonic Curation Pipeline.

Tasks:

Continuous Operation: Run the system over an extended period with a continuous stream of new, domain-relevant documents.

Track Learning Metrics: Monitor key performance indicators over time, including the rate of new ConceptFractal creation, the average semantic coherence (e.g., silhouette score) of identified clusters, and the growth of the HKG.

Measure Cognitive Improvement: Periodically re-run the Compositional Gauntlet benchmark against the evolving system. The goal is to demonstrate that the autonomously learned concepts are not just being created, but are actively improving the system's performance on complex reasoning tasks. This will provide evidence of true, cumulative, and beneficial learning.

Section 6: Conclusion and Strategic Recommendations

The analysis presented in this report provides a comprehensive evaluation of the TelOS MVA's proposed architecture, situating its core concepts within the broader landscape of neuro-symbolic AI research. The design exhibits significant strengths in its philosophical coherence, the novelty of its integration mechanisms, and its pragmatic approach to scalability. However, it also presents critical research challenges that must be addressed to validate its core hypotheses.

6.1 Summary of Architectural Strengths

Philosophical and Technical Coherence: The architecture's derivation from the principle of info-autopoiesis results in a remarkably consistent and tightly-coupled system. Each component, from the ZODB "Living Image" to the doesNotUnderstand_ protocol, is a logical consequence of this foundational mandate, creating a design of unusual integrity.

Novel Neuro-Symbolic Integration: The "Constrained Cleanup Operation" represents a significant innovation in neuro-symbolic design. By creating a dynamic, bidirectional feedback loop where the semantic system actively filters the search space for the symbolic system, it moves beyond simple pipelines to a more powerful, synergistic model of cognitive interaction.

Pragmatism and Scalability: The decision to repurpose mature, highly-optimized ANN libraries like FAISS and DiskANN as a VSA cleanup memory is an intelligent and pragmatic engineering choice. It elegantly addresses a major scalability challenge in the VSA field by leveraging existing, battle-tested technology.

6.2 Key Research Challenges and Risks

Robustness of ANN-based Cleanup: The primary technical risk lies in the trade-off between speed and accuracy. The research agenda must rigorously investigate whether the "approximate" nature of ANN search is sufficient for the high-precision demands of VSA cleanup, particularly as bundling noise increases in complex representations.

Dependency on RAG Performance: The hybrid reasoning engine's performance is critically dependent on the quality of the initial RAG retrieval. This makes the entire reasoning chain vulnerable to known RAG failure modes, most notably the "lost-in-the-middle" problem, which represents a potential single point of failure.

Lack of End-to-End Differentiability: The current architecture is a modular pipeline, not an end-to-end learnable system. While pragmatic for immediate implementation, this limits the system's ability to learn VSA representations from task-based feedback, a key frontier in NSAI research.

6.3 Strategic Recommendations

Based on this analysis, the following strategic recommendations are proposed to guide the future research and development roadmap for the TelOS MVA:

Prioritize the Compositional Gauntlet: The most critical next step is to empirically validate the core reasoning hypotheses. A successful demonstration of superior performance on the Gauntlet benchmark would provide strong, falsifiable evidence for the architecture's value and novelty.

Investigate and Mitigate RAG Vulnerabilities: Given the critical dependency of the symbolic reasoner on the semantic retriever, dedicated research should be conducted on strategies to make the initial RAG retrieval step more robust to the "lost-in-the-middle" effect and other known failure modes.

Explore a Long-Term Evolution Towards Learnable VSA: While the current pipeline approach is sound for the MVA, the long-term roadmap should include research into integrating differentiable VSA operations (e.g., Hadamard-derived Linear Binding 47) to enable deeper, end-to-end learning and representation tuning.

Expand Cognitive Modeling and Benchmarking: The system's implicit strength in analogical reasoning, inherited from its choice of FHRR, should be explicitly tested and developed. Future work could also explore more nuanced cognitive architectures, such as metacognitive control layers that dynamically choose between reasoning strategies (e.g., fast RAG vs. slow VSA) based on query complexity or confidence scores, a research direction hinted at in recent NSAI literature.27

By pursuing this expanded research agenda, the TelOS project can rigorously validate its core architectural hypotheses, mitigate its most significant risks, and continue its evolution from a promising prototype into a robust and provably capable neuro-symbolic intelligence.

Works cited

Fractal Cognition-Memory Symbiosis Architecture

TelOS: A Living System's Becoming

AI Architecture: A Living Codex

Fractal Cognition: Parameterized Internal Monologue

Metamorphosis: Prototypal Soul Manifested

VSA Integration for AI Reasoning

Programming as an Experience: The Inspiration for Self - ResearchGate, accessed September 13, 2025, https://www.researchgate.net/publication/2467120_Programming_as_an_Experience_The_Inspiration_for_Self

Self (programming language) - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Self_(programming_language)

Self: The Power of Simplicity - CMU School of Computer Science, accessed September 13, 2025, http://www-2.cs.cmu.edu/~aldrich/courses/819/self.pdf

Generative Kernel and Mnemonic Pipeline

Unifying Cognitive and Mnemonic Spaces

VSA and NN for RAG

arxiv.org, accessed September 13, 2025, https://arxiv.org/html/2502.11269v1

Neuro-symbolic AI - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Neuro-symbolic_AI

Neurosymbolic AI. How hybrid models combining logic-based… | by Zaina Haider | Medium, accessed September 13, 2025, https://medium.com/@thekzgroupllc/neurosymbolic-ai-2850dc2c7d8f

Neurosymbolic Artificial Intelligence (Why, What, and How) - Scholar Commons, accessed September 13, 2025, https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=1590&context=aii_fac_pub

Neuro-Symbolic methods for Trustworthy AI: a systematic review - Neurosymbolic Artificial Intelligence, accessed September 13, 2025, https://neurosymbolic-ai-journal.com/system/files/nai-paper-726.pdf

[2503.18213] A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives, accessed September 13, 2025, https://arxiv.org/abs/2503.18213

Dual Process Theory: Embodied and Predictive; Symbolic and Classical - PMC, accessed September 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC8979207/

Exploring Dual Process Theory - Structural Learning, accessed September 13, 2025, https://www.structural-learning.com/post/exploring-dual-process-theory

Dual Processes in Decision Making and Developmental Neuroscience: A Fuzzy-Trace Model - PMC - PubMed Central, accessed September 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC3214669/

Dual process theory - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Dual_process_theory

Full article: From theory to practice: a roadmap for applying dual-process theory in design cognition research - Taylor & Francis Online, accessed September 13, 2025, https://www.tandfonline.com/doi/full/10.1080/09544828.2024.2336837

Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning - arXiv, accessed September 13, 2025, https://arxiv.org/html/2503.12635v1

(PDF) Dual-process theories, cognitive architectures, and hybrid neural-symbolic models, accessed September 13, 2025, https://www.researchgate.net/publication/379092512_Dual-process_theories_cognitive_architectures_and_hybrid_neural-symbolic_models

Dual-process theories of thought as potential architectures for developing neuro-symbolic AI models - Frontiers, accessed September 13, 2025, https://www.frontiersin.org/journals/cognition/articles/10.3389/fcogn.2024.1356941/full

Neuro-Symbolic AI in 2024: A Systematic Review - arXiv, accessed September 13, 2025, https://arxiv.org/html/2501.05435v1

(PDF) A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations - ResearchGate, accessed September 13, 2025, https://www.researchgate.net/publication/360721967_A_Survey_on_Hyperdimensional_Computing_aka_Vector_Symbolic_Architectures_Part_I_Models_and_Data_Transformations

Some tests on geometric analogues of Holographic Reduced Representations and Binary Spatter Codes - Annals of Computer Science and Information Systems, accessed September 13, 2025, https://annals-csis.org/proceedings/2011/pliks/70.pdf

A comparison of geometric analogues of holographic reduced representations, original holographic reduced representations and binary spatter codes | Request PDF - ResearchGate, accessed September 13, 2025, https://www.researchgate.net/publication/261459899_A_comparison_of_geometric_analogues_of_holographic_reduced_representations_original_holographic_reduced_representations_and_binary_spatter_codes

[PDF] A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges | Semantic Scholar, accessed September 13, 2025, https://www.semanticscholar.org/paper/7809840eefc16f639b5f5d15fffc911b7b7d1ebc

Holographic Reduced Representation: Distributed Representation for Cognitive Structures - Stanford University, accessed September 13, 2025, https://web.stanford.edu/group/cslipublications/cslipublications/site/1575864304.shtml

Learning Vector Symbolic Architectures | Research | Automation Technology - TU Chemnitz, accessed September 13, 2025, https://www.tu-chemnitz.de/etit/proaut/en/research/vsa.html

An Introduction to Vector Symbolic Architectures and Hyperdimensional Computing - TU Chemnitz, accessed September 13, 2025, https://www.tu-chemnitz.de/etit/proaut/workshops_tutorials/vsa_ecai20/rsrc/vsa_slides.pdf

(PDF) Vector Symbolic Architectures: A New Building Material for ..., accessed September 13, 2025, https://www.researchgate.net/publication/215991898_Vector_Symbolic_Architectures_A_New_Building_Material_for_Artificial_General_Intelligence

Vector Symbolic Architectures As A Computing Framework For Emerging Hardware - Scribd, accessed September 13, 2025, https://www.scribd.com/document/808915541/Vector-Symbolic-Architectures-as-a-Computing-Framework-for-Emerging-Hardware

Vector Symbolic Architectures as a Computing Framework for Emerging Hardware - PMC, accessed September 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC10588678/

VSA-SD: A Service Discovery Method Based on Vector Symbol Architecture for Low-Cost IoT System Development - Haiming Chen, accessed September 13, 2025, http://www.chenhaiming.cn/papers/cwq-tcc24.pdf

Variable Binding for Sparse Distributed Representations: Theory and Applications - Redwood Center for Theoretical Neuroscience, accessed September 13, 2025, https://redwood.berkeley.edu/wp-content/uploads/2022/04/Variable_Binding_for_Sparse_Distributed_Representations_Theory_and_Applications.pdf

Associative Memories to Accelerate Approximate Nearest Neighbor Search - MDPI, accessed September 13, 2025, https://www.mdpi.com/2076-3417/8/9/1676

Understanding the approximate nearest neighbor (ANN) algorithm | Elastic Blog, accessed September 13, 2025, https://www.elastic.co/blog/understanding-ann

spotify/annoy: Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk - GitHub, accessed September 13, 2025, https://github.com/spotify/annoy

(PDF) VSAG: An Optimized Search Framework for Graph-based Approximate Nearest Neighbor Search - ResearchGate, accessed September 13, 2025, https://www.researchgate.net/publication/390141959_VSAG_An_Optimized_Search_Framework_for_Graph-based_Approximate_Nearest_Neighbor_Search

L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation, accessed September 13, 2025, https://arxiv.org/html/2507.02619v1

Demystifying Neural Networks: Variational AutoEncoders | by Dagang Wei | Medium, accessed September 13, 2025, https://medium.com/@weidagang/demystifying-neural-networks-variational-autoencoders-6a44e75d0271

Learning prior p(z) in VAEs - autoencoders - Stats StackExchange, accessed September 13, 2025, https://stats.stackexchange.com/questions/505171/learning-prior-pz-in-vaes

NeurIPS Poster A Walsh Hadamard Derived Linear Vector Symbolic Architecture, accessed September 13, 2025, https://neurips.cc/virtual/2024/poster/93583

Differentiable Tree Operations Promote Compositional Generalization | Request PDF - ResearchGate, accessed September 13, 2025, https://www.researchgate.net/publication/371222929_Differentiable_Tree_Operations_Promote_Compositional_Generalization

A Walsh Hadamard Derived Linear Vector Symbolic Architecture, accessed September 13, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/hash/0525fa17a8dbea687359116d01732e12-Abstract-Conference.html

Vector Symbolic Architectures - Emergent Mind, accessed September 13, 2025, https://www.emergentmind.com/topics/vector-symbolic-architectures-vsas

Developing a Foundation of Vector Symbolic Architectures Using Category Theory - arXiv, accessed September 13, 2025, https://arxiv.org/html/2501.05368v2

Neuro-Vector-Symbolic Architecture - IBM Research, accessed September 13, 2025, https://research.ibm.com/projects/neuro-vector-symbolic-architecture

A Walsh Hadamard Derived Linear Vector Symbolic Architecture - OpenReview, accessed September 13, 2025, https://openreview.net/forum?id=p3hNrpeWMe&referrer=%5Bthe%20profile%20of%20Stella%20Biderman%5D(%2Fprofile%3Fid%3D~Stella_Biderman1)

Torchhd is a Python library for Hyperdimensional Computing and Vector Symbolic Architectures - GitHub, accessed September 13, 2025, https://github.com/hyperdimensional-computing/torchhd

Analogy and Analogical Reasoning - Stanford Encyclopedia of Philosophy, accessed September 13, 2025, https://plato.stanford.edu/entries/reasoning-analogy/

‪Dedre Gentner‬ - ‪Google Scholar‬, accessed September 13, 2025, https://scholar.google.com/citations?user=JQ2VugwAAAAJ&hl=en

Metaphor as Structure Mapping: The Relational Shift - Dedre Gentner, accessed September 13, 2025, https://groups.psych.northwestern.edu/gentner/papers/Gentner88b.pdf

Evidence for a Structure-Mapping Theory of Analogy and Metaphor - DTIC, accessed September 13, 2025, https://apps.dtic.mil/sti/tr/pdf/ADA177300.pdf

Structure-Mapping: A Theoretical Framework for Analogy*, accessed September 13, 2025, http://matt.colorado.edu/teaching/highcog/readings/g83.pdf

Structure-mapping theory - Wikipedia, accessed September 13, 2025, https://en.wikipedia.org/wiki/Structure-mapping_theory

Structure- Mapping: A Theoretical Framework for Analogy - DTIC, accessed September 13, 2025, https://apps.dtic.mil/sti/pdfs/ADA122891.pdf

The analogical mind - PubMed, accessed September 13, 2025, https://pubmed.ncbi.nlm.nih.gov/9017931/

[PDF] The analogical mind. - Semantic Scholar, accessed September 13, 2025, https://www.semanticscholar.org/paper/The-analogical-mind.-Holyoak-Thagard/abfde397b2a1cfff9a1355e1cd86e90acd5d12dd

The Analogical Mind - UCLA Reasoning Lab, accessed September 13, 2025, https://reasoninglab.psych.ucla.edu/wp-content/uploads/sites/273/2021/04/HolyoakThagard.1997pdf.pdf

Analogy and the Roots of Creative Intelligence | The MIT Press Reader, accessed September 13, 2025, https://thereader.mitpress.mit.edu/analogy-and-the-roots-of-creative-intelligence/

Distributed Representations of Structure: A Theory of Analogical Access and Mapping - UCLA Reasoning Lab, accessed September 13, 2025, https://reasoninglab.psych.ucla.edu/wp-content/uploads/sites/273/2021/04/Hummel-Holyoak.1997.pdf

Lost-in-the-Middle Effect | LLM Knowledge Base - Promptmetheus, accessed September 13, 2025, https://promptmetheus.com/resources/llm-knowledge-base/lost-in-the-middle-effect

[2311.09198] Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2311.09198

Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training - arXiv, accessed September 13, 2025, https://arxiv.org/html/2311.09198v2

[2403.04797] Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2403.04797

Do LLM's get "lost in the middle" during summarization as well? [D] : r/MachineLearning, accessed September 13, 2025, https://www.reddit.com/r/MachineLearning/comments/1beb7vi/do_llms_get_lost_in_the_middle_during/

[2406.16008] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2406.16008

Lost in the Middle: How Language Models Use Long Contexts - ACL Anthology, accessed September 13, 2025, https://aclanthology.org/2024.tacl-1.9.pdf

HD/VSA - Software - Hyperdimensional Computing, accessed September 13, 2025, https://www.hd-computing.com/software

Torchhd: An Open-Source Python Library to Support Hyperdimensional Computing Research | DeepAI, accessed September 13, 2025, https://api.deepai.org/publication/torchhd-an-open-source-python-library-to-support-hyperdimensional-computing-research

Credible Plan-Driven RAG Method for Multi-Hop Question Answering - arXiv, accessed September 13, 2025, https://arxiv.org/html/2504.16787v2

From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs - arXiv, accessed September 13, 2025, https://arxiv.org/html/2508.01424v1

HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation - arXiv, accessed September 13, 2025, https://arxiv.org/html/2502.12442v1

Dynamic and Parametric Retrieval-Augmented Generation - arXiv, accessed September 13, 2025, https://arxiv.org/html/2506.06704v1

emulating retrieval augmented generation via prompt engineering for enhanced long context comprehension in llms - arXiv, accessed September 13, 2025, https://arxiv.org/pdf/2502.12462

mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs - arXiv, accessed September 13, 2025, https://arxiv.org/html/2505.11180v1

Fundamental Failure Modes in RAG Systems | PromptQL Blog, accessed September 13, 2025, https://promptql.io/blog/fundamental-failure-modes-in-rag-systems

Seven Failure Points When Engineering a Retrieval Augmented Generation System, accessed September 13, 2025, https://www.consensus.app/papers/seven-failure-points-when-engineering-a-retrieval-barnett-brannelly/a87b2ff30118550887f67d53f0ad1367/

Seven Failure Points When Engineering a Retrieval Augmented Generation System - arXiv, accessed September 13, 2025, https://arxiv.org/abs/2401.05856

Multimodal Iterative RAG for Knowledge Visual Question Answering - ChatPaper, accessed September 13, 2025, https://chatpaper.com/paper/185082

CRP-RAG: A Retrieval-Augmented Generation Framework for Supporting Complex Logical Reasoning and Knowledge Planning - MDPI, accessed September 13, 2025, https://www.mdpi.com/2079-9292/14/1/47

Understanding and Patching Compositional ... - ACL Anthology, accessed September 13, 2025, https://aclanthology.org/2024.findings-acl.576.pdf

Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen - ACL Anthology, accessed September 13, 2025, https://aclanthology.org/C16-1332.pdf

The (too Many) Problems of Analogical Reasoning ... - ACL Anthology, accessed September 13, 2025, https://aclanthology.org/S17-1017.pdf

Neuro-Symbolic AI in 2024: A Systematic Review - CEUR-WS.org, accessed September 13, 2025, https://ceur-ws.org/Vol-3819/paper3.pdf

Reasoning Pattern | Task Description | Example Query | Required VSA/RAG Operations | Baseline Failure Mode | Success Metric

Multi-Hop Inference | Chaining two or more distinct facts, where one fact is needed to discover the entity for the next fact. | "What is the population of the city where the founder of Microsoft was born?" | 1. ANN_Search("founder of Microsoft") -> Context_BillGates 2. VSA_Unbind(H_BillGates, H_born_in) -> H'_Seattle 3. Constrained_Cleanup(H'_Seattle) -> H_Seattle 4. VSA_Unbind(H_Seattle, H_has_population) -> H'_population 5. Constrained_Cleanup(H'_population) -> H_population_value | Requires retrieval of an intermediate entity ("Seattle") not present in the original query. Standard RAG often fails to retrieve all necessary evidence in a single pass.76 | Exact Match (EM)

Analogical Reasoning | Inferring and applying a relationship from a source domain to a target domain. | "If Paris is to France as Rome is to?" | 1. VSA_Unbind(H_Paris, H_France) -> H'_is_capital_of 2. Constrained_Cleanup(H'_is_capital_of) -> H_is_capital_of 3. VSA_Unbind(?, H_is_capital_of, H_Rome) -> H'_Italy 4. Constrained_Cleanup(H'_Italy) -> H_Italy | Simple linear offsets (e.g., king - man + woman) are brittle and sensitive to word idiosyncrasies. Purely semantic search may find related but incorrect concepts.88 | Exact Match (EM)

Structural Query | Querying the structure of the knowledge itself, not just its content. | "Find all entities that have a CAUSES relationship with 'economic recession'." | 1. VSA_Unbind(?, H_CAUSES, H_economic_recession) -> H'_causes 2. Constrained_Cleanup(H'_causes) -> List of cause entities | Semantic search is based on "aboutness," not explicit relational structure. It cannot reliably distinguish between documents that mention causes versus effects.12 | F1 Score (over set of correct entities)

Negation / Constraint Satisfaction | Answering a query that includes a negative constraint. | "Which US presidents served as governor but were NOT from Virginia?" | 1. VSA_Unbind(?, H_is_a, H_US_president) -> H'_presidents 2. VSA_Unbind(?, H_is_a, H_governor) -> H'_governors 3. Intersect results 4. VSA_Unbind(?, H_from, H_Virginia) -> H'_virginians 5. Subtract H'_virginians from intersection | Semantic search struggles with negation. A query for "presidents not from Virginia" will retrieve documents highly similar to both "presidents" and "Virginia," often returning the very items that should be excluded. | F1 Score (over set of correct entities)