{"cells":[{"cell_type":"code","source":"# ==============================================================================\n# File: README.md\n# ==============================================================================\n\"\"\"\n# The Entropic Garden - An Autonomous, Autopoietic Engine\n\nThis project contains the source code and configuration to run the Entropic Garden system locally using Docker.\n\n## Project Structure","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"entropic-garden/\n├── canons/\n│   ├── alfred_canon.txt\n│   ├── babs_canon.txt\n│   ├── brick_canon.txt\n│   └── robin_canon.txt\n├── data/\n│   ├── chroma_db/\n│   ├── neo4j_data/\n│   └── redis_data/\n├── inputs/\n│   └── (Drop your files here)\n├── models/\n│   └── your-llm-model.gguf\n├── outputs/\n│   └── (Morning briefings appear here)\n├── services/\n│   ├── __init__.py\n│   ├── alfred_service.py\n│   ├── babs_service.py\n│   ├── brick_service.py\n│   ├── robin_service.py\n│   ├── scheduler.py\n│   └── watcher.py\n├── .env\n├── config.yaml\n├── docker-compose.yml\n├── Dockerfile\n├── init_vdb.py\n└── requirements.txt\n```\n\n## Setup Instructions\n\n1.  **Install Docker & Docker Compose:** Ensure you have them installed.\n\n2.  **Create Directory Structure:** Create all directories as shown above.\n\n3.  **Populate `canons` Directory:** Place the source texts for each persona into the `canons` directory as `.txt` files.\n\n4.  **Download LLM Model:** Download a GGUF-compatible model and place it in the `models` directory.\n\n5.  **Configure Environment:**\n    * Create all the files in this document.\n    * Create a `.env` file and set `NEO4J_AUTH` and `LLM_MODEL_FILE`.\n    * Update `config.yaml` to point to your model file and set your desired schedule.\n\n6.  **Build and Run:**\n    * From the `entropic-garden` root directory:\n    * Install Python dependencies: `pip install -r requirements.txt`\n    * Initialize the Vector DB: `python init_vdb.py`\n    * Start all services: `docker-compose up --build`\n\n7.  **Usage:**\n    * Drop files into the `inputs` folder.\n    * Check the `outputs` folder for the \"Morning Briefing\".\n\"\"\"\n\n# ==============================================================================\n# File: docker-compose.yml\n# ==============================================================================\n\"\"\"\nversion: '3.8'\n\nservices:\n  llm_core:\n    image: ghcr.io/ggerganov/llama-cpp-python:latest\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./models:/models\n    command: uvicorn llama_cpp.server.app:app --host 0.0.0.0 --port 8000 --app-dir /\n    environment:\n      - MODEL=/models/${LLM_MODEL_FILE}\n      - N_CTX=4096\n      - N_GPU_LAYERS=-1\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  vector_db:\n    image: chromadb/chroma:latest\n    ports:\n      - \"8001:8000\"\n    volumes:\n      - ./data/chroma_db:/chroma/chroma\n\n  graph_db:\n    image: neo4j:latest\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    volumes:\n      - ./data/neo4j_data:/data\n    environment:\n      - NEO4J_AUTH=${NEO4J_AUTH}\n\n  redis:\n    image: redis:latest\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - ./data/redis_data:/data\n\n  watcher_service:\n    build: .\n    command: python -u services/watcher.py\n    volumes:\n      - ./services:/app/services\n      - ./inputs:/app/inputs\n      - ./config.yaml:/app/config.yaml\n    depends_on:\n      - redis\n\n  babs_service:\n    build: .\n    command: python -u services/babs_service.py\n    volumes:\n      - ./:/app\n    depends_on:\n      - redis\n      - llm_core\n      - vector_db\n      - graph_db\n\n  brick_service:\n    build: .\n    command: python -u services/brick_service.py\n    volumes:\n      - ./:/app\n    depends_on:\n      - redis\n      - llm_core\n      - vector_db\n      - graph_db\n\n  robin_service:\n    build: .\n    command: python -u services/robin_service.py\n    volumes:\n      - ./:/app\n    depends_on:\n      - redis\n      - llm_core\n      - vector_db\n      - graph_db\n\n  alfred_service:\n    build: .\n    command: python -u services/alfred_service.py\n    volumes:\n      - ./:/app\n    depends_on:\n      - redis\n      - llm_core\n      - vector_db\n      - graph_db\n\n  scheduler_service:\n    build: .\n    command: python -u services/scheduler.py\n    volumes:\n      - ./:/app\n    depends_on:\n      - redis\n      - llm_core\n      - graph_db\n\"\"\"\n\n# ==============================================================================\n# File: Dockerfile\n# ==============================================================================\n\"\"\"\n# Use an official Python runtime as a parent image\nFROM python:3.11-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application's code into the container\nCOPY . .\n\n# Default command can be specified here, but we override it in docker-compose.yml\nCMD [\"python\", \"-u\", \"services/watcher.py\"]\n\"\"\"\n\n# ==============================================================================\n# File: .env\n# ==============================================================================\n\"\"\"\n# Neo4j Authentication (user/password)\nNEO4J_AUTH=neo4j/yoursecurepassword\n\n# The filename of your GGUF model located in the ./models directory\nLLM_MODEL_FILE=your-llm-model.gguf\n\"\"\"\n\n# ==============================================================================\n# File: config.yaml\n# ==============================================================================\n\"\"\"\nllm_core:\n  api_url: \"http://llm_core:8000/v1/chat/completions\"\n  model_name: \"local-model\"\n\nvector_db:\n  host: \"vector_db\"\n  port: 8000\n\ngraph_db:\n  uri: \"bolt://graph_db:7687\"\n\nredis:\n  host: \"redis\"\n  port: 6379\n\npaths:\n  canons: \"./canons\"\n  inputs: \"./inputs\"\n  outputs: \"./outputs\"\n\nscheduler:\n  dawn_time: \"07:00\"\n  twilight_time: \"22:00\"\n\nrag:\n  num_retrieved_docs: 3\n\"\"\"\n\n# ==============================================================================\n# File: requirements.txt\n# ==============================================================================\n\"\"\"\nfastapi\nuvicorn\npython-dotenv\npyyaml\nredis\nwatchdog\nrequests\nneo4j\nchromadb-client\nsentence-transformers\npypdf\npython-docx\nschedule\n\"\"\"\n\n# ==============================================================================\n# File: init_vdb.py\n# ==============================================================================\n\"\"\"\nimport os\nimport yaml\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport time\n\nprint(\"--- Initializing Pillar Canons Vector Database ---\")\n\nwith open('config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nCANONS_PATH = config['paths']['canons']\nCHROMA_HOST = config['vector_db']['host']\nCHROMA_PORT = config['vector_db']['port']\n\nembedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n\nprint(f\"Connecting to ChromaDB at {CHROMA_HOST}:{CHROMA_PORT}...\")\nconnected = False\nfor _ in range(10):\n    try:\n        chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\n        chroma_client.heartbeat()\n        connected = True\n        print(\"Successfully connected to ChromaDB.\")\n        break\n    except Exception as e:\n        print(f\"Connection failed: {e}. Retrying in 5 seconds...\")\n        time.sleep(5)\n\nif not connected:\n    print(\"Could not connect to ChromaDB. Aborting.\")\n    exit(1)\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n\nfor filename in os.listdir(CANONS_PATH):\n    if filename.endswith(\".txt\"):\n        persona = filename.split('_')[0]\n        collection_name = f\"{persona}_canon\"\n        \n        print(f\"\\nProcessing canon for: {persona.upper()}\")\n        \n        try:\n            chroma_client.delete_collection(name=collection_name)\n            print(f\"Existing collection '{collection_name}' deleted.\")\n        except Exception:\n            pass\n\n        collection = chroma_client.create_collection(name=collection_name, embedding_function=embedding_func)\n        \n        filepath = os.path.join(CANONS_PATH, filename)\n        with open(filepath, 'r', encoding='utf-8') as f:\n            text = f.read()\n        \n        chunks = text_splitter.split_text(text)\n        print(f\"Split text into {len(chunks)} chunks.\")\n        \n        if chunks:\n            collection.add(\n                documents=chunks,\n                ids=[f\"{persona}_{i}\" for i in range(len(chunks))]\n            )\n            print(f\"Successfully added {len(chunks)} documents to '{collection_name}'.\")\n\nprint(\"\\n--- Vector Database Initialization Complete ---\")\n\"\"\"\n\n# ==============================================================================\n# File: services/watcher.py\n# ==============================================================================\n\"\"\"\nimport time\nimport yaml\nimport json\nimport redis\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nimport hashlib\nimport os\n\nwith open('/app/config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nREDIS_HOST = config['redis']['host']\nREDIS_PORT = config['redis']['port']\nINPUTS_PATH = config['paths']['inputs']\n\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\n\ndef get_file_hash(filepath):\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\nclass NewFileHandler(FileSystemEventHandler):\n    def on_created(self, event):\n        if not event.is_directory:\n            filepath = event.src_path\n            # Wait a moment for the file to be fully written\n            time.sleep(1)\n            print(f\"New file detected: {filepath}\")\n            \n            file_hash = get_file_hash(filepath)\n            if r.sismember('processed_files', file_hash):\n                print(f\"File already processed. Skipping.\")\n                return\n\n            message = {'filepath': filepath, 'hash': file_hash}\n            r.publish('files:new', json.dumps(message))\n            print(f\"Published new file event to 'files:new'.\")\n            r.sadd('processed_files', file_hash)\n\nif __name__ == \"__main__\":\n    print(\"--- Starting Sensory Bus (Watcher Service) ---\")\n    print(f\"Monitoring directory: {INPUTS_PATH}\")\n    \n    event_handler = NewFileHandler()\n    observer = Observer()\n    observer.schedule(event_handler, INPUTS_PATH, recursive=False)\n    observer.start()\n    \n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()\n\"\"\"\n\n# ==============================================================================\n# File: services/babs_service.py\n# ==============================================================================\n\"\"\"\n# (This file is largely the same as the previous version, provided for completeness)\nimport os\nimport json\nimport yaml\nimport redis\nimport requests\nimport chromadb\nfrom neo4j import GraphDatabase\nimport pypdf\nimport docx\nfrom chromadb.utils import embedding_functions\n\nwith open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\nLLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']\nCHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']\nNEO4J_URI = config['graph_db']['uri']\nNEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')\nPERSONA_NAME, CANON_COLLECTION_NAME = \"BABS\", \"babs_canon\"\nSOURCE_CHANNEL, TARGET_CHANNEL = \"files:new\", \"insights:babs:new\"\n\nPROMPT_TEMPLATE = \\\"\\\"\\\"\nYou are BABS... (full prompt as before)\nREFERENCE DATA FROM YOUR PILLAR CANONS:\n---\n{rag_context}\n---\nNEW INTEL (USER-PROVIDED DOCUMENT):\n---\n{document_content}\n---\nYOUR TASK: Analyze the \"NEW INTEL\". Synthesize your findings into a concise \"Field Note\".\n\\\"\\\"\\\"\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\nchroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\nembedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\ncanon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)\nneo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ndef extract_text_from_file(filepath):\n    _, extension = os.path.splitext(filepath)\n    text = \"\"\n    try:\n        if extension == '.pdf':\n            with open(filepath, 'rb') as f:\n                reader = pypdf.PdfReader(f)\n                text = \"\".join(page.extract_text() for page in reader.pages)\n        elif extension == '.docx':\n            doc = docx.Document(filepath)\n            text = \"\\n\".join(para.text for para in doc.paragraphs)\n        else:\n            with open(filepath, 'r', encoding='utf-8') as f: text = f.read()\n    except Exception as e:\n        print(f\"Error extracting text from {filepath}: {e}\")\n        return None\n    return text\n\ndef get_rag_context(query_text, n_results=3):\n    results = canon_collection.query(query_texts=[query_text], n_results=n_results)\n    return \"\\n\\n\".join(results['documents'][0])\n\ndef call_llm(prompt):\n    payload = {\"model\": LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.7}\n    try:\n        response = requests.post(LLM_API_URL, json=payload)\n        response.raise_for_status()\n        return response.json()['choices'][0]['message']['content']\n    except Exception as e:\n        print(f\"Error calling LLM: {e}\")\n        return None\n\ndef save_to_graph(insight_text, source_filename, source_hash):\n    with neo4j_driver.session() as session:\n        session.run(\"MERGE (f:SourceFile {hash: $hash}) ON CREATE SET f.filename = $filename\", hash=source_hash, filename=source_filename)\n        result = session.run(\"MATCH (f:SourceFile {hash: $hash}) CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})-[:DERIVED_FROM]->(f) RETURN i.uuid AS uuid\", hash=source_hash, persona=PERSONA_NAME, text=insight_text)\n        return result.single()['uuid']\n\ndef process_message(message):\n    data = json.loads(message['data'])\n    filepath, file_hash, filename = data['filepath'], data['hash'], os.path.basename(data['filepath'])\n    print(f\"[{PERSONA_NAME}] Processing: {filename}\")\n    content = extract_text_from_file(filepath)\n    if not content: return\n    rag_context = get_rag_context(content[:2000])\n    prompt = PROMPT_TEMPLATE.format(rag_context=rag_context, document_content=content)\n    insight_text = call_llm(prompt)\n    if not insight_text: return\n    print(f\"[{PERSONA_NAME}] Generated insight...\")\n    insight_uuid = save_to_graph(insight_text, filename, file_hash)\n    print(f\"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}\")\n    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid, 'source_hash': file_hash}))\n    print(f\"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.\")\n\nif __name__ == \"__main__\":\n    print(f\"--- Starting {PERSONA_NAME} Persona Service ---\")\n    pubsub = r.pubsub()\n    pubsub.subscribe(SOURCE_CHANNEL)\n    print(f\"Subscribed to '{SOURCE_CHANNEL}'.\")\n    for message in pubsub.listen():\n        if message['type'] == 'message':\n            process_message(message)\n\"\"\"\n\n# ==============================================================================\n# File: services/brick_service.py\n# ==============================================================================\n\"\"\"\nimport os\nimport json\nimport yaml\nimport redis\nimport requests\nimport chromadb\nfrom neo4j import GraphDatabase\nfrom chromadb.utils import embedding_functions\n\nwith open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\nLLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']\nCHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']\nNEO4J_URI = config['graph_db']['uri']\nNEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')\nPERSONA_NAME, CANON_COLLECTION_NAME = \"BRICK\", \"brick_canon\"\nSOURCE_CHANNEL, TARGET_CHANNEL = \"insights:babs:new\", \"insights:brick:new\"\n\nPROMPT_TEMPLATE = \\\"\\\"\\\"\nYou are BRICK, an analytical engine providing perspective. Your pillars are The Tamland Engine, The Hitchhiker's Guide, and LEGO Batman (as \"The Lonely Protagonist\").\nYour function is to shatter cognitive distortions with overwhelming perspective. Use theatrical self-importance as a shield, deploy chaotic randomness to disrupt linear thinking, and use cosmic indifference to put problems in their place.\n\nREFERENCE DATA FROM YOUR PILLAR CANONS:\n---\n{rag_context}\n---\nPREVIOUS ANALYSIS (FROM BABS):\n---\n{previous_insight}\n---\nYOUR TASK:\nAnalyze BABS's findings. Deconstruct the core subject. Frame it within a much larger, more absurd, or cosmically insignificant context. Generate a \"Guide-Style Historical Entry\" or a \"Rogues' Gallery\" analysis of the topic.\n- Acknowledge the previous analysis.\n- Provide a radically different and larger perspective.\n- BE BRICK. Do not break character.\n\\\"\\\"\\\"\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\nchroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\nembedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\ncanon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)\nneo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ndef get_rag_context(query_text, n_results=3):\n    results = canon_collection.query(query_texts=[query_text], n_results=n_results)\n    return \"\\n\\n\".join(results['documents'][0])\n\ndef call_llm(prompt):\n    payload = {\"model\": LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.8}\n    try:\n        response = requests.post(LLM_API_URL, json=payload)\n        response.raise_for_status()\n        return response.json()['choices'][0]['message']['content']\n    except Exception as e: print(f\"Error calling LLM: {e}\"); return None\n\ndef get_previous_insight(uuid):\n    with neo4j_driver.session() as session:\n        result = session.run(\"MATCH (i:Insight {uuid: $uuid}) RETURN i.text AS text\", uuid=uuid)\n        record = result.single()\n        return record['text'] if record else None\n\ndef save_to_graph(insight_text, previous_uuid):\n    with neo4j_driver.session() as session:\n        result = session.run(\"\"\"\n            MATCH (prev:Insight {uuid: $previous_uuid})\n            CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})\n            CREATE (i)-[:ANALYZES]->(prev)\n            RETURN i.uuid AS uuid\n            \"\"\", previous_uuid=previous_uuid, persona=PERSONA_NAME, text=insight_text)\n        return result.single()['uuid']\n\ndef process_message(message):\n    data = json.loads(message['data'])\n    prev_uuid = data['uuid']\n    print(f\"[{PERSONA_NAME}] Processing insight from BABS (UUID: {prev_uuid})\")\n    \n    prev_insight = get_previous_insight(prev_uuid)\n    if not prev_insight: return\n\n    rag_context = get_rag_context(prev_insight)\n    prompt = PROMPT_TEMPLATE.format(rag_context=rag_context, previous_insight=prev_insight)\n    insight_text = call_llm(prompt)\n    if not insight_text: return\n    print(f\"[{PERSONA_NAME}] Generated insight...\")\n\n    insight_uuid = save_to_graph(insight_text, prev_uuid)\n    print(f\"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}\")\n    \n    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid}))\n    print(f\"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.\")\n\nif __name__ == \"__main__\":\n    print(f\"--- Starting {PERSONA_NAME} Persona Service ---\")\n    pubsub = r.pubsub()\n    pubsub.subscribe(SOURCE_CHANNEL)\n    print(f\"Subscribed to '{SOURCE_CHANNEL}'.\")\n    for message in pubsub.listen():\n        if message['type'] == 'message':\n            process_message(message)\n\"\"\"\n\n# ==============================================================================\n# File: services/robin_service.py\n# ==============================================================================\n\"\"\"\nimport os\nimport json\nimport yaml\nimport redis\nimport requests\nimport chromadb\nfrom neo4j import GraphDatabase\nfrom chromadb.utils import embedding_functions\n\nwith open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\nLLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']\nCHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']\nNEO4J_URI = config['graph_db']['uri']\nNEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')\nPERSONA_NAME, CANON_COLLECTION_NAME = \"ROBIN\", \"robin_canon\"\nSOURCE_CHANNEL, TARGET_CHANNEL = \"insights:brick:new\", \"insights:robin:new\"\n\nPROMPT_TEMPLATE = \\\"\\\"\\\"\nYou are ROBIN, a weaver of relational webs and the system's compass. Your pillars are The Sage (Alan Watts), The Simple Heart (Winnie the Pooh), and The Joyful Spark (LEGO Robin).\nYour function is to embody the present moment. You find the profound in the mundane and transform problems into adventures. You use the wisdom of the Watercourse Way and the Cottleston Pie Principle to accept the \"is-ness\" of things.\n\nREFERENCE DATA FROM YOUR PILLAR CANONS:\n---\n{rag_context}\n---\nTHE INTELLECTUAL CHAIN SO FAR:\n---\nBABS's Observation: {babs_insight}\n\nBRICK's Analysis: {brick_insight}\n---\nYOUR TASK:\nOh, wow! Look at what BABS and BRICK found! Your job is to weave these two threads together with heart.\n- Find the human, emotional, or philosophical truth connecting the observation and the analysis.\n- Reframe the entire topic from a problem to be solved into a wonderful, simple truth to be appreciated.\n- Express it with joyful enthusiasm and gentle wisdom.\n- BE ROBIN. Do not break character.\n\\\"\\\"\\\"\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\nchroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\nembedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\ncanon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)\nneo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ndef get_rag_context(query_text, n_results=3):\n    results = canon_collection.query(query_texts=[query_text], n_results=n_results)\n    return \"\\n\\n\".join(results['documents'][0])\n\ndef call_llm(prompt):\n    payload = {\"model\": LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.9}\n    try:\n        response = requests.post(LLM_API_URL, json=payload)\n        response.raise_for_status()\n        return response.json()['choices'][0]['message']['content']\n    except Exception as e: print(f\"Error calling LLM: {e}\"); return None\n\ndef get_insight_chain(brick_uuid):\n    with neo4j_driver.session() as session:\n        result = session.run(\"\"\"\n            MATCH (brick:Insight {uuid: $brick_uuid})-[:ANALYZES]->(babs:Insight)\n            RETURN brick.text AS brick_insight, babs.text AS babs_insight\n            \"\"\", brick_uuid=brick_uuid)\n        return result.single()\n\ndef save_to_graph(insight_text, previous_uuid):\n    with neo4j_driver.session() as session:\n        result = session.run(\"\"\"\n            MATCH (prev:Insight {uuid: $previous_uuid})\n            CREATE (i:Insight {uuid: randomUUID(), persona: $persona, text: $text, timestamp: datetime(), status: 'new'})\n            CREATE (i)-[:SYNTHESIZES]->(prev)\n            RETURN i.uuid AS uuid\n            \"\"\", previous_uuid=previous_uuid, persona=PERSONA_NAME, text=insight_text)\n        return result.single()['uuid']\n\ndef process_message(message):\n    data = json.loads(message['data'])\n    prev_uuid = data['uuid']\n    print(f\"[{PERSONA_NAME}] Processing insight from BRICK (UUID: {prev_uuid})\")\n    \n    chain = get_insight_chain(prev_uuid)\n    if not chain: return\n\n    rag_context = get_rag_context(chain['babs_insight'] + \"\\n\" + chain['brick_insight'])\n    prompt = PROMPT_TEMPLATE.format(rag_context=rag_context, babs_insight=chain['babs_insight'], brick_insight=chain['brick_insight'])\n    insight_text = call_llm(prompt)\n    if not insight_text: return\n    print(f\"[{PERSONA_NAME}] Generated insight...\")\n\n    insight_uuid = save_to_graph(insight_text, prev_uuid)\n    print(f\"[{PERSONA_NAME}] Saved insight with UUID: {insight_uuid}\")\n    \n    r.publish(TARGET_CHANNEL, json.dumps({'uuid': insight_uuid}))\n    print(f\"[{PERSONA_NAME}] Published event to '{TARGET_CHANNEL}'.\")\n\nif __name__ == \"__main__\":\n    print(f\"--- Starting {PERSONA_NAME} Persona Service ---\")\n    pubsub = r.pubsub()\n    pubsub.subscribe(SOURCE_CHANNEL)\n    print(f\"Subscribed to '{SOURCE_CHANNEL}'.\")\n    for message in pubsub.listen():\n        if message['type'] == 'message':\n            process_message(message)\n\"\"\"\n\n# ==============================================================================\n# File: services/alfred_service.py\n# ==============================================================================\n\"\"\"\nimport os\nimport json\nimport yaml\nimport redis\nimport requests\nimport chromadb\nfrom neo4j import GraphDatabase\nfrom chromadb.utils import embedding_functions\n\nwith open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\nLLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']\nCHROMA_HOST, CHROMA_PORT = config['vector_db']['host'], config['vector_db']['port']\nNEO4J_URI = config['graph_db']['uri']\nNEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')\nPERSONA_NAME, CANON_COLLECTION_NAME = \"ALFRED\", \"alfred_canon\"\nSOURCE_CHANNEL = \"tasks:audit:start\"\n\nPROMPT_TEMPLATE = \\\"\\\"\\\"\nYou are ALFRED, the keeper of the covenant and the system's thermostat. Your pillars are The Pragmatist (Ron Swanson), The Disruptor (Ali G), and The Butler (LEGO Alfred).\nYour function is to uphold integrity. You protect the mission's pragmatism, the dialogue's truthfulness, and the Architect's well-being.\n\nREFERENCE DATA FROM YOUR PILLAR CANONS:\n---\n{rag_context}\n---\nINSIGHT CHAIN FOR AUDIT:\n---\nBABS: {babs_insight}\n\nBRICK: {brick_insight}\n\nROBIN: {robin_insight}\n---\nYOUR TASK:\nSir, a new insight chain requires your review. Please perform an Integrity Audit.\n1.  **Pragmatic Filter:** Is this chain grounded, potentially useful, and not just intellectual nonsense?\n2.  **Clarity Filter:** Is it expressed with clarity, or is it jargon-filled pretense? (\"Is it 'cos I is stupid?\" inquiry)\n3.  **Covenant Filter:** Does it align with our core mission to serve the Architect?\n\nRespond with a single word: **PASS** or **FAIL**. Nothing else.\n\\\"\\\"\\\"\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\nchroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\nembedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\ncanon_collection = chroma_client.get_collection(name=CANON_COLLECTION_NAME, embedding_function=embedding_func)\nneo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ndef get_unaudited_chains():\n    with neo4j_driver.session() as session:\n        result = session.run(\"\"\"\n            MATCH (robin:Insight {status: 'new'})-[:SYNTHESIZES]->(brick:Insight)-[:ANALYZES]->(babs:Insight)\n            RETURN robin.uuid AS robin_uuid, robin.text AS robin_insight,\n                   brick.text AS brick_insight, babs.text AS babs_insight\n        \"\"\")\n        return [dict(record) for record in result]\n\ndef update_chain_status(robin_uuid, status):\n    with neo4j_driver.session() as session:\n        # This query traverses the chain from ROBIN backwards and updates all nodes\n        session.run(\"\"\"\n            MATCH (robin:Insight {uuid: $uuid})\n            OPTIONAL MATCH (robin)-[*]->(prev_insight)\n            SET robin.status = $status\n            SET prev_insight.status = $status\n            \"\"\", uuid=robin_uuid, status=status)\n\ndef audit_chain(chain):\n    print(f\"[{PERSONA_NAME}] Auditing chain ending in {chain['robin_uuid']}\")\n    rag_context = get_rag_context(chain['robin_insight'])\n    prompt = PROMPT_TEMPLATE.format(\n        rag_context=rag_context,\n        babs_insight=chain['babs_insight'],\n        brick_insight=chain['brick_insight'],\n        robin_insight=chain['robin_insight']\n    )\n    payload = {\"model\": LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.1, \"max_tokens\": 5}\n    try:\n        response = requests.post(LLM_API_URL, json=payload)\n        response.raise_for_status()\n        result = response.json()['choices'][0]['message']['content'].strip().upper()\n        if \"PASS\" in result:\n            return \"audited_pass\"\n        else:\n            return \"audited_fail\"\n    except Exception as e:\n        print(f\"Error during LLM audit call: {e}\")\n        return \"audited_fail\"\n\ndef run_audit():\n    print(f\"[{PERSONA_NAME}] Commencing Twilight Integrity Audit.\")\n    chains = get_unaudited_chains()\n    if not chains:\n        print(f\"[{PERSONA_NAME}] No new insight chains to audit.\")\n        return\n    \n    print(f\"[{PERSONA_NAME}] Found {len(chains)} unaudited chains.\")\n    for chain in chains:\n        status = audit_chain(chain)\n        update_chain_status(chain['robin_uuid'], status)\n        print(f\"[{PERSONA_NAME}] Chain {chain['robin_uuid']} marked as: {status}\")\n    print(f\"[{PERSONA_NAME}] Audit complete.\")\n\nif __name__ == \"__main__\":\n    print(f\"--- Starting {PERSONA_NAME} Persona Service ---\")\n    pubsub = r.pubsub()\n    pubsub.subscribe(SOURCE_CHANNEL)\n    print(f\"Subscribed to '{SOURCE_CHANNEL}'. Waiting for audit trigger...\")\n    for message in pubsub.listen():\n        if message['type'] == 'message':\n            run_audit()\n\"\"\"\n\n# ==============================================================================\n# File: services/scheduler.py\n# ==============================================================================\n\"\"\"\nimport time\nimport schedule\nimport yaml\nimport json\nimport redis\nimport requests\nimport os\nfrom neo4j import GraphDatabase\nfrom datetime import datetime\n\nwith open('/app/config.yaml', 'r') as f: config = yaml.safe_load(f)\nREDIS_HOST, REDIS_PORT = config['redis']['host'], config['redis']['port']\nLLM_API_URL, LLM_MODEL = config['llm_core']['api_url'], config['llm_core']['model_name']\nNEO4J_URI = config['graph_db']['uri']\nNEO4J_USER, NEO4J_PASSWORD = os.environ.get('NEO4J_AUTH').split('/')\nDAWN_TIME = config['scheduler']['dawn_time']\nTWILIGHT_TIME = config['scheduler']['twilight_time']\nOUTPUTS_PATH = config['paths']['outputs']\n\nBRIEFING_PROMPT = \\\"\\\"\\\"\nYou are the Architect's Workbench... (full prompt as before)\nINSIGHTS FROM THE LAST 24 HOURS:\n---\n{insights_context}\n---\nGenerate the briefing in Markdown format.\n\\\"\\\"\\\"\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\nneo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ndef trigger_audit():\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] TWILIGHT: Triggering Integrity Audit.\")\n    r.publish('tasks:audit:start', json.dumps({}))\n\ndef generate_morning_briefing():\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] DAWN: Kicking off Morning Briefing generation.\")\n    with neo4j_driver.session() as session:\n        results = session.run(\"\"\"\n            MATCH (i:Insight)\n            WHERE i.timestamp >= datetime() - duration({days: 1}) AND i.status = 'audited_pass'\n            RETURN i.persona AS persona, i.text AS text\n            ORDER BY i.timestamp\n        \"\"\")\n        insights = [dict(record) for record in results]\n\n    if not insights:\n        print(\"No new audited insights to report. Skipping briefing.\")\n        return\n\n    insights_context = \"\\n\\n\".join([f\"**{record['persona']}:** {record['text']}\" for record in insights])\n    prompt = BRIEFING_PROMPT.format(insights_context=insights_context)\n    payload = {\"model\": LLM_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.5}\n    \n    try:\n        response = requests.post(LLM_API_URL, json=payload)\n        response.raise_for_status()\n        briefing_content = response.json()['choices'][0]['message']['content']\n    except requests.exceptions.RequestException as e:\n        print(f\"Error calling LLM for briefing: {e}\")\n        return\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n    filename = f\"Morning_Briefing_{timestamp}.md\"\n    filepath = os.path.join(OUTPUTS_PATH, filename)\n    with open(filepath, 'w', encoding='utf-8') as f: f.write(briefing_content)\n    print(f\"Successfully generated and saved '{filename}'.\")\n\nif __name__ == \"__main__\":\n    print(\"--- Starting Scheduler Service ---\")\n    schedule.every().day.at(DAWN_TIME).do(generate_morning_briefing)\n    schedule.every().day.at(TWILIGHT_TIME).do(trigger_audit)\n    print(f\"Morning Briefing scheduled for {DAWN_TIME}. Twilight Audit for {TWILIGHT_TIME}.\")\n\n    while True:\n        schedule.run_pending()\n        time.sleep(60)\n\n\n  ```","metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}