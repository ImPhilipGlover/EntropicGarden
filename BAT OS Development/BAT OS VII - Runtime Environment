{"cells":[{"cell_type":"code","source":"# ==============================================================================\n# BAT OS VII - Runtime Environment\n#\n# ARCHITECT: Philip\n# DATE: 2025-08-28\n#\n# DESCRIPTION:\n# This script is the main runtime for the BAT OS. It loads the persistent\n# universe from 'Data.fs' (created by 'awakening.py') and activates its\n# dynamic, cognitive capabilities.\n#\n# Key functionalities in this phase:\n#   1.  **Lazy-Loading the LLM**: The massive LLM is loaded from the ZODB Blob\n#       into GPU memory only when it's first needed for cognition.\n#   2.  **The 'doesNotUnderstand:' Protocol**: The core of the system's\n#       autopoietic nature. A failed message lookup is transformed from an\n#       error into a creative event.\n#   3.  **Cognitive Reflection**: The 'reflectOn_' method on the pLLM prototype\n#       constructs a detailed prompt, allowing the system to reason about its\n#       own capability gaps and generate code to fill them.\n#   4.  **Live Code Installation**: Generated Python code is compiled and\n#       installed directly into the live object graph as a new method,\n#       extending the system's capabilities in real-time.\n#   5.  **Interactive Shell**: Provides a direct interface for the Architect\n#       to interact with the living objects of the BAT OS universe.\n# ==============================================================================\n\nimport os\nimport ZODB, ZODB.FileStorage\nimport transaction\nimport persistent\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport shutil\nimport tempfile\nimport tarfile\nimport readline\nfrom contextlib import contextmanager\nimport atexit\nimport re\n\n# --- Configuration ---\nLLM_MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nDB_FILE = 'Data.fs'\nMODEL_CACHE_DIR = './model_cache'\n\n# ==============================================================================\n#   CORE ARCHITECTURAL COMPONENTS (Phase 2 Enhancements)\n# ==============================================================================\n\nclass UvmObject(persistent.Persistent):\n    \"\"\"\n    The fundamental building block of the BAT OS universe.\n    (Enhanced for Phase 2 to include the doesNotUnderstand: hook)\n    \"\"\"\n    def __init__(self, **kwargs):\n        # This method is primarily used by awakening.py.\n        # The runtime will load existing instances.\n        super().__init__()\n        self._slots = persistent.mapping.PersistentMapping()\n        for key, value in kwargs.items():\n            if key == 'parent_star':\n                self._slots['parent*'] = persistent.list.PersistentList(value)\n            else:\n                self._slots[key] = value\n\n    def __getattr__(self, name):\n        \"\"\"\n        The core of the message-passing and delegation mechanism.\n        (Updated to delegate to doesNotUnderstand_ on final failure)\n        \"\"\"\n        # Prevent infinite recursion on special methods\n        if name.startswith('__') or name.startswith('_p_') or name == '_slots':\n             raise AttributeError(f\"Special attribute '{name}' not found\")\n\n        # 1. Check local slots\n        if name in self._slots:\n            return self._slots[name]\n\n        # 2. Delegate to parents\n        if 'parent*' in self._slots:\n            for parent in self._slots['parent*']:\n                try:\n                    return getattr(parent, name)\n                except AttributeError:\n                    continue\n        \n        # 3. **NEW**: If lookup fails completely, invoke the generative protocol.\n        # We find 'doesNotUnderstand_' via the same delegation mechanism.\n        # This will find the method on the root 'traits_obj'.\n        try:\n            # The 'self' passed here is the original object that received the message.\n            # This is crucial for context.\n            return self.doesNotUnderstand_(self, name)\n        except AttributeError:\n             # This is a critical failure - the universe is missing its core protocol.\n             raise AttributeError(f\"'{name}' not found and 'doesNotUnderstand_' protocol is missing!\")\n\n\n    def __setattr__(self, name, value):\n        if name == '_slots' or name.startswith('_p_'):\n            super().__setattr__(name, value)\n        else:\n            self._slots[name] = value\n            self._p_changed = True\n\n    def set_slot(self, name, value):\n        self._slots[name] = value\n        self._p_changed = True\n\n    def __repr__(self):\n        oid_bytes = self._p_oid\n        oid = int.from_bytes(oid_bytes, 'big') if oid_bytes else 'transient'\n        keys = list(self._slots.keys())\n        return f\"<UvmObject OID:{oid} Slots:{keys}>\"\n\nclass BatOS_UVM:\n    \"\"\"\n    The Universal Virtual Machine for the runtime environment.\n    \"\"\"\n    def __init__(self, db_path):\n        self.db_path = db_path\n        self.storage = None\n        self.db = None\n        self.connection = None\n        self.root = None\n        self.genesis_obj = None\n        self.pLLM_obj = None\n        self.traits_obj = None\n        self.temp_model_dir = None\n\n    def startup(self):\n        \"\"\"Connects to the ZODB and loads the primordial prototypes.\"\"\"\n        if not os.path.exists(self.db_path):\n            print(f\"FATAL: Database file '{self.db_path}' not found.\")\n            print(\"Please run 'awakening.py' first to initialize the universe.\")\n            exit(1)\n\n        print(\"[UVM] Starting up runtime environment...\")\n        self.storage = ZODB.FileStorage.FileStorage(self.db_path)\n        self.db = ZODB.DB(self.storage)\n        self.connection = self.db.open()\n        self.root = self.connection.root()\n\n        # Load primordial prototypes into memory\n        self.genesis_obj = self.root.get('genesis_obj')\n        self.pLLM_obj = self.root.get('pLLM_obj')\n        self.traits_obj = self.root.get('traits_obj')\n\n        if not all([self.genesis_obj, self.pLLM_obj, self.traits_obj]):\n            print(\"FATAL: The database is corrupted or incomplete.\")\n            print(\"Primordial prototypes not found. Please re-run awakening.py.\")\n            exit(1)\n\n        # Dynamically attach the Python methods (the \"code\") to the\n        # persistent UvmObjects (the \"data\"). This is the heart of the\n        # live image concept.\n        self._attach_behaviors()\n        \n        # Register cleanup to run on exit\n        atexit.register(self.shutdown)\n\n        print(\"[UVM] Runtime is live. Universe is loaded.\")\n\n    def shutdown(self):\n        \"\"\"Closes the DB and cleans up temporary model files.\"\"\"\n        print(\"\\n[UVM] Shutting down runtime...\")\n        if self.connection:\n            self.connection.close()\n        if self.db:\n            self.db.close()\n        if self.storage:\n            self.storage.close()\n        \n        if self.temp_model_dir and os.path.exists(self.temp_model_dir):\n            print(f\"[UVM] Cleaning up temporary model directory: {self.temp_model_dir}\")\n            shutil.rmtree(self.temp_model_dir)\n        \n        print(\"[UVM] Shutdown complete.\")\n\n    @contextmanager\n    def transaction(self):\n        try:\n            yield\n            transaction.commit()\n            print(\"[UVM] Transaction committed.\")\n        except Exception as e:\n            print(f\"[UVM] TRANSACTION FAILED: {e}. Aborting.\")\n            transaction.abort()\n            raise\n\n    def _attach_behaviors(self):\n        \"\"\"\n        Binds the transient Python methods of the UVM to the persistent slots\n        of the primordial prototypes.\n        \"\"\"\n        with self.transaction():\n            # The pLLM's cognitive functions\n            self.pLLM_obj.set_slot('infer_', self._pLLM_infer)\n            self.pLLM_obj.set_slot('reflectOn_', self._pLLM_reflectOn)\n            \n            # The universal generative protocol\n            self.traits_obj.set_slot('doesNotUnderstand_', self._universal_doesNotUnderstand)\n        \n        print(\"[UVM] Core behaviors attached to primordial prototypes.\")\n\n    def _lazy_load_model(self):\n        \"\"\"\n        Loads the LLM from the ZODB Blob into GPU memory on first use.\n        This is a complex, one-time operation per session.\n        \"\"\"\n        if self.pLLM_obj._slots.get('_loaded_model') is not None:\n            return self.pLLM_obj._slots['_loaded_model'], self.pLLM_obj._slots['_loaded_tokenizer']\n\n        print(\"[pLLM] Cognitive function invoked. Lazy-loading model from BLOB...\")\n        \n        # Create a temporary directory to extract the model files\n        self.temp_model_dir = tempfile.mkdtemp()\n        print(f\"[pLLM] Created temporary directory for model: {self.temp_model_dir}\")\n\n        try:\n            # Read the tarball from the blob and extract it\n            with self.pLLM_obj.model_blob.open('r') as blob_file:\n                with tarfile.open(fileobj=blob_file, mode='r:gz') as tar:\n                    tar.extractall(path=self.temp_model_dir)\n            \n            # The model files are inside a subdirectory within the cache structure\n            # e.g., /tmp/xyz/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/....\n            # We need to find the actual model directory.\n            model_path = None\n            for root, dirs, files in os.walk(self.temp_model_dir):\n                if \"config.json\" in files:\n                    model_path = root\n                    break\n            \n            if not model_path:\n                raise RuntimeError(\"Could not find a valid model directory in the extracted blob.\")\n\n            print(f\"[pLLM] Model files extracted. Loading from '{model_path}'...\")\n\n            # Configure quantization for efficient memory usage\n            quant_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16\n            )\n\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quant_config,\n                device_map=\"auto\"\n            )\n            tokenizer = AutoTokenizer.from_pretrained(self.pLLM_obj.tokenizer_id)\n\n            # Store the loaded model and tokenizer in the object's volatile slots.\n            # These are not persisted.\n            self.pLLM_obj._slots['_loaded_model'] = model\n            self.pLLM_obj._slots['_loaded_tokenizer'] = tokenizer\n            \n            print(\"[pLLM] Model and tokenizer loaded successfully into GPU memory.\")\n            return model, tokenizer\n\n        except Exception as e:\n            print(f\"[pLLM] FATAL: Failed to load model from BLOB: {e}\")\n            # Clean up if loading fails\n            if self.temp_model_dir and os.path.exists(self.temp_model_dir):\n                shutil.rmtree(self.temp_model_dir)\n            raise\n\n    def _pLLM_infer(self, target_obj, prompt_string, max_new_tokens=1024):\n        \"\"\"The 'infer_' method for the pLLM prototype.\"\"\"\n        model, tokenizer = self._lazy_load_model()\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful and concise assistant.\"},\n            {\"role\": \"user\", \"content\": prompt_string},\n        ]\n\n        input_ids = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            return_tensors=\"pt\"\n        ).to(model.device)\n\n        outputs = model.generate(\n            input_ids,\n            max_new_tokens=max_new_tokens,\n            eos_token_id=tokenizer.eos_token_id\n        )\n        \n        response = outputs[0][input_ids.shape[-1]:]\n        return tokenizer.decode(response, skip_special_tokens=True)\n\n    def _pLLM_reflectOn(self, target_obj, message_obj):\n        \"\"\"The 'reflectOn_' method for the pLLM prototype.\"\"\"\n        model, tokenizer = self._lazy_load_model()\n\n        # Construct the detailed, zero-shot prompt from the spec\n        prompt = f\"\"\"You are the BAT OS Reflective Core. An object has received a message it does not understand.\nYour task is to generate the Python code for a new method to handle this message.\n\n**Architectural Constraints:**\n- The function definition must be `def method_name(self, *args, **kwargs):`.\n- The first argument MUST be `self`, representing the UvmObject instance.\n- Access object state ONLY through `self.slot_name`.\n- To ensure persistence, any state modification MUST be followed by `self._p_changed = True`.\n- The function should return a meaningful value.\n- **Output ONLY the raw Python code for the function definition. No explanations, no markdown, just the code.**\n\n**Context:**\n- Target Object OID: {int.from_bytes(target_obj._p_oid, 'big')}\n- Target Object Slots: {list(target_obj._slots.keys())}\n- Failed Message Selector: {message_obj.selector}\n- Message Arguments: args={message_obj.arguments}, kwargs={message_obj.kwargs}\n\n**GENERATE METHOD CODE:**\n\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a master Python programmer specializing in dynamic code generation for a unique object-oriented operating system. Follow all constraints precisely.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        input_ids = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, return_tensors=\"pt\"\n        ).to(model.device)\n\n        outputs = model.generate(\n            input_ids,\n            max_new_tokens=1024,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.eos_token_id, # Suppress padding warning\n            do_sample=True, # Encourage creativity\n            temperature=0.6,\n            top_p=0.9,\n        )\n        \n        response = outputs[0][input_ids.shape[-1]:]\n        generated_text = tokenizer.decode(response, skip_special_tokens=True)\n        \n        # Clean the output to extract only the Python code block\n        code_match = re.search(r\"```python\\n(.*?)```\", generated_text, re.DOTALL)\n        if code_match:\n            return code_match.group(1).strip()\n        \n        # Fallback for models that don't use markdown\n        if generated_text.strip().startswith('def '):\n            return generated_text.strip()\n            \n        print(f\"[pLLM] WARNING: Could not parse code from LLM output:\\n{generated_text}\")\n        return None\n\n    def _universal_doesNotUnderstand(self, target_obj, failed_message_name, *args, **kwargs):\n        \"\"\"The new, object-level generative mechanism.\"\"\"\n        print(f\"\\n[UvmObject] OID {int.from_bytes(target_obj._p_oid, 'big')} doesNotUnderstand: '{failed_message_name}'\")\n\n        with self.transaction():\n            # 1. Reify the failed message into a persistent UvmObject\n            message_obj = UvmObject(\n                selector=failed_message_name,\n                arguments=list(args),\n                kwargs=dict(kwargs),\n                receiver_oid_str=str(int.from_bytes(target_obj._p_oid, 'big'))\n            )\n            log = self.root.setdefault('message_log', persistent.list.PersistentList())\n            log.append(message_obj)\n            print(f\"[UvmObject] Message reified. Delegating to reflectOn_...\")\n\n        # 2. Send the 'reflectOn_' message to the target object itself.\n        # This will delegate up to the pLLM prototype.\n        generated_code = target_obj.reflectOn_(target_obj, message_obj)\n\n        if generated_code and isinstance(generated_code, str):\n            print(f\"[pLLM] Generated code for '{failed_message_name}':\\n---\\n{generated_code}\\n---\")\n            try:\n                # 3. Compile and install the generated method\n                namespace = {}\n                # The 'self' in exec is just a placeholder, not the real object\n                exec(generated_code, {'UvmObject': UvmObject}, namespace)\n                \n                # Infer method name from the 'def' line\n                method_name = generated_code.split('(')[0].split('def ')[1].strip()\n                method_obj = namespace[method_name]\n\n                # 4. Install the new method and re-invoke\n                with self.transaction():\n                    target_obj.set_slot(failed_message_name, method_obj)\n                \n                print(f\"[UvmObject] Successfully installed method '{failed_message_name}'. Re-invoking.\")\n                # Re-call the method on the original object\n                return getattr(target_obj, failed_message_name)(target_obj, *args, **kwargs)\n\n            except Exception as e:\n                print(f\"[UVM] ERROR: Failed to process generated code: {e}\")\n                return f\"Error: Code generation failed for '{failed_message_name}'\"\n        else:\n            print(f\"[UvmObject] Cognitive reflection did not yield code for '{failed_message_name}'.\")\n            return f\"Error: Unable to handle '{failed_message_name}'\"\n\n    def interactive_shell(self):\n        \"\"\"Starts an interactive shell to communicate with the OS.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"BAT OS VII Interactive Shell\")\n        print(\"The universe is live. 'genesis' is the primordial object.\")\n        print(\"Type 'exit' or 'quit' to shutdown.\")\n        print(\"Example: genesis.display_yourself()\")\n        print(\"=\"*60)\n\n        # Expose the genesis object to the shell's local scope\n        shell_scope = {'genesis': self.genesis_obj}\n\n        while True:\n            try:\n                command = input(\"BAT OS > \")\n                if command.lower() in ['exit', 'quit']:\n                    break\n                \n                result = eval(command, globals(), shell_scope)\n                print(f\"Result: {result}\")\n\n            except Exception as e:\n                print(f\"Shell Error: {e}\")\n\n# ==============================================================================\n#   MAIN EXECUTION BLOCK\n# ==============================================================================\n\nif __name__ == \"__main__\":\n    uvm = BatOS_UVM(DB_FILE)\n    try:\n        uvm.startup()\n        uvm.interactive_shell()\n    except Exception as e:\n        print(f\"\\nA critical error occurred in the UVM: {e}\")\n    finally:\n        # The shutdown is now handled by atexit, ensuring it runs\n        # even if the script is interrupted (e.g., with Ctrl+C).\n        pass","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}