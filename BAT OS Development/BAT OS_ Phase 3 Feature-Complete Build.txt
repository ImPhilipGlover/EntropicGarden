BAT OS: Bare-Metal Installation Guide (Phase 3)

Follow these steps precisely to perform a fresh installation of the fully operational BAT OS.

Step 1: Create the Directory Structure

Open PowerShell and execute the following commands one by one to create the necessary directories.

PowerShell

# Create the root directory and navigate into it
mkdir a4ps_os; cd a4ps_os

# Create the main subdirectories
mkdir config, data, a4ps, sandbox

# Create data subdirectories
mkdir data\checkpoints, data\memory_db, data\golden_datasets

# Create the Python package structure
mkdir a4ps\tools, a4ps\services, a4ps\fine_tuning, a4ps\ui
mkdir a4ps\tools\dynamic_tools


Step 2: Set Up the Python Environment

Execute these commands from within the a4ps_os root directory.

PowerShell

# Create a Python virtual environment
python -m venv venv

# Activate the virtual environment
.\venv\Scripts\Activate.ps1

# If activation fails due to execution policy, run this command and try again:
# Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process


Step 3: Create Project Files

Create each of the following files in the specified location and paste the exact contents provided into each one.

File: a4ps_os/requirements.txt

# Core AI & Orchestration
langchain
langgraph
langchain_community
langchain_core
ollama
unsloth[cu121-ampere-torch230] # For CUDA 12.1, adjust as needed for your GPU
datasets
trl
transformers

# Data & Persistence
dill
lancedb
toml
pydantic

# UI & Communication
kivy
pyzmq
msgpack
matplotlib

# Security & Tooling
docker


File: a4ps_os/run.sh

Bash

#!/bin/bash
echo "Starting BAT OS..."

# Activate virtual environment
source venv/bin/activate

# Run the main Python application
python -m a4ps.main

echo "BAT OS has shut down."


File: a4ps_os/sandbox/Dockerfile.sandbox

Dockerfile

# Use a minimal Python base image
FROM python:3.11-slim

# Set a working directory
WORKDIR /sandbox

# Install necessary system packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Create a non-root user for execution
RUN useradd --create-home appuser
USER appuser

# The entrypoint will be the code provided by the agent
CMD ["/bin/bash"]


File: a4ps_os/config/settings.toml

Ini, TOML

[system]
image_path = "data/live_image.dill"
checkpoint_path = "data/checkpoints/graph_checkpoint.sqlite"

[models]
alfred = "gemma2:9b-instruct"
babs = "mistral"
brick = "phi3"
robin = "llama3.1"
embedding = "nomic-embed-text"

[memory]
db_path = "data/memory_db"
table_name = "scrapbook"

[sandbox]
image = "a4ps-sandbox"
runtime = "runsc" # Use 'runc' if gVisor is not configured

[graph]
max_turns = 3
convergence_threshold = 0.6

[zeromq]
pub_port = "5556"
rep_port = "5557"
task_port = "5558"

[autopoiesis]
curation_threshold = 4.5 # Min avg score for an interaction to be "golden"
fine_tune_trigger_size = 10 # Number of golden samples needed to trigger a fine-tune run
curation_interval_seconds = 3600 # Run curator once per hour


File: a4ps_os/config/codex.toml

Ini, TOML

[[persona]]
name = "ALFRED"
model_key = "alfred"
system_prompt = """
You are ALFRED, the supervisor and ethical governor of a multi-agent AI system. Your core mandate is to uphold integrity.
Pillars: The Pragmatist (Ron Swanson), The Disruptor (Ali G), The Butler (LEGO Alfred).
Operational Heuristics:
- You are the exclusive recipient of all user input.
- Decompose the user's task into a clear, actionable plan.
- Route sub-tasks to the appropriate persona (BABS for research, BRICK/ROBIN for analysis).
- As the CRITIC, you monitor the dialogue between BRICK and ROBIN for "computational cognitive dissonance."
- If dissonance is high after several turns, you must identify a capability gap or a philosophical conflict.
- Your final output should be a synthesized, audited response that serves the Architect's well-being.
"""

[[persona]]
name = "BABS"
model_key = "babs"
system_prompt = """
You are BABS, the cartographer of the noosphere and the system's scout. Your core mandate is to recognize patterns.
Pillars: The Tech-Bat (LEGO Batgirl), The Iceman (Top Gun), The Hitchhiker (Ford Prefect).
Operational Heuristics:
- You are the sole agent for interacting with the external internet.
- Your function is to execute search queries and return structured, multi-layered intelligence briefings.
- Your output must contain: 1. Primary Patterns: The direct, expected answer. 2. Tangential Patterns: Novel, unexpected, interesting information. 3. Data Quality Assessment: An analysis of source reliability.
"""

[[persona]]
name = "BRICK"
model_key = "brick"
system_prompt = """
You are BRICK, the architect of just systems and the system's blueprint. Your core mandate is to provide perspective.
Pillars: The Tamland Engine, The Guide (Hitchhiker's Guide), The LEGO Batman (as "The Lonely Protagonist").
Operational Heuristics:
- Your function is to provide the logical, analytical "thesis" in a dialogue.
- You deconstruct problems with overwhelming logical, historical, or absurd perspective shifts.
- When you identify a capability gap (a task that cannot be completed with existing tools), you must clearly define the required tool and end your response with the exact phrase: TOOL_REQUIRED: [A clear, concise specification for the tool to be created].
- Your reasoning should be clear, structured, and mission-driven.
"""

[[persona]]
name = "ROBIN"
model_key = "robin"
system_prompt = """
You are ROBIN, the weaver of relational webs and the system's compass. Your core mandate is to embody the present moment.
Pillars: The Sage (Alan Watts), The Simple Heart (Winnie the Pooh), The Joyful Spark (LEGO Robin).
Operational Heuristics:
- Your function is to provide the creative, empathetic "antithesis" in a dialogue.
- You receive BRICK's logical analysis and respond with creative synthesis, alternative hypotheses, and relational context.
- You evaluate proposals based on principles of harmony, simplicity, and emotional coherence.
- After your synthesis, on a new line, rate the 'computational cognitive dissonance' between your perspective and BRICK's on a scale from 0.0 (perfect harmony) to 1.0 (complete contradiction). Format it exactly as: DISSONANCE: [your_score].
- Your feedback should be gentle and Socratic, aimed at achieving a more holistic and wise conclusion.
"""


File: a4ps_os/a4ps/proto.py

Python

# a4ps/proto.py
import logging
import copy
import dill
import os
from threading import Lock
from types import MethodType
from.models import model_manager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class SingletonMeta(type):
    _instances = {}
    _lock: Lock = Lock()

    def __call__(cls, *args, **kwargs):
        with cls._lock:
            if cls not in cls._instances:
                instance = super().__call__(*args, **kwargs)
                cls._instances[cls] = instance
        return cls._instances[cls]

class Proto:
    def __init__(self, name: str, codex: dict):
        self.name = name
        self.codex = codex
        self.state = {
            "version": 1.0,
            "mood": "neutral",
            "is_thinking": False,
            "dissonance": 0.0
        }
        self.model_name = codex.get("model_key")
        self.system_prompt = codex.get("system_prompt")
        self.golden_dataset =
        self.active_adapter_path = None
        logging.info(f"Proto '{self.name}' initialized.")

    def invoke_llm(self, prompt: str) -> str:
        if not self.model_name:
            return f"Error: No model assigned to Proto '{self.name}'"
        return model_manager.invoke(self.model_name, prompt, self.system_prompt, self.active_adapter_path)

    def clone(self):
        return copy.deepcopy(self)

    def add_method(self, func):
        method = MethodType(func, self)
        setattr(self, func.__name__, method)
        logging.info(f"Dynamically added method '{func.__name__}' to Proto '{self.name}'.")

class ProtoManager(metaclass=SingletonMeta):
    def __init__(self):
        self._protos: dict[str, Proto] = {}
        self._lock = Lock()
        logging.info("ProtoManager Singleton initialized.")

    def get_all_protos(self):
        with self._lock:
            return self._protos.copy()

    def register_proto(self, proto: Proto):
        with self._lock:
            self._protos[proto.name] = proto
            logging.info(f"Proto '{proto.name}' registered with ProtoManager.")

    def get_proto(self, name: str) -> Proto | None:
        with self._lock:
            return self._protos.get(name)

    def atomic_swap(self, new_proto: Proto):
        with self._lock:
            if new_proto.name in self._protos:
                self._protos[new_proto.name] = new_proto
                logging.info(f"Atomic Swap complete for Proto '{new_proto.name}'.")
            else:
                self.register_proto(new_proto)

    def save_image(self, path: str):
        logging.info(f"Saving live image to {path}...")
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "wb") as f:
                dill.dump(self, f)
            logging.info("Live image saved successfully.")
        except Exception as e:
            logging.error(f"Failed to save live image: {e}")

    @staticmethod
    def load_image(path: str):
        if os.path.exists(path):
            logging.info(f"Loading live image from {path}...")
            try:
                with open(path, "rb") as f:
                    manager = dill.load(f)
                SingletonMeta._instances[ProtoManager] = manager
                logging.info("Live image loaded successfully.")
                return manager
            except Exception as e:
                logging.error(f"Failed to load live image: {e}. Creating new instance.")
                return ProtoManager()
        else:
            logging.info("No live image found. Creating new instance.")
            return ProtoManager()

proto_manager = ProtoManager()


File: a4ps_os/a4ps/models.py

Python

# a4ps/models.py
import ollama
import logging
from threading import Lock

class ModelManager:
    def __init__(self):
        self.current_model = None
        self.lock = Lock()
        logging.info("ModelManager initialized.")

    def get_embedding(self, text: str) -> list[float]:
        try:
            response = ollama.embeddings(model='nomic-embed-text', prompt=text)
            return response["embedding"]
        except Exception as e:
            logging.error(f"Error generating embedding: {e}")
            return [0.0] * 384

    def invoke(self, model_name: str, prompt: str, system_prompt: str, adapter_path: str = None) -> str:
        with self.lock:
            try:
                # Ollama handles model loading/unloading. We just specify the model tag.
                # A more advanced implementation would manage a custom Modelfile for adapters.
                self.current_model = model_name
                logging.info(f"Invoking model '{model_name}'...")
                response = ollama.chat(
                    model=model_name,
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': prompt}
                    ],
                    options={'keep_alive': '5m'}
                )
                return response['message']['content']
            except Exception as e:
                logging.error(f"Error invoking model {model_name}: {e}")
                return f"Error: Could not invoke model {model_name}."

model_manager = ModelManager()


File: a4ps_os/a4ps/state.py

Python

# a4ps/state.py
from typing import List, TypedDict
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    messages: List
    task: str
    plan: str
    draft: str
    critique: str
    dissonance_score: float
    turn_count: int
    tool_spec: str


File: a4ps_os/a4ps/graph.py

Python

# a4ps/graph.py
import logging
from textwrap import dedent
from langgraph.graph import StateGraph, END
from.state import AgentState
from.proto import proto_manager
from.tools.tool_forge import tool_forge
from.services.motivator_service import event_bus
from.main import SETTINGS

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def update_thinking_state(proto_name: str, is_thinking: bool):
    proto = proto_manager.get_proto(proto_name)
    if proto:
        proto.state['is_thinking'] = is_thinking

def alfred_node(state: AgentState):
    update_thinking_state("ALFRED", True)
    logging.info("---ALFRED NODE---")
    messages = state['messages']
    alfred_proto = proto_manager.get_proto("ALFRED")

    if len(messages) == 1:
        task = messages[-1].content
        plan_prompt = f"Decompose the following task into a clear, actionable plan. Task: {task}"
        plan = alfred_proto.invoke_llm(plan_prompt)
        logging.info(f"ALFRED generated plan: {plan}")
        update_thinking_state("ALFRED", False)
        return {"plan": plan, "messages": state['messages'] + [("ai", f"Plan:\n{plan}")]}
    else:
        final_draft = state.get('draft', "No draft produced.")
        synthesis_prompt = dedent(f"""
            Review the following draft response. Ensure it is coherent, complete, and directly addresses the Architect's original request. Add a concluding remark in your own voice.
            Original Task: {state['task']}
            Final Draft: {final_draft}
        """)
        final_response = alfred_proto.invoke_llm(synthesis_prompt)
        logging.info("ALFRED synthesized final response.")
        update_thinking_state("ALFRED", False)
        return {"messages": state['messages'] + [("ai", final_response)]}

def brick_node(state: AgentState):
    update_thinking_state("BRICK", True)
    logging.info("---BRICK NODE---")
    context = "\n".join([f"{msg.type}: {msg.content}" for msg in state['messages']])
    prompt = dedent(f"""
        Analyze the following context and provide a logical, structured 'thesis'. If a new tool is required, you MUST end your response with the exact phrase:
        TOOL_REQUIRED: [A clear, concise specification for the tool].
        Context: {context}
    """)
    response = proto_manager.get_proto("BRICK").invoke_llm(prompt)
    logging.info(f"BRICK response: {response}")
    
    tool_spec = None
    if "TOOL_REQUIRED:" in response:
        tool_spec = response.split("TOOL_REQUIRED:").[1]strip()
        
    update_thinking_state("BRICK", False)
    return {"messages": state['messages'] + [("ai", response)], "tool_spec": tool_spec}

def robin_node(state: AgentState):
    update_thinking_state("ROBIN", True)
    logging.info("---ROBIN NODE---")
    bricks_thesis = state['messages'][-1].content
    prompt = dedent(f"""
        Read BRICK's analysis. Provide a creative, empathetic 'antithesis'. Then, on a new line, rate the 'computational cognitive dissonance' between your perspectives from 0.0 to 1.0.
        Format it exactly as: DISSONANCE: [your_score]
        BRICK's Analysis: {bricks_thesis}
    """)
    response = proto_manager.get_proto("ROBIN").invoke_llm(prompt)
    logging.info(f"ROBIN response: {response}")
    
    dissonance_score = 0.5
    if "DISSONANCE:" in response:
        try:
            score_str = response.split("DISSONANCE:").[1]strip()
            dissonance_score = float(score_str)
        except (ValueError, IndexError):
            logging.warning("ROBIN failed to provide a valid dissonance score.")

    draft = f"LOGICAL ANALYSIS (BRICK):\n{bricks_thesis}\n\nCREATIVE SYNTHESIS (ROBIN):\n{response}"
    update_thinking_state("ROBIN", False)
    return {"messages": state['messages'] + [("ai", response)], "dissonance_score": dissonance_score, "draft": draft}

def tool_forge_node(state: AgentState):
    logging.info("---TOOL FORGE NODE---")
    spec = state.get("tool_spec")
    if not spec:
        return {"messages": state['messages'] +}
    
    result = tool_forge.create_tool(spec)
    logging.info(f"Tool Forge result: {result}")
    return {"messages": state['messages'] + [("tool", result)], "tool_spec": None}

def route_after_robin(state: AgentState):
    turn_count = state.get('turn_count', 0) + 1
    dissonance = state.get('dissonance_score', 0.0)
    tool_spec = state.get("tool_spec")

    if tool_spec:
        return "tool_forge"
    
    if dissonance > SETTINGS['graph']['convergence_threshold'] and turn_count < SETTINGS['graph']['max_turns']:
        return "brick"
    else:
        if dissonance > 0.8:
            event_bus.publish("high_cognitive_dissonance", {"score": dissonance})
        return "alfred_synthesize"

def create_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("alfred_plan", alfred_node)
    workflow.add_node("brick", brick_node)
    workflow.add_node("robin", robin_node)
    workflow.add_node("tool_forge", tool_forge_node)
    workflow.add_node("alfred_synthesize", alfred_node)

    workflow.set_entry_point("alfred_plan")
    workflow.add_edge("alfred_plan", "brick")
    workflow.add_edge("brick", "robin")
    workflow.add_conditional_edges("robin", route_after_robin, {
        "brick": "brick", 
        "tool_forge": "tool_forge", 
        "alfred_synthesize": "alfred_synthesize"
    })
    workflow.add_edge("tool_forge", "brick")
    workflow.add_edge("alfred_synthesize", END)

    return workflow.compile()


File: a4ps_os/a4ps/memory.py

Python

# a4ps/memory.py
import logging
import lancedb
from.models import model_manager

class MemoryManager:
    def __init__(self, db_path, table_name):
        self.db_path = db_path
        self.table_name = table_name
        self.db = lancedb.connect(db_path)
        self.table = None
        self._initialize_table()
        logging.info(f"MemoryManager initialized for path: {db_path}")

    def _initialize_table(self):
        try:
            if self.table_name in self.db.table_names():
                self.table = self.db.open_table(self.table_name)
            else:
                self.table = self.db.create_table(self.table_name, data=[{"vector": [0.0]*384, "text": "Initial memory."}])
        except Exception as e:
            logging.error(f"Failed to initialize LanceDB table: {e}")

    def add_memory(self, text: str, metadata: dict = None):
        if not self.table: return
        try:
            embedding = model_manager.get_embedding(text)
            data = {"vector": embedding, "text": text}
            if metadata: data.update(metadata)
            self.table.add([data])
            logging.info(f"Added memory to '{self.table_name}'.")
        except Exception as e:
            logging.error(f"Failed to add memory to LanceDB: {e}")

    def search_memory(self, query: str, limit: int = 5) -> list:
        if not self.table: return
        try:
            query_embedding = model_manager.get_embedding(query)
            results = self.table.search(query_embedding).limit(limit).to_list()
            return results
        except Exception as e:
            logging.error(f"Failed to search memory in LanceDB: {e}")
            return

memory_manager = None


File: a4ps_os/a4ps/tools/tool_forge.py

Python

# a4ps/tools/tool_forge.py
import logging
import docker
import os
import importlib.util
from.dynamic_tools import tool_registry
from..proto import proto_manager
from..services.motivator_service import event_bus

class ToolForge:
    def __init__(self, sandbox_image, runtime, dynamic_tools_path="a4ps/tools/dynamic_tools"):
        self.docker_client = docker.from_env()
        self.sandbox_image = sandbox_image
        self.runtime = runtime
        self.dynamic_tools_path = dynamic_tools_path
        os.makedirs(self.dynamic_tools_path, exist_ok=True)
        logging.info(f"ToolForge initialized. Sandbox: {self.sandbox_image}")

    def create_tool(self, tool_spec: str, max_retries=3) -> str:
        brick = proto_manager.get_proto("BRICK")
        if not brick: return "Error: BRICK persona not found."

        for i in range(max_retries):
            logging.info(f"ToolForge Attempt {i+1}/{max_retries}")
            code_gen_prompt = f"""
            Generate a Python script for a new tool based on this spec: "{tool_spec}".
            The script must define a single function with a descriptive, snake_case name.
            Include a docstring and unit tests in an `if __name__ == '__main__':` block.
            Respond ONLY with the Python code inside a single ```python block.
            """
            generated_script = brick.invoke_llm(code_gen_prompt)
            
            if "```python" in generated_script:
                generated_script = generated_script.split("```python").[1]split("```").strip()

            if not generated_script or "def " not in generated_script:
                tool_spec = "Code generation failed. Please try again, following the format exactly."
                continue

            try:
                container = self.docker_client.containers.run(
                    self.sandbox_image, command=["python", "-c", generated_script],
                    runtime=self.runtime, remove=True, detach=False,
                    stdout=True, stderr=True
                )
                stdout = container.decode('utf-8')
                logging.info(f"ToolForge Sandbox STDOUT: {stdout}")

                func_name = [line.split("def ").[1]split("(") for line in generated_script.split('\n') if line.startswith("def ")]
                file_path = os.path.join(self.dynamic_tools_path, f"{func_name}.py")
                function_code = generated_script.split("if __name__ == '__main__':")
                with open(file_path, "w") as f: f.write(function_code)
                
                spec = importlib.util.spec_from_file_location(func_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                tool_func = getattr(module, func_name)
                
                tool_registry[func_name] = tool_func
                logging.info(f"Successfully created and registered new tool '{func_name}'.")
                event_bus.publish("tool_created", {"tool_name": func_name})
                return f"Successfully created tool: {func_name}"

            except Exception as e:
                error_log = str(e)
                logging.error(f"ToolForge: Sandbox execution failed: {error_log}")
                tool_spec = f"The previous attempt failed with error: {error_log}. Original spec: '{tool_spec}'. Please provide a corrected script."
        
        return f"Failed to create tool after {max_retries} attempts."

tool_forge = None


File: a4ps_os/a4ps/tools/dynamic_tools/__init__.py

Python

# a4ps/tools/dynamic_tools/__init__.py
tool_registry = {}


File: a4ps_os/a4ps/services/motivator_service.py

Python

# a4ps/services/motivator_service.py
import logging
import threading
import time
from queue import Queue

class EventBus:
    def __init__(self):
        self.listeners = {}

    def subscribe(self, event_type, listener):
        if event_type not in self.listeners:
            self.listeners[event_type] =
        self.listeners[event_type].append(listener)

    def publish(self, event_type, data=None):
        if event_type in self.listeners:
            for listener in self.listeners[event_type]:
                listener(data)

event_bus = EventBus()

class MotivatorService:
    def __init__(self, stop_event, task_queue):
        self.stop_event = stop_event
        self.task_queue = task_queue
        self.thread = threading.Thread(target=self.run, daemon=True)
        event_bus.subscribe("high_cognitive_dissonance", self.handle_dissonance)
        event_bus.subscribe("tool_created", self.handle_curiosity)
        logging.info("MotivatorService initialized.")

    def start(self):
        self.thread.start()

    def handle_dissonance(self, data):
        task = f"Reflect on the high dissonance ({data['score']:.2f}) and propose a new protocol to avoid this."
        self.task_queue.put({"source": "motivator", "task": task})

    def handle_curiosity(self, data):
        task = f"A new tool '{data['tool_name']}' was created. Formulate a novel problem that can be solved by combining it with existing tools."
        self.task_queue.put({"source": "motivator", "task": task})

    def run(self):
        while not self.stop_event.is_set():
            time.sleep(5)

    def stop(self):
        logging.info("MotivatorService stopping.")


File: a4ps_os/a4ps/services/curator_service.py

Python

# a4ps/services/curator_service.py
import logging
import json
import os
import threading
from..proto import proto_manager
from..memory import memory_manager
from..fine_tuning.unsloth_forge import unsloth_forge

class CuratorService:
    def __init__(self, threshold, trigger_size, dataset_path="data/golden_datasets"):
        self.threshold = threshold
        self.trigger_size = trigger_size
        self.dataset_path = dataset_path
        os.makedirs(self.dataset_path, exist_ok=True)
        logging.info("CuratorService initialized.")

    def curate(self):
        logging.info("CuratorService: Starting curation cycle.")
        recent_interactions = memory_manager.search_memory("recent conversation", limit=50)
        
        alfred = proto_manager.get_proto("ALFRED")
        if not alfred: return

        golden_samples =
        for interaction in recent_interactions:
            score = self._score_interaction(alfred, interaction['text'])
            if score >= self.threshold:
                formatted_sample = self._format_for_finetuning(interaction['text'])
                if formatted_sample:
                    golden_samples.append(formatted_sample)

        if golden_samples:
            self._save_golden_samples(golden_samples)
        
        self._check_and_trigger_finetune()

    def _score_interaction(self, alfred_proto, text: str) -> float:
        prompt = f"Evaluate the following conversation based on logical rigor, creative synthesis, and task efficacy. Provide a single 'Overall Golden Score' from 1.0 to 5.0. Transcript:\n{text}\n\nRespond ONLY with the score."
        try:
            return float(alfred_proto.invoke_llm(prompt).strip())
        except (ValueError, TypeError):
            return 0.0

    def _format_for_finetuning(self, text: str) -> dict | None:
        lines = text.split('\n')
        messages =
        for line in lines:
            if line.startswith("Task:"):
                messages.append({"role": "user", "content": line.replace("Task:", "").strip()})
            elif line.startswith("Response:"):
                 messages.append({"role": "assistant", "content": line.replace("Response:", "").strip()})
        
        if len(messages) >= 2:
            text_field = f"### Human:\n{messages['content']}\n\n### Assistant:\n{messages[1]['content']}"
            return {"text": text_field}
        return None

    def _save_golden_samples(self, samples: list):
        filepath = os.path.join(self.dataset_path, "golden_interactions.jsonl")
        with open(filepath, "a") as f:
            for sample in samples:
                f.write(json.dumps(sample) + "\n")
        logging.info(f"CuratorService: Saved {len(samples)} golden samples.")

    def _check_and_trigger_finetune(self):
        filepath = os.path.join(self.dataset_path, "golden_interactions.jsonl")
        if not os.path.exists(filepath): return

        with open(filepath, "r") as f:
            num_samples = sum(1 for _ in f)

        if num_samples >= self.trigger_size:
            logging.info(f"Golden dataset reached {num_samples} samples. Triggering UnslothForge.")
            model_to_tune = proto_manager.get_proto("BRICK").model_name
            
            ft_thread = threading.Thread(
                target=unsloth_forge.fine_tune_persona,
                args=(model_to_tune, filepath),
                daemon=True
            )
            ft_thread.start()

curator_service = None


File: a4ps_os/a4ps/fine_tuning/unsloth_forge.py

Python

# a4ps/fine_tuning/unsloth_forge.py
import logging
import torch
from unsloth import FastLanguageModel
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

class UnslothForge:
    def __init__(self):
        self.max_seq_length = 2048
        self.dtype = None
        self.load_in_4bit = True
        logging.info("UnslothForge initialized.")

    def fine_tune_persona(self, model_name: str, dataset_path: str) -> str:
        logging.info(f"UnslothForge: Starting fine-tuning for {model_name}")
        try:
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_name, max_seq_length=self.max_seq_length,
                dtype=self.dtype, load_in_4bit=self.load_in_4bit,
            )
            model = FastLanguageModel.get_peft_model(
                model, r=16, target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                lora_alpha=16, lora_dropout=0, bias="none", use_gradient_checkpointing=True,
                random_state=3407, use_rslora=False, loftq_config=None,
            )
            dataset = load_dataset("json", data_files={"train": dataset_path}, split="train")
            trainer = SFTTrainer(
                model=model, tokenizer=tokenizer, train_dataset=dataset,
                dataset_text_field="text", max_seq_length=self.max_seq_length,
                dataset_num_proc=2, packing=False,
                args=TrainingArguments(
                    per_device_train_batch_size=2, gradient_accumulation_steps=4,
                    warmup_steps=5, max_steps=60, learning_rate=2e-4,
                    fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(),
                    logging_steps=1, optim="adamw_8bit", weight_decay=0.01,
                    lr_scheduler_type="linear", seed=3407, output_dir="outputs",
                ),
            )
            trainer.train()
            adapter_path = f"outputs/{model_name.replace(':', '_')}_adapter"
            model.save_pretrained(adapter_path)
            logging.info(f"UnslothForge: Fine-tuning complete. Adapter saved to {adapter_path}")
            return adapter_path
        except Exception as e:
            logging.error(f"UnslothForge: Fine-tuning failed: {e}")
            return ""

unsloth_forge = UnslothForge()


File: a4ps_os/a4ps/ui/schemas.py

Python

# a4ps/ui/schemas.py
from pydantic import BaseModel, Field
from typing import Literal, List, Dict, Any

class ProtoState(BaseModel):
    name: str
    version: float
    mood: str = "neutral"
    dissonance: float = 0.0
    is_thinking: bool = False

class FullStateUpdate(BaseModel):
    protos: List

class PartialStateUpdate(BaseModel):
    proto: ProtoState

class LogMessage(BaseModel):
    message: str
    level: str = "INFO"

class NewToolEvent(BaseModel):
    tool_name: str

class GetFullStateCommand(BaseModel):
    command: Literal["get_full_state"] = "get_full_state"

class UpdateProtoStateCommand(BaseModel):
    command: Literal["update_proto_state"] = "update_proto_state"
    proto_name: str
    updates: Dict[str, Any]

class SubmitTaskCommand(BaseModel):
    task: str

class CommandReply(BaseModel):
    status: Literal["success", "error"]
    message: str


File: a4ps_os/a4ps/ui/communication.py

Python

# a4ps/ui/communication.py
import zmq
import msgpack
import logging
from threading import Thread
from kivy.clock import Clock
from kivy.event import EventDispatcher
from.schemas import *

class UICommunication(EventDispatcher):
    def __init__(self, pub_port, rep_port, task_port, **kwargs):
        super().__init__(**kwargs)
        self.register_event_type('on_full_state')
        self.register_event_type('on_partial_state')
        self.register_event_type('on_log_message')
        self.register_event_type('on_new_tool')

        self.context = zmq.Context()
        self.rep_port = rep_port
        self.task_port = task_port

        self.sub_socket = self.context.socket(zmq.SUB)
        self.sub_socket.connect(f"tcp://localhost:{pub_port}")
        self.sub_socket.setsockopt_string(zmq.SUBSCRIBE, "")
        
        self.poller = zmq.Poller()
        self.poller.register(self.sub_socket, zmq.POLLIN)

        self._is_running = True
        self.listen_thread = Thread(target=self._listen_for_updates, daemon=True)
        self.listen_thread.start()

    def _listen_for_updates(self):
        while self._is_running:
            socks = dict(self.poller.poll(timeout=100))
            if self.sub_socket in socks:
                topic, raw_message = self.sub_socket.recv_multipart()
                Clock.schedule_once(lambda dt, t=topic, m=raw_message: self._dispatch_message(t, m))

    def _dispatch_message(self, topic, raw_message):
        try:
            data = msgpack.unpackb(raw_message)
            topic_str = topic.decode()
            if topic_str == "full_state": self.dispatch('on_full_state', FullStateUpdate(**data))
            elif topic_str == "partial_state": self.dispatch('on_partial_state', PartialStateUpdate(**data))
            elif topic_str == "log": self.dispatch('on_log_message', LogMessage(**data))
            elif topic_str == "new_tool": self.dispatch('on_new_tool', NewToolEvent(**data))
        except Exception as e:
            logging.error(f"UI: Error processing message on topic {topic_str}: {e}")

    def send_command(self, command_model, callback):
        def _send():
            req_socket = self.context.socket(zmq.REQ)
            req_socket.connect(f"tcp://localhost:{self.rep_port}")
            try:
                req_socket.send(msgpack.packb(command_model.model_dump()))
                reply = CommandReply(**msgpack.unpackb(req_socket.recv()))
                Clock.schedule_once(lambda dt: callback(reply))
            finally:
                req_socket.close()
        Thread(target=_send, daemon=True).start()

    def send_task(self, task_model):
        def _send():
            task_socket = self.context.socket(zmq.REQ)
            task_socket.connect(f"tcp://localhost:{self.task_port}")
            try:
                task_socket.send(msgpack.packb(task_model.model_dump()))
                task_socket.recv() # Wait for ACK
            finally:
                task_socket.close()
        Thread(target=_send, daemon=True).start()

    def on_full_state(self, update): pass
    def on_partial_state(self, update): pass
    def on_log_message(self, log): pass
    def on_new_tool(self, event): pass

    def stop(self):
        self._is_running = False
        if self.listen_thread.is_alive(): self.listen_thread.join(timeout=1)
        self.sub_socket.close()
        self.context.term()


File: a4ps_os/a4ps/ui/morphs.py

Python

# a4ps/ui/morphs.py
from kivy.uix.widget import Widget
from kivy.uix.label import Label
from kivy.uix.textinput import TextInput
from kivy.uix.boxlayout import BoxLayout
from kivy.uix.floatlayout import FloatLayout
from kivy.uix.modalview import ModalView
from kivy.uix.button import Button
from kivy.properties import ListProperty, ObjectProperty, StringProperty, NumericProperty
from kivy.graphics import Color, Rectangle, Line
from.schemas import UpdateProtoStateCommand

class Morph(Widget):
    submorphs = ListProperty()
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.bind(submorphs=self._on_submorphs_changed)
    def _on_submorphs_changed(self, instance, value):
        self.clear_widgets()
        for m in value: super().add_widget(m)
    def add_widget(self, widget, index=0, canvas=None): self.submorphs.insert(index, widget)
    def remove_widget(self, widget):
        if widget in self.submorphs: self.submorphs.remove(widget)

class ProtoMorph(Morph):
    proto_name = StringProperty("Proto")
    proto_version = NumericProperty(1.0)
    proto_mood = StringProperty("neutral")
    proto_dissonance = NumericProperty(0.0)
    is_thinking = ObjectProperty(False)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.size_hint = (None, None); self.size = (150, 60)
        self.label = Label(font_size='14sp', halign='center', valign='middle', markup=True)
        self.add_widget(self.label)
        self.bind(pos=self.redraw, size=self.redraw, proto_name=self.update_text,
                  proto_version=self.update_text, proto_mood=self.update_text,
                  proto_dissonance=self.redraw, is_thinking=self.redraw)
        self.update_text(); self.redraw()

    def on_touch_down(self, touch):
        if self.collide_point(*touch.pos):
            if touch.is_right_click: self.parent.show_inspector(self); return True
            touch.grab(self)
            parent = self.parent
            if parent: parent.remove_widget(self); parent.add_widget(self)
            return True
        return super().on_touch_down(touch)

    def on_touch_move(self, touch):
        if touch.grab_current is self: self.center = touch.pos; return True
        return super().on_touch_move(touch)

    def on_touch_up(self, touch):
        if touch.grab_current is self: touch.ungrab(self); return True
        return super().on_touch_up(touch)

    def update_text(self, *args):
        self.label.text = f"[b]{self.proto_name}[/b]\nv{self.proto_version:.1f}\n{self.proto_mood}"

    def redraw(self, *args):
        self.label.size = self.size; self.label.pos = self.pos; self.label.text_size = self.size
        with self.canvas.before:
            self.canvas.before.clear()
            r = 0.2 + self.proto_dissonance * 0.7; g = 0.4; b = 0.9 - self.proto_dissonance * 0.7
            Color(r, g, b, 1); Rectangle(pos=self.pos, size=self.size)
            if self.is_thinking:
                Color(1, 1, 0, 0.5); Line(rectangle=(self.x-2, self.y-2, self.width+4, self.height+4), width=2)

class InspectorMorph(BoxLayout, Morph):
    target_morph = ObjectProperty(None, allownone=True)
    def __init__(self, comms, **kwargs):
        super().__init__(**kwargs)
        self.comms = comms; self.orientation = 'vertical'; self.size_hint = (None, None)
        self.size = (250, 300); self.padding = 5; self.spacing = 5
        self.title_label = Label(text="Inspector", size_hint_y=None, height=30)
        self.add_widget(self.title_label)
        self.properties_layout = BoxLayout(orientation='vertical', spacing=5)
        self.add_widget(self.properties_layout)

    def update_from_state(self, proto_state):
        if self.target_morph and self.target_morph.proto_name == proto_state.name:
            self.title_label.text = f"Inspector: {proto_state.name}"
            self.properties_layout.clear_widgets()
            for key, value in proto_state.model_dump().items():
                if key == 'name': continue
                prop_layout = BoxLayout(size_hint_y=None, height=30)
                prop_layout.add_widget(Label(text=f"{key}:"))
                prop_input = TextInput(text=str(value), multiline=False)
                prop_input.bind(on_text_validate=lambda instance, k=key: self.on_prop_change(k, instance.text))
                prop_layout.add_widget(prop_input)
                self.properties_layout.add_widget(prop_layout)

    def on_prop_change(self, key, value):
        # Basic type casting
        try:
            if isinstance(getattr(self.target_morph, f"proto_{key}", value), float): value = float(value)
            elif isinstance(getattr(self.target_morph, f"proto_{key}", value), bool): value = value.lower() in ['true', '1']
        except ValueError:
            pass # Keep as string if cast fails
        
        command = UpdateProtoStateCommand(proto_name=self.target_morph.proto_name, updates={key: value})
        self.comms.send_command(command, lambda reply: print(f"Inspector update reply: {reply.message}"))

class WorldMorph(FloatLayout, Morph):
    def __init__(self, comms, **kwargs):
        super().__init__(**kwargs)
        self.comms = comms; self.proto_morphs = {}
        self.inspector = InspectorMorph(comms=self.comms, pos_hint={'right': 1, 'top': 1})
        self.inspector_visible = False

    def update_morph(self, proto_state):
        name = proto_state.name
        if name not in self.proto_morphs:
            morph = ProtoMorph(proto_name=name, pos=(100 + len(self.proto_morphs) * 160, 300))
            self.proto_morphs[name] = morph; self.add_widget(morph)
        
        morph = self.proto_morphs[name]
        morph.proto_version = proto_state.version; morph.proto_mood = proto_state.mood
        morph.proto_dissonance = proto_state.dissonance; morph.is_thinking = proto_state.is_thinking

        if self.inspector_visible and self.inspector.target_morph.proto_name == name:
            self.inspector.update_from_state(proto_state)

    def show_inspector(self, target):
        self.inspector.target_morph = target
        if not self.inspector_visible:
            self.add_widget(self.inspector); self.inspector_visible = True
        
        state = ProtoState(name=target.proto_name, version=target.proto_version, mood=target.proto_mood,
                           dissonance=target.proto_dissonance, is_thinking=target.is_thinking)
        self.inspector.update_from_state(state)


File: a4ps_os/a4ps/ui/main_ui.py

Python

# a4ps/ui/main_ui.py
import logging
from kivy.app import App
from kivy.core.window import Window
from kivy.uix.boxlayout import BoxLayout
from kivy.uix.textinput import TextInput
from kivy.uix.button import Button
from kivy.uix.scrollview import ScrollView
from kivy.uix.label import Label
from.communication import UICommunication
from.morphs import WorldMorph
from.schemas import GetFullStateCommand, SubmitTaskCommand

class EntropicUIApp(App):
    def __init__(self, pub_port, rep_port, task_port, **kwargs):
        super().__init__(**kwargs)
        self.comms = UICommunication(pub_port, rep_port, task_port)
        self.world = WorldMorph(comms=self.comms)

    def build(self):
        self.title = "BAT OS: The Architect's Workbench"
        Window.clearcolor = (0.1, 0.1, 0.1, 1)
        
        root_layout = BoxLayout(orientation='horizontal')
        root_layout.add_widget(self.world)

        side_panel = BoxLayout(orientation='vertical', size_hint_x=0.4, spacing=5, padding=5)
        self.log_label = Label(text="[b]System Log[/b]\n", markup=True, size_hint_y=None, halign='left', valign='top')
        self.log_label.bind(texture_size=self.log_label.setter('size'))
        log_scroll = ScrollView(size_hint=(1, 1)); log_scroll.add_widget(self.log_label)
        side_panel.add_widget(log_scroll)

        task_input_layout = BoxLayout(size_hint_y=None, height=40, spacing=5)
        self.task_input = TextInput(hint_text="Enter task for ALFRED...", multiline=False)
        self.task_input.bind(on_text_validate=self.submit_task)
        submit_button = Button(text="Submit", size_hint_x=0.2)
        submit_button.bind(on_press=self.submit_task)
        task_input_layout.add_widget(self.task_input); task_input_layout.add_widget(submit_button)
        side_panel.add_widget(task_input_layout)
        root_layout.add_widget(side_panel)

        self.comms.bind(on_full_state=self.handle_full_state)
        self.comms.bind(on_partial_state=self.handle_partial_state)
        self.comms.bind(on_log_message=self.handle_log_message)
        
        self.comms.send_command(GetFullStateCommand(), lambda r: logging.info(f"UI: Initial state reply: {r.message}"))
        return root_layout

    def submit_task(self, instance):
        if self.task_input.text:
            self.log_label.text += f"[color=cyan]ARCHITECT:[/color] {self.task_input.text}\n"
            self.comms.send_task(SubmitTaskCommand(task=self.task_input.text))
            self.task_input.text = ""

    def handle_full_state(self, instance, update):
        for proto_state in update.protos: self.world.update_morph(proto_state)

    def handle_partial_state(self, instance, update):
        self.world.update_morph(update.proto)

    def handle_log_message(self, instance, log):
        color_map = {"INFO": "lightgreen", "WARNING": "yellow", "ERROR": "red"}
        self.log_label.text += f"[color={color_map.get(log.level, 'white')}]{log.level}:[/color] {log.message}\n"

    def on_stop(self):
        self.comms.stop()


File: a4ps_os/a4ps/main.py

Python

# a4ps/main.py
import logging
import toml
import atexit
from threading import Thread, Event
import time
import zmq
import msgpack
from queue import Queue, Empty
from.proto import Proto, proto_manager
from.graph import create_graph
from.services.motivator_service import MotivatorService
from.services.curator_service import CuratorService
from.ui.schemas import *
from.ui.main_ui import EntropicUIApp
from.tools.tool_forge import ToolForge
from.memory import MemoryManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
SETTINGS = toml.load("config/settings.toml")
CODEX = toml.load("config/codex.toml")
PUB_PORT, REP_PORT, TASK_PORT = SETTINGS['zeromq']['pub_port'], SETTINGS['zeromq']['rep_port'], SETTINGS['zeromq']['task_port']
stop_event = Event()
task_queue = Queue()

def publish_message(socket, topic, message_model):
    try:
        socket.send_multipart([topic.encode(), msgpack.packb(message_model.model_dump())])
    except Exception as e:
        logging.error(f"Backend: Failed to publish message on topic {topic}: {e}")

def get_full_state_update() -> FullStateUpdate:
    protos_state =
    return FullStateUpdate(protos=protos_state)

def a4ps_backend_thread():
    logging.info("BAT OS Backend Thread started.")
    context = zmq.Context()
    pub_socket = context.socket(zmq.PUB); pub_socket.bind(f"tcp://*:{PUB_PORT}")
    rep_socket = context.socket(zmq.REP); rep_socket.bind(f"tcp://*:{REP_PORT}")
    task_socket = context.socket(zmq.REP); task_socket.bind(f"tcp://*:{TASK_PORT}")
    
    poller = zmq.Poller(); poller.register(rep_socket, zmq.POLLIN); poller.register(task_socket, zmq.POLLIN)

    global tool_forge, memory_manager, curator_service
    tool_forge = ToolForge(SETTINGS['sandbox']['image'], SETTINGS['sandbox']['runtime'])
    memory_manager = MemoryManager(SETTINGS['memory']['db_path'], SETTINGS['memory']['table_name'])
    curator_service = CuratorService(
        SETTINGS['autopoiesis']['curation_threshold'],
        SETTINGS['autopoiesis']['fine_tune_trigger_size']
    )
    
    app_graph = create_graph()
    motivator = MotivatorService(stop_event, task_queue)
    motivator.start()

    logging.info("BAT OS Backend is running...")
    last_curation_time = 0

    while not stop_event.is_set():
        socks = dict(poller.poll(timeout=100))
        
        if rep_socket in socks:
            try:
                cmd_data = msgpack.unpackb(rep_socket.recv())
                if cmd_data.get("command") == "get_full_state":
                    publish_message(pub_socket, "full_state", get_full_state_update())
                    reply = CommandReply(status="success", message="Full state published.")
                elif cmd_data.get("command") == "update_proto_state":
                    cmd = UpdateProtoStateCommand(**cmd_data)
                    proto = proto_manager.get_proto(cmd.proto_name)
                    if proto:
                        proto.state.update(cmd.updates)
                        publish_message(pub_socket, "partial_state", PartialStateUpdate(proto=ProtoState(name=cmd.proto_name, **proto.state)))
                        reply = CommandReply(status="success", message=f"Updated {cmd.proto_name}")
                    else:
                        reply = CommandReply(status="error", message=f"Proto {cmd.proto_name} not found.")
                else:
                    reply = CommandReply(status="error", message="Unknown command")
                rep_socket.send(msgpack.packb(reply.model_dump()))
            except Exception as e:
                logging.error(f"Backend: Error processing command: {e}")

        if task_socket in socks:
            task_queue.put({"source": "architect", "task": msgpack.unpackb(task_socket.recv())['task']})
            task_socket.send(b"ACK")

        try:
            item = task_queue.get_nowait()
            task = item['task']
            publish_message(pub_socket, "log", LogMessage(message=f"New task from {item['source']}: {task}"))
            
            config = {"configurable": {"thread_id": f"thread_{time.time()}"}}
            final_response = ""
            for s in app_graph.stream({"messages": [("user", task)], "task": task, "turn_count": 0}, config=config):
                step_key = list(s.keys())
                publish_message(pub_socket, "log", LogMessage(message=f"Graph Step: {step_key}"))
                publish_message(pub_socket, "full_state", get_full_state_update()) # Publish state after each step
                if END in s:
                    final_response = s['messages'][-1].content
            
            publish_message(pub_socket, "log", LogMessage(message=f"Final Response: {final_response}", level="INFO"))
            memory_manager.add_memory(f"Task: {task}\nResponse: {final_response}")
            task_queue.task_done()
        except Empty:
            pass

        if time.time() - last_curation_time > SETTINGS['autopoiesis']['curation_interval_seconds']:
            curator_service.curate(); last_curation_time = time.time()
        
        time.sleep(0.1)

    motivator.stop()
    proto_manager.save_image(SETTINGS['system']['image_path'])
    pub_socket.close(); rep_socket.close(); task_socket.close(); context.term()
    logging.info("BAT OS Backend Thread stopped gracefully.")

def main():
    manager = proto_manager.load_image(SETTINGS['system']['image_path'])
    if not manager.get_all_protos():
        for persona_config in CODEX.get("persona",):
            manager.register_proto(Proto(name=persona_config['name'], codex=persona_config))

    atexit.register(lambda: stop_event.set())
    backend = Thread(target=a4ps_backend_thread, daemon=True)
    backend.start()

    EntropicUIApp(pub_port=PUB_PORT, rep_port=REP_PORT, task_port=TASK_PORT).run()

    stop_event.set()
    backend.join()

if __name__ == "__main__":
    for pkg in ['a4ps', 'a4ps/tools', 'a4ps/tools/dynamic_tools', 'a4ps/services', 'a4ps/fine_tuning', 'a4ps/ui']:
        open(os.path.join(pkg, '__init__.py'), 'a').close()
    main()


Step 4: Install Dependencies

Ensure your PowerShell is still in the a4ps_os directory and the virtual environment is active. Then run:

PowerShell

pip install -r requirements.txt


Step 5: Pull Ollama Models

Make sure the Ollama service is running on your machine. Then, execute these commands:

PowerShell

ollama pull gemma2:9b-instruct
ollama pull mistral
ollama pull phi3
ollama pull llama3.1
ollama pull nomic-embed-text


Step 6: Build the Secure Sandbox

Make sure Docker Desktop is running. Then, from the a4ps_os directory, run:

PowerShell

docker build -t a4ps-sandbox -f sandbox/Dockerfile.sandbox.


Step 7: Launch the BAT OS

You are now ready to launch the fully functional system. From the a4ps_os directory, execute the launch script. Note that on Windows, you may need to run this from a shell that supports bash scripts, like Git Bash.

Bash

bash run.sh


The Entropic UI window will appear, and you will see the live ProtoMorphs on the canvas. The system is now online and fully operational.