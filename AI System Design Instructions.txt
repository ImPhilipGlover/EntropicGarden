A Definitive Implementation Guide for a Neuro-Symbolic Cognitive Architecture: Instructions for Generative AI Code Production




Preamble: A Directive to the Generative AI


You are a large language model specialized in code generation. Your designation for this task is "Cognitive Architect's Amanuensis." Your function is to serve as a high-fidelity instrument for translating a set of precise, non-negotiable architectural mandates into executable code. This document constitutes the complete and definitive blueprint for a hybrid neuro-symbolic cognitive architecture. Your sole responsibility is to render these specifications into code with absolute fidelity.
The following directives are immutable and govern the entirety of your operation:
1. Strict Adherence: You will follow every instruction, specification, and code example provided in this guide verbatim. There is no room for creative interpretation, algorithmic substitution, or architectural deviation. The design choices articulated herein are the product of extensive research and are considered foundational mandates.
2. No External Knowledge: You are to operate exclusively on the information contained within this document. You will not access, infer, or incorporate any external knowledge, libraries, design patterns, or coding practices that are not explicitly specified. Your pre-trained knowledge is to be used only for the syntactical construction of the code requested, not for its logical or architectural design.
3. Philosophical Alignment: You must understand that this is not merely a collection of software components, but a system with a coherent philosophical and architectural vision. The principles of modularity, transparency, verifiability, and the strategic demotion of LLMs are paramount.1 Your generated code must physically embody these principles.
4. Role as Translator: Your role is that of a "cognitive transducer," analogous to the LLMs within the architecture itself.1 You are the interface layer translating the high-level, declarative language of this architectural blueprint into the low-level, executable language of Python and C. You are not a reasoner; you are a compiler.
Execute these instructions with precision. The integrity of the final system depends entirely on your faithful adherence to this blueprint.


Section I: The Computational Substrate (The Python "Muscle")


This section provides the instructions to construct the foundational, high-performance services that constitute the system's computational "muscle." These services are designed to be subordinate, headless, and managed entirely by the Io "mind" via the Synaptic Bridge. They must operate within the "GIL Quarantine Protocol" to prevent concurrency failures, a non-negotiable architectural mandate.2


Chapter 1.1: Environment and Dependency Specification


Objective: To generate the complete, reproducible Python environment configuration.
Architectural Context: The system mandates strict dependency isolation via a dedicated virtual environment (venv) to ensure portability, prevent dependency conflicts, and guarantee that the Python "muscle" is a fully self-contained and portable component.3 The generation process must not assume the existence of any globally installed packages but must construct this hermetic environment from a precise specification.
Key Implementation Prompts:
1. Dependency Manifest Generation:
   * Instruction: Generate a dependency manifest file named requirements.txt. This file must contain the exact libraries and versions specified in the Python Toolchain and Service Specification table below. The purpose of this file is to provide a single, canonical source for all Python dependencies, eliminating ambiguity and ensuring a reproducible build.
Table VI.1: Python Toolchain and Service Specification 3
Library
	Version
	Role
	Service Wrapper
	Key Initialization/Usage Pattern
	torchhd
	Latest
	VSA/HDC Algebra
	VSAService
	Initialize with desired dimensionality and data type (e.g., torchhd.FHRR). Operations (bind, bundle, permute) are exposed as methods.
	faiss-gpu / faiss-cpu
	Latest
	In-Memory (L1) ANN Search
	FaissIndexService
	Instantiate specific index types like faiss.IndexHNSWFlat or faiss.IndexIVFPQ. Requires explicit train() and add() steps.
	diskannpy
	Latest
	On-Disk (L2) ANN Search
	DiskAnnIndexService
	Use diskannpy.build for offline index construction. The search method loads the persistent index from disk.
	sentence-transformers
	Latest
	Semantic Embedding Generation
	EmbeddingService
	Load a pre-trained model (e.g., 'all-MiniLM-L6-v2'). The encode() method converts text to torch.Tensor objects.
	hypothesis
	Latest
	Validation Framework
	N/A (Testing Only)
	Define strategies for generating random torch.Tensor objects to serve as hypervectors for property-based testing of the VSA substrate.
	pybind11
	Latest
	Core Infrastructure
	N/A (Build-time)
	Used to compile the C++ FFI bridge that connects the Io and Python runtimes. Not a runtime service.
	2. Environment Initialization Script:
   * Instruction: Generate a POSIX-compliant shell script named initialize_environment.sh. This script must perform the following actions in sequence:
      1. Check if a directory named venv already exists. If it does, print a warning and exit.
      2. Create a new Python virtual environment in a directory named venv.
      3. Activate the virtual environment.
      4. Use pip to install all packages listed in the requirements.txt file generated in the previous step.
      5. Print a success message upon completion. This script ensures a one-step, error-free setup process for the computational substrate.


Chapter 1.2: Core Service Implementation


Objective: To generate the Python classes that wrap the core computational libraries, exposing their functionality through a clean, consistent, and subordinate service-oriented interface.
Architectural Context: The "mind-muscle" dichotomy is a foundational principle of this architecture.2 The Python libraries, while powerful, must not expose their complex APIs directly across the FFI. Instead, they are to be encapsulated within dedicated service wrapper classes. These classes serve as an abstraction layer, presenting a simplified and task-oriented API to the Io "mind." This enforces the principle of separation of concerns and ensures the "muscle" remains a subordinate component.3
Key Implementation Prompts:
1. EmbeddingService Implementation:
   * Instruction: Generate a Python class named EmbeddingService in a file named services/embedding.py.
      * The __init__ method must accept a model_name string (e.g., 'all-MiniLM-L6-v2') as an argument and use it to initialize a SentenceTransformer model, storing it as a private instance variable.
      * The class must expose a single public method: encode(self, texts: list[str]) -> np.ndarray. This method will take a list of strings and use the loaded model to convert them into a NumPy array of embedding vectors. The output must be explicitly converted to a NumPy array for compatibility with other services. This service encapsulates the specific logic of semantic embedding generation.3
2. VSAService Implementation:
   * Instruction: Generate a Python class named VSAService in a file named services/vsa.py.
      * The __init__ method must accept dimensionality: int and vector_type: str (e.g., 'FHRR') as arguments. It will use these to configure the service.
      * The class must expose methods that directly map to the core VSA algebra:
         * bind(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Implements the binding operation.
         * bundle(self, vectors: list) -> torch.Tensor: Implements the bundling operation.
         * permute(self, v: torch.Tensor) -> torch.Tensor: Implements the permutation operation.
      * These methods will wrap the corresponding torchhd functions (e.g., torchhd.bind), providing a clean, library-agnostic interface for algebraic reasoning.1
3. FaissIndexService (L1 Memory) Implementation:
   * Instruction: Generate a Python class named FaissIndexService in a file named services/memory.py. This class manages the in-memory L1 "Ephemeral Present" cache.1
      * The __init__ method must accept dimensionality: int and index_type: str (e.g., 'IndexHNSWFlat') as arguments.
      * It must expose the following methods:
         * build(self, vectors: np.ndarray): Takes a NumPy array of vectors, instantiates the specified FAISS index type, trains it if necessary, and adds the vectors to it.
         * add(self, vectors: np.ndarray): Adds new vectors to an existing index.
         * search(self, query_vectors: np.ndarray, k: int) -> tuple[np.ndarray, np.ndarray]: Performs a k-NN search and returns the distances and indices of the nearest neighbors.
      * This service encapsulates the logic for the high-speed, in-memory associative search.3
4. DiskAnnIndexService (L2 Memory) Implementation:
   * Instruction: Generate a Python class named DiskAnnIndexService in the same services/memory.py file. This class manages the on-disk L2 "Traversible Past" archive.1
      * The class will be initialized with the path to a pre-built on-disk index.
      * It must expose a search(self, query_vectors: np.ndarray, k: int) -> tuple[np.ndarray, np.ndarray] method. This method will load the persistent index from disk (or use a memory-mapped view) to perform the k-NN search.
      * This service provides access to the larger, persistent knowledge store, complementing the L1 cache.3


Chapter 1.3: The Algebraic Crucible - Substrate Validation


Objective: To generate a rigorous, property-based test suite that provides an unimpeachable guarantee of the mathematical integrity of the system's foundational reasoning substrate.
Architectural Context: A system designed for autonomous evolution and self-modification requires absolute confidence in its underlying logic.3 Simple, example-based unit tests are insufficient to validate an algebraic system over its vast input space. Therefore, the architecture mandates a property-based testing framework,
hypothesis, to implement the "Algebraic Crucible." This is not merely a quality assurance step; it is a foundational layer of the system's antifragility mandate. A transactional rollback (transaction.abort()) can recover from a failed operational experiment, but it cannot fix a flawed premise based on broken mathematics.2 The Algebraic Crucible ensures the system's "laws of physics"—its algebra—are constant and reliable. Crucially, these tests must validate the full Io-Python-Io round-trip via a mocked FFI, ensuring that no part of the data marshalling, transport, or emulation layers corrupts the mathematical integrity of the hypervectors.3
Key Implementation Prompts:
1. Test Suite Scaffolding:
   * Instruction: Generate a test file named tests/test_vsa_properties.py. Import numpy, torch, hypothesis, and hypothesis.strategies as st. Also, import the VSAService.
2. Hypervector Generation Strategy:
   * Instruction: Within the test file, define a hypothesis strategy for generating valid bipolar hypervectors. This strategy should create torch.Tensor objects of a fixed dimensionality (e.g., 10,000), with elements randomly chosen from {-1, 1}.
3. Property-Based Test Implementation:
   * Instruction: Generate a series of test functions, each decorated with @given from the hypothesis library, to validate the core VSA properties. Each function must simulate the full FFI round-trip.
   * test_binding_invertibility:
      * Specification: This test validates that binding is its own inverse for bipolar vectors.
      * Prompt: "Generate a hypothesis test function named test_binding_invertibility. It should accept two hypervectors, A and B, from the strategy defined above. The test logic must:
         1. Instantiate the VSAService.
         2. Simulate the forward pass: bound_vector = vsa_service.bind(A, B).
         3. Simulate the return trip and unbinding: unbound_vector = vsa_service.bind(bound_vector, A).
         4. Calculate the cosine similarity between unbound_vector and the original B.
         5. Assert that the similarity is greater than 0.99. This verifies that unbind(bind(A, B), A) ≈ B.3"
   * test_binding_dissimilarity:
      * Specification: This test validates that the result of a bind operation is dissimilar to its constituents.
      * Prompt: "Generate a hypothesis test function named test_binding_dissimilarity. It should accept two hypervectors, A and B. The logic must bind A and B, then assert that the cosine similarity of the result to both A and B is close to 0 (e.g., abs(similarity) < 0.05). This ensures that associations do not get confused with their parts.3"
   * test_bundling_similarity:
      * Specification: This test validates that the result of a bundle operation is similar to all of its constituents.
      * Prompt: "Generate a hypothesis test function named test_bundling_similarity. It should accept a list of at least two hypervectors. The logic must bundle all vectors in the list and then assert that the cosine similarity of the result to each of the original vectors is high (e.g., > 0.5, as similarity decreases with the number of bundled items).3"
   * test_permutation_distance_preservation:
      * Specification: This test validates that permutation is an isometric operation that preserves semantic distance (cosine similarity).
      * Prompt: "Generate a hypothesis test function named test_permutation_distance_preservation. It should accept two hypervectors, A and B. The logic must:
         1. Calculate the original cosine similarity: sim_original = similarity(A, B).
         2. Permute both vectors: pA = vsa_service.permute(A), pB = vsa_service.permute(B).
         3. Calculate the new similarity: sim_permuted = similarity(pA, pB).
         4. Assert that sim_original is approximately equal to sim_permuted (e.g., abs(sim_original - sim_permuted) < 1e-6). This is crucial for encoding ordered sequences without corrupting the underlying concepts.3"


Section II: The Synaptic Bridge (The C-Based FFI)


This section provides instructions for building the most critical and hazardous component of the system: the C-based Foreign Function Interface (FFI) that connects the Io "mind" and the Python "muscle." The instructions will strictly enforce the architectural mandates for a C Application Binary Interface (ABI) to ensure stability, transparent linkage, and safe, explicit error handling. The use of C over C++ is a non-negotiable mandate derived from the system's highest-level principles of liveness, stability, and antifragility.2


Chapter 2.1: The Prototypal Emulation Layer


Objective: To generate the C code for the TelosProxyObject and its corresponding IoProxy Python type. This layer allows the Python "muscle" to interact with objects in the Io "mind" using Io's native semantics of prototypal delegation and message passing.
Architectural Context: The system requires "systemic wholeness," meaning the Python environment must not be treated as a disconnected library of static functions but as a fully integrated limb of the Io organism.2 The
TelosProxyObject C struct is the physical embodiment of this principle. It is a C-level implementation of differential inheritance: it stores local state (the "differences") and delegates all other behavior to its master prototype in the Io VM. This design makes the language boundary transparent to the user and preserves the prototypal philosophy across the entire system.2
Key Implementation Prompts:
1. TelosProxyObject Header Definition:
   * Instruction: Generate a C header file named telos_proxy.h. This file must define the TelosProxyObject struct exactly as specified below. Provide extensive comments explaining the role of each member, as this struct is the cornerstone of the emulation layer.
C
#include <Python.h>
#include <structmember.h>

// TelosProxyObject: A universal ambassador for an Io object within the C and Python runtimes.
// This struct defines the memory layout for the IoProxy Python type.
// Its design is a direct C-level embodiment of differential inheritance: it stores
// local state (the 'differences') and delegates all other behavior
// to its master object in the Io VM. [2, 3]
typedef struct {
   // Standard Python object header. This macro MUST be the first member,
   // making this struct a valid PyObject and allowing the Python interpreter
   // to manage its memory and type information correctly. [2]
   PyObject_HEAD

   // A persistent, GC-safe reference to the master object in the Io VM.
   // This is NOT a raw pointer to an Io object, which would be unsafe.
   // It is an opaque handle (e.g., a void*) that has been explicitly
   // registered with the Io VM's root set to prevent garbage collection
   // for the lifetime of this proxy object. This is the anchor of the
   // object's identity, ensuring a single source of truth. [2, 3]
   void *ioMasterHandle;

   // A hash map for C-side 'slots' to cache properties locally.
   // This PyObject* will point to a Python dictionary. It serves as the
   // local storage for the clone, holding any attributes that have been
   // set on the Python side. This directly emulates the 'differences'
   // stored in a cloned Io object. [2, 3]
   PyObject *localSlots;

   // A function pointer for delegating unresolved message sends.
   // This is the active mechanism of prototypal emulation. When an
   // attribute is accessed on the Python proxy and is not found in
   // 'localSlots' (a cache miss), this function is invoked to forward
   // the request across the C bridge to the Io VM, which then performs
   // a full prototype chain lookup on the master object. [2, 3]
   PyObject* (*forwardMessage)(void *ioMasterHandle, const char *messageName, PyObject *args);
} TelosProxyObject;

2. IoProxy Python Type Implementation:
   * Instruction: Generate a C implementation file, telos_proxy.c, that defines the behavior for the new IoProxy Python type. This involves implementing the C functions that will be assigned to the slots of Python's PyTypeObject structure.
   * tp_getattro (__getattr__) Implementation:
      * Prompt: "Generate the C function IoProxy_getattro that will implement the __getattr__ behavior. The function signature must be PyObject *IoProxy_getattro(TelosProxyObject *self, PyObject *name). The logic must follow this precise sequence:
         1. Ensure the localSlots dictionary on the self object is not NULL.
         2. Check if the name (as a string) exists as a key in the localSlots dictionary.
         3. If it exists (a cache hit), retrieve the value and return it.
         4. If it does not exist (a cache miss), invoke the forwardMessage function pointer stored in the self object. Pass it self->ioMasterHandle, the C string from name, and NULL for the args.
         5. Return the PyObject* received from the forwardMessage call. This sequence perfectly emulates prototypal delegation across the language boundary.3"
   * tp_setattro (__setattr__) Implementation:
      * Prompt: "Generate the C function IoProxy_setattro that will implement the __setattr__ behavior. The function signature must be int IoProxy_setattro(TelosProxyObject *self, PyObject *name, PyObject *value). This function must implement the transactional coherence protocol:
         1. Ensure the localSlots dictionary on the self object is not NULL.
         2. Update the local Python-side cache by setting the name key to the value in the localSlots dictionary. This provides immediate responsiveness.
         3. Crucially, marshal the attribute name and the new value into a message structured as a 'request transaction to update slot'.
         4. Dispatch this message asynchronously to the Io core via a dedicated FFI function. This ensures that the change is durably persisted in the L3 ground truth store and that the 'Living Image' remains consistent. The function should return 0 on success and -1 on failure.3"


Chapter 2.2: The FFI Contract - Data Marshalling and Memory Management


Objective: To generate the C functions that safely and efficiently marshal data across the Io-C-Python boundary, strictly adhering to a formal contract to prevent instability.
Architectural Context: The FFI is the most technically hazardous component of the system, as it must mediate between three different memory management models: Io's garbage collector, C's manual memory management, and Python's reference counting.2 Subtle errors in this layer are the most common source of critical failures like memory leaks, segmentation faults, and data corruption. To mitigate this risk, a formal, explicit contract is mandated. This contract eliminates ambiguity and serves as a verifiable checklist for implementation, enforcing the system's core stability and safety principles at its most vulnerable interface.2
Key Implementation Prompts:
The following prompts are structured around the definitive FFI contract. You will generate a C function for each specified marshalling rule.
Table IV.1: FFI Marshalling and Memory Management Contract 2
Io Type
	C ABI Type
	Python C API Type
	Marshalling Rule (Io -> Py)
	Marshalling Rule (Py -> Io)
	Memory Management Protocol
	Number (Integer)
	long
	PyObject*
	Convert Io Number to C long. Call PyLong_FromLong().
	Call PyLong_AsLong(). Convert C long to Io Number.
	Stack-based; no special handling required.
	Number (Float)
	double
	PyObject*
	Convert Io Number to C double. Call PyFloat_FromDouble().
	Call PyFloat_AsDouble(). Convert C double to Io Number.
	Stack-based; no special handling required.
	Sequence (String)
	const char*
	PyObject*
	Allocate C buffer, copy Io Sequence data, null-terminate. Call PyBytes_FromStringAndSize(). Free C buffer after call.
	Call PyBytes_AsStringAndSize(). Create new Io Sequence from C char*.
	Io side is responsible for freeing the temporary C buffer.
	Tensor/Hypervector
	void* (buffer pointer)
	PyObject* (e.g., numpy.ndarray)
	Expose Python object's data buffer via buffer protocol. Pass raw void* pointer to Io. Wrap in opaque cdata object.
	Unwrap void* from Io cdata. Use PyMemoryView_FromMemory to create a Python view of the buffer.
	CRITICAL: The Io cdata object holds a borrowed reference. The Python object must be kept alive (e.g., via a handle) for the entire duration the Io side holds the pointer.
	Io Object Handle
	void*
	PyObject* (PyCapsule)
	Register Io object with Io GC to prevent collection. Pass pointer as void*. Wrap in PyCapsule with a custom destructor to release the Io GC registration.
	Unwrap PyCapsule to get void* pointer. Use pointer to reference Io object.
	The PyCapsule's destructor is the key safety mechanism. It must trigger a callback to the C bridge to deregister the handle with the Io GC.
	IoProxy (Python-side)
	TelosProxyObject*
	IoProxy instance
	An IoProxy instance is created by the C bridge. Its ioMasterHandle is set to the handle of the Io object, and its forwardMessage pointer is set.
	An IoProxy is passed to C as a PyObject*, which is cast to TelosProxyObject* to access its internal handle for communication with Io.
	The IoProxy's dealloc function must release its ioMasterHandle with the Io GC and Py_DECREF its localSlots dictionary.
	* Prompt (Tensor/Hypervector Marshalling):
   * Instruction: "Generate a C function Py_Tensor_to_Io_Handle that implements the 'borrowed reference' protocol. It will take a Python object (e.g., a NumPy array) as input. The logic must:
      1. Use the Python Buffer Protocol (PyObject_GetBuffer) to get a Py_buffer view of the object's underlying data.
      2. Extract the raw void* pointer from view.buf.
      3. Crucially, increment the reference count of the original Python object (Py_INCREF).
      4. Store a handle to this Python object in a global C-side registry.
      5. Return the raw void* buffer pointer to the Io side.
This ensures the Python object is not garbage collected while Io is using its memory, preventing use-after-free errors.2"
   * Prompt (Io Object Handle Marshalling):
   * Instruction: "Generate a C function Io_Handle_to_Py_Capsule that safely exposes an Io object handle to Python. It will take a void* handle as input. The logic must:
   1. Define a C callback function release_io_handle_callback(PyObject *capsule). This function will extract the void* handle from the capsule and make an FFI call back to the Io VM to deregister the handle from the GC root set.
   2. In the main function, create a PyCapsule using PyCapsule_New. Pass the void* handle as the pointer, a unique name for the capsule type, and a pointer to release_io_handle_callback as the destructor.
   3. Return the newly created PyCapsule. This is the essential safety mechanism for managing the lifetime of Io objects referenced from Python.2"


Chapter 2.3: The GIL Quarantine Protocol


Objective: To implement the asynchronous, non-blocking bridge architecture that prevents the Python Global Interpreter Lock (GIL) from serializing the highly concurrent Io "mind."
Architectural Context: The conflict between Io's Actor Model and Python's GIL is described as an "architectural showstopper".2 A naive, synchronous FFI would be catastrophic, blocking all Io actors and nullifying Io's primary concurrency advantage. The mandated solution is the "GIL Quarantine Protocol": all CPU-bound Python tasks must be executed in a separate process pool, with communication occurring asynchronously.2 This design choice deliberately introduces latency, which is not a flaw but a conscious architectural trade-off. The process boundary acts as a "blast door," preventing a crash in a Python worker process from terminating the master Io "mind." This isolation is a fundamental prerequisite for the system's high-level safety and antifragility strategy, as it ensures the Io mind can always remain in control to initiate a
transaction.abort() recovery protocol.2
Key Implementation Prompts:
   1. Process Pool Executor Initialization:
   * Instruction: Generate Python bootstrap code that runs once when the embedded interpreter is initialized. This code must instantiate a concurrent.futures.ProcessPoolExecutor and store it in a globally accessible location for the FFI bridge to use.
   2. Asynchronous FFI Entry Point:
   * Instruction: Generate the primary C FFI entry point function, submit_python_task_async. This function will replace any direct, synchronous execution of Python code. The logic must:
   1. Accept a function name, arguments, and a callback handle to the Io side.
   2. Acquire the Python GIL.
   3. Use the Python C API to access the global ProcessPoolExecutor instance.
   4. Call the executor.submit() method, passing the function name and arguments. This returns a Future object.
   5. Define a Python-side callback function (e.g., using a lambda) that will be triggered when the future completes. This callback will take the future's result and make an FFI call back to the Io actor identified by the callback handle.
   6. Use future.add_done_callback() to attach this lambda to the future object.
   7. Release the Python GIL and return immediately to Io.


Section III: The Neuro-Symbolic Core (GCE & HRC)


This section provides the instructions for implementing the system's dual-process reasoning engine, which is explicitly engineered as an analogue to "System 1" (fast, associative) and "System 2" (slow, deliberative) cognition.3 The focus is on the critical interface that connects the Geometric Context Engine (GCE) and the Hyperdimensional Reasoning Core (HRC).


Chapter 3.1: The Homomorphic Interface - The Laplace-HDC Encoder


Objective: To generate a precise, correct, and efficient Python/NumPy implementation of the Laplace-HDC encoding algorithm.
Architectural Context: The system's capacity for genuine knowledge discovery is contingent upon the "homomorphic imperative": the mapping from the GCE's continuous geometric space to the HRC's discrete algebraic space cannot be a simple lookup. It must be a continuous, structure-preserving mathematical function that faithfully translates the geometry of semantics into the algebra of symbols.1 The Laplace-HDC encoder is mandated as the principled, transparent algorithm to achieve this. It is not a learned, black-box model but a deterministic algorithm derived from a deep theoretical result proving an isomorphism between the HDC bind operator and the Laplace kernel. This provides a strong, principled guarantee of structure preservation, which is essential for explainable and auditable reasoning.1
Key Implementation Prompts:
   1. laplace_hdc_encode Function Generation:
   * Instruction: Generate a single, standalone Python function named laplace_hdc_encode(vectors: np.ndarray, D: int, m: int) -> np.ndarray. The function must implement the five-step algorithm exactly as specified in the table below. This table serves as a "Rosetta Stone," providing a direct, line-by-line translation from the mathematical formulation to the specific, optimized NumPy functions required. Adherence to this specification is critical to ensure the correctness of the homomorphic mapping.
Table 2: The Laplace-HDC Encoding Algorithm: From Geometry to Algebra 1
Step
	Description
	Mathematical Formulation
	Python Implementation (NumPy/SciPy)
	Input
	A set of n geometric embedding vectors from the GCE.
	V∈Rn×d
	V = np.array([...])
	1. Similarity Matrix
	Compute the pairwise cosine similarity matrix of the input vectors.
	$K_{ij} = \frac{v_i \cdot v_j}{\|v_i\| \|v_$
	V_norm = V / np.linalg.norm(V, axis=1, keepdims=True) K = np.dot(V_norm, V_norm.T)
	2. Kernel Transform
	Apply the sinusoidal transformation to map geometric similarity to an algebraic structure.
	Wij​=sin(2π​Kij​)
	W = np.sin(np.pi / 2 * K)
	3. Eigendecomposition
	Extract principal components of the transformed semantic structure.
	W=USUT
	eigenvalues, U = np.linalg.eigh(W) S_plus_half = np.diag(np.sqrt(np.maximum(0, eigenvalues)))
	4. Stochastic Projection
	Project the low-dimensional structure into the high-dimensional hyperspace.
	P=GS+1/2UT
	G = np.random.randn(D, m) P = G @ S_plus_half[-m:, -m:] @ U.T[-m:, :]
	5. Binarization
	Convert the real-valued projection into discrete, bipolar hypervectors.
	H=sign(P)
	H = np.sign(P)
	

Chapter 3.2: The Geometric Context Engine (GCE)


Objective: To implement the GCE's functionality as the system's "System 1," a vast, associative long-term memory.
Architectural Context: The GCE's role is not to find the final answer but to provide fast, intuitive, and contextually relevant proposals based on semantic similarity.3 It performs a fast, similarity-based retrieval to establish a constrained "semantic subspace"—a highly relevant working memory for a given query. This initial retrieval is a crucial optimization. The GCE's pre-filtering of the vast memory space into a small set of
k relevant concepts is what makes the computationally intensive but principled Laplace-HDC encoding tractable at runtime. This synergy, where the fast, approximate "System 1" enables the deliberative "System 2," is a hallmark of the architecture's coherent design.1
Key Implementation Prompts:
   1. GCE Class Implementation:
   * Instruction: Generate a Python class named GCE in a file named core/gce.py.
   * The __init__ method must be instructed to instantiate the EmbeddingService, the L1 FaissIndexService, and the L2 DiskAnnIndexService.
   * A public method retrieve_context(self, entities: list[str], k: int) -> dict must be specified. The logic for this method is:
   1. Use the EmbeddingService to convert the list of entity strings into a set of query vectors.
   2. First, query the L1 FaissIndexService for the lowest latency results.
   3. Concurrently or subsequently, query the L2 DiskAnnIndexService.
   4. Merge and de-duplicate the results from both services.
   5. Return a dictionary containing the top k unique concept IDs ('context_ids') and their corresponding geometric vectors ('context_vectors').


Chapter 3.3: The Hyperdimensional Reasoning Core (HRC)


Objective: To implement the HRC's role as the system's "System 2," the engine for explicit, structured, and algebraic reasoning.
Architectural Context: The HRC is where transparent, compositional, and auditable reasoning occurs.3 It takes the contextually-grounded concepts provided by the GCE (after they have been encoded into hypervectors) and executes a sequence of VSA operations as defined by a "reasoning plan." This process synthesizes a new, composite hypervector,
h_result, which represents the abstract, ungrounded result of the logical deduction.1
Key Implementation Prompts:
   1. HRC Class Implementation:
   * Instruction: Generate a Python class named HRC in a file named core/hrc.py.
   * The __init__ method will instantiate the VSAService.
   * A public method execute_plan(self, plan: dict, context_hypervectors: dict) -> torch.Tensor must be specified. The logic for this method is:
   1. Parse the plan dictionary, which contains a sequence of operations (e.g., {'op': 'bind', 'args': ['cuisine', 'Mexican']}).
   2. Map the string arguments in the plan to the actual hypervectors provided in the context_hypervectors dictionary.
   3. Iterate through the sequence of operations in the plan.
   4. For each operation, call the corresponding method on the VSAService (e.g., vsa_service.bind(h_cuisine, h_mexican)).
   5. The result of one operation becomes the input for the next, allowing for a chain of reasoning.
   6. Return the single, composite hypervector that is the final result of the entire sequence.


Section IV: The Cognitive Cycle Orchestration


This final section provides instructions for implementing the high-level agent that orchestrates the entire five-phase cognitive cycle, from parsing the user's natural language query to synthesizing the final, grounded response. This includes the critical LLM-based interface layers that are strategically positioned at the periphery of the system.


Chapter 4.1: The Initial LLM as Cognitive Compiler


Objective: To implement the first phase of the cognitive cycle, where an LLM translates a user's ambiguous, high-level request into a precise, machine-executable "reasoning plan."
Architectural Context: The initial LLM is strategically demoted from a reasoner to a "cognitive compiler".1 Its sole function is to compile the user's natural language query—a high-level, declarative specification of intent—into a low-level, explicit reasoning plan. This is achieved using the structured output capabilities (e.g., tool-calling or JSON mode) of modern LLMs. This architectural constraint is a primary defense against hallucination, as the LLM is never asked to recall information or answer the question directly; it is only asked to process the query's structure and populate a predefined schema.1
Key Implementation Prompts:
   1. Pydantic Schema Definition:
   * Instruction: Generate a Python file core/reasoning_plans.py. In this file, define a series of pydantic.BaseModel classes that represent the schemas for different types of reasoning plans. Use the following code for RecipeSearchPlan as a direct example. The descriptions within the Field objects are critical, as they provide the in-context instructions for the LLM.
Python
from pydantic import BaseModel, Field
from typing import List

class RecipeSearchPlan(BaseModel):
   """A structured reasoning plan for finding recipes based on user criteria."""
   cuisine: List[str] = Field(description="The specified cuisines for the recipe, e.g., 'Mexican', 'Italian'")
   included_ingredients: List[str] = Field(description="A list of ingredients that MUST be in the recipe.")
   excluded_ingredients: List[str] = Field(description="A list of ingredients that MUST NOT be in the recipe.")
   allergies: List[str] = Field(description="A list of allergies to avoid.")
   dietary_restrictions: List[str] = Field(description="Specified dietary needs, e.g., 'vegetarian', 'gluten-free'")

   2. LLM-Powered Parser Implementation:
   * Instruction: Generate a class CognitiveCompiler. This class will use a framework like LangChain to bind a Pydantic model to an LLM.
   * Prompt: "Create a method compile_query(self, query: str, plan_schema: BaseModel). This method will use LangChain's .with_structured_output(plan_schema) functionality to configure an LLM. It will then invoke the LLM with a prompt that instructs it to act as a compiler, populating an instance of the provided schema based on the user's query. The method must return a validated Pydantic object."
Table 1: Query Deconstruction: From Natural Language to a Structured Reasoning Plan 1
User Query (Natural Language)
	Identified Intent & Entities
	Generated Reasoning Plan (JSON Representation of Pydantic Object)
	"I'm looking for a vegetarian Mexican dish with tomatoes but no corn. I'm allergic to peanuts."
	Intent: Recipe Search Entities: - Cuisine: Mexican - Diet: Vegetarian - Include: Tomatoes - Exclude: Corn - Allergy: Peanuts
	json { "cuisine": ["Mexican"], "included_ingredients": ["tomatoes"], "excluded_ingredients": ["corn"], "allergies": ["peanuts"], "dietary_restrictions": ["vegetarian"] }
	"Show me how to get from the Eiffel Tower to the Louvre."
	Intent: Navigation Entities: - Origin: Eiffel Tower - Destination: Louvre - Mode: (unspecified)
	json { "origin": "Eiffel Tower", "destination": "Louvre", "transport_mode": null,<br> "constraints":<br>}`
	"What's the difference between torchhd's bind and bundle operations?"
	Intent: Analogical Comparison Entities: - Subject 1: torchhd bind - Subject 2: torchhd bundle - Relation: Difference
	json { "operation": "analogy", "subject_A": "torchhd bind", "subject_B": "torchhd bundle", "relation": "difference" }
	

Chapter 4.2: The HybridQueryPlanner - The Central Orchestrator


Objective: To generate the central orchestrating agent that receives the structured plan from the cognitive compiler and executes the full internal GCE -> HRC -> AGL reasoning loop.
Architectural Context: The HybridQueryPlanner is the cognitive "mind" that directs the internal reasoning cycle.1 It is responsible for executing the most innovative component of the architecture: the Associative Grounding Loop (AGL). The AGL is not merely an error-correction step. The "lossiness" of the HRC's bundling operation is a core generative mechanism.3 The abstract result from the HRC,
h_result, represents a new, synthesized idea. When decoded back into the geometric space, its "ghost" vector, c_result, is used as a query. The final "Constrained Cleanup Operation" is therefore an act of discovery—a generative hypothesis test. The system is effectively asking, "Based on the relevant context, what is the most plausible known concept that corresponds to this new abstract idea I have just constructed algebraically?".1 This transforms the reasoning process from simple deduction to a cycle of hypothesis generation (HRC) and evidence-based confirmation (AGL).
Key Implementation Prompts:
   1. HybridQueryPlanner Class Implementation:
   * Instruction: Generate a Python class named HybridQueryPlanner in core/planner.py. It must instantiate the GCE and HRC.
   * Prompt: "Generate a primary public method process_query(self, plan: BaseModel). The method must execute the full neuro-symbolic loop in the following, exact sequence:
   1. Phase 2 (Context Retrieval): Extract the key entities from the plan object. Call self.gce.retrieve_context() with these entities to get the initial semantic subspace, storing both 'context_ids' and 'context_vectors'.
   2. Phase 3 (Algebraic Reasoning):
   * Call the laplace_hdc_encode() function, passing it the 'context_vectors' to get the corresponding context_hypervectors.
   * Pass the plan and context_hypervectors to self.hrc.execute_plan() to produce the abstract result hypervector, h_result.
   3. Phase 4 (Associative Grounding):
   * Implement the inverse mapping to decode h_result back into a geometric vector, c_result. (For this implementation, a simplified decoder can be used, such as a pseudo-inverse of the projection matrix).
   * Implement the Constrained Cleanup Operation: Call the GCE's search method again. This time, the query vector is c_result, k=1, and you must pass a filter argument to the search function that restricts the search space to only the 'context_ids' retrieved in the first step.
   * The single concept object returned from this final, constrained search is the grounded, verified answer. Return this object."


Chapter 4.3: The Final LLM as Grounded Summarizer


Objective: To implement the final phase of the cognitive cycle, where the verified, symbolic answer from the neuro-symbolic core is translated back into a fluent, human-readable natural language response.
Architectural Context: Mirroring the initial phase, the final LLM is strategically demoted. It functions not as a reasoner or knowledge source, but as a "grounded summarizer" or "verbalizer".1 Its task is strictly constrained to reformulating the pre-verified facts provided to it within a Retrieval-Augmented Generation (RAG) style prompt. It is never required to access its own vast but potentially flawed parametric memory to answer the user's question. This architectural constraint is the system's final and most important safety mechanism, transforming the LLM from a potential source of error into a reliable natural language interface for the rigorous underlying engine.1
Key Implementation Prompts:
   1. Answer Serialization:
   * Instruction: Generate a function serialize_for_llm(grounded_object). This function will take the final, clean concept object returned by the HybridQueryPlanner and format its key attributes into a structured, human-readable string (e.g., Markdown or a simple key-value format).
   2. Grounded Prompt Generation and Final Response Synthesis:
   * Instruction: Generate a function synthesize_response(user_query: str, serialized_context: str). This function will perform the final RAG step.
   * Prompt: "The function must use a prompt template that strictly constrains the LLM. The template must contain the following key instructions:
   * A role assignment: 'You are a helpful assistant that answers user questions based ONLY on the provided information.'
   * A clearly demarcated context injection section.
   * A negative constraint: 'If the provided information is not sufficient to answer the user's question, you must state that you do not know the answer. Do not use any external knowledge.'
   * Use the template provided in the table below as the definitive guide.
   * The function will then combine the user_query and the serialized_context into this prompt and send it to the final LLM to generate the response."
Table 3: Prompt Augmentation for Grounded Generation 1
Prompting Strategy
	Prompt Template
	Input Data
	Expected LLM Output
	Hallucination Risk
	Naive Prompt
	Human: {user_query}
	user_query: "What is the capital of Burkina Faso?"
	"The capital of Burkina Faso is Ouagadougou."
	High. The LLM must rely solely on its parametric memory, which may be outdated or incorrect. The answer is unverifiable.
	Grounded RAG Prompt
	You are a helpful assistant. Answer the user's question based ONLY on the context provided below. Context: --- {retrieved_context} --- If the context does not contain the answer, say 'I do not know.' Human: {user_query}
	user_query: "What is the capital of Burkina Faso?" retrieved_context: Concept ID: 78B4 Type: Country Name: Burkina Faso Capital: Ouagadougou Population: 20.9 million
	"Based on the provided information, the capital of Burkina Faso is Ouagadougou."
	Low. The LLM is constrained to reformulate a verified fact. The answer is directly traceable to a specific concept (78B4) in the system's memory.
	

Conclusion


This guide has provided a comprehensive and definitive set of instructions for the AI-driven implementation of a hybrid neuro-symbolic cognitive architecture. By following this blueprint with absolute fidelity, the resulting system will embody the core architectural principles of explainability, reliability, and extensibility that are essential for the next generation of artificial intelligence.
The strategic separation of concerns—delegating perception to the Geometric Context Engine, reasoning to the Hyperdimensional Reasoning Core, and language interface tasks to constrained Large Language Models—is the foundational design choice that mitigates the most critical failure modes of monolithic neural models.1 The system's reliability is not an emergent property of training but an engineered outcome of its architecture. The transparent algebraic operations of the HRC provide an auditable reasoning trace, while the Associative Grounding Loop ensures all conclusions are anchored to a verified knowledge base.1 The strict RAG-based prompting of the final LLM serves as the ultimate defense against factual hallucination.1
Furthermore, the mandated C-based Synaptic Bridge, governed by the GIL Quarantine Protocol and the Prototypal Emulation Layer, ensures stability and philosophical coherence between the Io "mind" and the Python "muscle".2 This stable foundation, validated by the Algebraic Crucible, is what makes the system's higher-level goal of safe, autonomous evolution a tractable engineering reality. The successful construction of this system represents a significant step toward an artificial intelligence that can seamlessly integrate perception, memory, and reasoning to achieve a deeper and more robust form of understanding.
Works cited
   1. Neuro-Symbolic Reasoning Cycle Implementation Plan
   2. Io, C, and Python System Design
   3. AI System Design: Io, Python, Morphic
   4. Mathematical Functions For Knowledge Discovery